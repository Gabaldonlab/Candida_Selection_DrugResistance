#!/usr/bin/env python

# this contains all the functions related to CandidaMine. It can be activated from perSVade_env and Candida_mine_env



"""
INFO:

This is where I got the reference genomes from:

C. auris: 

    the gDNA from GCA_002759435.2 and the mtDNA from ASM716870v1. 
    updated in august 2020 cgd

    This is the B8441 strain, bioproject PRJNA328792 and BioSample SAMN05379624

C. glabrata: reference genome from CGD: the latest version by 12/03/2019, which is v_s02-m07-r35 
C. albicans: 

    ref genome CGD: http://www.candidagenome.org/download/sequence/C_albicans_SC5314/Assembly22/current/C_albicans_SC5314_version_A22-s07-m01-r110_chromosomes.fasta.gz

    all genome from CGD: http://www.candidagenome.org/download/gff/C_albicans_SC5314/Assembly22/C_albicans_SC5314_version_A22-s07-m01-r110_features.gff

    From here I keep 'haplotype A' for both files

C. tropicalis: ref genome from NCBI:

    https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/006/335/GCF_000006335.3_ASM633v3/GCF_000006335.3_ASM633v3_genomic.fna.gz
    https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/006/335/GCF_000006335.3_ASM633v3/GCF_000006335.3_ASM633v3_genomic.gff.gz
    
    cgd has 2010

    This is strain Strain: MYA-3404 and bioproject PRJNA13675
    
C. parapsilosis: ref genome from NCBI:

    https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/182/765/GCA_000182765.2_ASM18276v2/GCA_000182765.2_ASM18276v2_genomic.fna.gz
    https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/182/765/GCA_000182765.2_ASM18276v2/GCA_000182765.2_ASM18276v2_genomic.gff.gz

C. orthopsilosis:

    gDNA from NCBI:

        https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/315/875/GCF_000315875.1_ASM31587v1/GCF_000315875.1_ASM31587v1_genomic.fna.gz
        https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/315/875/GCF_000315875.1_ASM31587v1/GCF_000315875.1_ASM31587v1_genomic.gff.gz
    
    mtDNA from NCBI:

        https://www.ncbi.nlm.nih.gov/nuccore/NC_006972
        (fasta and gff obtained from 'Send to' tab)

    cgd is from 2012, corrected in ncbi but not in cgd

C. metapsilosis: Veronica's assembly from 
    
    genome: ~/samba_bsc40/vdepinho/Candida_metapsilosis/reference_nanopore/CANME.fa
    gff: ~/samba_bsc40/vdepinho/Candida_metapsilosis/reference_nanopore/EMBL/CANME.gff3 # this includes the 'COMPLETE_GENES'. In addition, we merged this to the interpro annotation from ~/samba_bsc40/vdepinho/Candida_metapsilosis/reference_nanopore/interproscan/CANME.gff3


At the end you have these two files under each dir
annotations.gff  genome.fasta

Some comments: prefetch sometimes works well, but then fastq-dump does not. I had to rerun some prefetched datasets a couple of times.

OUTGROUPS:

The C. bracarensis genome was obtained from NCBI (https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/001/077/315/GCA_001077315.1_ASM107731v1/GCA_001077315.1_ASM107731v1_genomic.fna.gz)


"""
# module imports
import os
import re
import string
import random
import pandas as pd
import numpy as np
import sys
from collections import ChainMap
import warnings
from Bio.Seq import Seq
from mpl_toolkits.axes_grid1.inset_locator import inset_axes

from Bio.SeqRecord import SeqRecord
import statsmodels
import pickle
import itertools
import copy as cp
import prince
import re
import shutil
from datetime import date
import multiprocessing as multiproc
import scipy.stats
import numpy.polynomial.polynomial as poly
from scipy.optimize import curve_fit
import time
from matplotlib.patches import Patch

from collections import Counter, defaultdict
import inspect
import collections
from shutil import copyfile
import urllib
from subprocess import STDOUT, check_output
import random
import subprocess
import subprocess, datetime, signal
from Bio import Entrez
import Bio
import pickle
from matplotlib import colors
from statsmodels.nonparametric.kernel_density import KDEMultivariate
import scipy.stats as stats
from statsmodels.stats import multitest
from Bio import SeqIO
import matplotlib
import matplotlib.path as mpath
import matplotlib.lines as mlines
import matplotlib.patches as mpatches
from matplotlib.collections import PatchCollection
from matplotlib.lines import Line2D
from matplotlib.patches import Circle
import matplotlib.patches as patches
from scipy.stats import mode, gaussian_kde
from scipy.optimize import minimize, shgo

from goatools.goea.go_enrichment_ns import GOEnrichmentStudyNS
from goatools.obo_parser import GODag
from goatools.godag_plot import plot_gos, plot_results, plot_goid2goobj
import goatools.semantic as goatools_semantic
import scipy.spatial
import scipy.cluster.hierarchy
from sklearn.manifold import MDS as MDS_sklearn


from reportlab.pdfgen import canvas
from reportlab.lib.utils import ImageReader
from PIL import Image as PILImage
from PIL import ImageColor
import warnings
import plotly.graph_objects as go
import plotly.offline as off_py
from math import radians, cos, sin, asin, sqrt

import requests
from shapely.geometry import mapping, shape
from shapely.prepared import prep
from shapely.geometry import Point


# functions needed on top
def get_current_clusterName_mareNostrum():

    """Returns the cluster name in which you are. It can be MN4, Nord3"""

    bscMachine_to_name = {"mn4":"MN4", "knl":"CTE_KNL", "nord3":"Nord3"}

    return bscMachine_to_name[os.environ["BSC_MACHINE"]]


# things installed only in Cmine_env
try: from colour import Color
except: print("colour could not be loaded")

# define the parent dir of the cluster or not
ParentDir = "%s/samba"%(os.getenv("HOME")); # local
if os.path.exists(ParentDir):
    run_in_cluster = False    
    threads = 4
else:
    run_in_cluster = True    
    cluster_name = get_current_clusterName_mareNostrum()
    ParentDir = "/gpfs/projects/bsc40/mschikora"
    if cluster_name=="MN4": threads = 48
    elif cluster_name=="Nord3": threads=4


# print teh cluster
if run_in_cluster is True: print("running in %s"%cluster_name)

# imports that depend on the system
if run_in_cluster is False or cluster_name!="Nord3":
    from ete3 import NCBITaxa, Tree, TreeStyle, TreeStyle, TextFace, NodeStyle, RectFace, StackedBarFace, SequenceFace, ProfileFace, ImgFace, CircleFace, SeqMotifFace
    

# define the dir where all perSVade code is
perSVade_dir = "%s/scripts/perSVade/perSVade_repository/scripts"%ParentDir
sys.path.insert(0, perSVade_dir)

# import functions
#import sv_functions as fun

warnings.simplefilter(action='ignore', category=pd.core.common.SettingWithCopyWarning) # avoid the slicing warning
#pd.options.mode.chained_assignment = 'raise'
warnings.filterwarnings(action='ignore', message='Mean of empty slice')

# load a specific matplotlib library for cluster envs
if run_in_cluster is True:

    try:
        import matplotlib as mpl
        mpl.use('Agg')
        import matplotlib.pyplot as plt

    except: import matplotlib.pyplot as plt

else: 
    print("running locally")
    import matplotlib as mpl
    #mpl.use('TkAgg')
    import matplotlib.pyplot as plt

import seaborn as sns

########## variables related to the genomes ###########

taxID_to_sciName = { 5476:"Candida_albicans", # albicans
                     5480:"Candida_parapsilosis", # parapsilosis
                     273371:"Candida_orthopsilosis", # orthopsilosis
                     5482:"Candida_tropicalis", # tropicalis
                     5478:"Candida_glabrata", # glabrata
                     498019:"Candida_auris" # auris
                        }

sciName_to_taxID = {sciName:taxID for taxID, sciName in taxID_to_sciName.items()}

# define wrong samples
species_to_shortName =  {"Candida_metapsilosis":"meta.", 
                         "Candida_parapsilosis":"para.", 
                         "Candida_orthopsilosis":"ortho.", 
                         "Candida_tropicalis":"trop.", 
                         "Candida_glabrata":"glab.",
                         "Candida_albicans":"alb.",
                         "Candida_auris":"auris"}

# define the outlayer samples. This comes from looking at the SNP densities
sciName_to_outlayerNumericSampleIDs = {"Candida_metapsilosis":{25}, 
                                       "Candida_parapsilosis":set(), 
                                       "Candida_orthopsilosis":set(), 
                                       "Candida_tropicalis":set(), 
                                       "Candida_glabrata":set(),
                                       "Candida_albicans":{512, 390, 391, 522, 523, 524, 525, 526, 527, 400, 398, 402, 403, 404, 405, 401, 399, 528, 543, 544, 415, 416, 420, 550, 551, 552, 553, 554, 555, 556, 428, 448, 489, 490, 619, 620, 500, 501, 502, 503},
                                       "Candida_auris":{725}}

# define the diploid samples. This is from the SNP distribution frequencies.
sciName_to_diploidNumericSampleIDs = {"Candida_glabrata":set(), "Candida_auris":set()}

# define wrong samples
sciName_to_badSamples = {"Candida_metapsilosis":set(), 
                         "Candida_parapsilosis":set(), 
                         "Candida_orthopsilosis":set(), 
                         "Candida_tropicalis":set(), 
                         "Candida_glabrata":set(),
                         "Candida_albicans":set(),
                         "Candida_auris":{270, 562}} # these two samples had a lot of heterozygous SNPs at ~35% of AF, so they are likely contaminants

# define the best Ks and type of analysis
species_to_K_and_typePopStructureAnalysis = {

    "Candida_albicans": [(17, "all_samples")],
    "Candida_metapsilosis":[(2, "all_samples")],
    "Candida_auris": [(4, "all_samples")],
    "Candida_glabrata": [(9, "all_samples")],
    "Candida_parapsilosis": [(6, "all_samples")],
    "Candida_orthopsilosis": [ (4, "all_samples")],
    "Candida_tropicalis": [(6, "all_samples")]
}


# map each taxID, to the taxIDs, got with the comment
#from ete3 import NCBITaxa; ncbi = NCBITaxa(); taxID_to_taxIDs = {taxID: sorted(set(ncbi.get_descendant_taxa(taxID, collapse_subspecies=False, return_tree=False, intermediate_nodes=True) + [taxID])) for taxID in interesting_taxIDs}

taxID_to_taxIDs = {5476: [5476, 237561, 294748, 559299, 1027996, 1040604, 1040605, 1040606, 1040607, 1040608, 1040609, 1040610, 1040611, 1040612, 1040613, 1040614, 1040615, 1040616, 1040617, 1040618, 1040619, 1040620, 1040621, 1040622, 1040623, 1040624, 1040625, 1040626, 1040627, 1040628, 1040629, 1040630, 1040631, 1040632, 1041952, 1094981, 1094982, 1094983, 1094984, 1094985, 1094986, 1094987, 1094988, 1094989, 1094990, 1094991, 1094992, 1094993, 1094994, 1094995, 1094996, 1094997, 1094998, 1094999, 1095000, 1165368, 1182531, 1182532, 1182533, 1182534, 1182535, 1182536, 1182537, 1182538, 1182539, 1182540, 1308523, 1325634, 1333669, 1334023, 1356448, 1356449, 1356450, 1356451, 1439353, 1439354, 1439355, 1439356, 1439357, 1439358, 1451121, 1451122, 1451123, 1451124, 1451125, 1451126, 1451127, 1451128, 1451129, 1451130, 1451131, 1451132], 

                   5480: [5480, 578454, 1231521, 1431529, 1431530, 1431531], 
                   273372: [273372], 
                   273371: [273371, 1136231, 1213353, 1382548], 
                   5482: [5482, 294747, 559300, 1231520, 1308530, 1312901, 1451133, 1451134, 1451135, 1451136, 1451137, 1451138], 
                   5478: [5478, 284593, 1231519, 1308524, 1308525, 1308526, 1308527, 1308528, 1308529, 1398155, 1403403, 1406948], 
                   498019: [498019]}


# map each taxID to the mtDNA
taxID_to_mtChromosome = {5476:"Ca22chrM_C_albicans_SC5314", # albicans
                         5480:"HE605210.1", # parapsilosis
                         273372:"scaffold9", # metapsilosis --> the length is 236193, which makes sense
                         273371:"NC_006972.1", # orthopsilosis
                         5482:"NW_003020054.1", # tropicalis
                         5478:"mito_C_glabrata_CBS138", # glabrata
                         498019:"AP018713.1" # auris
                        }

species_to_mtChromosome = {species : taxID_to_mtChromosome[taxID] for taxID, species in taxID_to_sciName.items()}

taxID_to_outgroup = {5478 : "Candida-bracarensis.fasta"}


taxID_to_ploidy = {5476: 2, # albicans
                   5480: 2, # parapsilosis
                   273372: 2, # metapsilosis
                   273371: 2, # orthopsilosis
                   5482: 2, # tropicalis
                   5478: 1, # glabrata
                   498019: 1 # auris
                   }  

taxID_to_gDNA_code = {5476: 12,
                      5480: 12,
                      273372: 12, # metapsilosis
                      273371: 12,
                      5482: 12,
                      5478: 1,
                      498019: 12
                    }

taxID_to_mDNA_code = {5476: 4,
                      5480: 4,
                      273372: 4,
                      273371: 4,
                      5482: 4,
                      5478: 3,
                      498019: 3
                    }                    

# the fraction of branch lengths to consider to define clades
species_to_min_fraction_branchLen = {"Candida_glabrata":0.025, "Candida_auris":0.05, "Candida_parapsilosis":0.082, "Candida_albicans":0.059, "Candida_tropicalis":0.05, "Candida_metapsilosis":0.01, "Candida_orthopsilosis":0.07} 
species_to_linspace_range = {"Candida_glabrata":(0.001, 0.2, 20), "Candida_auris":(0.001, 0.2, 20), "Candida_parapsilosis":(0.001, 0.1, 300), "Candida_albicans":(0.03, 0.1, 20), "Candida_tropicalis":(0.001, 0.12, 70), "Candida_metapsilosis":(0.001, 0.02, 300), "Candida_orthopsilosis":(0.001, 0.08, 300)} 

#######################################################

######## graphical vars ###########

species_to_color = {'Candida_albicans': 'tomato', 'Candida_parapsilosis': 'black', 'Candida_metapsilosis': 'darkgray', 'Candida_orthopsilosis': 'rosybrown', 'Candida_tropicalis': 'olive', 'Candida_glabrata': 'cyan', 'Candida_auris': 'blue'}

#sorted_species_byPhylogeny = ["Candida_glabrata", "Candida_auris", "Candida_tropicalis", "Candida_albicans", "Candida_parapsilosis", "Candida_orthopsilosis", "Candida_metapsilosis"]
sorted_species_byPhylogeny = ["Candida_glabrata", "Candida_auris", "Candida_tropicalis", "Candida_albicans", "Candida_parapsilosis", "Candida_orthopsilosis"]

repeatFamily_to_color = {'DNA':'red',
                       'LINE':'sienna',
                       'SINE':'peru',                       
                       'LTR': 'salmon', 
                                           
                       'rRNA': 'olive', 
                       'snRNA': 'yellow', 
                       'tRNA': 'green',
                       
                       'Satellite': 'cyan', 
                       'Simple_repeat': 'deepskyblue', 
                       'Low_complexity': 'navy',

                       'Unknown': 'darkorchid', 
                       'multiSpecies_cluster': 'magenta', 
                       'singleSpecies_cluster': 'crimson', 
                       
                       'no_repeat': 'grey'}


###################################

########### variables ##########

# seletion piN piS
sel_to_valid_thresholds_piNpiS = {"positive":{1, 1.5, 2}, "negative":{0.01, 0.1, 0.5, 1}}


# sorted vars
sorted_consequences = ['', 'intergenic_variant', 'non_coding_transcript_variant', 'non_coding_transcript_exon_variant', 'stop_retained_variant', 'start_retained_variant', 'incomplete_terminal_codon_variant', 'coding_sequence_variant', 'synonymous_variant', 'intron_variant', '5_prime_UTR_variant', 'upstream_gene_variant', 'downstream_gene_variant', '3_prime_UTR_variant', 'splice_region_variant', 'splice_donor_variant', 'splice_acceptor_variant', 'missense_variant', 'stop_lost', 'inframe_insertion', 'inframe_deletion', 'protein_altering_variant', 'start_lost', 'stop_gained', 'frameshift_variant', 'feature_elongation', 'feature_truncation', 'transcript_ablation', 'transcript_amplification']

# IDXs
var_to_IDX = dict(zip(sorted_consequences, reversed(range(0, len(sorted_consequences)))))

# add the BND
for var, IDX in cp.deepcopy(var_to_IDX).items(): var_to_IDX["%s_BND"%var] = IDX

consequence_to_abbreviation  = {'frameshift_variant':"FS",
                                 'inframe_deletion':"del",
                                 'inframe_insertion':"ins",
                                 'missense_variant':"mis",
                                 'protein_altering_variant':"FS",
                                 'splice_acceptor_variant':"spliceAcc",
                                 'splice_donor_variant':"spliceDon",
                                 'splice_region_variant':"spliceReg",
                                 'start_lost':"lostATG",
                                 'stop_gained':"PTC",
                                 'stop_lost':"lostSTOP",
                                 'non_coding_transcript_exon_variant':"nonCodExon",
                                 '3_prime_UTR_variant':"UTR3",
                                 '5_prime_UTR_variant':"UTR5",
                                 'downstream_gene_variant':"down",
                                 'intron_variant':"intr",
                                 'non_coding_transcript_variant':"nonCodTrans",
                                 'start_retained_variant':"retATG",
                                 'stop_retained_variant':"retSTOP",
                                 'synonymous_variant':"syn",
                                 'upstream_gene_variant':"up",
                                 'intergenic_variant':"ig", 
                                 'incomplete_terminal_codon_variant':"incTermCodonVar",
                                 "coding_sequence_variant":"codSeqVar",
                                 '':"empty",
                                 "transcript_ablation":"DEL",
                                 "transcript_amplification":"DUP",
                                 "feature_truncation":"trunc",
                                 "feature_elongation":"elongation"}



# map each type of variant to either CDS position, Codons or Protein_position, Amino_acids
info_codons = ("c", "CDS_position", "Codons")
info_aa = ("p", "Protein_position", "Amino_acids")
info_spliciingAndNoncoding = ("g", "CDS_position", "#Uploaded_variation")

protVar_to_info =  {'frameshift_variant':info_aa,
                     'inframe_deletion': info_aa,
                     'inframe_insertion': info_aa,
                     'missense_variant': info_aa,
                     'protein_altering_variant': info_aa,
                     'splice_acceptor_variant': info_spliciingAndNoncoding,
                     'splice_region_variant': info_spliciingAndNoncoding,
                     'start_lost': info_codons,
                     'stop_gained': info_aa,
                     'stop_lost': info_codons,
                     'non_coding_transcript_exon_variant': info_spliciingAndNoncoding,
                     'splice_donor_variant':info_spliciingAndNoncoding,
                     '3_prime_UTR_variant': info_spliciingAndNoncoding,
                     '5_prime_UTR_variant': info_spliciingAndNoncoding,
                     'downstream_gene_variant': info_spliciingAndNoncoding,
                     'non_coding_transcript_variant': info_spliciingAndNoncoding,
                     'start_retained_variant': info_spliciingAndNoncoding,
                     'stop_retained_variant': info_spliciingAndNoncoding,
                     'synonymous_variant': info_codons,
                     'upstream_gene_variant': info_spliciingAndNoncoding,
                     'intron_variant':info_spliciingAndNoncoding,
                     'intergenic_variant':info_spliciingAndNoncoding,
                     "coding_sequence_variant":info_codons,
                     "incomplete_terminal_codon_variant":info_spliciingAndNoncoding,
                     "":info_spliciingAndNoncoding}


cglab_breakponts_dict = {"FLC":16, "ANI":0.06, "MIF":0.03, "AMB":1}

cglab_inHouse_breakponts_dict = {"ITR":4, "KET":2, "ANI":0.06}


cglab_gene_to_color = {"PDR1":"black", "CDR1":"red", "ERG11":"green", "ERG4":"blueviolet", "ERG3":"orange", "FKS1":"gray", "FKS2":"firebrick", "Scer_CNE1":"olive", "none":"darkseagreen", "UPC2A":"cyan", "Scer_YMR102C":"navy"}

# from Carrete2017
cglab_strain_to_clade  =  {'M17': 3,
                     'F1019': 3,
                     'F1822': 3,
                     'M12': 3,
                     'CST78': 3,
                     'F2229': 3,
                     'I1718': 3,
                     'EB0911': 2,
                     'CST35': 2,
                     'CST34': 1,
                     'CST109': 1,
                     'CST80': 1,
                     'M7': 1,
                     'EB101M': 1,
                     'BO101S': 1,
                     'B1012S': 1,
                     'B1012M': 1,
                     'EF1237': 4,
                     'EI1815': 4,
                     'EF1620': 4,
                     'EF0616': 4,
                     'F15': 5,
                     'F11': 5,
                     'E1114': 7,
                     'M6': 7,
                     'CST110': 7,
                     'EG01004': 7,
                     'F15021': 7,
                     'F03013': 7,
                     'BG2': 7,
                     'P35_2': 6,
                     'P35_1': 6,
                     'CBS138': 5}

cglab_clade_to_color = {1:"red", 2:"greenyellow", 3:"mediumpurple", 4:"cyan", 5:"gray", 6:"blue", 7:"orange", np.nan:"white"}

# hotspot info
cglab_gene_to_hotspotPositions = {"FKS1": {625, 626, 627, 628, 629, 630, 631, 632, 633},
                                  "FKS2": {659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383},
                                  "ERG11": {141, 152}}


# define the SRRs to remove, from manual curation of re-running. In some the reads were not properly paired
srrs_to_remove = ['ERR570064', 'ERR331060'] + ['ERR246509', 'ERR246505', 'ERR246510'] + ['ERR321926'] + ['ERR331061', 'ERR570065'] + ["SRR5083834", "SRR1811019", "SRR1106666"] + ["SRR10461151", "SRR7909148", "SRR7909184", "SRR7909183", "SRR7909204", "SRR7909181", "SRR7909233", "SRR7909235", "SRR7909298"] #  ['SRR5133904', 'SRR5133887', 'SRR5133897', 'SRR5133900', 'SRR5133895', 'SRR5133898', 'SRR5133893', 'SRR5133889', 'SRR5133903', 'SRR5133885', 'SRR5133894', 'SRR5133892', 'SRR5133905', 'SRR5133888', 'SRR5133890', 'SRR5133899', 'SRR5133891', 'SRR5133884', 'SRR5133902', 'SRR5133901', 'SRR5133896', 'SRR5133886']
# The last list are reads that don't have properly paired reads (22 C. albicans samples)

# There are some SRRs where gridss did not work the first time
#srrs_gridss_didNotWork = ["SRR5083834", "SRR1811019", "SRR1106666"] + ["SRR10461151", "SRR7909148", "SRR7909184", "SRR7909183", "SRR7909204", "SRR7909181", "SRR7909233", "SRR7909235", "SRR7909298"]


# from the GWAS df, define the interesting fields
all_pval_fields = ["%s_%s"%(pval_seed, pval_m) for pval_seed in ["pval_chi_square", "pval_GenoAndPheno"] for pval_m in  ["phenotypes"]] # RelToBranchLen
all_pval_fields_wCorrection = ["%s_%s"%(pval_f, corr_method) for pval_f in all_pval_fields for corr_method in ["bonferroni", "fdr_bh", "fdr_by"]]

interesting_fields_gwas_df = ['method', 'phenotype_f', 'genotype_f', 'nodes_withGeno', 'nodes_withoutGeno', 'nodes_unkownGeno', 'nodes_withPheno', 'nodes_withoutPheno', 'nodes_unkownPheno', 'nodes_GenoAndPheno', 'nodes_noGenoAndPheno', 'nodes_GenoAndNoPheno', 'nodes_noGenoAndNoPheno', 'epsilon', 'OR', 'chi_square', 'group_name', 'ASR_methods_phenotypes', 'ASR_methods_mutations', 'classification_method_phenotypes', 'classification_method_mutations', 'type_vars', 'type_genes', 'type_mutations', 'type_collapsing', 'gwas_method', 'min_support', 'drug', 'species'] + all_pval_fields + all_pval_fields_wCorrection + ['pval_chi_square_maxT', 'pval_epsilon_maxT']

################################


def save_object(obj, filename):
    
    """ This is for saving python objects """

    filename_tmp = "%s.tmp"%filename
    remove_file(filename_tmp)
    
    with open(filename_tmp, 'wb') as output:  # Overwrites any existing file.
        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)

    os.rename(filename_tmp, filename)


def id_generator(size=10, chars=string.ascii_uppercase + string.digits, already_existing_ids=set()):

    """ already_existing_ids is a set that indicates whihc IDs can't be picked """

    ID = ''.join(random.choice(chars) for _ in range(size))
    while ID in already_existing_ids:
        ID = ''.join(random.choice(chars) for _ in range(size))

    return ID

def load_object_direct(filename):
    
    """ This is for loading python objects  in a fast way"""
    
    return pickle.load(open(filename,"rb"))



def load_object(filename):
    
    """ This is for loading python objects  in a fast way"""

    # define the machine name of the linux laptop
    linux_laptop_machine_ID = "Linux localhost.localdomain 5.3.18-lp152.60-default #1 SMP Tue Jan 12 23:10:31 UTC 2021 (9898712) x86_64 x86_64 x86_64 GNU/Linux"

    # get the machine name
    machine_ID = str(subprocess.check_output("uname -a", shell=True)).split("'")[1].split("\\")[0]

    # in the cluster the access is fast
    if machine_ID!=linux_laptop_machine_ID: 
        return load_object_direct(filename)

    # in the local it is not, so that we rsync first to a fast access local_dir
    else:
        print("loading %s faster"%filename)

        # check that this is a full path
        if not filename.startswith(ParentDir): raise ValueError("filename '%s' should start with %s"%(filename, ParentDir))

        # define the origin file
        origin_file = filename.replace(ParentDir, "/gpfs/projects/bsc40/mschikora")

        # define a local dir where to make the file
        local_folder = "/home/mschikora/dumping_samba_files"; make_folder(local_folder)
        local_dir = get_dir(filename).replace(ParentDir, local_folder)
        local_file = "%s/%s"%(local_dir, get_file(filename))

        # make the local_dir
        os.makedirs(local_dir, exist_ok=True)

        # create the local file
        print("rsyncing to local dir")
        run_cmd("rsync bsc40395@dt01.bsc.es:%s %s"%(origin_file, local_file), env="base")

        # loading local file 
        print("returning file")
        return load_object_direct(local_file)

def file_is_empty(path): 
    
    """ask if a file is empty or does not exist """

    if not os.path.isfile(path):
        return_val = True
    elif os.stat(path).st_size==0:
        return_val = True
    else:
        return_val = False
            
    return return_val



def run_cmd(cmd, env="Candida_mine_env"):

    """This function runs a cmd with a given env"""

    # define the cmds
    CondaDir = "/gpfs/projects/bsc40/mschikora/anaconda3"
    SOURCE_CONDA_CMD = "source %s/etc/profile.d/conda.sh"%CondaDir
    cmd_prefix = "%s && conda activate %s &&"%(SOURCE_CONDA_CMD, env)

    # define the running
    cmd_to_run = "%s %s"%(cmd_prefix, cmd)

    # run
    out_stat = os.system(cmd_to_run) 
    if out_stat!=0: raise ValueError("\n%s\n did not finish correctly. Out status: %i"%(cmd_to_run, out_stat))


def make_flat_listOflists(LoL):

    return list(itertools.chain.from_iterable(LoL))


def plot_coverage_allSRRs(df, filename, min_coverage=40, min_pct_covered=90):

    """
    This function takes a df with coverage and outputs some statistics of coverage-based filering. It outputs histograms. """

    print("getting coverage")
    all_species = sorted(set(df.sciName))
    fig = plt.figure(figsize=(10, len(all_species)*3)); I=1

    print("filtering out coverage<%i and pct_covered<%i"%(min_coverage, min_pct_covered))

    # go through each species
    for Ispp, spp in enumerate(all_species):

        df_spp = df[df.sciName==spp]

        # print statistics
        print("There are %i/%i samples that pass the filters in %s"%(sum((df_spp.mean_coverage>=min_coverage) & (df_spp.pct_covered>=min_pct_covered)), len(df_spp), spp))

        # go through each field
        for Xfield in ["mean_coverage", "pct_covered"]:

            ax = plt.subplot(len(all_species), 2, I); I+=1

            sns.distplot(df_spp[Xfield], rug=True, hist=True, kde=False)

            # add vlines
            xfield_to_y = {"mean_coverage":min_coverage, "pct_covered":min_pct_covered}
            plt.axvline(xfield_to_y[Xfield], linestyle="--", color="k", linewidth=.8)

            if Ispp==(len(all_species)-1): ax.set_xlabel(Xfield)
            else: ax.set_xlabel("")
                
            ax.set_title(spp)
            if Xfield=="mean_coverage": ax.set_ylabel("# samples")


    # write
    fig.savefig(filename, bbox_inches='tight')
    plt.close(fig)

def run_cmd_Candida_mine_env(cmd):

    """This function runs a cmd from the Candida_mine_env"""

    # define the cmds
    CondaDir = "/gpfs/projects/bsc40/mschikora/anaconda3"
    SOURCE_CONDA_CMD = "source %s/etc/profile.d/conda.sh"%CondaDir
    cmd_prefix = "%s && conda activate Candida_mine_env &&"%SOURCE_CONDA_CMD

    # define the running
    run_cmd("bash -c '%s %s'"%(cmd_prefix, cmd))


def get_BioSample_df_for_BioProject_Rpackage(bioproject, outfile):

    """Get the bioproject info with an R package. This will not always work"""

    get_BioSampleInfo_R = "%s/scripts/get_BioSampleInfo.R"%ParentDir

    if file_is_empty(outfile):

        outfile_tmp = "%s.tmp"%outfile
        run_cmd_Candida_mine_env("export NCBI_API_KEY=a757b8a9248e85ca292413e1671152308707 && %s --bioproject %s --outfile %s"%(get_BioSampleInfo_R, bioproject, outfile_tmp))

        # check that the outoput is full
        if len(pd.read_csv(outfile_tmp, sep="\t"))==0: raise ValueError("The file %s is empty "%outfile_tmp)
        os.rename(outfile_tmp, outfile)

def get_BioSample_df_for_BioProject_parsing_xmls(bioproject, outfile):

    """Get bioproject data parsing vrious xmls. 
    You may have to export"""

    if file_is_empty(outfile):

        # run efetch for this bioproject many times until it works
        efetch_outfile = "%s.efetch_output.xml"%outfile
        if file_is_empty(efetch_outfile): 
            ntries = 30
            for Itry in range(ntries):

                try: 

                    stderr_efetch = "%s.stderr"%efetch_outfile
                    run_cmd("export NCBI_API_KEY=a757b8a9248e85ca292413e1671152308707 && esearch -db sra -query %s | elink -target biosample | efetch -format docsum > %s 2>%s "%(bioproject, efetch_outfile, stderr_efetch))
                    break

                except: print("esearch did not work on try %i"%Itry)

            if file_is_empty(efetch_outfile): raise ValueError("esearch did not work after %i tries"%ntries)

        # get the groups of lines
        attribute_names_to_BioSamples = {}
        current_accession = None
        for l in open(efetch_outfile, "r").readlines():

            # if possible, define the accession
            if len(l.split())>0 and l.split()[0].startswith("<Accession>"): current_accession = l.split("<Accession>")[1].split("<")[0]

            # get the attribute names
            attribute_names = tuple([x.split('attribute_name="')[1].split('"')[0] for x in re.split("<|>", l) if x.startswith("Attribute") and 'attribute_name' in x])

            # if it is an attribute line, keep
            if len(attribute_names)>0: 
                if current_accession is None: raise ValueError("current_accession can't be none. Check %s"%efetch_outfile)
                attribute_names_to_BioSamples.setdefault(attribute_names, set())
                attribute_names_to_BioSamples[attribute_names].add(current_accession)

        # get the parsed xml as a table
        xtract_outfile = "%s.xtract.tab"%efetch_outfile
        run_cmd("cat %s | xtract -pattern DocumentSummary -element Attribute -element Accession > %s"%(efetch_outfile, xtract_outfile))

        # get into df
        longest_attributes = list(max(attribute_names_to_BioSamples.keys(), key=(lambda x: len(x)))) + ["Accession"]
        df_xtract = pd.read_csv(xtract_outfile, sep="\t", header=None, names=longest_attributes)

        # define all the columns
        all_columns = sorted(set(make_flat_listOflists(attribute_names_to_BioSamples.keys()))) + ["Accession"]

        # go through each set of df parts that have the same attributes
        df_final = pd.DataFrame(columns=all_columns)
        for attribute_names, bioSamples in attribute_names_to_BioSamples.items():

            # get the df
            df = df_xtract[df_xtract.apply(lambda r: any([r[k] in bioSamples for k in r.keys()]), axis=1)]

            #print(df, "\n", df.keys(), "\n", attribute_names)

            # very specific parts of bioprojects
            if bioproject=="PRJNA595978" and df.isolate.iloc[0]=="CNRMA17-624": 

                attribute_names = ("isolate", "host", "collection_date", "geo_loc_name", "sample_type", "biomaterial_provider") # strange attrubutes
                df = df[[c for c in df.columns if all(~pd.isna(df[c]))]]

            # most bioprojects
            else:

                # only keep the non-NaN cols and change the names (there are some bioprojects where you should skip this)
                not_skip_bioprojects = {"PRJNA470683"}
                if bioproject not in not_skip_bioprojects: df = df[[c for c in df.columns if not all(pd.isna(df[c]))]]
                #if bioproject not in not_skip_bioprojects: df = df[[c for c in df.columns if all(~pd.isna(df[c]))]]
                elif bioproject=="PRJNA470683": df = df[[c for c in df.columns if all(~pd.isna(df[c])) or c=="strain"]] # strains are nans
                else: raise ValueError("The bioproject %s has not been considered"%bioproject)

            # change the names
            df.columns = list(attribute_names) + ["Accession"]

            # add the missing cols
            for col in all_columns:
                if col not in df.keys(): df[col] = np.nan

            # keep
            df_final = df_final.append(df[all_columns])

        # write as a file
        outfile_tmp = "%s.tmp"%outfile
        df_final.to_csv(outfile_tmp, sep="\t", index=False, header=True)
        os.rename(outfile_tmp, outfile)

def get_BioSample_df_for_BioProject(bioproject, outfile):

    """Rteurns a df with bioproject info writing into outfile"""


    # try several ways
    try: get_BioSample_df_for_BioProject_Rpackage(bioproject, outfile)

    except:

        print("getting bioproject info with get_BioSampleInfo.R failed for bioproject. Trying with the parsing of various xmls")

        get_BioSample_df_for_BioProject_parsing_xmls(bioproject, outfile)

    df = pd.read_csv(outfile, sep="\t")
    if len(df)==0: raise ValueError("There should be metadata")

    # rename
    df = df.rename(columns={"Accession":"BioSample"})

    return df


def get_fitness_and_susceptibility_dfs_Carrete2017(manually_curated_data):

    # format df of susceptibility and fitness dfs Carrete 2017

    # map each sample to the BioSample
    expected_sampleNames = {'M12_CANGA', 'B1012M_CANGA', 'CST110_CANGA', 'EF1237Blo1_CANGA', 'CST35_CANGA', 'CST80_CANGA', 'F1019_CANGA', 'B1012S_CANGA', 'CST34_CANGA', 'F1822_CANGA', 'EF1535Blo1_CANGA', 'CST78_CANGA', 'EF0313Blo1_CANGA', 'EG01004Sto_CANGA', 'P35_3_CANGA', 'BG2_CANGA', 'I1718_CANGA', 'M17_CANGA', 'EB101M_CANGA', 'EF1620Sto_CANGA', 'EB0911Sto_CANGA', 'F2229_CANGA', 'EI1815Blo1_CANGA', 'EF1117Blo1_CANGA', 'BO101S_CANGA', 'EF1521Blo1_CANGA', 'P35_2_CANGA', 'M7_CANGA', 'E1114_CANGA', 'CST109_CANGA', 'M6_CANGA', 'EF0616Blo1_CANGA', 'CBS138_CANGA'}

    ######### susceptibility #########

    # load
    df_susc = pd.read_csv("%s/TableS4_susceptibility_Cglabrata_Carrete2017.csv"%(manually_curated_data), sep=";")

    # refomat the reistance  so that it is a boolean
    for col in [c for c in df_susc.keys() if "_resistance" in c]: df_susc[col] = df_susc[col]=="R"

    # reformat the MIC so that it is a float
    def reformat_MIC(x):

        # replace the commas by '.'
        if type(x)==str: x = x.replace(",", ".")

        # ask if it is floatable 
        x_is_floatable = False
        try: 
            float(x)
            x_is_floatable = True
        except:pass

        # define the strange chars
        strange_chars = []
        if type(x)==str: strange_chars = [y for y in x if y.isdigit() is False and y not in {" ", ".", ","}]

        if type(x)==float or type(x)==int or x_is_floatable is True: final_x =  float(x)
        elif ">" in x: final_x =  float(x.split(">")[1])*2
        else:
            if len(strange_chars)!=1: raise ValueError("error in MIC parsing")
            final_x = np.mean([float(y) for y in x.split(strange_chars[0])])

        # malformated vars
        if final_x==-1.0: final_x = np.nan

        return final_x

    for col in [c for c in df_susc.keys() if "_MIC" in c]: df_susc[col] = df_susc[col].apply(reformat_MIC)

    ####################################

    ########## add strain info ###########

    # load
    df_info = pd.read_csv("%s/Table1_strainInfo_Carrete2017.txt"%manually_curated_data, sep="\t")

    # change the Sample_ID
    expected_Sample_IDs = set(df_susc.strain)
    SampleID_to_synonyms = dict(df_info.set_index("Sample_ID")["Synonymous_ID"].apply(lambda x: set(x.split(","))))

    # add syns
    SampleID_to_synonyms["EB0911Sto"].add("EB0911")
    SampleID_to_synonyms["EF0616Blo1"].add("EF0616")
    SampleID_to_synonyms["EF1237Blo1"].add("EF1237")
    SampleID_to_synonyms["EF1620Sto"].add("EF1620")
    SampleID_to_synonyms["EI1815Blo1"].add("EI1815")
    SampleID_to_synonyms["EG01004Sto"].add("EG01004")
    SampleID_to_synonyms["P35_3"].add("P35_1") # there is a typo
    SampleID_to_synonyms["B1012M a"].add("B1012M")
    SampleID_to_synonyms["B1012S a"].add("B1012S")
    SampleID_to_synonyms["BO101S a"].add("BO101S")
    SampleID_to_synonyms["EB101M a"].add("EB101M")

    # add new
    SampleID_to_synonyms["P35-2"] = {"P35_2"}
    SampleID_to_synonyms["P35-3"] = {"P35_3", "P35_1"}
    
    def get_expected_sampleID(s):
        
        if s in expected_Sample_IDs: return s
        else:

            # find synonsyms
            potential_synIDs = SampleID_to_synonyms[s].intersection(expected_Sample_IDs)
            if len(potential_synIDs)==1: return next(iter(potential_synIDs))
            else: raise ValueError("%s is not valid"%s)

    df_info["Sample_ID"] = df_info.Sample_ID.apply(get_expected_sampleID)

    # merge
    if set(df_susc.strain)!=set(df_info.Sample_ID): raise ValueError("keys are not unique")
    df_all = df_susc.merge(df_info, left_on="strain", right_on="Sample_ID", validate="one_to_one")
    df_all = df_all.set_index("Sample_ID", drop=False)

    ######################################

    ######### fitness #########

    # load df
    df_GR = pd.read_excel("%s/TableS3_Fitness_Cglabrata_Carrete2017.xls"%(manually_curated_data))
    df_GR["r"] = df_GR.r.apply(float)

    # change names
    cond_to_new_cond = {"YPD":"YPD", "Temp41.5":"Temp-41.5", "pH9":"pH-9", "pH2":"pH-2", "NaCl":"NaCl-1M", "H2O2":"H2O2-10mM", "DTT":"DTT-1.5mM"}
    df_GR["Condition"] = df_GR.Condition.apply(lambda x: cond_to_new_cond[x])


    # map each sample to the real one
    sample_to_df_allSample = df_GR.set_index

    # add to df_all
    for cond in set(df_GR["Condition"]):

        # get the df
        df_c = df_GR[df_GR.Condition==cond]

        # add the correct sample
        df_c["Sample"] = df_c.Sample.apply(get_expected_sampleID)

        if set(df_c.Sample)!=set(df_all.Sample_ID): raise ValueError("samples are not unique")

        # add the growth rate
        df_c = df_c.set_index("Sample")
        df_all["%s_GR"%cond] = df_c.loc[df_all.index, "r"]

        if any(pd.isna(df_all["%s_GR"%cond])): raise ValueError("there are errors in the samples")

    ###########################

    sample_to_correctSample = {"EB0911":"EB0911Sto",
                               "EF1237":"EF1237Blo1",
                               "EI1815":"EI1815Blo1",
                               "EF1620":"EF1620Sto",
                               "EF0616":"EF0616Blo1",
                               "F15":"EF1535Blo1",
                               "F11":"EF1117Blo1",
                               "EG01004":"EG01004Sto",
                               "F15021":"EF1521Blo1",
                               "F03013":"EF0313Blo1",
                               "P35_1":"P35_3"}   
    # get correct sampleName
    def get_SampleName(s):

        # redefine s
        if s in sample_to_correctSample: s = sample_to_correctSample[s]
        
        # define the sampleName
        SampleName = "%s_CANGA"%s

        # keep
        if SampleName in expected_sampleNames: return SampleName
        else: raise ValueError("%s is not correct"%s)
   
       
    df_all["SampleName"] = df_all["strain"].apply(get_SampleName)

    # define the resistance 
    def get_resistance(r):

        r = "+".join([k.split("_")[0] for k in ["AMB_resistance", "FLC_resistance", "MIF_resistance"] if r[k] is True])

        if len(r)==0: return np.nan
        else: return r


    df_all["resistance"] = df_all.apply(get_resistance, axis=1)

    return df_all

def get_MIC_to_float(mic):

    """Takes a MIC value and returns the float"""

    mic = str(mic)

    if '-' in mic: 
        mics = [float(x) for x in mic.split("-")]
        if len(mics)!=2: raise ValueError("mics is not valid")
        return np.mean(mics)

    elif "–" in mic: 
        mics = [float(x) for x in mic.split("–")]
        if len(mics)!=2: raise ValueError("mics is not valid")
        return np.mean(mics)

    elif ">" in mic: return float(mic.split(">")[1])*2

    elif "<" in mic: return float(mic.split("<")[1])/2

    elif "—" in mic: return np.nan

    else: return float(mic)
        
def get_susceptibility_df_Carrete2019(manually_curated_data):

    """Takes the Table S5 of Carreté 2019 and returns the df"""

    df = pd.read_excel("%s/Table_S5_MICs_Cglabrata_Carrete2019.xls"%(manually_curated_data))

    drug_to_abbreviation = {'Fluconazole': 'FLC', 'Voriconazole': 'VRC', 'Amphotericin B': 'AMB', 'Anidulafungin': 'ANI', 'Micafungin': 'MIF', 'Caspofungin': 'CAS', '5- fluorocytosine': '5FC', 'Posaconazole': 'POS', 'Itraconazole': 'ITR', 'Isavuconazol': 'IVZ'}

    # initialize the interesting fields
    interesting_fields = ["Strains",  "Source"]
    for drug, abb in drug_to_abbreviation.items(): 

        field_MIC = "%s_MIC"%abb
        df[field_MIC] = df[drug].apply(get_MIC_to_float)

        # keep
        interesting_fields.append(field_MIC)

    return df[interesting_fields]


def remove_file(f):

    if os.path.isfile(f): 

        try: run_cmd("rm %s > /dev/null 2>&1"%f)
        except: pass

def delete_folder(f):

    if os.path.isdir(f): shutil.rmtree(f)


def make_folder(f):

    if not os.path.isdir(f): os.mkdir(f)

def delete_file_or_folder(f):

    """Takes a path and removes it"""

    if os.path.isdir(f): shutil.rmtree(f)
    if os.path.isfile(f): os.unlink(f)

def rsync_file(origin, dest):

    """syncs one file to the other"""

    if file_is_empty(dest):

        dest_tmp = "%s.tmp"%dest
        run_cmd("rsync %s %s"%(origin, dest_tmp))
        os.rename(dest_tmp, dest)


def get_integrated_metadata_fields(r, taxID, breakpoints_dict):

    """Takes a row of the metadata fields, and returns some extra fields:

    -   *_resistance: a boolean that indicates whether the sample is resistant to drug *. This is so if any of the azoles' MIC is >2xBreakpoint by EUCAST
    - *_MICfc: The log2 ratio in MIC between the MIC of drug * and the breakpoint MIC.
    - azole_mean_MICfc: The mean of all *_MICfc
    - azole_resistance: Whether any of the *_resistance is True
    - echinocandidn_mean_MICfc: The mean of all *_MICfc for azoles
    - echinocandidn_resistance: Whether any of the *_resistance is True for echinocandins

    It considers MIC50 for all drugs except AMB, which is MIC90. This is equivalent to the 

    """

    print(taxID, r)

    kjadjhgg


def print_df_keys(df):

    """print df keys"""

    for k in df.keys(): 
        if any(~pd.isna(df[k])): print( "\n", k, "\n", df[k],  "\n")


def get_Chow2020_df(manually_curated_data):

    """Takes the manually curated data dir and returns the df with the SRR and extra fields"""
    
    # load
    df_Chow2020 = pd.read_excel("%s/Chow2020_TableS1_strainsUsed.xlsx"%manually_curated_data, header=1).rename(columns={"FCZ* (AFST)":"FLC", "AMB* (AFST)":"AMB", "MCF* (AFST)":"MIF", "SRA Run":"SRR"})

    # add the resistance
    df_Chow2020["resistance"] = df_Chow2020[["FLC", "AMB", "MIF"]].apply(lambda r: "+".join([k for k in r.keys() if r[k]=="Resistant"]), axis=1)

    # add the no resistance
    df_Chow2020["susceptibility"] = df_Chow2020[["FLC", "AMB", "MIF"]].apply(lambda r: "+".join([k for k in r.keys() if r[k]=="Not resistant"]), axis=1)

    #return important fields
    return df_Chow2020[["SRR", "resistance", "susceptibility", "ID"]]


def get_df_GermanInternationalTurism(manually_curated_data):

    """There is a paper (https://wwwnc.cdc.gov/eid/article/25/9/19-0262_article) where they sequenced C. auris isolates for several bioprojects (PRJNA485145, PRJNA485239, PRJNA485259, PRJNA485409, PRJNA485414, and PRJNA485415.). This function takes the Appendix Table and returns a df.

    There are 3 isolates for patienyt 394"""

    # load
    df = pd.read_excel("%s/Hamprecht2019_ExtendedTable_Cauris_AFresistance.xlsx"%manually_curated_data)
    df = df[df.Isolate!="unkown"]

    # add the strain
    df["strain"] = "strain_" + df.Isolate.apply(lambda x: x.split("-")[1]).apply(str)

    # redefine the MIC fields
    MICfields = [k for k in df.keys() if k.endswith("_MIC")]
    def get_correctMIC(x):
        x = str(x)
        if x=="ND": return np.nan
        
        if x.startswith(">"): multiplier = 2
        elif  x.startswith("<"): multiplier = 0.5
        else: multiplier = 1

        return (float(x.replace(">","").replace("<", ""))*multiplier)


    for f in MICfields: df[f] = df[f].apply(get_correctMIC)

    # define the patient ID
    df["patient_ID"] = ["germanInternationalTourism_patient%i"%I for I in range(len(df))]

    return MICfields, df[["strain", "treatment", "patient_ID"] + MICfields]


def get_sra_metadata_df(sra_df, metadata_dir, manually_curated_data, taxID):

    """This function returns the sra_df with metadata. It downloads the necessary files into metadata_dir and reads manually curated data from manually_curated_data"""

    metadata_cols = ["Run", "BioSample", 

                     "AMB_MIC", 
                     "AMB_MIC90",

                     "ANI_MIC", 
                     "CAS_MIC", 
                     "MIF_MIC", 

                     "FLC_MIC", 
                     "ITR_MIC", 
                     "POS_MIC",
                     "VRC_MIC",
                     "IVZ_MIC",
                     "KET_MIC",
                     "CLZ_MIC",
                     "MIZ_MIC",

                     "UCA_MIC",

                     "BVN_MIC",

                     "5FC_MIC", 

                     "TRB_MIC",

                     "YPD_GR",
                     "Temp-41.5_GR",
                     "pH-9_GR",
                     "pH-2_GR",
                     "NaCl-1M_GR",
                     "H2O2-10mM_GR",
                     "DTT-1.5mM_GR", 

                     "Temp-18_GR",
                     "Temp-30_GR",
                     "VSM-Temp-30_GR",
                     "VSM-Temp-37_GR",
                     "SSM-Temp-30_GR",
                     "SSM-Temp-37_GR",

                     "Temp-16_GR",
                     "Temp-37_GR",
                     "Temp-42_GR",
                     "CaCl2-50mM_GR",
                     "CaCl2-300mM_GR",
                     "NACl-1.3M_GR",
                     "noCarbon_GR",
                     "Galactose_GR",
                     "Glycerol_GR",
                     "Ethanol_GR",
                     "CalcofluorW_GR",
                     "SDS_GR",
                     "Sorbitol_GR",
                     "Caffeíne_GR",
                     "EDTA_GR",
                     "Menadione-90uM_GR",
                     "Menadione-80uM_GR",
                     "H2O2-6mM_GR",
                     "H2O2-4.5mM_GR",
                     "pH-4.5_GR",
                     "pH-10_GR",
                     "CuSO4-13mM_GR",
                     "CuSO4-15mM_GR",
                     "LiCl- 300mM_GR",
                     "Guanidine_HCl- 3mM_GR",
                     "Guanidine_HCl-5mM_GR",
                     "Urea-12.5mM_GR",
                     "Urea-25mM_GR",
                     "Ethanol-5%_GR",
                     "noNitrogen_GR",
                     "Isoleucine_GR",
                     "Proline_GR",      

                     "CAS_GR",
                     "ITR_GR",
                     "FLC_GR",           

                     "treatment", "resistance", "susceptibility",
                     "patient_ID", "timepoint", "host_age", "host_sex",
                     "source", "type", "strain", "parent_strains", "Hospital"]

    df_metadata_all = pd.DataFrame(columns=metadata_cols)

    # this is the meaning of the columns
    """
    *_MIC: is a float with the MIC 50 in mg/L 
    treatment: can be any combination of drugs (sepparated by '/' (either), '+' (additive) or '>' (followed by)), none . It can also include conditions like temperature or some stress.
    resistance: can be any combination of drugs (sepparated by '/' (either), '+' (additive) or '>' (followed by)), none 
    susceptibility: the same as resistance, but for drugs for which there was susceptibility found
    source: is the place of isolation (things like Blood, Feces ...)
    type: can be clinical/environmental/reference/invitro_evol_population/invitro_evol_clone/one_homozygous_chromosome/genome_engineered/inmouse_evol_clone
    parent_strains: should be the parent strains (unique ID) in the dataset, comma sepparated.
    *_GR: is a float with  the growth rate estimate in several conditions. It is important to compare always to a control for inter-study comparison 
    
    # these are the drugs
    AMB: amphotericin B

    ANI: anidulafungin 
    CAS: caspofungin
    MIF: micafungin

    FLC: fluconazole
    ITR: itraconazole
    POS: posaconazole
    VRC: voriconazole
    IVZ: isavuconazole
    KET: ketoconazole
    CLZ: clotriminazole
    MIZ: miconazole

    BVN: beauvericin

    UCA: undecanoic acid

    TRB: terbinafine

    5FC: 5-flucytosine
    #### MISC COMMENTS ###
    
    PRJEB8951 is related to https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4636289/


    #####################
    """

    # define wrong bioprojects 
    wrong_bioprojects = {"PRJNA437988", "PRJNA579121"}


    # define the samples with no available metadata
    bioprojects_with_noBioSample_available = {"PRJNA231221", "PRJNA595978"}

    # define the bioprojects with multiple runs for one BioSample
    bps_multipleRuns_one_BioSample = {"PRJEB27862", "PRJNA120009", "PRJNA165021", "PRJNA165023", "PRJNA165025", "PRJNA165027", "PRJNA165029", "PRJNA165031", "PRJNA165033", "PRJNA165035", "PRJNA165037", "PRJNA165039", "PRJNA200311", "PRJNA271803", "PRJNA322852", "PRJNA395439", "PRJNA73979", "PRJNA75209", "PRJNA75211", "PRJNA75213"}.union(bioprojects_with_noBioSample_available)


    # define bioprojects were the data can't be easily parsed
    make_folder(metadata_dir)

    # go through each bioproject
    for Ibioproject, bioproject in enumerate(sorted(set(sra_df.BioProject))):
        print("%i/%i"%(Ibioproject+1, len(set(sra_df.BioProject))), bioproject)

        # get the bioproject metadata
        df = sra_df[sra_df.BioProject==bioproject]

        if bioproject not in wrong_bioprojects:

            # get the biosampleMetadata
            bioproject_bioSample_file = "%s/%s_BioSampleData.tab"%(metadata_dir, bioproject)

            # get the metadata
            df_BioSample = get_BioSample_df_for_BioProject(bioproject, bioproject_bioSample_file)
            df_BioSample = df_BioSample[df_BioSample.BioSample.isin(set(df.BioSample))]
          
            if bioproject not in bioprojects_with_noBioSample_available:

                missing_bioSamples = set(df.BioSample).difference(set(df_BioSample.BioSample))
                if len(missing_bioSamples)>0: raise ValueError("biosamples %s are missing from df_BioSample"%missing_bioSamples)

            # write the bioproject_bioSample_file with only the available biosample data
            bioproject_bioSample_file_onlyInterestingSRRs = "%s.onlySRRsrun"%bioproject_bioSample_file
            df_BioSample.to_csv(bioproject_bioSample_file_onlyInterestingSRRs, sep="\t", header=True, index=False)
            print("The biosample file is in %s"%bioproject_bioSample_file_onlyInterestingSRRs)

      
            # init the df_metadata with the merging of dfs. In some cases there can be repeated biosamples
            try: df_metadata = df[["Run", "BioSample", "SampleName"]].merge(df_BioSample, on="BioSample", validate="one_to_one", how="left")

            except: df_metadata = df[["Run", "BioSample", "SampleName"]].merge(df_BioSample, on="BioSample", validate="many_to_one", how="left")

        else: df_metadata = df[["Run", "BioSample", "SampleName"]]

        # keep the df_metadata_raw for the end
        df_metadata_raw = cp.deepcopy(df_metadata)


        if len(df)!=len(df_metadata): raise ValueError("df and df_metadata should have the same length")

        # get only available biosamples
        if bioproject=="PRJEB20459":

            """
            # The paper is 'Understand the genomic diversity and evolution of fungal pathogen Candida glabrata by genome-wide analysis of genetic variations' (https://pubmed.ncbi.nlm.nih.gov/31059831/). 

            They have 48 isolates, some of them treated with fluconazole. The susceptibility data was taken from 'S. Poláková, C. Blume, J.Á. Zárate, et al., Formation of new chromosomes as a virulence mechanism in yeast Candida glabrata, Proc. Natl. Acad. Sci. USA 106 (8)'. 

            This is how they measure flz susceptibility: 'Fluconazole susceptibility was measured on RPMI medium 1640 agar plates (0.84% RPMI medium 1640 with L-glutamine and no bicarbonate, 3.45% Mops, 2% glucose, 1.5% bacto agar adjusted to pH 7 with 1 M NaOH) with 6 different concentrations of fluconazole (4.8, 14.4, 43.2, 129.6, 388.8, and 1166.4 mg/L). Appropriate 10 times dilutions of each cell suspension (containing 10, 102, and 103 cells) were plated on RPMI medium 1640 agar plates. The plates were incubated for 48 h at 37 °C. In the case of strain Y663, the aneuploid single colonies were additionally grown for 70 generations in YPD + 40 mg/L fluconazole. Every day the cultures were reinoculated by transfer of 2 μL of the old culture into a new liquid YPD + fluconazole. Appropriate dilutions of each individual suspension were plated on YPD, and the 14 resulting single colonies were checked by PFGE for chromosome loss.'. 

            The fluconazole MIC data, in mg/L can be found in Table S1 of this paper (a pdf in https://www.pnas.org/content/suppl/2009/02/05/0809793106.DCSupplemental/0809793106SI.pdf#nameddest=ST1).
            """

            # change the BioSampleDF naming
            df_BioSample.columns = list(df_BioSample)[0:-1] + ["Strain_id"]

            # load the MIC table
            df_MIC = pd.read_csv("%s/TableS1_flz_susceptibility_Cglabrata_Polajova2009.txt"%(manually_curated_data), sep=" ")
            df_BioSample = df_BioSample.merge(df_MIC, how="left", validate="one_to_one", on="Strain_id")[["BioSample", "Strain_id", "fluconazole_MIC_mgL", "Source"]]

            # get the df_metadata
            df_metadata = df[["Run", "BioSample"]].merge(df_BioSample, on="BioSample", how="left", validate="many_to_one")
            df_metadata = df_metadata.rename(columns={"fluconazole_MIC_mgL":"FLC_MIC", "Source":"source", "Strain_id":"strain"})
            df_metadata["type"] = "clinical"
            df_metadata = df_metadata[["Run", "BioSample", "FLC_MIC", "source", "type", "strain"]]

        elif bioproject=="PRJEB8571":

            """
            this is from an ARS-capture experiment. The paper is entitled 'Genome-wide replication landscape of Candida glabrata' and can be found in https://bmcbiol.biomedcentral.com/articles/10.1186/s12915-015-0177-6. There is no interesting metadata. It is all based on CBS138
            """

            # initialize metadata
            df_metadata = df[["Run", "BioSample"]]

            # set the strain
            df_metadata["strain"] = "CBS138"

            # add other metadata
            df_metadata["treatment"] = "none"

            df_metadata["source"] = "reference"
            df_metadata["type"] = "reference"

        elif bioproject=="PRJNA184888":

            """
            This includes the sequencing of all the isolated genomes of one day in a clinical setting.
            Next generation sequencing technology is available to many clinical laboratories; however, it is not yet widely used in routine microbiology practice. To demonstrate the feasibility of using whole genome sequencing in a routine clinical microbiology workflow, we sequenced the genome of every organism isolated in our laboratory for one day.

            The paper is 'A genomic day in the life of a clinical microbiology laboratory' from http://jcm.asm.org/cgi/pmidlookup?view=long&pmid=23345298.

            There is metadata in Table 1 about the source of isolation

            """

            # initialize metadata
            df_metadata = df[["Run", "BioSample"]]

            # set the strain
            df_metadata["strain"] = df.SampleName
            df_metadata["source"] = "Urine"
            df_metadata["type"] = "clinical"

        elif bioproject=="PRJNA297263":

            """
            Six Candida glabrata isolates from blood of patients with recurrent candedemia. Two isolates were taken from each of three patients, minimum three months apart. All of them with candidaemia.

            There is a paper called 'Draft Genome Sequences of Candida glabrata Isolates 1A, 1B, 2A, 2B, 3A, and 3B'. It can be found here: https://mra.asm.org/content/5/10/e00328-16. There is no further metadata, but they state that these have been obtained from a study ('Twenty-two years of candidemia surveillance - Results from a Norwegian national study.', from https://www.clinicalmicrobiologyandinfection.com/article/S1198-743X(15)00621-7/fulltext)

            I could not find any susceptibility data. I could not find any paper citing this (not very trustable because there are many papers with no full text).
            """ 

            # initialize
            df_metadata = df[["Run", "BioSample", "SampleName"]].merge(df_BioSample[["host_age", "host_sex", "isolate", "BioSample"]], how="left", validate="one_to_one", on="BioSample")

            # get patient and timepoint
            df_metadata["patient_ID"] = bioproject + "_" + df_metadata.isolate.apply(lambda x: x[0])
            df_metadata["timepoint"] = df_metadata.isolate.apply(lambda x: {"A":1, "B":2}[x[1]])

            # set the strain
            df_metadata["strain"] = df.SampleName
            df_metadata["source"] = "Blood"
            df_metadata["type"] = "clinical"


            df_metadata = df_metadata[["Run", "BioSample", "patient_ID", "timepoint", "host_age", "host_sex", "source", "type", "strain"]]

        elif bioproject=="PRJNA310957":

            """
            Next generation sequencing of clinical Candida glabrata isolates. 

            I can only find a paper by the same instiute called 'Whole Genome Sequencing of Candida glabrata for Detection of Markers of Antifungal Drug Resistance' in https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5908403/ where they claim that they sequenced 13 C. glabrata isolates (the same ones as this  BioProject) under PRJNA310057. However, this bioproject has no data associated. They also state that they analyze 12 clinical and 1 real strain.

            In this other paper, they state that there are samples resistant to many drugs.

            Thirteen C. glabrata comprising C. glabrata ATCC 90030 and 12 isolates from the Clinical Mycology Reference laboratory (isolates CMRL1 to CMRL12), Westmead Hospital, Sydney were studied (Table 1). These included three pairs of isolates CMRL-1/CMRL-2, CMRL-3/CMRL-4 and CMRL-5/CMRL-6 obtained before and after antifungal therapy with no epidemiological links between them 24 (Table 1).

            The MICs are in mg/L

            As the names match (SampleName), I can assume that this is the associated metadata.

            In vitro susceptibility of 13 Candida glabrata isolates including CMRL-1/CMRL-2, CMRL-3/CMRL-4 and CMRL-5/CMRL-6 isolate pairs obtained before and after antifungal therapy.

            """

            # map each sample to the patient ID
            sampleName_to_patientID = {"CMRL-1":1, "CMRL-2":1, "CMRL-3":2, "CMRL-4":2, "CMRL-5":3, "CMRL-6":3, "ATCC_90030":"reference"}
            sampleName_to_patientID = {**sampleName_to_patientID, **{"CMRL-%i"%s : "CMRL-%i"%s for s in range(7, 13)}}

            # map to the timepoint
            sampleName_to_timepoint = {"CMRL-1":1, "CMRL-2":2, "CMRL-3":1, "CMRL-4":2, "CMRL-5":1, "CMRL-6":2, "ATCC_90030":np.nan}
            sampleName_to_timepoint = {**sampleName_to_timepoint, **{"CMRL-%i"%s : np.nan for s in range(7, 13)}}

            # map to the treatment
            sampleName_to_treatment = {"CMRL-1":"none", "CMRL-2":"ANI/MIF/CAS", "CMRL-3":"none", "CMRL-4":"ANI/MIF/CAS", "CMRL-5":"none", "CMRL-6":"5FC", "ATCC_90030":"none"}
            sampleName_to_treatment = {**sampleName_to_treatment, **{"CMRL-%i"%s : np.nan for s in range(7, 13)}}

            # get the MIC data
            def convert_MIC_to_float(x):

                # ask if it is 
                x_is_floatable = False
                try: 
                    float(x)
                    x_is_floatable = True
                except:pass

                # edit
                if type(x)==float or type(x)==int or x_is_floatable is True: return float(x)
                elif "<" in x: return float(x.split("<")[1])/2

                elif ">" in x: return float(x.split(">")[1])*2
                elif type(x)==str: return x
                else: raise ValueError("%s is not valid"%x)

            df_MIC = pd.read_csv("%s/Table1_susceptibility_Biswas2017.txt"%(manually_curated_data), sep="\t").applymap(convert_MIC_to_float)

            # initialize the metadata
            df_metadata = df[["Run", "BioSample", "SampleName"]].merge(df_BioSample[["isolation_source", "BioSample"]], how="left", validate="one_to_one", on="BioSample").merge(df_MIC, how="left", left_on="SampleName", right_on="Isolate", validate="one_to_one")

            suscept_to_resistance = {'echinocandin_resistant':"ANI+CAS+MIF", '5FC_azole_resistant':"FLC+ITR+POS+VRC+5FC", 'susceptible_all':"none", 'azole_resistant':"FLC+ITR+POS+VRC"}
            df_metadata["resistance"] = df_metadata.antifungal_susceptibility.apply(lambda x: suscept_to_resistance[x])

            # add the MICs
            drugs = ["AMB", "ANI", "CAS", "MIF", "FLC", "ITR", "POS", "VRC", "5FC"]
            all_drugMICs = ["%s_MIC"%d for d in drugs]
            df_metadata = df_metadata.rename(columns=dict(zip(drugs, all_drugMICs)))

            # add general metadata
            df_metadata["type"] = "clinical"
            df_metadata["source"] = df_metadata.isolation_source
            df_metadata["strain"] = df_metadata.SampleName

            # get patient and timepoint
            df_metadata["patient_ID"] = bioproject + "_" + df_metadata.SampleName.apply(lambda x: sampleName_to_patientID[x]).apply(str)
            df_metadata["timepoint"] = df_metadata.SampleName.apply(lambda x: sampleName_to_timepoint[x])

            # add the treatment
            df_metadata["treatment"] = df_metadata.SampleName.apply(lambda x: sampleName_to_treatment[x])

            # get final
            df_metadata = df_metadata[["Run", "BioSample"] + all_drugMICs + ["treatment", "resistance", "patient_ID", "timepoint", "source", "type", "strain"]]

            # change a misannotation. CMRL-1 is actually CMRL-2
            strain_to_run = dict(df_metadata.set_index("strain")["Run"])
            strain_to_run["CMRL-1"] = "SRR3154234"
            strain_to_run["CMRL-2"] = "SRR3154166"
            df_metadata["Run"] = df_metadata.strain.apply(lambda s: strain_to_run[s])

        elif bioproject=="PRJNA329124":

            """
            Candida glabrata multidrug resistance study. I can't find a paper, but it seems that they are clinical data

            Index(['collected_by', 'collection_date', 'culture_collection', 'genotype',
               'geo_loc_name', 'host', 'host_disease', 'isolate', 'isolation_source',
               'lat_lon', 'strain', 'BioSample'],

            """
            df_metadata = df[["Run", "BioSample", "SampleName"]].rename(columns={"SampleName":"strain"})
            df_metadata["type"] = "clinical"

        elif bioproject=="PRJNA361477":

            """
            Candida glabrata population genomic analysis. These are the data from Laia's paper. The paper is called 'Patterns of Genomic Variation in the Opportunistic Pathogen Candida glabrata Suggest the Existence of Mating and a Secondary Association with Humans', found here https://www.cell.com/current-biology/fulltext/S0960-9822(17)31472-0?_returnURL=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS0960982217314720%3Fshowall%3Dtrue
                
            There is growth data in several condittions, including Dithiothreitol (DTT).

            The MIC data is already in mg/mL, and I take the average of the range, if provided, and 2x or 1/2x of the <, >, respectively.

            Cultures were grown in 96-well plates at 37°C or 41.5°C, shaking, for 24 or 72 h depending on the growth rate in each condition, and monitored to determine the optical density at 600 nm every ten min by a TECAN Infinite M200microplate reader. Finally, results from growth conditions were analyzed using an R package called Growthcurver v0.2.1.

            There is a mix of population and clone data, and the 

            In the biosample information 
            """

            # initialize 
            df_metadata = df[["Run", "BioSample", "SampleName"]]

            # get metadata from the Carrete paper
            df_metadata_Carrete = get_fitness_and_susceptibility_dfs_Carrete2017(manually_curated_data)

            # join 
            df_metadata = df_metadata.merge(df_metadata_Carrete, how="left", on="SampleName", validate="one_to_one")

            # add metadata
            df_metadata["source"] = df_metadata.Site
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = df_metadata.SampleName

            # keep important metadata
            df_metadata = df_metadata[["Run", "BioSample", "AMB_MIC90", "5FC_MIC", "FLC_MIC", "VRC_MIC", "POS_MIC", "IVZ_MIC", "MIF_MIC", "CAS_MIC", 'pH-2_GR', 'DTT-1.5mM_GR', 'YPD_GR', 'Temp-41.5_GR', 'H2O2-10mM_GR', 'pH-9_GR', 'NaCl-1M_GR', "resistance", "source", "type", "strain"]]

        elif bioproject=="PRJNA374542":

            """
            Sequencing of one patient, before () and after azole treatment isolates. The paper is in https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5555451/
            
            In this study, we were interested to compare genomes of two specific C. glabrata-related isolates, one of which was azole susceptible (DSY562) while the other was azole resistant (DSY565). DSY565 contained a PDR1 mutation (L280F) and was isolated after a time-lapse of 50 d of azole therapy.

            The metadata about the resistant isolate is found in another paper 'https://pubmed.ncbi.nlm.nih.gov/10543759/'

            """

            # initialize
            df_metadata = df[["Run", "BioSample", "SampleName"]]

            # get patient metadata
            strain_to_timepoint = {"DSY562":1, "DSY565":2}
            strain_to_treatment = {"DSY562":"none", "DSY565":"FLZ"}

            df_metadata["strain"] = df_metadata.SampleName
            df_metadata["patient_ID"] = "patient_%s"%bioproject
            df_metadata["timepoint"] = df_metadata.strain.apply(lambda s: strain_to_timepoint[s])
            df_metadata["treatment"] = df_metadata.strain.apply(lambda s: strain_to_treatment[s])
            df_metadata["source"] = "oral cavity"
            df_metadata["type"] = "clinical"

            # add the MICs
            strain_to_MICs = {"DSY562": {"FLC_MIC":4, "KET_MIC":0.031, "ITR_MIC":0.125, "AMB_MIC":0.5},
                              "DSY565": {"FLC_MIC":128, "KET_MIC":2, "ITR_MIC":4, "AMB_MIC":0.5}}

            for mic in ["FLC_MIC", "KET_MIC", "ITR_MIC", "AMB_MIC"]: df_metadata[mic] = df_metadata.strain.apply(lambda s: strain_to_MICs[s][mic])

            # keep important metadata
            df_metadata = df_metadata[["Run", "BioSample", "strain", "patient_ID", "timepoint", "treatment", "source", "type", "FLC_MIC", "KET_MIC", "ITR_MIC", "AMB_MIC"]]

        elif bioproject=="PRJNA393577":

            """
            They induce termal stress and H2O2 in the lab. This is the bioproject description: In the present study, we used a high-throughput sequencing method to identify genetic determinants that contribute to the improved tolerance to various environmental stresses (hydrogen peroxide and hyperthermal stress) in adaptive isolates and population samples from laboratory adaptive evolution.

            The associated paper is called 'CgSTE11 mediates cross tolerance to multiple environmental stressors in Candida glabrata' in https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6863853/

            They induce periodic hypethermal challenge and H2O2 exposure in parallel populations. There are 3 parallel populations (T1, T2 and T3) in which heat shocks were induced from time to time. In addition there are 2 populations (H2 and H3) treated with H2O2. There are both whole-population and clone samples. 

            interesting fields: 5isolate, 7isolation_source, 10sample_type, 11Selective Stress

            """

            # initialize
            df_metadata = df[["Run", "BioSample", "SampleName"]].merge(df_BioSample, on="BioSample", validate="one_to_one")

            # define general things
            df_metadata["strain"] = "ATCC2001" # all these are from this stain.

            # add the treatment as the periodic treatment
            stress_to_treatment = {np.nan:"none",
                                   "Periodic challenge at 47 degree celsius for 30 minutes":"Temp-47",
                                   "Periodic challenge at 48 degree celsius for 30 minutes":"Temp-48",
                                   "Periodic challenge at 49 degree celsius for 30 minutes":"Temp-49",
                                   "Periodic challenge at 50 degree celsius for 30 minutes":"Temp-50",
                                   "Periodic challenge with 140 mM of hydrogen peroxide for 1 hour":"H2O2-140mM",
                                   "Periodic challenge with 300 mM of hydrogen peroxide for 1 hour":"H2O2-300mM",
                                   "Periodic challenge with 350 mM of hydrogen peroxide for 1 hour":"H2O2-350mM",
                                   "Periodic challenge with 80 mM of hydrogen peroxide for 1 hour":"H2O2-80mM"}

            df_metadata["treatment"] = df_metadata["Selective Stress"].apply(lambda x: stress_to_treatment[x])

            # add the 'patient ID' as the population
            def get_metadata(r):

                # reference strains
                if r["SampleName"].startswith("MHCg"):

                    r_type = "reference"
                    r_patient_ID = bioproject + "_" + r["SampleName"]
                    r_timepoint = 0

                else: 

                    # define the type
                    if r["sample_type"]=="Single isolate": r_type = "invitro_evol_clone"
                    elif r["sample_type"]=="Population isolate": r_type = "invitro_evol_population"
                    else: raise ValueError("%s is not valid"%r["sample_type"])

                    r_patient_ID = bioproject + "_" + r["SampleName"].split("-")[0]

                    if r_type=="invitro_evol_clone": r_timepoint = int(r["isolate"].split()[-1].split("-")[1])
                    elif r_type=="invitro_evol_population":  r_timepoint = int(r["SampleName"].split("-")[1])

                # define the resistance (from table1)
                sampleName_to_resistance = {"T1-27G":"H2O2-100mM+pH-1+pH-3+EtOH-16pct+1Butanol-4pct+1Isobutanol-4pct",
                                            "T2-5G":"H2O2-100mM+pH-1+pH-3+EtOH-16pct+1Isobutanol-4pct",
                                            "T2-10G":"H2O2-100mM+pH-1+pH-3+EtOH-16pct+1Isobutanol-4pct",
                                            "T2-17G":"H2O2-100mM+pH-1+pH-3+EtOH-16pct+1Isobutanol-4pct",
                                            "T2-27G":"H2O2-100mM+pH-1+pH-3+EtOH-16pct+1Isobutanol-4pct",
                                            "T3-24Y":"H2O2-100mM+pH-1+pH-3+EtOH-16pct+1Butanol-4pct+1Isobutanol-4pct"
                                            }

                if r["SampleName"] in sampleName_to_resistance: r_resistance = sampleName_to_resistance[r["SampleName"]]
                else: r_resistance = np.nan
                
                return pd.Series({"type": r_type, "patient_ID":r_patient_ID, "timepoint":r_timepoint, "resistance":r_resistance})

            df_metadata[["type", "patient_ID", "timepoint", "resistance"]] = df_metadata.apply(get_metadata, axis=1)

            # keep important metadata
            df_metadata = df_metadata[["Run", "BioSample", "strain", "patient_ID", "timepoint", "treatment", "type", "resistance"]]

        elif bioproject=="PRJNA480138":

            """
            There is no interesting metadata in the BioSample. This is a project of WGS of clinical isolates. The associated paper is in https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6287553/.

            They do WGS of 51 Australian clinical isolates with antifungal susceptibility testing. They perform phylogeny analyses and also private-SNP analyses to see whih SNPs are related to antifungal susceptibility.
    
            Most isolates are from different patients. There are only 2 of them that are from the same patient, recovered 3 weeks apart.
            The two isolates from the same patient (WM_18.63 and WM_18.64) were both ST8. We don't know the order, so that we can't assign timepoints.

            """

            # initialize
            df_metadata = df[["Run", "BioSample", "SampleName"]].merge(df_BioSample, on="BioSample", validate="one_to_one")

            # load the MIC data
            df_MIC = pd.read_csv("%s/Table1_drug_susceptibility_Cglabrata_Biswas2018.tab"%manually_curated_data, sep="\t")[["isolateID", "FLC_MIC", "VRC_MIC", "POS_MIC", "AMB_MIC", "CAS_MIC"]]
            df_MIC = df_MIC[df_MIC.isolateID!="ATCC 90030"]

            # add
            df_metadata = df_metadata.merge(df_MIC, left_on="SampleName", right_on="isolateID", validate="one_to_one")

            # add the patient ID
            def get_patientID(sname):

                if sname not in {"WM_18.63", "WM_18.64"}: return "%s_%s"%(bioproject, sname)
                else: return "%s_WM_18.63"%(bioproject)

            df_metadata["patient_ID"] = df_metadata.SampleName.apply(get_patientID)

            # define general things
            df_metadata["type"] = "clinical"
            df_metadata["source"] = df_metadata.isolation_source
            df_metadata["strain"] = df_metadata.SampleName

            # keep important metadata
            df_metadata = df_metadata[["Run", "BioSample", "strain", "type", "source", "FLC_MIC", "VRC_MIC", "POS_MIC", "AMB_MIC", "CAS_MIC", "patient_ID"]]

        elif bioproject=="PRJNA483064":

            """
            This is a project of clinical isolates that have acquired equinocandin resistance. It is the same patient, two serial isolates. The paper is in https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6355595/

            Table 1 contains the MICs

            Figure 1 has the dynamics of drugs given. The patient had long administration of voriconazole, and it was sequenced before and after caspofungin+anidulafungin exposure (this is correlated with acquisition of echinocandin resistance)
            """

            # initialize
            df_metadata = df[["Run", "BioSample", "SampleName"]].merge(df_BioSample, on="BioSample", validate="one_to_one")

            # load the MICs
            df_MIC = pd.read_csv("%s/Table1_drug_susceptibility_Cglabrata_Barber2019.tab"%manually_curated_data, sep="\t")
            df_metadata = df_metadata.merge(df_MIC, left_on="SampleName", right_on="sample", validate="one_to_one")

            # load specific things
            sampleName_to_timepoint = {"NG-9780_NRZ_2016_57": 1, "NG-9780_NRZ_2016_58":2}
            sampleName_to_resistance = {"NG-9780_NRZ_2016_57": "VRC+POS+ITR+FLC+IVZ", "NG-9780_NRZ_2016_58":"VRC+POS+ITR+FLC+IVZ+ANI+CAS"}
            sampleName_to_treatment = {"NG-9780_NRZ_2016_57": "VRC", "NG-9780_NRZ_2016_58":"VRC>(ANI+CAS)"}

            df_metadata["timepoint"] = df_metadata.SampleName.apply(lambda x: sampleName_to_timepoint[x])
            df_metadata["resistance"] = df_metadata.SampleName.apply(lambda x: sampleName_to_resistance[x])
            df_metadata["treatment"] = df_metadata.SampleName.apply(lambda x: sampleName_to_treatment[x])

            # add trivial things
            df_metadata["type"] = "clinical"
            df_metadata["source"] = df_metadata.isolation_source
            df_metadata["strain"] = df_metadata.SampleName
            df_metadata["patient_ID"] = "%s_singlePatient"%bioproject

            # get important fields
            df_metadata = df_metadata[["Run", "BioSample", "timepoint", "resistance", "treatment", "type", "source", "strain", "patient_ID", "AMB_MIC", "POS_MIC", "VRC_MIC", "ITR_MIC", "FLC_MIC", "IVZ_MIC", "ANI_MIC", "CAS_MIC"]]

        elif bioproject=="PRJNA506893":

            """
            Three isolates from the same patient but different isolation sites. 

            The paper is in https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6379656/ (from Laia Carreté)
            
            In addition, we sequenced three serial isolates (SAT01BAL, SAT02PL, SAT03BL, here collectively referred to as the SAT strains) obtained from different body sites over the course of a week from the same leukemic patient suffering candidiasis. This bioproject is for this trio.

            These were isolated sequentially in 3 sepparatae days. There is MICs measured for these and others

            """

            # initialize
            df_metadata = df[["Run", "BioSample", "SampleName"]].merge(df_BioSample, on="BioSample", validate="one_to_one")

            # chnage the name
            sampleName_to_NewSampleName = {"SAT01BAL":"SAT01BAL", "SAT02PC":"SAT02PL", "SAT03BC":"SAT03BC"}
            df_metadata["SampleName"] = df_metadata.SampleName.apply(lambda x: sampleName_to_NewSampleName[x])

            # add the timepoint
            sampleName_to_timepoint = {"SAT01BAL":0, "SAT02PL":1, "SAT03BC":6}
            df_metadata["timepoint"] = df_metadata.SampleName.apply(lambda x: sampleName_to_timepoint[x])

            # load the MICs:
            df_MICs = get_susceptibility_df_Carrete2019(manually_curated_data)
            df_MICs = df_MICs[df_MICs.Strains.isin(sampleName_to_timepoint)]

            df_metadata = df_metadata.merge(df_MICs, left_on="SampleName", right_on="Strains", validate="one_to_one")

            # trivial metadata
            df_metadata["type"] = "clinical"
            df_metadata["patient_ID"] = "%s_singlePatient"%bioproject
            df_metadata["source"] = df_metadata.isolation_source
            df_metadata["strain"] = df_metadata.SampleName

            # get only the interesting fields
            df_metadata = df_metadata[['Run', 'BioSample', 'strain', 'timepoint', 'source', 'type', 'patient_ID', 'FLC_MIC', 'VRC_MIC', 'AMB_MIC', 'ANI_MIC', 'MIF_MIC', 'CAS_MIC', '5FC_MIC', 'POS_MIC', 'ITR_MIC', 'IVZ_MIC']]

        elif bioproject=="PRJNA524686":

            """
            >200 isolates from the CDC. WGS data of Candida glabrata isolates from bloodstream infections. I can't find a paper or metadata elsewhere.

            I have sent an email to see if I can get some extra metadata
            """

            # initialize
            df_metadata = df[["Run", "BioSample", "SampleName"]].merge(df_BioSample, on="BioSample", validate="one_to_one")

            # add trivial things
            df_metadata["source"] = df_metadata.isolation_source
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = df_metadata.SampleName

            # get only the interesting fields
            df_metadata = df_metadata[['Run', 'BioSample', 'strain', 'source', 'type']]

        elif bioproject=="PRJNA525402":

            """
            These are three clinical isolates. The paper is in https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31467089/, called 'Draft Genome Sequences of Three Clinical Isolates of the Pathogenic Yeast Candida glabrata'

            040_cg
            044_cg
            OL152_cg

            Susceptibility data is not available
            """

            # initialize
            df_metadata = df[["Run", "BioSample", "SampleName"]].merge(df_BioSample, on="BioSample", validate="one_to_one")

            # add trivial things
            df_metadata["source"] = df_metadata.isolation_source
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = df_metadata.SampleName

            # get only the interesting fields
            df_metadata = df_metadata[['Run', 'BioSample', 'strain', 'source', 'type']]

        elif bioproject=="PRJNA593955":

            """
            I can't find the study, but the BioSample metadata contains the resistance definition

            """

            # initialize
            df_metadata = df[["Run", "BioSample", "SampleName"]].merge(df_BioSample, on="BioSample", validate="one_to_one")

            # add the resistance
            sampleName_to_resistance = {"IHEM:26116":"ANI/CAS/MIF",
                                        "IHEM:26304":"FLC",
                                        "IHEM:26780":"(ANI/CAS/MIF)+FLC",
                                        "IHEM:9556":"none"}

            df_metadata["resistance"] = df_metadata.culture_collection.apply(lambda x: sampleName_to_resistance[x])

            # add trivial things
            df_metadata["source"] = df_metadata.isolation_source
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = df_metadata.SampleName

            # get only the interesting fields
            df_metadata = df_metadata[['Run', 'BioSample', 'strain', 'source', 'type', 'resistance']]

        elif bioproject=="PRJNA596170":

            """
            This contains only one sample with the referebce strain. There is no antifungal susceptibility measured.
            """

            # initialize
            df_metadata = df[["Run", "BioSample", "SampleName"]].merge(df_BioSample, on="BioSample", validate="one_to_one")

            # add trivial things
            df_metadata["source"] = df_metadata.isolation_source
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = df_metadata.SampleName

            # get only the interesting fields
            df_metadata = df_metadata[['Run', 'BioSample', 'strain', 'source', 'type']]

        elif bioproject=="PRJNA600925":

            """
            Genome assembly of BG2. This contains no data
            """

            # initialize
            df_metadata = df[["Run", "BioSample", "SampleName"]].merge(df_BioSample, on="BioSample", validate="one_to_one")

            # add trivial things
            df_metadata["source"] = df_metadata.isolation_source
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = df_metadata.SampleName

            # get only the interesting fields
            df_metadata = df_metadata[['Run', 'BioSample', 'strain', 'source', 'type']]

        elif bioproject=="PRJEB11905":

            """
            The paper is in https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4814463/.

            They engineer C. albicans strains to contain different tRNAs and then evolve them for 100 generations. They sequence them (presumably clones) after the generation.

            They measure several fitness attributes before the evolution of the transformants, but they are shown in an image that can't be parsed.

            There are 2 timepoints (0 and 100) for the generations of the experiment. They do replicates for each strain.
            """

            # add timepoint
            def get_timepoint(x):
                if "G0" in x: return 0
                elif "G100" in x: return 100
                else: raise ValueError("timepoint is not correct for %s"%x)
            df_metadata["timepoint"] = df_metadata.Title.apply(get_timepoint)

            # add the patient ID
            def get_patientID(x):
                if ")" in x: return "%s)_%s"%(x.split(")")[0], x[-1])
                else: return "%s_%s"%(x[0:7], x[-1])

            df_metadata["patient_ID"] = bioproject + "_" + df_metadata.Title.apply(get_patientID)

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "invitro_evol_clone"
            df_metadata["strain"] = df_metadata.SampleName
            df_metadata["source"] = "inVitro_evolution"


            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type', 'timepoint', 'patient_ID']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJEB12264":

            """
            I could not find an associated paper. They express several aberrrant tRNAs and perform invitro evolution in increasing concentrations of AmpB

            The link can be found here https://www.ncbi.nlm.nih.gov/bioproject/PRJEB12264/

            I can recover a treatment. We can assume that they are the last timepoint of the evolution experiment, treated and resistant to AmpB. They did two replicates for several alterred tRNAs.
            """

            
            # add the treatment
            df_metadata["treatment"] = "AMB"
            df_metadata["resistance"] = "AMB"

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "invitro_evol_clone"
            df_metadata["strain"] = df_metadata.Alias
            df_metadata["source"] = "inVitro_evolution"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type', 'treatment', 'resistance']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJEB1755":

            """
            They have reengineered C. albicans strains to mis-incorporate increasing levels of Leu at protein CUG sites.

            This is the paper: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3704024/

            All samples have the following description:

            These are laboratory strains genetically engineered from strain SN148 which were not submitted to infection assays. Strains CIP,t1,t2 have 0, 1 and 2 copies of a Leu-tRNA(CAG) gene inserted at RPS10 locus, respectively. Strain t2ko1 is derived from strain t2 by deletion of one copy of the Ser-tRNA(CAG) gene. Strain t2ko2 is derived from t2ko1 by deletion of the second copy of the Ser-tRNA(CAG) gene. Samples were taken from the stock and plated onto YPD agar media containing fluconazole. One colony was selected for growth in liquid YPD until exponential phase. DNA extraction was carried out using the Genomic-tip 100/G kit (Qiagen) according to the manufacturer's protocol. Quantification and quality assessment were performed using the Picogreen fluorescence based quantification assay. Libraries were constructed using Illumina TruSeq DNA Sample Prep standard protocol. Briefly, 5 ug of high molecular weight genomic DNA (gDNA) was fragmented by Covaris sonication device. Following sonication, DNA fragments were end-repaired and A-tailed. Adapters were then ligated via a 3' thymine overhang. Finally, ligated fragments were amplified by PCR (10 cycles). insert sizes were about 400-500 bp as evaluated in an Agilent DNA 1000 Analyzer Chip.

            There is metadata (table S5) that contains a growth score relative to a control.

            These are in vitro evolved strains with these aberrations.

            """

            # load the table with the phenotypic data
            df_GR = pd.read_excel("%s/Bezerra2013_Table_S5_growRate.xlsx"%manually_curated_data)[["condition", "CIP", "t1", "t2", "t2KO1"]].set_index("condition").transpose()

            # change the cols to include GR
            GR_fields = ["%s_GR"%c for c in df_GR.columns]
            df_GR.columns = GR_fields

            # add to the metadata df
            df_metadata = df_metadata.merge(df_GR, left_on="title", right_on=None, left_index=False, right_index=True, validate="one_to_one")

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "invitro_evol_clone"
            df_metadata["strain"] = df_metadata.title
            df_metadata["source"] = "inVitro_evolution"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type'] + GR_fields

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJEB1831":

            """
            They plate human feces on YPD plates and isolate canida albicals form ITS sequencing. The paper is in https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5780349/, where they report all the phenotypic info about these strains.

            They measure several things not related to antifungal resistance, so that I drop them. 
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = df_metadata.Title
            df_metadata["source"] = "feces"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJEB27862":

            """
            They isolate three C. albicans from oaks. Paper in https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6325710/. There is some phenotypic metadata on growth in several media, but not AF resistance.
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["treatment"] = "none"
            df_metadata["type"] = "environmental"
            df_metadata["strain"] = df_metadata.Title
            df_metadata["source"] = "oak"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type', 'treatment']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJEB8387":

            """
            They evolve 6 candida albicans strains that misstranslate CUG with and without flz. It seems that the paper is this https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5549176/, but it is unclear. There are two parental strains with different misstranslation of CUG. T0 has low misstranslation and T1 has high misstranslation. 

            This is the meainmng of each sample:

            T1FH: strain T1 resistant to flz MIC > 256 ug/mL
            T1FM: strain T1 in the middle of FLZ evolution
            T1UH: strain T1 with no treatment

            T0FH: strain T0 resistant to flz MIC > 256 ug/mL
            T0FM: strain T0 in the middle of FLZ evolution
            T0UH: strain T0 with no treatment

            """

            # define the strain
            df_metadata["strain"] = df_metadata.Title.apply(lambda x: x.split(":")[1])

            # define mappings
            strain_to_treatment = {"T1FH":"FLC", "T1FM":"FLC", "T1UH":"none", "T0FH":"FLC", "T0FM":"FLC", "T0UH":"none"}
            strain_to_MICflc = {"T1FH":256, "T1FM":np.nan, "T1UH":np.nan, "T0FH":256, "T0FM":np.nan, "T0UH":np.nan}
            strain_to_resistance = {"T1FH":"FLC", "T1FM":"FLC", "T1UH":np.nan, "T0FH":"FLC", "T0FM":"FLC", "T0UH":np.nan}

            df_metadata["treatment"] = df_metadata.strain.apply(lambda x: strain_to_treatment[x])
            df_metadata["FLC_MIC"] = df_metadata.strain.apply(lambda x: strain_to_MICflc[x])
            df_metadata["resistance"] = df_metadata.strain.apply(lambda x: strain_to_resistance[x])

            # add the patient and timepoint
            df_metadata["patient_ID"] = bioproject + "_" + df_metadata.strain.apply(lambda x: x[0:2])
            string_to_timepoint = {"UH":0, "FM":1, "FH":2}
            df_metadata["timepoint"] = df_metadata.strain.apply(lambda x: string_to_timepoint[x[2:]])

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "invitro_evol_clone"
            df_metadata["source"] = "inVitro_evolution"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type', 'treatment', 'FLC_MIC', "resistance", "patient_ID", "timepoint"]

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA120009":

            """
            There is only one bioproject for all of them. This is a genome assembly project for strain SC5314. I don't know any treatment regimes.
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = "SC5314"
            df_metadata["source"] = "unknown"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA165021":

            """
            There are multiple libraries of sequencing the strain 3153A, with no metadata
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = "3153A"
            df_metadata["source"] = "unknown"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA165023":

            """
            The sequence many libraries from the same strain CHN1, without metadata
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = "CHN1"
            df_metadata["source"] = "unknown"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA165025":

            """
            The sequence many libraries of strain A20, without metadata
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = "A20"
            df_metadata["source"] = "unknown"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA165027":

            """
            They sequence many libraries of strain A48, without metadata
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = "A48"
            df_metadata["source"] = "unknown"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA165029":

            """
            Several libraries of strain A67 of Candida albicans, without metadata
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = "A67"
            df_metadata["source"] = "unknown"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA165031":

            """
            Several isolates from strain A92 with no metadata
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = "A92"
            df_metadata["source"] = "unknown"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA165033":

            """
            Several libraries for strain A123 with no metadata
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = "A123"
            df_metadata["source"] = "unknown"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA165035":

            """
            Several libraries for strain A155 with no metadata
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = "A155"
            df_metadata["source"] = "unknown"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA165037":

            """
            Libraries from strain A84 with no metadata
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = "A84"
            df_metadata["source"] = "unknown"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA165039":

            """
            Libraries from strain A203
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = "A203"
            df_metadata["source"] = "unknown"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA194436":

            """
            Whole genome sequencing of wild-type and clinical isolates as well as laboratory-evolved strains of Candida albicans to identify and characterize mutations conferring amphotericin resistance. 

            Paper in https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3812114/

            They show that resistance to AmpB comes at a great cost resulting in avirulent strains. These are the strains (label in the df_metadata):

            0    BV01: ATCC10231 strain drug-susceptible clinical isolate --> this is the parent of the IV evolution samples
            1    BV08: ATCC200955 AmB-resistant clinical isolate
            2    BV05: SC5314 wild-type sample
            3    BV02: lab-evolved AmB-resistant #2
            4    BV03: lab-evolved AmB-resistant #3
            5    BV04: lab-evolved AmB-resistant #4

            The process of IV evolution was performed until the AMB=32 ug/mL. From figure 1 we can infer the AMB MIC of some isolates. We can assume that the MIC of the evolved strains has 32 ug/mL. 


            """

            # define mappings
            label_to_treatment = {"BV01":"none", "BV08":"AMB", "BV05":"none", "BV02":"AMB", "BV03":"AMB", "BV04":"AMB"}
            label_to_resistance = {"BV01":"none", "BV08":"AMB", "BV05":"none", "BV02":"AMB", "BV03":"AMB", "BV04":"AMB"}
            label_to_AMB_MIC = {"BV01":0.4, "BV08":np.nan, "BV05":0.05, "BV02":32, "BV03":32, "BV04":32}
            label_to_patientID = {"BV01":"1", "BV08":"2", "BV05":"3", "BV02":"1", "BV03":"1", "BV04":"1"}
            label_to_timepoint = {"BV01":0, "BV08":0, "BV05":0, "BV02":1, "BV03":1, "BV04":1}
            label_to_strain = {"BV01":"ATCC10231_WT", "BV08":"ATCC200955_AMBresistant", "BV05":"SC5314_WT", "BV02":"ATCC10231_expEvol1", "BV03":"ATCC10231_expEvol2", "BV04":"ATCC10231_expEvol3"}
            label_to_source = {"BV01":"unknown", "BV08":"unknown", "BV05":"unknown", "BV02":"inVitro_evolution", "BV03":"inVitro_evolution", "BV04":"inVitro_evolution"}
            label_to_type = {"BV01":"clinical", "BV08":"clinical", "BV05":"clinical", "BV02":"invitro_evol_clone", "BV03":"invitro_evol_clone", "BV04":"invitro_evol_clone"}

            # add things
            df_metadata["treatment"] = df_metadata.label.apply(lambda x: label_to_treatment[x])
            df_metadata["resistance"] = df_metadata.label.apply(lambda x: label_to_resistance[x])
            df_metadata["AMB_MIC"] = df_metadata.label.apply(lambda x: label_to_AMB_MIC[x])
            df_metadata["patient_ID"] = bioproject + "_" + df_metadata.label.apply(lambda x: label_to_patientID[x])
            df_metadata["timepoint"] = df_metadata.label.apply(lambda x: label_to_timepoint[x])
            df_metadata["strain"] = df_metadata.label.apply(lambda x: label_to_strain[x])
            df_metadata["source"] = df_metadata.label.apply(lambda x: label_to_source[x])
            df_metadata["type"] = df_metadata.label.apply(lambda x: label_to_type[x])

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type', 'treatment', 'resistance', 'AMB_MIC', 'patient_ID', 'timepoint']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA200311":

            """
            Two libraries of sample Ca529L. The paper is in https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6588375/, with no metadata
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = "Ca529L"
            df_metadata["source"] = "oral_cavity"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA201506":

            """
            The goal of this study was to resolve the fully diploid genome sequence of Candida albicans, a common fungal pathogen. Deep sequencing of a panel of strains selected to be homozygous for single chromosomes helped elucidate with sequence variants were present on each chromosome. 

            There is no associated metadata. I will mark as type 'one_homozygous_chromosome'
            """

            # add the type as one_homozygous_chromosome
            def get_type(x):

                if "(WT)" in x: return "clinical"
                else: return "one_homozygous_chromosome"

            df_metadata["type"] = df_metadata.strain.apply(get_type)

            # add trivial things, like the source, the type or the strain
            df_metadata["strain"] = df_metadata.strain
            df_metadata["source"] = "unknown"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA210804":

            """
            This is a study of serial isolates with no paper available. I will just assign one patient to each df. There are missing isolates.
            """ 

            # add the patient ID

            df_metadata["patient_ID"] = bioproject + "_" + df_metadata.SampleName.apply(lambda x: x.split("-")[0])

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = df_metadata.SampleName
            df_metadata["source"] = "unknown"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type', 'patient_ID']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA231221":

            """
            These are samples from the FDS-Argos project, with no associated metadata from my automatic parsing. There is no metadata.
            """

            # I need to re-initialize df_metadata
            df_metadata = df[["Run", "BioSample", "SampleName"]]

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = df_metadata.SampleName
            df_metadata["source"] = "unknown"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA257929":

            """
            To study the evolution of azole resistance in Candida albicans, we analyzed 43 isolates from 11 HIV-infected patients with oralpharyngeal candidiasis.

            The paper is in https://elifesciences.org/articles/00662.

            They measured MICs for several treatments and stored them in table 1

            Note that all have FLC measured but patient 1 has Clotriminazole. From the paper I understand that the MIC is to FLC

            """

            # load the metadata table 
            df_tab1 = pd.read_csv("%s/Table1_Ford2015_Calbicans.tab"%manually_curated_data, sep="\t").rename(columns={"E-test MIC (ug/mL)":"FLC_MIC"})

            # add metadata to this treatment
            def get_treatment(r):

                if r["Drug treatment"]=="Fluconazole" and r["Dose"]>0: return "FLC"
                elif r["Drug treatment"]=="Clotriminazole" and r["Dose"]>0: return "CLZ"
                else: return "none"

            df_tab1["treatment"] = df_tab1.apply(get_treatment, axis=1)
            df_tab1["patient_ID"] = bioproject + "_" + df_tab1.PT.apply(str)
            df_tab1["timepoint"] = df_tab1.Strain

            # add the strain as in df_metadata
            def get_strain_as_df_metadata(r):

                if r["PT"]==1: return "TWTC%i"%r["Strain"]
                else: return str(r["Strain"])


            df_tab1["strain_as_df_metadata"] = df_tab1.apply(get_strain_as_df_metadata, axis=1)

            # add to df metadata
            df_metadata = df_metadata.merge(df_tab1[["strain_as_df_metadata", "treatment", "patient_ID", "timepoint", "FLC_MIC"]], left_on="strain", right_on="strain_as_df_metadata", how="left", validate="one_to_one")


            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = df_metadata.strain
            df_metadata["source"] = df_metadata.isolation_source

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type', "treatment", "FLC_MIC", "patient_ID", "timepoint"]

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA271803":

            """
            Candida albicans isolates from CF patients. Whole genome sequencing to reveal genetic basis for morphological differences. The paper is likely in https://journals.plos.org/plospathogens/article?id=10.1371/journal.ppat.1005308#ppat.1005308.s016.


            They performed whole-genome sequencing of three C. albicans isolates from patient CF170, one with the filamentous phenotype under standard conditions, designated F1, and two that grew as yeast, designated Y1 and Y2

            Here we only have Y1 (or CF170 (patient) isolate P1A1)samples. I could find a CP1A1 isolate in File S1 for which resistance was measured, but it is susceptible to azoles.

            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = df_metadata.isolate
            df_metadata["source"] = "sputum"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA277890":

            """
            Whole genome sequencing of wild-type C. albicans and its seven mutants. The paper is in https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5177745/

            They knock-out several cell wall factors and do WGS to check that the KOs worked well. They don't report any drug resistance metadata.

            """

            # define the type of data
            def get_type(x):

                if "Ca WT" in x: return "clinical"
                else: return "genome_engineered"

            df_metadata["type"] = df_metadata.Title.apply(get_type)


            # define the patient ID and timepoints (they are all from the same patient)
            df_metadata["patient_ID"] = "%s_1"%bioproject 

            type_to_timepoint = {"clinical":0, "genome_engineered":1}
            df_metadata["timepoint"] = df_metadata["type"].apply(lambda x: type_to_timepoint[x])

            # add trivial things
            df_metadata["strain"] = df_metadata.Title
            df_metadata["source"] = "unknown"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA305340":

            """
            This is one patient with bronchitis caused by candida albicans.

            NGS for understanding non-invasiveness, pH non-responsiveness and Azole resistance in Candida albicans ATCC10231. The paper is just a preprint from https://www.biorxiv.org/content/10.1101/302943v1.full.pdf+html

            They state that it is a fluconazole resistance strain, so we'll keep this. There is no extra metadata
            """

            # in the paper they state that there is FLC resistance
            df_metadata["resistance"] = "FLC"

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = df_metadata.SampleName
            df_metadata["source"] = "lungs"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA322852":

            """
            Plating cells on a high concentration of the combination of DTPA and caspofungin allowed identification of spontaneous resistant isolates. This experiment was performed with both wild type cells and with a caspofungin-resistant clinical isolate. Sequencing of resistant isolates allowed for the identification of mutations that confer resistance to the combination. 

            They plate the CAS resistant isolate in CAS+DTPA and pick the colonies resistant to both. DTPA is another drug.

            This paper (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5047589/) is somehow related to this data. The available genome is a diploid made of a CAS-resistant isolate and a genetically engineered strain, so it's better not to take as resistant.
            
            All strains called from SN95/DPL15. They are the strains definitions:

            CaLC990 (DPL15) Clinical isolate,  FKS1T1922C/FKS1T1922C (from 1. Singh SD, Robbins N, Zaas AK, Schell WA, Perfect JR, Cowen LE. Hsp90 governs echinocandin resistance in the pathogenic yeast Candida albicans via calcineurin. PLoS Pathog. 2009;5:  e1000532.)

            CaLC239 (SN95)  arg4∆/arg4∆  his1∆/his1∆  URA3/ura3∆::imm434 IRO1/iro1∆::imm434 (from 2. Noble SM, Johnson AD. Strains and strategies for large-scale gene deletion studies of the diploid human fungal pathogen Candida albicans. Eukaryot Cell. 2005;4: 298-309.)

            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "genome_engineered"
            df_metadata["strain"] = df_metadata.strain
            df_metadata["source"] = "unknown"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA323475":

            """

            They investigate how the strain CaCi-17 can become azole resistant. These are a background strain and 4 invitro evolved resistant isolates. They say that the invitro clones become resistant in high concentrations of azoles (18-26 ug/mL).

            The paper in which these strains were generated is in https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5922528/
            
            These are the strain names of the azole resistant strains:

            1    CaLC4349 -> they treat with miconazole (18 ug/mL) and test MIC on fluconazole
            2    CaLC4353 -> same  as CaLC4349
            3    CaLC4356 -> they treat with miconazole (20 ug/mL) and test MIC on fluconazole
            4    CaLC4358 -> same as CaLC4356

            The parent is (CaLC3601), which has an homozygous RGD1 deletion from CaCi-17ç

            In the paper they measure miconazole resistance for all strains (Figure 4)

            """

            # define mappings
            data = [["field", "CaLC3601", "CaLC4349", "CaLC4353", "CaLC4356", "CaLC4358"], # strains
                    ["CAS_MIC", 0.015, np.nan, np.nan, np.nan, np.nan],   # caspofungin MIC
                    ["AMB_MIC", 0.12, np.nan, np.nan, np.nan, np.nan], # AMB MIC
                    ["MIZ_MIC", 0.31, 2.5, 2.5, 2.5, 2.5], # miconazole MIC
                    ["treatment", "none", "MIZ", "MIZ", "MIZ", "MIZ"], # treatment
                    ["type", "genome_engineered", "invitro_evol_clone", "invitro_evol_clone", "invitro_evol_clone", "invitro_evol_clone"], # the type of isolate
                    ["resistance", np.nan, "MIZ", "MIZ", "MIZ", "MIZ"],
                    ["timepoint", 0, 1, 1, 1, 1]
                    ]
            df_info = pd.DataFrame(data, columns=data.pop(0)).set_index("field").transpose()

            # add general things
            df_info["patient_ID"] = bioproject + "_1"

            # merge
            df_metadata = df_metadata.merge(df_info, left_on="strain", right_on=None, right_index=True, how="left", validate="one_to_one")

            # add trivial things, like the source, the type or the strain
            df_metadata["strain"] = df_metadata.strain
            df_metadata["source"] = "unknown"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type', "CAS_MIC", "AMB_MIC", "MIZ_MIC", "treatment", "resistance", "patient_ID", "timepoint"]

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA324272":

            """
            The goal of our study is to determine the mechanism of action of a microbial natural product in the human fungal pathogen Candida albicans. For this we used an unbiased genomic approach to select for mutants resistant to the compound followed by whole genome sequencing to identify the target. By determining the mechanism of action we hope to uncover novel regulatory circuitry mediating drug resistance in the pathogen.

            Paper in https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5119031/

            The available are isolates resistant to beauvericin. The parent was generated by deleting ZCF29 and TAC1, so they are genome_engineered
            """

            # define mappings
            data = [["field", "parent", "D1", "D7", "E1", "E2"], 
                    ["BVN_MIC", 3.125, 100, 100, 100, 100],  
                    ["treatment", "none", "BVN", "BVN", "BVN", "BVN"], 
                    ["type", "genome_engineered", "invitro_evol_clone", "invitro_evol_clone", "invitro_evol_clone", "invitro_evol_clone"], 
                    ["resistance", np.nan, "BVN", "BVN", "BVN", "BVN"],
                    ["timepoint", 0, 1, 1, 1, 1]
                    ]

            df_info = pd.DataFrame(data, columns=data.pop(0)).set_index("field").transpose()

            # add general things
            df_info["patient_ID"] = bioproject + "_1"

            # merge
            df_metadata = df_metadata.merge(df_info, left_on="isolate", right_on=None, right_index=True, how="left", validate="one_to_one")

            # add trivial things, like the source, the type or the strain
            df_metadata["strain"] = df_metadata.isolate
            df_metadata["source"] = "unknown"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type', "BVN_MIC", "treatment", "resistance", "patient_ID", "timepoint"]


            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA345600":

            """

            Be careful with the biosample accessions, as they are misplaced. You should rely in SampleName

            There are 28/28 samples considered.

            The paper is in https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6140516/

            They do microevolution passing on gastrointestingal tract (two different diets), bloodstream and YPD. they select 4 clinical isolates (SC5314, P78048, P76055, and P57055)) which are susceptible to antifungal treatments.

            I will consider as clinical all isolates except the invitro evolved

            """

            # load an excel with the metadata
            df_info = pd.read_excel("%s/Iuliana_2018_Calbican_InVitroEvol.xlsx"%manually_curated_data)

            # make unique IDs
            df_info["strain"] = bioproject + "_" + df_info.strain

            def get_parent_strains(x):
                if pd.isna(x): return np.nan
                else: return ";".join(["%s_%s"%(bioproject, s) for s in x.split(";")])
            df_info["parent_strains"] = df_info.parent_strains.apply(get_parent_strains)

            df_info["patient_ID"] = bioproject + "_" + df_info.patient_ID

            # add fields 
            df_metadata = df_metadata[["Run", "BioSample"]]

            for f in ["strain", "parent_strains", "source", "type", "patient_ID", "timepoint", "resistance"]: df_metadata[f] = df_info[f].values

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', "strain", "parent_strains", "source", "type", "patient_ID", "timepoint", "resistance"]

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA356375":

            """
            Microbes from strain WO-1. There are isolates 1 to 12. Genome sequencing data of 12 Candida albicans transformants containing various gene drive. There is one isolate missing.

            The paer is in https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5832965/

            They generate several deletions and then do WGS to test that these deletions are real. There are some WT strains (clinical), but most of them are engineered.

            There is no metadata on antifungal drug resistance

            """

            # define the type according to which are the WT strains
            wt_strains = {"Candida_142", "Candida_29", "Candida_32"}
            bool_to_type = {True:"clinical", False:"genome_engineered"}
            df_metadata["type"] = df_metadata.SampleName.isin(wt_strains).apply(lambda x: bool_to_type[x])

            # add trivial things, like the source, the type or the strain
            df_metadata["strain"] = df_metadata.SampleName
            df_metadata["source"] = "unknown"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA394762":

            """
            Several isolates from strain SC5314. Genome sequencing of a subset of fluconazole-resistant parasexual progeny from C. albicans

            The paper is in https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5676243/

            They generate parasexual progeny from the parent SC5314 and measure fluconazole resistance.

            They measure FLC MICs for several isolates (selected on FLC), and then let them grow for 4 weeks (to see if they lose the mutations). We have the FLC_MICs in the first timepoint, but not the second. Figure 5 helps us get the MIC of the second passage.

            There is one extra samople here (Ss2b) which is described in this work for the first time.
            """

            # load metadata as excel
            df_info = pd.read_excel("%s/Hirakawa_2017_Calbicans_parasexual.xlsx"%manually_curated_data)

            # make unique IDs
            df_info["strain"] = bioproject + "_" + df_info.strain
            df_info["parent_strains"] = bioproject + "_" + df_info.parent_strains
            df_info["patient_ID"] = bioproject + "_" + df_info.patient_ID

            # merge
            df_metadata = df_metadata.merge(df_info, on="SampleName", validate="one_to_one").rename(columns={"strain_y":"strain"})

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'patient_ID', 'timepoint', 'parent_strains', 'treatment', 'FLC_MIC', '5FC_MIC', 'CAS_MIC', 'VRC_MIC', 'AMB_MIC', 'type', 'source']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]
         
        elif bioproject=="PRJNA395439":

            """
            To test the synergistic effect of these stress conditions on C. albicans genome, reference strain SC5314 was subjected to an in vitro evolution scheme under hypoxia and 37°C, with two different carbon sources (glycerol and dextrose) for up to 48 weeks (approximately 3,000 generations).

            I have 3 runs which correspond to some of these conditions

            SC5314 GTH12: sample grow in glycerol and hypoxia for 12 weeks
            SC5314 P0 (twice): blood clinical isolates pre-experimental evolution. These would be timepoint 0

            They don't measure any drug resistance
            """

            # add variables according to the order ["SC5314 GTH12", " SC5314 P0", " SC5314 P0"]
            df_metadata["type"] = ["invitro_evol_clone", "clinical", "clinical"]
            df_metadata["strain"] = "SC5314"
            df_metadata["source"] = ["inVitro_evolution", "blood", "blood"]
            df_metadata["patient_ID"] = "%s_1"%bioproject
            df_metadata["timepoint"] = [12, 0, 0]
            df_metadata["treatment"] = ["hypoxia+Gly", np.nan, np.nan]

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type', "patient_ID", "timepoint", "treatment"]

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA400662":

            """
            Evolution of C. albicans in the mouse gastrointestinal tract. The paper is in https://science.sciencemag.org/content/362/6414/589.long

            They passage the same strain from one mouse to another. They do this in WT and Rag- mouse strains. There is no metadata. I will just keep who is whose parental.

            """

            # load an excel with all the data
            excel_file = "%s/curatedData_%s.xlsx"%(manually_curated_data, bioproject)
            df_info = pd.read_excel(excel_file)            

            # get unique IDs
            df_info["strain"] = bioproject + "_" + df_info.strain

            def get_parent_strains(x):
                if pd.isna(x): return np.nan
                else: return ";".join(["%s_%s"%(bioproject, s) for s in x.split(";")])
            df_info["parent_strains"] = df_info.parent_strains.apply(get_parent_strains)

            # add the patientID
            df_info["patient_ID"] = bioproject + "_1" 

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = ""
            df_metadata["strain"] = ""
            df_metadata["source"] = ""

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', "strain", "parent_strains", "source", "type", "patient_ID", "timepoint"]

            # get only the interesting fields
            df_metadata = df_info[interesting_fields]

        elif bioproject=="PRJNA417396":

            """
            There are just two isolates. The paper is in https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6336345/. They don't measure antifungal resistance, so that I just keep the names
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "genome_engineered"
            df_metadata["strain"] = df_metadata.SampleName
            df_metadata["source"] = "unknown"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]


        elif bioproject=="PRJNA421185":

            """
            Mice were infected with C. albicans strain AF485 using an oropharyngeal Candidiasis (OPC) model and single colony isolates were recovered from the tongue tissue of mice after 5 days. Three strains with specific single chromosome aneuploidies and AF485 were subjected to a second round of OPC and again multiple single colony isolates were recovered as described above. AF485 and single colony isolates from the 1st and 2nd OPC were subjected to whole genome sequencing.

            The paper is in https://www.biorxiv.org/content/10.1101/537340v1.full

            There are 15 samples. They are all inmouse_evol_clone and genome_engineered, so that I will keep both. There is no AF testing.

            """

            # add the aneuploid chromosome
            def get_aneuploid_GT(r):
                if "Chr" in r["genotype"]: return r["genotype"].split("; ")[-1]
                else: return np.nan
            df_metadata["aneuploid_GT"] = df_metadata.apply(get_aneuploid_GT, axis=1)

            # define the type
            def get_type(t):
                if t=="progenitor 1st OPC": return "genome_engineered"
                else: return "genome_engineered/inmouse_evol_clone"

            df_metadata["type"] = df_metadata.Title.apply(get_type)

            # define the source
            type_to_source = {"genome_engineered":"unknown", "genome_engineered/inmouse_evol_clone":"mouse_tongue"}
            df_metadata["source"] = df_metadata.type.apply(lambda x: type_to_source[x])

            # add strain
            df_metadata["strain"] = bioproject + "_" + df_metadata.strain
            
            # define the parental strains
            def get_parent_strains(r):

                # the first has no parents
                if r["Title"]=="progenitor 1st OPC": return np.nan

                elif r["Title"]=="single colony recovered from 1nd OPC experiment": 

                    return df_metadata[df_metadata.Title=="progenitor 1st OPC"].iloc[0].strain

                elif r["Title"]=="progenitor 2nd OPC": 

                    return df_metadata[(df_metadata.Title=="single colony recovered from 1nd OPC experiment") & (df_metadata.aneuploid_GT==r["aneuploid_GT"])].iloc[0].strain

                elif r["Title"]=="single colony recovered from 2nd OPC experiment":


                    if pd.isna(r["aneuploid_GT"]): return np.nan
                    else:

                        df_progenitor2nd = df_metadata[(df_metadata.Title=="progenitor 2nd OPC") & (df_metadata.aneuploid_GT==r["aneuploid_GT"])]
                        if len(df_progenitor2nd)!=1: raise ValueError("the df should be 1")

                        return df_progenitor2nd.iloc[0].strain

                

                else: raise ValueError("can't find the parent strains for %s"%r)

            df_metadata["parent_strains"] = df_metadata.apply(get_parent_strains, axis=1)

            # add the patient ID
            df_metadata["patient_ID"] = "%s_1"%bioproject

            # add the timepoint
            title_to_timepoint = {"progenitor 1st OPC":0, "single colony recovered from 1nd OPC experiment":1, "progenitor 2nd OPC":2, "single colony recovered from 2nd OPC experiment":3}
            df_metadata["timepoint"] = df_metadata.Title.apply(lambda x: title_to_timepoint[x])

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type', "patient_ID", "timepoint", "parent_strains"]

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA432884":

            """
            There are 177 samples here, which look like clinical isolates. The papee is here: https://www.nature.com/articles/s41467-018-04787-4. This is a nice example of population genomics analyses that can be performed.

            They measure fitness in several media (temperatures and salivary-like (SSM) and vaginal-like (VSM) media) in FigS7
            """

            # load phenotipic data
            df_pheno = pd.read_excel("%s/SuppTableS7_Ropars2018_Calbicans.xlsx"%manually_curated_data) # it has 181 strains

            # load the MIC data from another paper that measured data of these samples
            df_tableS1 = pd.read_csv("%s/TableS1_Sitterle2020_antifungalResistance_Calbicans.tab"%manually_curated_data, sep="\t")[["strain", "CAS_MIC", "FLC_MIC"]] 
            df_table2 = pd.read_csv("%s/Table2_Sitterle2020_antifungalResistance_Calbicans.tab"%manually_curated_data, sep="\t")[["strain", "FLC_MIC", "ITR_MIC", "VRC_MIC" ,"CAS_MIC"]]

            df_MICs = df_table2.append(df_tableS1) # it has 162 strains
            df_MICs.index = list(range(len(df_MICs)))

            # debug
            if len(df_MICs)!=len(set(df_MICs.strain)): raise ValueError("strains are not unique")

            # add to df pheno
            df_pheno = df_pheno.merge(df_MICs, left_on="Strain", right_on="strain", how="left", validate="one_to_one")

            # adapt the names
            def get_correct_colName(c):
                if "_" in c and "MIC" not in c:
                    if "YPD" in c: return "Temp-%s_GR"%(c.split("_")[1])
                    else: return "%s-Temp-%s_GR"%(c.split("_")[0], c.split("_")[1])
                else: return c
            df_pheno.columns = [get_correct_colName(c) for c in df_pheno.columns]

            # merge
            df_metadata = df_metadata.merge(df_pheno, left_on="SampleName", right_on="Strain", how="left", validate="one_to_one")

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = df_metadata.SampleName
            df_metadata["source"] = df_metadata.isolation_source

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type', "Temp-18_GR", "Temp-30_GR", "Temp-37_GR", "Temp-42_GR", "VSM-Temp-30_GR", "VSM-Temp-37_GR", "SSM-Temp-30_GR", "SSM-Temp-37_GR", "FLC_MIC", "ITR_MIC", "VRC_MIC" ,"CAS_MIC"]

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA433858":

            """
            They select mutants caspofungin-resistant to evaluate drug resistance. They use 2 parental strains (JRCT1 and SC5314)

            Paper in https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5404545/

            They generate mutatnts by plating them in CAS. They next test which mutants have Chr5 aneuploidies.

            SC5314 are the parents of SMC* strains
            JRCT1 are the parents of JMC* strains

            """
            # 
            excel_file = "%s/curatedData_%s.xlsx"%(manually_curated_data, bioproject)
            df_info = pd.read_excel(excel_file)            

            # get unique IDs
            df_info["strain"] = bioproject + "_" + df_info.strain

            def get_parent_strains(x):
                if pd.isna(x): return np.nan
                else: return ";".join(["%s_%s"%(bioproject, s) for s in x.split(";")])
            df_info["parent_strains"] = df_info.parent_strains.apply(get_parent_strains)

            df_info["patient_ID"] = "%s_1"%bioproject

            # add trivial things, like the source, the type or the strain
            df_info["source"] = "unknown"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type', "treatment", "resistance", "CAS_MIC", "timepoint", "patient_ID", "parent_strains"]

            # get only the interesting fields
            df_metadata = df_info[interesting_fields]


        elif bioproject=="PRJNA437988":

            """
            A bioproject with transcriptomics, where the biosample information is not available. This is only one SRR, which corresponds to a GFP mCherry strain. There is no metadata available
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "genome_engineered"
            df_metadata["strain"] = df_metadata.SampleName
            df_metadata["source"] = "unknown"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]


        elif bioproject=="PRJNA489773":

            """

            Paper in https://www.nature.com/articles/s41598-019-38768-4. They collect 9 samples from healthy individuals and 3 x 3 clonal isolates, in total 18.
            """

            # add the patient ID and timepoint
            df_metadata["patient_ID"] = bioproject + "_" + df_metadata.strain.apply(lambda s: s.split("-")[0])
            df_metadata["timepoint"] = 0

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["source"] = df_metadata.isolation_source

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type', "patient_ID", "timepoint"]

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA510147":

            """
            The goal of this research was to identify the copy number breakpoints associated with segmental aneuploidies in diverse Candida albicans isolates.

            Paper in https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6591007/

            They infect mice with YJB9318 and collect singel colonies after 1, 2, 3, and 5 days of infection. 

            These are the samples
            AMS3411, AMS3413 and AMS3432: single colony isolates from the OPC mouse (inmouse_evol_clone)
            AMS3743, AMS3744, AMS3747, AMS3748: In vitro evolution in FLZ for 100 generations
            """

            # define mappings
            title_to_type = {"Candida albicans murine OPC isolate":"inmouse_evol_clone", "Candida albicans drug exposed isoalte":"invitro_evol_clone"}
            strain_to_parent = {"AMS3411":"SC5314", "AMS3413":"SC5314", "AMS3432":"SC5314", "AMS3743":"P78042", "AMS3744":"SC5314", "AMS3747":"P75063", "AMS3748":"T490"}
            type_to_drug = {"inmouse_evol_clone":"none", "invitro_evol_clone":"FLC"}

            # define strains
            inmouse_strains = {"AMS3411", "AMS3413", "AMS3432"}
            invitro_strains = {"AMS3743", "AMS3744", "AMS3747", "AMS3748"}

            # add timepoint
            df_metadata["timepoint"] = 1

            # add patient_ID
            def get_patientID(s):
                if s in inmouse_strains: return "SC5314"
                else: return "inVitro_" + strain_to_parent[s]

            df_metadata["patient_ID"] = bioproject + "_" + df_metadata.strain.apply(get_patientID)

            # add the treatment and resistance
            df_metadata["type"] = df_metadata.Title.apply(lambda t: title_to_type[t])
            df_metadata["treatment"] = df_metadata.type.apply(lambda s: type_to_drug[s])
            df_metadata["resistance"] = df_metadata.type.apply(lambda s: type_to_drug[s])

            # add trivial things, like the source, the type or the strain
            df_metadata["source"] = df_metadata.isolation_source

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type', "patient_ID", "timepoint", "treatment", "resistance"]

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA546118":

            """
            Paper in https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6595153/

            They analyze a deletion collection and find that there is resistance to undecanoic acid. There is no MIC data, we just know that the dal is resistant to undecanoic acid.

            There are only the WT (SN152) and the dal81D/D strain (undecanoic acid highly susceptible)
            """

            df_metadata["type"] = ["clinical", "genome_engineered"]
            df_metadata["strain"] = df_metadata.SampleName
            df_metadata["source"] = "unknown"
            df_metadata["parent_strains"] = [np.nan, df_metadata.strain.iloc[0]]

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type', "parent_strains"]

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA563993":

            """
            We provide the genome sequences for 10 isolates of the major fungal pathogen of humans Candida albicans. These isolates are phenotypically resistant to fluconazole antifungal drug.

            The df is in https://academic-oup-com.sire.ub.edu/jac/article/75/4/835/5700447

            They test resistance to several antifungals.

            Sup= superficial; com= commensal; inv=invasive; NA = not applicable; NC = no cladable; UK= United Kingdom; FLC= fluconazole; CAS= caspofungin, DST= Diploid Sequence Type.  

            """

            # load df with MICs
            df_table2 = pd.read_csv("%s/Table2_Sitterle2020_antifungalResistance_Calbicans.tab"%manually_curated_data, sep="\t")

            # add to df_metadata
            df_metadata = df_metadata.merge(df_table2[["strain", "patient_ID", "FLC_MIC", "ITR_MIC", "VRC_MIC", "CAS_MIC"]], on="strain", validate="one_to_one", how="left")

            # rename patient ID
            df_metadata["patient_ID"] = bioproject + "_" + df_metadata.patient_ID.apply(lambda x: "_".join(x.split()))

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = df_metadata.strain
            df_metadata["source"] = df_metadata.isolation_source

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type', "patient_ID", "FLC_MIC", "ITR_MIC", "VRC_MIC", "CAS_MIC"]

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA605578":

            """
            Libraries were sequenced using illumina NovaSeq PE150 for Candida albicans haploid genome assembly. I can't find the paper, I just know that it is a clinical isolate.

            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = df_metadata.SampleName
            df_metadata["source"] = "unknown"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]


        elif bioproject=="PRJNA634197":

            """
            isolates obtained via culturing on D-tagatose, which can be considered as IV evolution. This is a specific monosaccaryde.. I can't find the paper

            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = ["clinical", "invitro_evol_clone", "invitro_evol_clone", "invitro_evol_clone", "invitro_evol_clone"]
            df_metadata["strain"] = df_metadata.isolate
            df_metadata["source"] = "unknown"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]


        elif bioproject=="PRJNA73979":

            """
            They sequence several isolates of C. glabrata.
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = df_metadata.SampleName
            df_metadata["source"] = "unknown"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]


        elif bioproject=="PRJNA75209":

            """
            Genome assembly of canida albicans 12C
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = "12C"
            df_metadata["source"] = "unknown"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]


        elif bioproject=="PRJNA75211":

            """
            Genome assembly of canida albicans L26
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = "L26"
            df_metadata["source"] = "unknown"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]


        elif bioproject=="PRJNA75213":

            """
            Genome assembly of canida albicans P94015
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = "P94015"
            df_metadata["source"] = "unknown"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]


        elif bioproject=="PRJNA75215":

            """
            Candida albicans P87 Genome sequencing and assembly
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = "P87"
            df_metadata["source"] = "unknown"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]


        elif bioproject=="PRJNA75217":

            """
            Assembly of strain P37005
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = "P37005"
            df_metadata["source"] = "unknown"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]


        elif bioproject=="PRJNA75219":

            """
            Strain P60002
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = "P60002"
            df_metadata["source"] = "unknown"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]


        elif bioproject=="PRJNA75221":

            """
            Candida albicans 19F Genome sequencing and assembly
    
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = "19F"
            df_metadata["source"] = "unknown"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]


        elif bioproject=="PRJNA75223":

            """
            Candida albicans GC75 Genome sequencing and assembly. 

            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = "GC75"
            df_metadata["source"] = "unknown"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]


        elif bioproject=="PRJNA75225":

            """
            Candida albicans P78048 Genome sequencing and assembly
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = "P78048"
            df_metadata["source"] = "unknown"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA75227":

            """
            Candida albicans P57072 Genome sequencing and assembly

            """
            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = "P57072"
            df_metadata["source"] = "unknown"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA75229":

            """
            Candida albicans P34048 Genome sequencing and assembly

            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = "P34048"
            df_metadata["source"] = "unknown"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA75231":

            """
            Candida albicans P37037 Genome sequencing and assembly

            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = "P37037"
            df_metadata["source"] = "unknown"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA75233":

            """
            Candida albicans P37039 Genome sequencing and assembly

            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = "P37039"
            df_metadata["source"] = "unknown"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]


        elif bioproject=="PRJNA75235":

            """
            Candida albicans P75010 Genome sequencing and assembly

            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = "P75010"
            df_metadata["source"] = "unknown"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]


        elif bioproject=="PRJNA75237":

            """
            Candida albicans P75016 Genome sequencing and assembly
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = "P75016"
            df_metadata["source"] = "unknown"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]


        elif bioproject=="PRJNA75239":

            """
            Candida albicans P57055 Genome sequencing and assembly

            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = "P57055"
            df_metadata["source"] = "unknown"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]


        elif bioproject=="PRJNA75241":

            """
            Candida albicans P75063 Genome sequencing and assembly

            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = "P75063"
            df_metadata["source"] = "unknown"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]


        elif bioproject=="PRJNA75243":

            """
            Candida albicans P76055 Genome sequencing and assembly

            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = "P76055"
            df_metadata["source"] = "unknown"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]


        elif bioproject=="PRJNA75245":

            """
            Candida albicans P76067 Genome sequencing and assembly
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = "P76067"
            df_metadata["source"] = "unknown"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]


        elif bioproject=="PRJNA75247":

            """
            Candida albicans P76055 Genome sequencing and assembly
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = "P78042"
            df_metadata["source"] = "unknown"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]



        elif bioproject=="PRJDB6988":

            """
            Paper in https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0223433
            They sequence several clinical isolates from C. auris and also perform some AF testing
            """

            # load mic table
            df_MIC = pd.read_csv("%s/Sekizuka2019_Table1_AFresistance_Cauris.tab"%manually_curated_data, sep="\t")
            df_metadata = df_metadata.merge(df_MIC, left_on="Run", right_on="SRR", validate="one_to_one", how="left")

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = "unknown"
            df_metadata["source"] = df_metadata.env_material

            # add patient data
            df_metadata["patient_ID"] = [bioproject+"_%i"%x for x in range(len(df_metadata))]
            df_metadata["timepoint"] = 1

            # ADD THE MIC

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type', "patient_ID", "timepoint"] + [x for x in df_MIC.keys() if x.endswith("_MIC")]

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]



        elif bioproject=="PRJEB14717":

            """
            The paper is in https://www.sciencedirect.com/science/article/pii/S2052297516300749?via%3Dihub
            They sequence several bloodstream isolates from C. auris. They report treatment and AF resistance
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = df_metadata.Alias
            df_metadata["source"] = "Blood"

            # add patient data
            df_metadata["patient_ID"] = [bioproject+"_%i"%x for x in range(len(df_metadata))]
            df_metadata["timepoint"] = 1

            # add the MICs
            df_MIC = pd.read_csv("%s/Sharma2016_Table1_AFresistance_Cauris.tab"%manually_curated_data, sep="\t")
            df_metadata = df_metadata.merge(df_MIC, left_on="strain", right_on="Isolate", validate="one_to_one", how="left")

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type',  "patient_ID", "timepoint"] + [x for x in df_MIC.keys() if x.endswith("_MIC") or x=="treatment"]

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJEB20230":

            """
            They sequence several isolates of C. auris (we have one sample) of the same outbreak

            Paper in https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5874254/

            They do ON and Illumina

            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = df_metadata.Title
            df_metadata["source"] = "Blood"


            # map each isolate to the characteristics
            strain_to_drug_to_MIC = {"16B31":{"ANI":0.12, "MIF":0.06, "CAS":0.12, "5FC":0.06, "POS":0.03, "VRC":0.5, "ITR":0.06, "FLC":256, "AMB":1}}
            for drug in strain_to_drug_to_MIC["16B31"].keys(): df_metadata["%s_MIC"%drug] = df_metadata.strain.apply(lambda s: strain_to_drug_to_MIC[s][drug])

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type'] + [k for k in df_metadata.keys() if k.endswith("_MIC")]

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJEB21518":

            """
            They sequence several isolates from this paper: https://aricjournal.biomedcentral.com/articles/10.1186/s13756-016-0132-5
            They state that this is a pre-publication release, indicating that there is no paper. I can't find any data
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = df_metadata.Title
            df_metadata["source"] = "unknown"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type'] 

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJEB29190":

            """
            Candida auris B2 whole genome. The paper is in https://www.frontiersin.org/articles/10.3389/fmicb.2019.01445/full
        
            They sequence one genome and analyze the presence of ABC transporters, showing that their expression is related to drug resistance.
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = df_metadata.Title
            df_metadata["source"] = "unknown"

            # add the MICS from table S2
            df_metadata["AMB_MIC"] = 0.70
            df_metadata["FLC_MIC"] = 6.0
            df_metadata["TRB_MIC"] = 0.75

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type'] + ["AMB_MIC", "TRB_MIC", "FLC_MIC"]

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJEB9463":

            """
            They sequence a FLC resistant C. auris sample. The paper is in https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4505117/. The FLC treatment and sequencing is equivalent to PRJEB14717
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = "VPCI_479_P_13"
            df_metadata["source"] = "unknown"

            # add patient data
            df_metadata["patient_ID"] = bioproject+"_1"
            df_metadata["timepoint"] = 1

            # load the MIC
            df_MIC = pd.read_csv("%s/Sharma2016_Table1_AFresistance_Cauris.tab"%manually_curated_data, sep="\t")
            df_metadata = df_metadata.merge(df_MIC, left_on="strain", right_on="Isolate", validate="one_to_one", how="left")

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type',  "patient_ID", "timepoint"] + [x for x in df_MIC.keys() if x.endswith("_MIC") or x=="treatment"]

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]


        elif bioproject=="PRJNA267757":

            """
            It is the same isolate, three different replicates.
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = df_metadata.strain
            df_metadata["source"] = df_metadata.isolation_source

            # add patient data
            df_metadata["patient_ID"] = bioproject+"_1"
            df_metadata["timepoint"] = 1

            # add MICs from table 2
            df_metadata["FLC_MIC"] = 64
            df_metadata["AMB_MIC"] = 16
            df_metadata["5FC_MIC"] = 1
            df_metadata["CAS_MIC"] = 0.25

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type', 'patient_ID', 'timepoint'] + ["FLC_MIC", "AMB_MIC", "5FC_MIC", "CAS_MIC"]

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA328792":

            """
            The paper is in https://www.nature.com/articles/s41467-018-07779-6

            They sequence many C. auris isolates and analyze polymorphisms/rearrangements.

            The strains tested in Chow2020 are here (PRJNA328792,PRJNA470683,PRJNA493622, andPRJNA595978.)

            There is AF testing for a few strains, whic I also add.

            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = df_metadata.strain
            df_metadata["source"] = df_metadata.isolation_source

            # load the Chow2020 data that tells resistance/susceptibility
            df_Chow2020 = get_Chow2020_df(manually_curated_data)
            df_metadata = df_metadata.merge(df_Chow2020[["SRR", "resistance", "susceptibility"]], left_on="Run", right_on="SRR", validate="one_to_one", how="left")
    
            # load the MIC data
            df_MIC = pd.read_csv("%s/TableS1_Munoz2018_AFresistance_Cauris.tab"%manually_curated_data, sep=" ")
            df_metadata =  df_metadata.merge(df_MIC, on="strain", validate="one_to_one", how="left")
            MIC_fields = [k for k in df_MIC.keys() if k.endswith("_MIC")]

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type', 'resistance', 'susceptibility'] + MIC_fields

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA415955":

            """
            The paper is in https://www.nejm.org/doi/10.1056/NEJMoa1714373. They sequence several patients. They measure AF resistance, but it is not reported. However, they state that "No micafungin or flucytosine resistance was identified."

            There are different timepoints per patient, which is interesting.
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = df_metadata.strain
            df_metadata["source"] = "unknown"

            # add patient data (on title)
            df_metadata["patient_ID"] = df_metadata.Title

            # add timepoint data
            df_metadata["collection_date_tuple"] = df_metadata.collection_date.apply(lambda x: tuple([int(y) for y in x.split("-")]))
            def get_timepoint_from_r(r):

                # map each collection date to a timepoint
                df_patient = df_metadata[df_metadata.patient_ID==r.patient_ID]["collection_date_tuple"].drop_duplicates().sort_values()
                collectionDate_to_timepoint = dict(zip(df_patient.values, range(len(df_patient))))

                # get it for r
                return (collectionDate_to_timepoint[r.collection_date_tuple]+1)

            df_metadata["timepoint"] = df_metadata.apply(get_timepoint_from_r, axis=1)

            # all samples are susceptible to micafungin and 5FC
            df_metadata["susceptibility"] = "MIF+5FC"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type', 'susceptibility', 'patient_ID', "timepoint"]

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]



        elif bioproject=="PRJNA470683":

            """
            This paper has also samples measured in Chow2020. The paper is in https://academic.oup.com/cid/article/68/1/15/4996781.
            They analize tens of C. auris isolates from Colombia . Table S1 has the MICs. This was also found in Chow2020, from which we take data.
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = df_metadata.SampleName
            df_metadata["source"] = df_metadata.isolation_source

            # load the Chow2020 data that tells resistance/susceptibility
            df_Chow2020 = get_Chow2020_df(manually_curated_data)
            df_metadata = df_metadata.merge(df_Chow2020[["SRR", "resistance", "susceptibility"]], left_on="Run", right_on="SRR", validate="one_to_one", how="left")
            

            # load the MIC data
            df_MIC = pd.read_excel("%s/Escandon2019_TableS1_AFresistance_Cauris.xlsx"%(manually_curated_data))
            MIC_fields = [f for f in df_MIC.keys() if f.endswith("_MIC")]
            def get_correct_MICval(x):
                if pd.isna(x): return x
                else: return float(str(x).replace("*", ""))

            for f in MIC_fields: df_MIC[f] = df_MIC[f].apply(get_correct_MICval)

            df_metadata =  df_metadata.merge(df_MIC[["Isolate", "Hospital"] + MIC_fields], left_on="strain", right_on="Isolate", validate="one_to_one", how="left")

            # patient data
            df_metadata["patient_ID"] = [bioproject+"_%i"%x for x in range(len(df_metadata))]
            df_metadata["timepoint"] = 1

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type', "resistance", "susceptibility", "patient_ID", "timepoint", "Hospital"] + MIC_fields

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA480539":

            """
            They sequence one isolate of C. auris. Paper in https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6555181/.

            They state that "The California isolate demonstrated low MICs to amphotericin (1 μg/mL), flucytosine (0.5 μg/mL), and voriconazole (0.032 μg/mL). The isolate had an elevated fluconazole MIC of 32 μg/mL. High caspofungin MIC of 8 μg/mL. "
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = "Candida_auris_CA_isolate"
            df_metadata["source"] = df_metadata.isolation_source

            # add the manually curated MICs
            df_metadata["AMB_MIC"] = 1
            df_metadata["5FC_MIC"] = 0.5
            df_metadata["VRC_MIC"] = 0.032
            df_metadata["FLC_MIC"] = 32
            df_metadata["CAS_MIC"] = 8

            # we know that there was treatment with echinocandins
            df_metadata["treatment"] = "MIF/ANI/CAS"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type', "treatment"] + ["AMB_MIC", "5FC_MIC", "VRC_MIC", "FLC_MIC", "CAS_MIC"]

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]



        elif bioproject=="PRJNA485022":

            """
            One isolate. Paper in https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6302580/. They measure antifungal resistance
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = df_metadata.SampleName
            df_metadata["source"] = df_metadata.isolation_source

            # add AF resistance
            df_metadata["AMB_MIC"] = 1.0
            df_metadata["ANI_MIC"] = 0.12
            df_metadata["CAS_MIC"] = 0.12
            df_metadata["MIF_MIC"] = 0.12
            df_metadata["ITR_MIC"] = 0.25
            df_metadata["POS_MIC"] = 0.06
            df_metadata["5FC_MIC"] = 0.12
            df_metadata["VRC_MIC"] = 2.0
            df_metadata["FLC_MIC"] = 512

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type'] + [k for k in df_metadata.keys() if k.endswith("_MIC")]

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA485145":

            """
            [Candida] auris strain:214 Genome sequencing and assembly. The MIC data can be obtained from get_df_GermanInternationalTurism
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = "strain_214"
            df_metadata["source"] = df_metadata.isolation_source

            # add the MIC vals
            MIC_fields, df_MIC = get_df_GermanInternationalTurism(manually_curated_data)
            df_metadata = df_metadata.merge(df_MIC, on="strain", how="left", validate="one_to_one")

            # add the timepoint
            df_metadata["timepoint"] = 1

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type', "treatment", "patient_ID", "timepoint"] + MIC_fields

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA485239":

            """
            [Candida] auris strain:288 Genome sequencing and assembly. Only one sample. I can't find the paper.

            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = "strain_288"
            df_metadata["source"] = df_metadata.isolation_source

            # add the MIC vals
            MIC_fields, df_MIC = get_df_GermanInternationalTurism(manually_curated_data)
            df_metadata = df_metadata.merge(df_MIC, on="strain", how="left", validate="one_to_one")

            # add the timepoint
            df_metadata["timepoint"] = 1

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type', "treatment", "patient_ID", "timepoint"] + MIC_fields

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA485259":

            """
            [Candida] auris strain:367 Genome sequencing and assembly. ONly one sample. This is associated to the get_df_GermanInternationalTurism paper

            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = "strain_367"
            df_metadata["source"] = df_metadata.isolation_source

            # add the MIC vals
            MIC_fields, df_MIC = get_df_GermanInternationalTurism(manually_curated_data)
            df_metadata = df_metadata.merge(df_MIC, on="strain", how="left", validate="one_to_one")

            # add the timepoint
            df_metadata["timepoint"] = 1
            
            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])   
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type', "treatment", "patient_ID", "timepoint"] + MIC_fields

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA485409":

            """
            Another sample from get_df_GermanInternationalTurism. This is the first timepoint
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = "strain_394"
            df_metadata["source"] = df_metadata.isolation_source

            # add the MIC vals
            MIC_fields, df_MIC = get_df_GermanInternationalTurism(manually_curated_data)
            df_metadata = df_metadata.merge(df_MIC, on="strain", how="left", validate="one_to_one")


            # add the timepoint
            df_metadata["timepoint"] = 1
            
            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type', "treatment", "patient_ID", "timepoint"] + MIC_fields

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA485414":

            """
            Another sample from get_df_GermanInternationalTurism. Note that PRJNA485409 and PRJNA485414 are 3 timepoints from the same sample. This is the 2nd timepoint
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = "strain_394"
            df_metadata["source"] = df_metadata.isolation_source

            # add the MIC vals
            MIC_fields, df_MIC = get_df_GermanInternationalTurism(manually_curated_data)
            df_metadata = df_metadata.merge(df_MIC, on="strain", how="left", validate="one_to_one")
            
            # add the timepoint
            df_metadata["timepoint"] = 2
            
            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type', "treatment", "patient_ID", "timepoint"] + MIC_fields

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA485415":

            """
            Another sample from get_df_GermanInternationalTurism. This is the 3d timepoint
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = "strain_394"
            df_metadata["source"] = df_metadata.isolation_source

            # add the MIC vals
            MIC_fields, df_MIC = get_df_GermanInternationalTurism(manually_curated_data)
            df_metadata = df_metadata.merge(df_MIC, on="strain", how="left", validate="one_to_one")
            
            # add the timepoint
            df_metadata["timepoint"] = 3
            
            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type', "treatment", "patient_ID", "timepoint"] + MIC_fields

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA493622":

            """
            There is data from Chow2020 here. This is a paper where they sequence ~300 strains of C. auris. Paper in https://www-thelancet-com.sire.ub.edu/journals/laninf/article/PIIS1473-3099(18)30597-8/fulltext. There is no AF resistance measured.

            Chow2020 measured AF resistance for all of them.

            They state that these are from similar 133 patients, and there are 294 WGS datasets. It is possible that some of these are from the same patient.
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = df_metadata.SampleName
            df_metadata["source"] = df_metadata.isolation_source
            
            # add the Chow2020 data that tells resistance/susceptibility
            df_Chow2020 = get_Chow2020_df(manually_curated_data)
            df_metadata = df_metadata.merge(df_Chow2020[["SRR", "resistance", "susceptibility", "ID"]], left_on="Run", right_on="SRR", validate="one_to_one", how="left")

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type', "resistance", "susceptibility"]

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]



        elif bioproject=="PRJNA540907":

            """
            Paper in https://www-sciencedirect-com.sire.ub.edu/science/article/pii/S0924857919302675?via%3Dihub. (Yen2019)

            They analyze 7 C. auris isolates with WGS and AF testing. Each of them is from a different patient.
                
            Table 1 has the AF data
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = df_metadata.strain
            df_metadata["source"] = df_metadata.isolation_source

            # add patient data
            df_metadata["patient_ID"] = [bioproject+"_%i"%I for I in range(len(df_metadata))]
            df_metadata["timepoint"] = 1

            # load MICs and reformat 
            df_MICs = pd.read_csv("%s/Yen2019_Table1_AFresistance_Cauris.tab"%manually_curated_data, sep="\t")
            MIC_fields = [k for k in df_MICs.keys() if k.endswith("_MIC")]
            def get_correct_MIC_yen2019(x):
                if pd.isna(x): return x
                else:

                    if ">" in x or "≥" in x: mult=2
                    elif "<" in x or "≤" in x: mult=0.5
                    elif x[0].isdigit(): mult=1
                    else: raise ValueError("not valid %s"%x)

                    return float(x.replace(">", "").replace("≥", "").replace("<", "").replace("≤", "").split("(")[0])*mult


            for f in MIC_fields: df_MICs[f] = df_MICs[f].apply(get_correct_MIC_yen2019)
            df_metadata = df_metadata.merge(df_MICs[["Isolate"] + MIC_fields], left_on="strain", right_on="Isolate", validate="one_to_one", how="left")

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type', "patient_ID", "timepoint"] + MIC_fields

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA541007":

            """
            One C. auris isolate from iran resistant to all classes of antifungals.

            There is info in the CDC about this: https://wwwnc.cdc.gov/eid/article/25/9/19-0686_article

            It refers to an isolate for which drug resistance was measured in "https://onlinelibrary-wiley-com.sire.ub.edu/doi/epdf/10.1111/myc.12886"
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = "IranStrain"
            df_metadata["source"] = df_metadata.isolate

            # add patient data
            df_metadata["patient_ID"] = bioproject+"_patient1"
            df_metadata["timepoint"] = 1

            # add the MIC fields
            df_metadata["FLC_MIC"] = 16
            df_metadata["ITR_MIC"] = 0.063
            df_metadata["VRC_MIC"] = 0.125
            df_metadata["MIF_MIC"] = 0.031
            df_metadata["ANI_MIC"] = 0.016
            df_metadata["POS_MIC"] = 0.016
            df_metadata["IVZ_MIC"] = 0.063
            df_metadata["AMB_MIC"] = 0.5

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type', "patient_ID", "timepoint"] + ["FLC_MIC", "ITR_MIC", "VRC_MIC", "MIF_MIC", "ANI_MIC", "POS_MIC", "IVZ_MIC", "AMB_MIC"]

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]



        elif bioproject=="PRJNA549561":

            """
            In vitro evolution paper of one parental clinical isolate. 

            The paper is in https://aac-asm-org.sire.ub.edu/content/65/1/e01466-20

            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "invitro_evol_clone"
            df_metadata["strain"] = df_metadata.isolate
            df_metadata["source"] = "inVitro_evolution"

            # get the patient ID, timepoint, treatment and MIC
            def get_metadata_from_r(r):

                sampleID = re.split("_|-", r.strain)[1]
                letter = sampleID[0]
                number = int(sampleID[1:])

                # timepoint 1. Several colonies passaged in fluconazole
                if letter=="A":
                    patient_ID = bioproject+"_%i"%number
                    timepoint = 1
                    treatment = "FLC"
                    if number<=8: FLC_MIC = 128
                    elif number>8 and number<=14: FLC_MIC = 64
                    elif number>14: FLC_MIC = 32

                # timepoint 2. colonies A1-A8 passaged in FLC to B1-B8
                elif letter=="B":
                    patient_ID = bioproject+"_%i"%number
                    timepoint = 2
                    treatment = "FLC"
                    FLC_MIC = 128

                # timepoint 3: B1 sample passaged in YPD to R1-to-R3
                elif letter=="R":
                    patient_ID = bioproject+"_1"
                    timepoint = 3
                    treatment = "none"
                    FLC_MIC = 2

                else: raise ValueError("%s is not valid"%letter)

                r["patient_ID"] = patient_ID
                r["timepoint"] = timepoint
                r["treatment"] = treatment
                r["FLC_MIC"] = FLC_MIC

                return r

            df_metadata = df_metadata.apply(get_metadata_from_r, axis=1)

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type', "patient_ID", "timepoint", "treatment", "FLC_MIC"]

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA560710":

            """
            The two first described cases of  C. auris in the Netherlands
            Paper in https://www.researchgate.net/publication/336141623_The_First_Two_Cases_of_Candida_auris_in_The_Netherlands
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = df_metadata.isolate
            df_metadata["source"] = "unknown"

            # no treatment
            df_metadata["treatment"] = "none"

            # add patient data
            df_metadata["patient_ID"] = [bioproject+"_%i"%I for I in range(len(df_metadata))]
            df_metadata["timepoint"] = 1

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type', "patient_ID", "timepoint", "treatment"]

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]


        elif bioproject=="PRJNA595978":

            """
            These are samples from Chow2020. They sequence many C. auris isolates.
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = df_metadata.isolate
            df_metadata["source"] = df_metadata.isolation_source

            df_Chow2020 = get_Chow2020_df(manually_curated_data)
            df_metadata = df_metadata.merge(df_Chow2020[["SRR", "resistance", "susceptibility", "ID"]], left_on="Run", right_on="SRR", validate="one_to_one", how="left")


            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type', "resistance", "susceptibility"]

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]


        elif bioproject=="PRJNA603602":

            """
            Paper in https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7255648/
            They study an outbreak in Australia with AF measured.
            No treatment available.

            There are several isolates of patient 1.
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = df_metadata.Title
            df_metadata["source"] = df_metadata.isolation_source


            # load df mics
            MIC_fields = ["AMB_MIC", "5FC_MIC", "FLC_MIC", "ITR_MIC", "VRC_MIC", "POS_MIC", "ANI_MIC", "MIF_MIC", "CAS_MIC"]
            df_MICs = pd.read_csv("%s/Biswas2020_Table1_Cauris_AFresistance.tab"%manually_curated_data, sep="\t")[["Isolate_ID", "patient_ID", "Date"]+MIC_fields]

            df_metadata = df_metadata.merge(df_MICs, left_on="strain", right_on="Isolate_ID", validate="one_to_one", how="left")

            # add the patient ID and metadata
            def get_timepoint_from_r(r):

                if r.patient_ID==1:
                    if r.Date=="July 2018": return 1
                    elif r.Date=="05.09.2018": return 2
                    elif r.Date=="06.09.2018": return 3
                    elif r.Date=="07.09.2018": return 4
                    elif r.Date=="11.09.2018": return 5
                    else: raise ValueError("%s not considered"%r.Date)

                else: return 1
            
            df_metadata["timepoint"]  = df_metadata.apply(get_timepoint_from_r, axis=1)
            df_metadata["patient_ID"] =  bioproject + "_" + df_metadata.patient_ID.apply(str)

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type', "timepoint", "patient_ID"] + MIC_fields

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA432377":

            """
            Draft hybrid long-read assembly of Candida metapsilosis ATCC 96143

            Paper in https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6499006/. No AF resistance measured.

            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = df_metadata.SampleName
            df_metadata["source"] = "unknown"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA579121":

            """
            I can't find any metadata. It seems taht they are from mycobiome studies
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = df_metadata.SampleName
            df_metadata["source"] = "unknown"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA326748":

            """

            They sequence azole-resistant isolates.

            The genome sequences of Candida parapsilosis BC014RPSC (azole resistant evolved in posaconazole) and BC014 (susceptible isolate) strains were sequenced using Illumina, to identify Single Nucleotide Polymorphisms.

            paper in https://www.clinicalmicrobiologyandinfection.com/article/S1198-743X(17)30090-3/fulltext

            I get the MICs from table 2.

            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = ["clinical", "invitro_evol_clone"]
            df_metadata["strain"] = df_metadata.strain
            df_metadata["source"] = ["unknown", "inVitro_evolution"]

            # same patient and timepoint
            df_metadata["patient_ID"] = bioproject+"_1"
            df_metadata["timepoint"] = [1, 2]

            # treatment
            df_metadata["treatment"] = ["unknown", "POS"]

            # get MICs
            df_metadata["FLC_MIC"] = [1, 128]
            df_metadata["VRC_MIC"] = [0.03, 16]
            df_metadata["POS_MIC"] = [0.03, 32]

            # check
            if df_metadata.iloc[0].strain!="BC014S": raise ValueError("The order changed")

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type', "patient_ID", "timepoint", "treatment", "FLC_MIC", "VRC_MIC", "POS_MIC"]

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA493002":

            """
            They evolve C. parapsilosis strains in 3 different echinocandins and get resistant isolates.

            Paper in https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6236803/
            """

            # get the MICs df and add
            df_MICs = pd.read_csv("%s/Table1_Papp2018_AFresistance_Cparapsilosis.tab"%manually_curated_data, sep="\t")
            MIC_fields = [k for k in df_MICs.keys() if k.endswith("_MIC")]
            df_metadata = df_metadata.merge(df_MICs, on="strain", how="left", validate="one_to_one")

            # add timepoint
            df_metadata["patient_ID"] = bioproject+"_1"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type', "patient_ID", "timepoint", "treatment"] + MIC_fields

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA516045":

            """
            5 isolates from the same individual. Paper in https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7481391/.

            They state that the isolates are divergent. Table 2 has AF testing data.
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = df_metadata.SampleName
            df_metadata["source"] = df_metadata.isolate

            # add the MICs
            df_MICs = pd.read_csv("%s/GomezMolero2020_Table2_phenotypes_Cparapsilosis.tab"%manually_curated_data, sep="\t").set_index("field").transpose()
            MIC_fields = [f for f in df_MICs.keys() if f.endswith("_MIC")]
            df_MICs = df_MICs[["strain"] + MIC_fields]

            df_metadata = df_metadata.merge(df_MICs, on="strain", how="left", validate="one_to_one")

            # add the patient ID
            df_metadata["patient_ID"] = bioproject+"_1"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type', "patient_ID"] + MIC_fields

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA563885":

            """
            Paper in https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7263194/
            They study centromere formation
            There is no AF resistance data
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = df_metadata.SampleName
            df_metadata["source"] = df_metadata.env_medium

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA322245":

            """
            Paper in https://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1006404
            This is a population genomics study of  the origin C. orthopsilosis hybrids.
            There is no metadata
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = df_metadata.strain
            df_metadata["source"] = df_metadata.isolation_source

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA431439":

            """
            This is a hybrid assembly of C. orthopsilosis. No AF measured.
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = df_metadata.strain
            df_metadata["source"] = "unknown"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJEB40738":

            """
            I isolate from a collection of 1-day isolates from Denemark. I can't find AF resistance data.
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = "unknown"
            df_metadata["source"] = df_metadata.isolation_source

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA194439":

            """
            Whole genome sequencing of a clinical isolate of of Candida tropicalis as part of a project to identify and characterize mutations conferring amphotericin resistance in Candida albicans as well as C. tropicalis.

            Paper in https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1001692#s1

            The strain is called ATCC:200956, OY5, MY1012

            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["source"] = "unknown"

            df_metadata["resistance"] = "AMB"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type', "resistance"]

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA432250":

            """
            One sample. Clinical or host-associated sample from Candida tropicalis MYA-3404
            Draft hybrid long-read assembly of Candida tropicalis MYA-3404
            I can't find the paper or AF resistance data.
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = df_metadata.strain
            df_metadata["source"] = "unknown"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA555042":

            """
            Paper in https://bmcbiol.biomedcentral.com/articles/10.1186/s12915-020-00776-6

            This is Veronica's paper on the hybrid origin of C. albicans.
            There is no evidence fpr
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = df_metadata.SampleName
            df_metadata["source"] = "unknown"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA604451":

            """
            Preprint at https://www.biorxiv.org/content/10.1101/2020.11.25.397612v1.full

            They sequence and charcaterize phenotipically 77 clinical and environmental isolates of C. tropicalis.

            There is a table 2 that has the phenotype scores
            """ 

            def get_phenotypicScores_OBrien2021_Ctropicalis_MICs():

                df_pheno = pd.read_excel("%s/phenotypicScores_OBrien2021_Ctropicalis.xlsx"%manually_curated_data)
                drug_to_MICfield = {"Caspofungin":"CAS_MIC", "Fluconazole":"FLC_MIC", "Ketoconazole":"KET_MIC"}

                # get the MIC for each drug
                for drug, MICf in drug_to_MICfield.items():

                    # map each field to the drug concentration
                    field_to_drugConc = {f : float(f.split("%s ("%drug)[1].split()[0]) for f in df_pheno.keys() if f.startswith(drug)}
                    
                    # get as floats
                    for f in field_to_drugConc: 
                        df_pheno[f] = df_pheno[f].apply(float)
                        if any(df_pheno[f]<0) or any(df_pheno[f]>2): raise ValueError("phenotype score not measured")

                    # get the MIC
                    fitness_fields = list(field_to_drugConc.keys())
                    max_drug_conc = max(field_to_drugConc.values())*2
                    df_pheno[MICf] = df_pheno[fitness_fields].apply(lambda r: min([field_to_drugConc[fit_field] for fit_field in r[r<=0.5].keys()] + [max_drug_conc]), axis=1)

                MICfields = sorted(drug_to_MICfield.values())
                return df_pheno[["Strain"] + MICfields], MICfields

            # load MICs df
            df_MICs, MICfields = get_phenotypicScores_OBrien2021_Ctropicalis_MICs()
            df_metadata = df_metadata.merge(df_MICs, left_on="Title", right_on="Strain", how="left", validate="one_to_one")

            # define the type
            host_to_type = {'Coconut':"environmental", 'soil':"environmental", 'clinical':"clinical", 'Homo sapiens':"clinical"}
            df_metadata["type"] = df_metadata.host.apply(lambda x: host_to_type[x])

            # add trivial things, like the source, the type or the strain
            df_metadata["strain"] = df_metadata.Title
            df_metadata["source"] = df_metadata.host

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type'] + MICfields

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]


        elif bioproject=="PRJNA610214":

            """

            Antifungal drug resistance is a global problem. Here, we use whole genome sequence analysis to investigate pairs of isolates from single infection episodes (by definition isolated less than 6 months apart) from individual patients where the initial isolate is susceptible and a subsequent isolate is resistant. The goal was to use whole genome sequence analysis to investigate whether the 2 isolates represented the same strain and to perform comparative genome analysis to elucidate mechanisms of resistance. Less...
        
            Paper in https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7449159/
            No treatment available

            AF resistance in table 4.

            There are 4 isolates

            """
            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = df_metadata.isolate
            df_metadata["source"] = df_metadata.isolation_source

            # load the metadata
            df_MICs = pd.read_csv("%s/McTaggart2020_AFresistance_Table4.tab"%manually_curated_data, sep="\t")
            df_metadata = df_metadata.merge(df_MICs, on="strain", how="left", validate="one_to_one")
            MIC_fields = [f for f in df_metadata.keys() if f.endswith("_MIC")]

            # load the patient ID
            df_metadata["patient_ID"] = bioproject+"_"+df_metadata.patient_ID

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type', "patient_ID", "timepoint"] + MIC_fields

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA677456":

            """
            Genome sequencing of yeast species that harbor traits desirable for biofuel production. No AF susceptibility measured.
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = df_metadata.strain
            df_metadata["source"] = "unknown"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA677457":

            """
            Genome sequencing of yeast species that harbor traits desirable for biofuel production. Same as PRJNA677456
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = df_metadata.strain
            df_metadata["source"] = "unknown"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]


        elif bioproject=="PRJNA677458":

            """
            Genome sequencing of yeast species that harbor traits desirable for biofuel production
            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = df_metadata.strain
            df_metadata["source"] = "unknown"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        elif bioproject=="PRJNA677459":

            """
            Genome sequencing of yeast species that harbor traits desirable for biofuel production

            """

            # add trivial things, like the source, the type or the strain
            df_metadata["type"] = "clinical"
            df_metadata["strain"] = df_metadata.strain
            df_metadata["source"] = "unknown"

            # define the interesting_fields that will be kept (ex: ['Run', 'BioSample', 'strain', 'source', 'type'])
            interesting_fields = ['Run', 'BioSample', 'strain', 'source', 'type']

            # get only the interesting fields
            df_metadata = df_metadata[interesting_fields]

        # unknown bioprojects, just set everything to NaN except sampleName, BioSample and bioproject
        elif bioproject in {}: pass

        else: raise ValueError("BioProject %s has not been considered"%bioproject)

        # check that the df_metadata is unique
        if len(df_metadata)!=len(df): raise ValueError("Something went wrong with the metadata")

        # check that the added fields are only metadata cols
        strange_fields = set(df_metadata.keys()).difference(set(metadata_cols))
        if len(strange_fields)>0: raise ValueError("There are strange fields in df_metadata for %s: %s"%(bioproject, strange_fields))

        # get the metadata_cols
        df_metadata = df_metadata[[c for c in metadata_cols if c in df_metadata.keys()]]

        # add the remaining cols
        for col in metadata_cols:
            if col not in df_metadata.keys(): df_metadata[col] = np.nan

        # change the AMB_MIC90 to AMB_MIC. All the AMB_MICs are  MIC90
        if any(~pd.isna(df_metadata.AMB_MIC90)) and all(pd.isna(df_metadata.AMB_MIC)): df_metadata["AMB_MIC"] = df_metadata.AMB_MIC90
        df_metadata["AMB_MIC90"] = df_metadata.AMB_MIC

        # add the bioproject
        df_metadata["BioProject"] = bioproject

        # add further metadata, uncurated
        uncurated_metadata_cols = []
        for k in df_metadata_raw.keys():
            if k in set(metadata_cols): continue

            # define filed and keep
            uncurated_col = "uncurated_%s"%k; uncurated_metadata_cols.append(uncurated_col)
            run_to_value = dict(df_metadata_raw.set_index("Run")[k])
            df_metadata[uncurated_col] = df_metadata.Run.apply(lambda x: run_to_value[x])

        # check that the parent strains are in strains
        def get_parentStrainsSet(ps):
            if pd.isna(ps): return set()
            else: return set(ps.split(";"))
        all_parent_strains = set.union(*df_metadata.parent_strains.apply(get_parentStrainsSet))
        unmatched_parent_strains = all_parent_strains.difference(set(df_metadata.strain))

        if len(unmatched_parent_strains)>0: raise ValueError("there are unmatched parent strains: %s"%unmatched_parent_strains)

        # keep
        df_metadata_all = df_metadata_all.append(df_metadata[metadata_cols + uncurated_metadata_cols + ["BioProject"]])

        print("already got %i/%i runs"%(len(df_metadata_all), len(sra_df)))

    # define the breakpoints dict, based on EUCAST (https://www.eucast.org/fileadmin/src/media/PDFs/EUCAST_files/AFST/Clinical_breakpoints/AFST_BP_v10.0_200204.pdf)
    #breakpoints_dict

    # add extra info
    #df_metadata_all.apply(lambda r: get_integrated_metadata_fields(r, taxID, breakpoints_dict), axis=1)


    print("metadata df was obtained!!")
    
    return df_metadata_all


def get_interesting_name_FeatsDF(row):
    
    """Takes a features dataframe row that has gene_name and Scerevisiae_orthologs and returns the interesting name"""

    if not pd.isna(row["gene_name"]): return row["gene_name"]
    elif not pd.isna(row["Scerevisiae_orthologs"]): return "Scer_%s"%(row["Scerevisiae_orthologs"])
    else: return row["gff_upmost_parent"]


def calculate_GCcontent(GenomeFile, chrom_start_end_df):

    """Takes a genome file (fasta) and a df that has the gene as index, chromosome, start and end (index1-based). It returns a series where the index is the gene and the values are the GC content, as the fraction of bases that are C or G."""

    # map each chromosome to a fasta file
    chromID_to_seq = {rec.id : rec for rec in Bio.SeqIO.parse(GenomeFile, "fasta")}

    def getGCcontent(chromosome, start, end):

        # debug the fact that start and end
        if end<start: real_start = end; real_end = start
        else: real_start = start; real_end = end

        # get sequence
        seq = str(chromID_to_seq[chromosome][real_start-1 : real_end].seq).upper()

        return (seq.count("G") + seq.count("C"))/len(seq)


    # get the GC content
    return chrom_start_end_df.apply(lambda r: getGCcontent(r["chromosome"], r["start"], r["end"]), axis=1)

def convert_NaN_to_empty_strings(serie):
    
    """ Takes a pandas series containing NaN and strings, and converts one that has the NaN into empty "" """
    
    serie[serie.isna()] = [""]*len(serie[serie.isna()])

def get_geneFeatures_df(TABANNOdir, geneFeatures_df_filepath, GenomeFile, replace=False):

    """Takes a tab-file with gene annotations and writes it under geneFeatures_df_filepath"""

    if file_is_empty(geneFeatures_df_filepath) or replace is True:
        print("getting gene features df")

        geneFeatures_df = pd.read_csv(TABANNOdir, sep="\t", names=["geneID", "gene_name", "aliases", "feature_type", "chromosome", "start", "end", "strand", "primary_CGDID", "secondary_CGDID", "description", "date", "sequence_coordinate_version_date", "blank1", "blank2", "date_geneName_reservation", "is_it_standardName", "Scerevisiae_orthologs"], skiprows=list(range(len([line for line in open(TABANNOdir, "r") if line.startswith("!")]))))                           
        geneFeatures_df = geneFeatures_df.set_index("geneID", drop=False)
        geneFeatures_df["final_name"] = geneFeatures_df.apply(get_interesting_name_FeatsDF, axis=1)
        geneFeatures_df["small_chr"] = geneFeatures_df.chromosome.apply(lambda x: x.split("_")[0])
        geneFeatures_df["start_str"] = geneFeatures_df.start.apply(str)

        # load the genomes and add GC content
        geneFeatures_df["GCcontent"] = calculate_GCcontent(GenomeFile, geneFeatures_df)

        # change the NAN values 
        for field in {"description"}: convert_NaN_to_empty_strings(geneFeatures_df[field])

        # save
        geneFeatures_df.to_csv(geneFeatures_df_filepath, sep="\t", header=True, index=False)

    else: geneFeatures_df = pd.read_csv(geneFeatures_df_filepath, sep="\t").set_index("geneID", drop=False)

    # add ID
    geneFeatures_df["ID"] = geneFeatures_df.geneID

    return geneFeatures_df





def merge_perSampleVARScalled_and_varINFO_into_VARSdf(perSampleVARSdf, varINFOdf, left_on="#Uploaded_variation", right_on="#Uploaded_variation"):

    """Takes the two small dataframes perSampleVARSdf and adds the columns of varINFO accordingly."""

    # any things that overlap, remove them from varINFO_df
    overlappingColumns = set(perSampleVARSdf.columns).intersection(set(varINFOdf.columns))
    varINFOdf = varINFOdf[[x for x in varINFOdf.columns if x not in overlappingColumns or x=="var"]]

    #print("Merging perSampleVARSdf and varINFOdf")
    merged_df = perSampleVARSdf.merge(varINFOdf, how="left", on=None, left_on=left_on, right_on=right_on, left_index=False, right_index=True, suffixes=("", "_varInfo"))

    # check that there is no NaNs
    NaNs_sum = set(pd.isna(merged_df).apply(lambda r: sum(r), axis=1))
    if NaNs_sum!={0}: 

        #raise ValueError("There are NaNs in the merged DF")
        #print("Warning: there are NaNs in the merged DF. These are the affected columns")
        for col in merged_df.columns:

            if sum(pd.isna(merged_df[col]))>0: pass #print(col)



    return merged_df


def get_type_mut(srr, vars_df, genes):

    """returns the type of mutations for a gene"""

    # get the df
    fields = ["gene_name", "strongest_consequence"]
    if srr in set(vars_df.index):

        df = vars_df.loc[{srr}][fields]
        df = df[df.gene_name.isin(genes)].sort_values(by=fields).drop_duplicates()

        # convert to string
        type_mut = ", ".join(df.gene_name + "-" + df.strongest_consequence)

    else: type_mut = "no_mut"

    # get the closest 
    return type_mut


def get_strongest_consequence(c):

    """Gets the strongest consequence"""

    # map each consequence to the ID
    cons_to_number = dict(zip(sorted_consequences, range(len(sorted_consequences))))

    # get the consequences set
    cons_set = set(c.split(","))

    # sort 
    sorted_c = sorted(cons_set, key=(lambda c: cons_to_number[c]))

    return consequence_to_abbreviation[sorted_c[-1]]



def save_df_as_tab(df, file):

    """Takes a df and saves it as tab"""

    file_tmp = "%s.tmp"%file
    df.to_csv(file_tmp, sep="\t", index=False, header=True)
    os.rename(file_tmp, file)


def get_tab_as_df_or_empty_df(file):

    """Gets df from file or empty df"""

    nlines = len([l for l in open(file, "r").readlines() if len(l)>1])

    if nlines==0: return pd.DataFrame()
    else: return pd.read_csv(file, sep="\t")

def plot_MIC_for_differentMutations(metadata_df, vars_df, var_annot_df, geneFeatures_df, filename, drugs=["MIF", "CAS", "ANI"], genes={"FKS1", "FKS2"}, only_prot_altering=False, figsize=(5, 3), replace=False):

    """Takes some variant calling data and plots the MIC distribution into filename """


    df_plot_file = "%s.df_plot.py"%filename
    if file_is_empty(df_plot_file) or replace is True:

        # get the fields
        micFields = ["%s_MIC"%d for d in drugs]

        # get the metadata for the important SRRs
        interesting_srrs = set(vars_df.index)

        # calculate the number of samples considered
        nonEmpty_metadata = metadata_df[(metadata_df.apply(lambda r: any(~pd.isna(r[micFields])), axis=1)) & (metadata_df.Run.isin(interesting_srrs))]
        print("There are %i/%i SRRs with metadata"%(len(nonEmpty_metadata), len(metadata_df)))

        # get the important vars
        interesting_genes = set(geneFeatures_df.loc[genes, "geneID"])
        interesting_vars = set(var_annot_df[var_annot_df.Gene.isin(interesting_genes)]["#Uploaded_variation"])

        vars_df = vars_df[(vars_df.ID.isin(interesting_vars)) & (vars_df.SRR.isin(set(nonEmpty_metadata.Run)))]

        # merge
        vars_df = merge_perSampleVARScalled_and_varINFO_into_VARSdf(vars_df, var_annot_df, left_on="ID")
        vars_df = vars_df[vars_df.Gene.isin(interesting_genes)].set_index("SRR", drop=False)

        # get only protein altering 
        if only_prot_altering is True: vars_df = vars_df[vars_df.is_protein_altering]
        
        # add fields
        print("getting strongest consequence")
        vars_df["gene_name"] = vars_df.Gene.apply(lambda g: geneFeatures_df[geneFeatures_df.geneID==g].final_name.iloc[0])
        vars_df["strongest_consequence"] = vars_df.Consequence.apply(get_strongest_consequence)

        # plot
        print("ploting")
        df_plot = pd.DataFrame()
        for field in micFields:
            
            # get the df with the data
            df = metadata_df[~(pd.isna(metadata_df[field]))]
            print("There are %i samples for %s"%(len(df), field))
            
            # add the type of fks 
            df["type_mut"] = df.Run.apply(lambda x: get_type_mut(x, vars_df, genes))

            print("in %s, these are the types of mutations:"%field, Counter(df.type_mut))
                    
            # keep
            df["MIC"] = df[field]
            df["drug"] = field
            df_plot = df_plot.append(df[["drug", "type_mut", "MIC", "Run"]])

        print("saving")
        save_object(df_plot, df_plot_file)

    else: df_plot = load_object(df_plot_file)
            
    print("These are the types of mutations:")
    for t, n in Counter(df_plot.drop_duplicates(subset=["type_mut", "Run"]).type_mut).items(): print(t, ":", n)


    # get the plot
    fig = plt.figure(figsize=figsize)
    box = sns.boxplot(x="drug", y="MIC", data=df_plot, hue="type_mut", notch=True, boxprops=dict(alpha=.7), linewidth=0.5, palette="Set2")
    #ax = sns.stripplot(x="drug", y="MIC", hue="type_mut", data=df_plot, dodge=True, palette="Set2")
    ax = sns.swarmplot(x="drug", y="MIC", hue="type_mut", data=df_plot, dodge=True, palette="Set2")

    # add the breakpoints
    for ticklabel in box.get_xmajorticklabels():

        text = ticklabel.get_text()
        xpos = ticklabel.get_position()[0]

        drug = text.split("_")[0]

        if drug in cglab_breakponts_dict:

            # define the breakpoint
            ybreakpoint = cglab_breakponts_dict[drug]

            plt.plot([xpos-0.5, xpos+0.5], [ybreakpoint, ybreakpoint], linewidth=.9, linestyle="--", color="gray")

    ax.set_yscale("log")

    ax.legend(bbox_to_anchor=(1.01, 1), loc=2, borderaxespad=0.)
    fig.savefig(filename, bbox_inches='tight')

    return df_plot
        
def get_speciesTree_multipleGenomes_JolyTree(input_dir_withGenomes, outdir, threads=4, replace=False, correct_genomes=True):

    """This function generates a species tree under outdir with all the genomes (files ending with .fasta) in input_dir_withGenomes. It returns the newick file with the tree.

    This can be installed with 'conda install -c bioconda jolytree'"""

    # make the outdir
    make_folder(outdir)

    # define the outprefix and the expected species tree file
    outprefix = "%s/outputJolyTree"%outdir
    species_treefile = "%s.nwk"%outprefix

    if file_is_empty(species_treefile) or replace is True:

        if correct_genomes is True:

            # move all the fasta files in input_dir_withGenomes into input_dir
            input_dir = "%s/input_genomes"%outdir; make_folder(input_dir)
            for file in os.listdir(input_dir_withGenomes):
                origin_file = "%s/%s"%(input_dir_withGenomes, file)

                # if it is a fasta file, softlink to input_dir
                if file.split(".")[-1] in {"fasta", "fa"} and os.path.isfile(origin_file):
                    dest_file = "%s/%s"%(input_dir, file)
                    if file_is_empty(dest_file): run_cmd("ln -s %s %s"%(origin_file, dest_file))

        else: input_dir = input_dir_withGenomes

        # run JolyTree
        print("running JolyTree to get species tree")
        run_cmd_Candida_mine_env("JolyTree.sh -i %s -b %s -t %i"%(input_dir, outprefix, threads))

    return species_treefile


def get_string_representation_of_var(row):
    
    """Takes a row of a vars df such as the one provided in get_genesToSamplesToPrivateMutations_df. It returns a string with the meaning of the var."""
    
    # define the most important consequence according to how you ranked the consequences in var_to_IDX
    cons = sorted(row["consequences_set"], key=lambda y: var_to_IDX[y])[0]
    
    # get the meaning
    return "%s|%s.%s|%s"%(consequence_to_abbreviation[cons], protVar_to_info[cons][0], row[protVar_to_info[cons][1]], row[protVar_to_info[cons][2]]) 
     

def get_simple_var_representation(var):

    """Takes the get_string_representation_of_var and returns the simple version. For example, for missense mutations H240F"""

    # split
    type_mut, pos, change = var.split("|")

    ref, alt = change.split("/")
    aa_pos = pos.split(".")[1].split("-")[0]
    return "%s%s%s"%(ref, aa_pos, alt)


def get_sample_to_gene_to_mutation_set_df(vars_df, samples, sample_field="SRR"):

    """Takes a (united) vars_df and returns the mapping between and mutations. This is a df with the sample in the index and the gene in the column. THe values are a set with the formatted mutations."""

    # add the variant
    vars_df["consequences_set"] = vars_df.Consequence.apply(lambda x: set(x.split(",")))
    vars_df["var_as_str"] = vars_df.apply(get_string_representation_of_var, axis=1)
    #vars_df["simple_var_as_str"] = vars_df.var_as_str.apply(get_simple_var_representation)
    vars_df = vars_df[["final_name", sample_field, "var_as_str"]]

    if len(vars_df)==0: return pd.DataFrame(index=samples)

    def get_sample_to_mutations_series(df_g):

        # takes a  df where all mutations are for one gene and returns a dict that maps each sample (in samples) to the mutations

        # rename
        df_g["sample_name"] = df_g[sample_field] 

        # get the current mutations
        sample_to_mutations_series = df_g.groupby("sample_name").apply(lambda df_s: set(df_s.var_as_str))

        # add for empty samples
        empty_samples = set(samples).difference(set(sample_to_mutations_series.index))
        empty_samples_series = pd.Series([set()]*len(empty_samples), index=empty_samples)
        sample_to_mutations_series = sample_to_mutations_series.append(empty_samples_series)

        return  sample_to_mutations_series
    
    sample_and_gene_mutations_series = vars_df.groupby("final_name").apply(lambda df_g: get_sample_to_mutations_series(df_g))

    # if there is a structure to be changed
    first_index = sample_and_gene_mutations_series.index[0]
    if type(first_index)==tuple:

        df = pd.DataFrame()
        df["mutations"] = sample_and_gene_mutations_series
        df["gene"] = [x[0] for x in df.index]
        df["sample_name"] = [x[1] for x in df.index]

        square_df = df.pivot(index='sample_name', columns='gene', values='mutations').loc[samples]

    elif type(first_index)==str and type(sample_and_gene_mutations_series)==pd.core.frame.DataFrame:

        square_df = sample_and_gene_mutations_series.transpose().loc[samples]


    return square_df

def get_log10_or_NaN(x):

    """Returns the log2 of a float"""

    if pd.isna(x): return np.nan
    elif x==0.0: raise ValueError("x can't be 0")
    else: return np.log10(x)


def find_nearest(a, a0):
    """Element in nd array `a` closest to the scalar value `a0`"""
    
    # Debug elements that are inf
    if a0 not in [np.inf, -np.inf]:
        a = np.array(a)
        idx = np.abs(a - a0).argmin()
        closest_in_a = a.flat[idx]
        
    elif a0==np.inf:
        closest_in_a = max(a)
        
    elif a0==-np.inf:
        closest_in_a = min(a)        

    return closest_in_a

def get_value_to_color(values, palette="mako", n=100, type_color="rgb", center=None):

    """TAkes an array and returns the color that each array has. Checj http://seaborn.pydata.org/tutorial/color_palettes.html"""

    # get the colors
    colors = sns.color_palette(palette, n)

    # change the colors
    if type_color=="rgb": colors = colors
    elif type_color=="hex": colors = [rgb_to_hex(c) for c in colors]
    else: raise ValueError("%s is not valid"%palette)

    # if they are strings
    if type(list(values)[0])==str:

        palette_dict = dict(zip(values, colors))
        value_to_color = palette_dict

    # if they are numbers
    else:

        # map eaqually distant numbers to colors
        if center==None:
            min_palette = min(values)
            max_palette = max(values)
        else: 
            max_deviation = max([abs(fn(values)-center) for fn in [min, max]])
            min_palette = center - max_deviation
            max_palette = center + max_deviation

        all_values_palette = list(np.linspace(min_palette, max_palette, n))
        palette_dict = dict(zip(all_values_palette, colors))

        # get value to color
        value_to_color = {v : palette_dict[find_nearest(all_values_palette, v)] for v in values}

    return value_to_color, palette_dict

def get_val_from_dict_withNaN(key, dict_obj, nan_value="white"):

    if key in dict_obj: return dict_obj[key]
    elif pd.isna(key): return nan_value
    else: raise ValueError("%s is not valid"%key) 


def rgb_to_hex(rgb):

    # Helper function to convert colour as RGB tuple to hex string
    rgb = tuple([int(255*val) for val in rgb])
    hex_val = '#%02x%02x%02x'%(rgb[0], rgb[1], rgb[2])

    if len(hex_val)!=7: raise ValueError("%s is not valid"%hex_val)

    return hex_val

def rgb_to_hex_old(rgb):

    # Helper function to convert colour as RGB tuple to hex string
    rgb = tuple([int(255*val) for val in rgb])
    hex_val = '#' + ''.join([hex(val)[2:] for val in rgb]).upper()
    if len(hex_val)!=7:
        print(rgb, hex_val)
        khagdahgadj
        
    return 



def get_correct_color(x):

    """Takes a color and returns it as string or hex"""

    # if it is RGB, change
    if type(x)==tuple: return rgb_to_hex(x)

    elif type(x)==str: return x
    
    else: raise ValueError("%s is not valid"%x)

def get_formatted_floatStr_or_str(x, decs=1):

    """Takes an object, converts it to float with ndecs if possible"""

    try: 

        if decs==1: return "%.1f"%x
        elif decs==2: return "%.2f"%x
        elif decs==3: return "%.3f"%x
        elif decs==4: return "%.4f"%x

    except: return x

def get_heatmap_tree_provided_colors(tree, df_colors, df_annotations, filename=None, leaf_attributes=["name"], orientation="r", df_annotation_colors=None, df_annotation_sizes=None, color_map_list=None, df_outline_colors=None):

    """Takes a tree and plots a heatmap arround it, determined by df_colors and df_annotations. It is saved into filename.

    df_colors and df_annotations should have the leaf names of the tree. Required attributes for the tree are:
        - bgcolor

    df_annotation_colors has the fgcolor of the annotations.

    color_map_list can be a list of tuples ("name_colormap", dictionary mapping values_to_numbers)

    df_outline_colors can be a df where all the outlines are defined """

    # keep the tree
    tree = cp.deepcopy(tree)

    # map each column to an index of the heatmap
    columns = df_colors.columns
    col_to_I = {col : i+1 for i, col in enumerate(columns)}

    # determine the leaf bgcolor based on monophyletic leafs
    for n in tree.traverse():

        if n.is_leaf(): continue

        # define the leafs bgcolor
        leafs_bgcolor = {l.bgcolor for l in n.get_leaves()}

        # if they are monophyletic for one and white, set it 
        if len(leafs_bgcolor)==2 and "white" in leafs_bgcolor: 

            bgcolor = [c for c in leafs_bgcolor if c!="white"][0]
            for l in n.get_leaves(): l.bgcolor =  bgcolor

    # iterate through the nodes setting branch colors if they are all of one color or white
    for n in tree.traverse():

        # init node style by setting the line thicks
        nst = NodeStyle()
        nst["hz_line_width"] = 8
        nst["vt_line_width"] = 8
        nst["size"] = 0

        # if tree is leaf, get bgcolor
        if n.is_leaf(): leafs = n
        else: leafs = n.get_leaves()

        # define the leafs bgcolor
        leafs_bgcolor = {l.bgcolor for l in leafs}

        # define the branch color
        if n.is_leaf(): branch_color = n.bgcolor
        elif leafs_bgcolor=={"white"}: branch_color = "black"
        elif len(leafs_bgcolor)==1: branch_color = next(iter(leafs_bgcolor))
        else: branch_color = "black"

        # set the branch color
        nst["hz_line_color"] = branch_color
        nst["vt_line_color"] = branch_color

        #if n.is_leaf(): nst["bgcolor"] = n.bgcolor

        # set the style
        n.set_style(nst)


    # iterate through the leaves
    for l in tree.get_leaves():

        # change in the case of filename rendering
        size_leaf_face = 12
        if filename is not None: size_leaf_face = size_leaf_face/7

        # get the face
        if leaf_attributes is None: textLeaf = ""
        else: textLeaf = "   ".join([str(l.__dict__[feat]) for feat in leaf_attributes])
        l.add_face(TextFace(textLeaf, fsize=12, bold=False, fgcolor="black"), column=0)

        # add the rectangles
        for col, I in col_to_I.items():

            # These are the real values
            color = get_correct_color(df_colors.loc[l.name, col])
            annotation = df_annotations.loc[l.name, col]

            # define the annotation color
            if df_annotation_colors is not None: annot_color = df_annotation_colors.loc[l.name, col]
            else: annot_color = "red"

            # define the annotation size
            if df_annotation_sizes is not None: annot_size = df_annotation_sizes.loc[l.name, col]
            else: annot_size = 8

            # define the outline color
            if df_outline_colors is not None: outline_color = df_outline_colors.loc[l.name, col]
            else:

                if col.startswith("blank"): outline_color = "white" # or color=="white":
                elif color=="white": outline_color = "gray"
                else: outline_color = "gray"

            # define the opacity
            if color=="white": opacity = 0
            else: opacity = 1
            
            # define the label
            label = {"text":annotation, "font":"Verdana", "color":annot_color,"fontsize":annot_size, "bold":False} 

            if orientation=="r": rect_size = 20
            else: rect_size = 55

            rectFace = RectFace(rect_size, rect_size, fgcolor=outline_color, bgcolor=color, label=label)
            rectFace.opacity = opacity
            l.add_face(rectFace, position="aligned", column=I)

    # define the style
    ts = TreeStyle()
    ts.show_branch_length = False
    ts.show_branch_support = False
    ts.show_leaf_name = False
    ts.draw_guiding_lines = True
    ts.guiding_lines_type = 2 # 0=solid, 1=dashed, 2=dotted.
    ts.guiding_lines_color = "black" 

    # add the footer
    for col, I in col_to_I.items():
        if col.startswith("blank"): continue

        # add the footer
        nameF = TextFace("  %s  "%col, fsize=10, bold=True); nameF.rotation = -90
        nameF.vt_align = 0
        ts.aligned_foot.add_face(nameF, column=I) 

        # add the header
        nameF = TextFace("  %s  "%col, fsize=10, bold=True); nameF.rotation = 90
        nameF.vt_align = 0
        ts.aligned_header.add_face(nameF, column=I) 


    # add the legend if color_map_list is provied
    if color_map_list is not None:

        # get the longest colorbar length
        longest_cbar_len = max([len(val_to_color) for groupID, val_to_color in color_map_list])

        # go through each colorbar item
        for Icbar, (groupID, val_to_color) in enumerate(color_map_list):

            # plot the label 
            ts.legend.add_face(TextFace("\n %s  "%groupID, bold=True, fsize=20), column=0)
            ts.legend.add_face(TextFace(" ", bold=True, fsize=20), column=0)

            # go through each box of the colorbar
            for i, (val, color) in enumerate(val_to_color.items()):

                # add the rectangle with the color
                ts.legend.add_face(RectFace(35, 35, fgcolor="black", bgcolor=get_correct_color(color)), column=i+1)

                # add the annotation
                nameF = TextFace("%s "%val, bold=True, fsize=12); nameF.rotation = -90
                ts.legend.add_face(nameF, column=i+1)

            # add bars to get to longest_cbar_len
            extra_len = longest_cbar_len-len(val_to_color)
            if extra_len>0:
                for iextra in range(i+1, i+1+extra_len):

                    # add the rectangle with the color
                    ts.legend.add_face(RectFace(35, 35, fgcolor="white", bgcolor="white"), column=iextra+1)
                    nameF = TextFace(" ", bold=True, fsize=12); nameF.rotation = -90
                    ts.legend.add_face(nameF, column=iextra+1)


        ts.legend_position=3


    # set the orientation
    if orientation=="r":
        ts.mode = orientation
        ts.root_opening_factor = 1

    elif orientation=="c":
        ts.mode = orientation
        ts.root_opening_factor = 10 # the higher the morecompact
        ts.optimal_scale_level = "full"
        ts.arc_start = 0 # 0 degrees = 3 o'clock
        ts.arc_span = 340
        ts.force_topology = False

        

    elif orientation=="half-c":
        ts.mode = "c"
        ts.root_opening_factor = 1
        ts.arc_start = -180 # 0 degrees = 3 o'clock
        ts.arc_span = 180
    
    # show
    if filename is None: print("Showing..."); tree.show(tree_style=ts)
    # write
    else: 
        print("rendering %s"%filename)
        tree.render(file_name=filename, tree_style=ts)


def get_annotationColor_on_bgcolor(bgcolor, threshold_gray=0.4):

    """Takes a bgcolor and retrives the optimum color to write an annotation on. """

    # change to RGB tuple
    if type(bgcolor)==str and not bgcolor.startswith("#"): bgcolor = colors.to_rgb(bgcolor)

    # debug
    elif type(bgcolor)==str and bgcolor.startswith("#"): bgcolor = tuple([x/255 for x in ImageColor.getcolor(bgcolor, "RGB")])

    # get the gray (a number between 0(black) and 1(white))
    gray = np.mean(bgcolor, -1)


    if gray>1: raise ValueError("gray can't be above 1. bgcolor %s is strange"%bgcolor)

    if gray<threshold_gray: return "white"
    else: return "black"

def get_mutation_annotation(mut, gene, previous_mutations):

    """This function takes a mutation and a gene and returns the annotation"""

    # define the position of the mutation
    mut_pos = int(mut.split("|")[1].split(".")[1].split("-")[0])

    # define the position of previous mutations
    previous_mutations_positions = {int(m.split("|")[1].split(".")[1].split("-")[0]) for m in previous_mutations}

    # define the hotspot positions
    if gene in cglab_gene_to_hotspotPositions: hotspot_positions = cglab_gene_to_hotspotPositions[gene]
    else: hotspot_positions = set()

    # define the type of mutation
    type_mut = mut.split("|")[0]
    if type_mut in {"PTC", "FS"}: return "*"
    elif not type_mut in {"mis", "del", "ins"}: raise ValueError("%s has  not been considered"%mut)

    # if it is not a diruptive mutation, look for other interesting patterns
    if mut in previous_mutations: return "="

    elif mut_pos in hotspot_positions: return "H"

    elif mut_pos in previous_mutations_positions: return "~"

    else: return ""  

def get_outlineColor_on_bgcolor(bgcolor, col, colcolor):

    """Takes a bgcolor and a column name and returns the outline color"""

    # if they are mutations and they are empty
    if "|" in col and bgcolor=="white": return colcolor

    # if it starts with blank
    elif col.startswith("blank"): return "white"

    # if it is white
    else: return "gray"

def get_timepoint_color(r, patient_palette):

    """Takes a row of the metadata index and returns the timepoint color, related to the patient_palette"""

    if pd.isna(r["timepoint"]): return "white"
    else: return patient_palette[r["patient_ID"]]

def get_only_repeated_patientIDs(pID, patient_ID_to_nSamples, min_samples=2):

    """Takes a patient ID and returns it if it is in >=min_samples"""

    if not pd.isna(pID) and patient_ID_to_nSamples[pID]>=min_samples: return pID
    else: return np.nan


def get_annotation_resitance(r, drug, threshold_largeR=3):

    """Takes a row of metadata. It returs the annotation. If threshold_largeR marks where the resistance is in the range of what we observe in our data."""

    is_resistant = r["%s_resistant"%drug]
    fc_mic = r["%s_MIC_FCtoBp"%drug]

    if pd.isna(is_resistant): return ""
    elif is_resistant is False: return "s"
    elif is_resistant is True and fc_mic<threshold_largeR: return "r"
    elif is_resistant is True and fc_mic>=threshold_largeR: return "R"

def get_MICdata_coverage_and_mutations_heatmap_forTree(metadata_df, vars_df, coverage_df, treefile, fileprefix, drugs=["ANI", "MIF", "CAS"], genes=["FKS1", "FKS2", "ERG3"], only_samples_withDrug=False, bg_color_field=None, only_samples_closeToResistant=False, previous_gene_to_mutations={}, only_samples_patientID=False, only_MICs=False, orientation="r"):

    """This function takes a vars df, metadata and coverage df and plots a tree that has all the data.
    - bg_color_field defines a field in metadata_df that will determine the background color in the field"""

    # copy dfs
    metadata_df = cp.deepcopy(metadata_df)

    ######### CREATE DFS ######## 

    # create a df_colors and df_annotations. Both will have index as sample name and column as the field name (it can be a MIC, a mutation or the coverage). The colors will be in hex

    # keep only clinical isolates and add the MIC data
    metadata_df = metadata_df[(metadata_df.type=="clinical")].set_index("Run", drop=False)
    mic_fileds = ["%s_MIC_FCtoBp"%d for d in drugs]
    metadata_df["has_MIC_data"] = metadata_df.apply(lambda r: any(~pd.isna(r[mic_fileds])), axis=1)
    print("There are %i/%i SRRs with metadata for %s"%(sum(metadata_df.has_MIC_data), len(metadata_df), ",".join(drugs)))

    # change the definition of patient_ID. Only keep patient IDs where you have multiple times
    all_patient_IDs = set(metadata_df[~pd.isna(metadata_df.patient_ID)].patient_ID)
    patient_ID_to_nSamples = {p : len(metadata_df[metadata_df.patient_ID==p]) for p in all_patient_IDs}
    metadata_df["patient_ID"] = metadata_df.patient_ID.apply(lambda pID: get_only_repeated_patientIDs(pID, patient_ID_to_nSamples, min_samples=2))

    # define the interesting samples
    if only_samples_withDrug is True: interesting_samples = set(metadata_df[metadata_df.has_MIC_data].Run)
    else: interesting_samples = set(metadata_df.Run)

    # get only those that have patient ID data
    if only_samples_patientID is True: interesting_samples = interesting_samples.intersection(set(metadata_df[~pd.isna(metadata_df.patient_ID)].index))

    # load the tree with the interesting samples
    tree = Tree(treefile)
    tree.prune(interesting_samples)

    if only_samples_closeToResistant is True:
        
        # get resistant samples
        resistant_samples = {srr for srr in interesting_samples if any([metadata_df.loc[srr, "%s_resistant"%d] for d in drugs])}
        print("There are %i/%i resistant samples to any of the drugs"%(len(resistant_samples), len(interesting_samples)))

        # get the susceptible ones
        susceptible_samples = interesting_samples.difference(resistant_samples)

        if len(susceptible_samples)>0:

            # get the distance of each 
            nclose_ssamples = 1

            # initialize the close sssamples
            close_ssamples = set()

            # go through each resistant sample
            for rsample in resistant_samples:

                # iterate through the acesors
                for ancestor in (tree&rsample).get_ancestors():

                    # get the leaf names of the ancestors
                    susceptible_ancestor_leafs = set(ancestor.get_leaf_names()).intersection(susceptible_samples)

                    # if it is enough, take randomly some 
                    if len(susceptible_ancestor_leafs)>=nclose_ssamples:


                        #close_ssamples.update(set(sorted(susceptible_ancestor_leafs)[]))

                        break





            rsample_to_ssample_to_distance_df = pd.DataFrame({rsample : {ssample : tree.get_distance(rsample, ssample) for ssample in susceptible_samples} for rsample in resistant_samples}).transpose() # index are R samples

            print(rsample_to_ssample_to_distance_df.apply(lambda r: [x for x in sorted([(ssample, r[ssample]) for ssample in r.keys()], key=(lambda x: x[1]))], axis=1).iloc[0])



            thisdoesnotwork


            rsample_to_closest_ssamples = rsample_to_ssample_to_distance_df.apply(lambda r: {x[0] for x in sorted([(ssample, r[ssample]) for ssample in r.keys()], key=(lambda x: x[1]))[:nclose_ssamples]}, axis=1)


            print(rsample_to_ssample_to_distance_df)
            print(rsample_to_closest_ssamples)

            adlhjdhjkdah

            # define the samples with patient data
            samples_with_patientID = set(metadata_df[~pd.isna(metadata_df["patient_ID"])].index).intersection(interesting_samples)

            # keep only the closest n samples
            ssamples_to_keep = set.union(*rsample_to_closest_ssamples)
            interesting_samples = ssamples_to_keep.union(resistant_samples).union(samples_with_patientID)
    
            # prune again
            tree.prune(interesting_samples)

    print("The tree will have %i samples"%(len(interesting_samples)))

    # initialize col_to_color
    col_to_color = {}

    # get only the metadata of the interesting samples
    metadata_df = metadata_df.loc[interesting_samples]

    ### MIC ###

    # change the mic fields so that they only contain log tans
    all_MIC_vals = make_flat_listOflists([list(metadata_df[~pd.isna(metadata_df[f])][f]) for f in mic_fileds])

    # get the colormap for the mic
    mic_palette_type = "coolwarm"
    mic_palette_center = 0

    mic_value_to_color, mic_palette = get_value_to_color(all_MIC_vals, palette=mic_palette_type, type_color="rgb", center=mic_palette_center)
    mic_palette = {get_formatted_floatStr_or_str(mic, decs=2) : color for mic, color in  get_value_to_color(all_MIC_vals, palette=mic_palette_type, n=10, type_color="rgb", center=mic_palette_center)[1].items()} # get only the palette to plot

    # get the colors
    df_colors = pd.DataFrame({d : {srr : get_val_from_dict_withNaN(metadata_df.loc[srr, "%s_MIC_FCtoBp"%d], mic_value_to_color, nan_value="white") for srr in interesting_samples} for d in drugs})[drugs]

    # get the annotations
    df_annotations = pd.DataFrame({d : {srr : get_annotation_resitance(metadata_df.loc[srr], d, threshold_largeR=3) for srr in interesting_samples} for d in drugs})

    ###### CLINICAL METADATA ######

    if only_MICs is False:

        # treatment
        all_treatments = sorted(set(metadata_df[~pd.isna(metadata_df.treatment)].treatment))
        treatment_palette = {"ANI/MIF/CAS":"magenta", "VRC>(ANI+CAS)":"purple", "FLZ":"blue", "VRC":"cyan", "none":"gray"}
        df_colors["treatment"] = metadata_df.treatment.apply(lambda x: get_val_from_dict_withNaN(x, treatment_palette, nan_value="white"))
        df_annotations["treatment"] = ""

        # patientID
        all_patient_IDs = sorted(set(metadata_df[~pd.isna(metadata_df.patient_ID)].patient_ID))
        patient_palette = get_value_to_color(all_patient_IDs, palette="tab20", type_color="rgb", n=len(all_patient_IDs))[1]
        patient_to_annotation = {p : str(Ip+1) for Ip, p in enumerate(all_patient_IDs)}
        df_colors["patient"] = metadata_df.patient_ID.apply(lambda x: get_val_from_dict_withNaN(x, patient_palette, nan_value="white"))
        df_annotations["patient"] = metadata_df.patient_ID.apply(lambda x: get_val_from_dict_withNaN(x, patient_to_annotation, nan_value=""))

        # timepoint (color from patient)
        df_colors["timepoint"] = metadata_df.apply(lambda r: get_timepoint_color(r, patient_palette), axis=1)
        all_timepoints = sorted(set(metadata_df[~pd.isna(metadata_df.timepoint)].timepoint))
        timepoint_to_anntation = {t : str(int(t)) for t in all_timepoints}
        df_annotations["timepoint"] = metadata_df.timepoint.apply(lambda x: get_val_from_dict_withNaN(x, timepoint_to_anntation, nan_value=""))
        
        #patients_with_timepoint = {p for p in set(metadata_df.patient_ID) if  any(~pd.isna(metadata_df[metadata_df.patient_ID==p].timepoint))}

    ###############################

    ### MUTATIONS ###

    if only_MICs is False:

        # keep only interesting samples
        vars_df = vars_df[vars_df.SRR.isin(interesting_samples)]

        # get mutations df. The samples are 
        mutations_df = get_sample_to_gene_to_mutation_set_df(vars_df, list(interesting_samples))

        # redefine genes as those that are in the df
        genes = [g for g in genes if g in mutations_df.columns]
        mutations_df = mutations_df[genes]

        # define graphics
        gene_to_color = cglab_gene_to_color
        gene_to_color = {g : gene_to_color[g] for g in genes}

        # go through each gene
        for Ig, gene in enumerate(genes): 

            # get all mutations
            all_mutations = set.union(*mutations_df[gene])
            sorted_mutations = sorted(all_mutations, key=(lambda x: int(x.split("|")[1].split(".")[1].split("-")[0])))

            # get a blank before each new gene
            if len(sorted_mutations)>0:
                df_colors["blank%i"%Ig] = "white"
                df_annotations["blank%i"%Ig] = "-"

            # go through each mutation
            for mut in sorted_mutations:

                # record the color
                col_to_color[mut] = gene_to_color[gene]

                # get the mutation annotation
                previous_mutations = previous_gene_to_mutations[gene]
                mut_annotation = get_mutation_annotation(mut, gene, previous_mutations)

                # decide if this mutation is interesting
                if mut_annotation=="": # only skip mutations that don't have any interesting annotation

                    # get the number of samples that have the mutation, and the ones that don't
                    samples_mut = set(mutations_df[mutations_df[gene].apply(lambda muts: mut in muts)].index)
                    all_samples = set(mutations_df.index)
                    samples_no_mut = all_samples.difference(samples_mut)

                    if len(samples_mut)>=8: # only discard if the mutations are in a lot of samples

                        # discard if the fraction of samples with the mutation is above 90%
                        if (len(samples_mut)/len(all_samples)) >= 0.75: continue

                        # calculate a ttest and a kstest for the resistance for samples that have and don't have this
                        pvals = []

                        for d in drugs:

                            # get the resistance vals
                            mut_resistance = metadata_df.loc[samples_mut, "%s_MIC_FCtoBp"%d]
                            no_mut_resistance = metadata_df.loc[samples_no_mut, "%s_MIC_FCtoBp"%d]

                            # correct the NaNs
                            mut_resistance = mut_resistance[~pd.isna(mut_resistance)]
                            no_mut_resistance = no_mut_resistance[~pd.isna(no_mut_resistance)]

                            if len(mut_resistance)==0 or len(no_mut_resistance)==0: continue

                            # get the tests
                            tstat, pttest = stats.ttest_ind(mut_resistance, no_mut_resistance, equal_var=True)
                            kstat, pkstest = stats.ks_2samp(mut_resistance, no_mut_resistance)

                            median_mut_resistance = np.median(mut_resistance)
                            median_no_mut_resistance = np.median(no_mut_resistance)

                            pvals += [pttest, pkstest]

                            # print
                            if (pttest<0.1 or pkstest<0.1) and median_mut_resistance>median_no_mut_resistance: 


                                print("%s of %s is sig. pKStest=%.4f, pTtest=%.4f"%(mut, gene, pkstest, pttest))
                                print("median(resistance)=%.3f, median(susceptible)=%.3f"%(median_mut_resistance, median_no_mut_resistance))
                              
                        # if none of the mutations are associated to mutations, skip the printing
                        if all(np.array(pvals)>=0.05): continue

                # add the annotation
                presence_mutation_to_annot = {True: mut_annotation, False:""}
                df_annotations[mut] = mutations_df[gene].apply(lambda muts: presence_mutation_to_annot[mut in muts])

                # add color depending on if the mutation is there or not
                presence_mutation_to_color = {True: gene_to_color[gene], False:"white"}
                df_colors[mut] = mutations_df[gene].apply(lambda muts: presence_mutation_to_color[mut in muts])

    ######### GET TREE ########

    # get the annotation colors
    df_annotation_colors = pd.DataFrame({col : {srr : get_annotationColor_on_bgcolor(df_colors.loc[srr, col]) for srr in interesting_samples} for col in df_colors.columns})

    # get the outline colors
    for col in set(df_colors.columns).difference(set(col_to_color)): col_to_color[col] = "gray"
    df_outline_colors = pd.DataFrame({col : {srr : get_outlineColor_on_bgcolor(df_colors.loc[srr, col], col, col_to_color[col]) for srr in interesting_samples} for col in df_colors.columns})

    # get the annotation sizes
    annot_to_size = {"R":40, "s":30, "r":30, "":8, "H":13, "=":13, "~":13, "*":13, "|":8, "-":14}
    for I in range(0, 100): annot_to_size[str(I)] = 12
    df_annotation_sizes = pd.DataFrame({col : {srr : annot_to_size[df_annotations.loc[srr, col]] for srr in interesting_samples} for col in df_colors.columns})


    # define the filename
    bool_to_str_samplesDrug = {True:"onlySamplesDrug", False:"allSamples"}
    bool_to_str_samplesCloseToResistant = {True:"onlySamplesCloseToR", False:"allSamples"}
    filename = "%s_%s_%s_%s_%s.pdf"%(fileprefix, "-".join(drugs), "-".join(genes), bool_to_str_samplesDrug[only_samples_withDrug], bool_to_str_samplesCloseToResistant[only_samples_closeToResistant])
    #filename = None

    # define attributes
    if bg_color_field is not None: leafName_to_bgColor = dict(metadata_df[bg_color_field])
    else: leafName_to_bgColor = {l.name : "white" for l in tree.get_leaves()}

    # add them to the tree
    for l in tree.get_leaves(): 
        l.bgcolor = leafName_to_bgColor[l.name]
        l.strain = metadata_df.loc[l.name, "strain"]
        l.patientID = metadata_df.loc[l.name, "patient_ID"]
        l.timepoint = metadata_df.loc[l.name, "timepoint"]

    # define the colorMAP lists
    if only_MICs is False: color_map_list = [("log2(MIC / MIC bp)", mic_palette), ("treatment", treatment_palette), ("patient", patient_palette), ("genes", gene_to_color)]

    else: color_map_list = [("log2(MIC / MIC bp)", mic_palette)]


    # define the leaf attributes
    #leaf_attributes = ["strain"]#, "patientID", "timepoint"]
    leaf_attributes = None

    # get tree
    get_heatmap_tree_provided_colors(tree, df_colors, df_annotations, filename=filename, df_annotation_colors=df_annotation_colors, df_annotation_sizes=df_annotation_sizes, color_map_list=color_map_list, leaf_attributes=leaf_attributes, df_outline_colors=df_outline_colors, orientation=orientation)

    ##########################

def plot_stacked_bar(df, filename, x="gene", y="n mutations", hue="type mut", figsize=(7,3), palette="tab10", rotation_xticks=75):

    """Plots a stacked bar for a df"""

    df = cp.deepcopy(df)
    df = df[[x, y, hue]]

    # init fig
    fig = plt.figure(figsize=figsize)

    # define colors
    all_hue_vals = sorted(set(df[hue]))
    if type(palette)==str: palette = get_value_to_color(all_hue_vals, palette=palette, n=10, type_color="rgb", center=None)[0]

    # reformat df to inclue all hue vals for all x vals
    all_x_and_hue = set(df.apply(lambda r: (r[x], r[hue]), axis=1))
    for xval in set(df[x]):
        for hueval in set(df[hue]):

            # if not existing, add a 0
            if (xval, hueval) not in all_x_and_hue:
                df_dict = {(xval, hueval): {x : xval, hue: hueval, y: 0.0}}
                df = df.append(pd.DataFrame(df_dict).transpose())

    # initialize legend items
    legend_items = []
    legend_names = []

    df = df.sort_values(by=[hue, x])

    # go through ordered hue values
    for I, hue_val in enumerate(all_hue_vals):
        color = palette[hue_val]
        print(hue_val, color)

        # get the df hue
        df_hue = df[df[hue]==hue_val]

        # get the array of bottom
        if I==0: bottom = [0]*len(df_hue)
        else: bottom = np.add(bottom, previous_Ys).tolist()

        # create the vars
        ax = plt.bar(df_hue[x], df_hue[y], color=color, bottom=bottom) # edgecolor='white', width=barWidth

        # keep for the next 
        previous_Ys = list(df_hue[y])

        # get the legend item
        legend_items.append(plt.Rectangle((0,0),1,1,fc=color, edgecolor = 'none'))
        legend_names.append(hue_val)




    # get the legend
    l = plt.legend(legend_items, legend_names, loc=2, ncol = 1, prop={'size':12}, bbox_to_anchor=(1.01, 1), borderaxespad=0.)

    #ax.legend(bbox_to_anchor=(1.01, 1), loc=2, borderaxespad=0.)
    #l.draw_frame(False)

    # get the labels
    plt.xlabel(x)
    plt.ylabel(y)

    # rotation
    for item in fig.axes[0].get_xticklabels(): item.set_rotation(rotation_xticks)

    # save
    fig.savefig(filename, bbox_inches='tight')

    return fig


def get_sample_to_closest_samples(samples_from, samples_to, tree, nsamples=2):

    """Takes a tree and matches each interesting sample to the closets samples, ordered by distance.
    sfrom would be the samples that are in the keys of the returned series"""

    # get a df with all the samples
    distance_df = pd.DataFrame({sto : {sfrom : float(tree.get_distance(sto, sfrom)) for sfrom in samples_from} for sto in samples_to})

    # get the series where each value is a list of samples, sorted by distance
    sample_to_closest_samples = distance_df.apply(lambda r: [y[0] for y in sorted([(sto, float(r[sto])) for sto in samples_to], key=(lambda x: x[1]))][0:nsamples], axis=1)

    return sample_to_closest_samples


def get_fdr_corrected_pvals(pvals):

    """Takes an array of pvals and returns them FDR corrected"""

    return multitest.fdrcorrection(pvals)[1]

def get_NaN_to_1(x):

    if pd.isna(x): return 1
    else: return x 

def get_mutation_info(metadata_df, vars_df, treefile, inVitro_gene_to_mutations, drugs="all"):

    """
    This function gets a df where each line is one muatation and there is info about the clinical impact on MIC
    """

    # copy dfs
    metadata_df = cp.deepcopy(metadata_df)

    # keep only clinical isolates and add the MIC data
    metadata_df = metadata_df[(metadata_df.type=="clinical")].set_index("Run", drop=False)

    if drugs=="all": drugs = [c.split("_")[0] for c in metadata_df.columns if c.endswith("_MIC_FCtoBp")]
    
    # initialzie the final df data
    data_dict_all = {}

    # go through each drug
    for drug in drugs:
        print(drug)

        # get the mic_f
        mic_f = "%s_MIC_FCtoBp"%drug

        # get only those samples that are interesting
        interesting_samples = set(metadata_df[~pd.isna(metadata_df[mic_f])].Run)

        # change the definition of patient_ID. Only keep patient IDs where you have multiple times
        """
        all_patient_IDs = set(metadata_df[~pd.isna(metadata_df.patient_ID)].patient_ID)
        patient_ID_to_nSamples = {p : len(metadata_df[metadata_df.patient_ID==p]) for p in all_patient_IDs}
        metadata_df["patient_ID"] = metadata_df.patient_ID.apply(lambda pID: get_only_repeated_patientIDs(pID, patient_ID_to_nSamples, min_samples=2))
        """

        # load the tree with the interesting samples
        tree = Tree(treefile)
        tree.prune(interesting_samples)

        # keep only interesting samples
        vars_df = vars_df[vars_df.SRR.isin(interesting_samples)]

        #print(interesting_samples)

        # get mutations df. The samples are 
        mutations_df = get_sample_to_gene_to_mutation_set_df(vars_df, list(interesting_samples))

        # go through each gene
        for Ig, gene in enumerate(mutations_df.columns): 

            # get all mutations
            all_mutations = set.union(*mutations_df[gene])

            # go through each mutation
            for mut in all_mutations:

                # get the mutation annotation
                if gene in inVitro_gene_to_mutations: previous_mutations = inVitro_gene_to_mutations[gene]
                else: previous_mutations = set()
                mut_annotation = get_mutation_annotation(mut, gene, previous_mutations)

                # get the number of samples that have the mutation, and the ones that don't
                samples_mut = set(mutations_df[mutations_df[gene].apply(lambda muts: mut in muts)].index)
                samples_no_mut = interesting_samples.difference(samples_mut)

                #print("There are %i/%i samples with %s in %s and MIC measured for %s"%(len(samples_mut), len(interesting_samples), mut, gene, drug))

                # map each sample mut to the samples no mut
                sampleMut_to_closest_samplesNoMut = get_sample_to_closest_samples(samples_mut, samples_no_mut, tree, nsamples=2)

                # initialize the data_dict
                data_dict = {}

                # get the log2 ratio in MIC
                data_dict["MIC_fc_list"] = [np.median([metadata_df.loc[smut, mic_f] - metadata_df.loc[snomut, mic_f] for snomut in close_nomut_samples]) for smut, close_nomut_samples in dict(sampleMut_to_closest_samplesNoMut).items()]

                # add metadata
                data_dict["drug"] = drug
                data_dict["gene"] = gene
                data_dict["mut"] = mut
                data_dict["mut_annotation"] = mut_annotation

                # add the number of samples
                data_dict["n_samples_mut"] = len(samples_mut)
                data_dict["n_samples_total"] = len(interesting_samples)

                # add compound things
                data_dict["MIC_fc_median"] = np.median(data_dict["MIC_fc_list"])
                data_dict["MIC_fc_mad"] = stats.median_absolute_deviation(data_dict["MIC_fc_list"])

                # keep
                data_dict_all["%s_%s_%s"%(drug, gene, mut)] = data_dict


    # get the mutations info
    df_mutation_info = pd.DataFrame(data_dict_all).transpose()

    # add the pvalues of the MICfc being different from 0
    df_mutation_info["pval_MICdifFrom0"] = df_mutation_info.MIC_fc_list.apply(lambda mics: get_NaN_to_1(stats.ttest_1samp(mics, 0)[1]))
    df_mutation_info["pval_MICdifFrom0_fdr"] = get_fdr_corrected_pvals(df_mutation_info.pval_MICdifFrom0)
    

    return df_mutation_info


def get_gene_and_mut_annot(r):

    if r["mut_annotation"]=="": return r["gene"]
    else: return "%s (%s)"%(r["gene"], r["mut_annotation"])

def get_seaborn_swarmplot_with_marker_and_color(x, y, df, hue, style, palette, styleVal_to_marker, figsize=(5,5)):

    """This function generates a swarmplot that has colors and markers"""

    # get a deepcopy
    df = cp.deepcopy(df)

    # get the swarm to get the positions 
    fig = plt.figure(figsize=figsize)
    swarm = sns.swarmplot(x=x, y=y, data=df, dodge=True, hue=hue, palette=palette)
    plt.close(fig)

    # initialize the dicts
    index_to_newX = {}

    # calculate the order of the legend and add it to the df
    hue_order_legend = dict(zip([x for x in [col.properties()["label"] for col in swarm.collections] if x in palette], range(len(set(df[hue])))))
    df["order_legend_by_hue"] = df[hue].apply(lambda x: hue_order_legend[x])
    df = df.sort_values(by=[x, "order_legend_by_hue", y])

    # get all the points
    all_points_xy_vectors = [(x,y) for x,y in [np.array(col.get_offsets()).T for col in swarm.collections] if len(x)>0 and len(y)>0]

    for I, (x_and_hue, df_group) in enumerate(df.groupby([x, "order_legend_by_hue"])):



        # get the points of the swarm:
        xswarm, yswarm = all_points_xy_vectors[I]

        print(x_and_hue)

        # check that they are the same, indicating that the ordering is correct
        if any([np.round(yswarm[idx], 10)!=np.round(df_group[y].values[idx], 10) for idx in range(len(yswarm))]):  print(yswarm, df_group[y].values); raise ValueError("error in adding errorbars because of the ordering")

        # check that all hue values are the sane
        if len(set(df_group[hue]))!=1: raise ValueError("There are the following hue values when there sjould only be one: %s"%set(df_group[hue]))

        # keep the 
        df_group["new_X"] = xswarm
        index_to_newX = {**index_to_newX, **dict(df_group.new_X)}


    # define the xticklabels
    xticklabels = [x.get_text() for x in swarm.axes.get_xticklabels()]
    xticks = swarm.axes.get_xticks()

    # redefine the x
    df[x] = [index_to_newX[idx] for idx in df.index]

    # make a scatterplot with the new coordinates
    ax = sns.scatterplot(x=x, y=y, data=df, hue=hue, palette=palette, style=style, markers=styleVal_to_marker, edgecolor="gray")
    
    # set xticks
    ax.set_xticklabels(xticklabels)
    ax.set_xticks(xticks)

    return ax

def plot_mutation_effect_vs_frequency(df, fileprefix, drug="azole", genes="all", max_freq=0.50, figsize=(5,5), add_annotations=True):

    """Takes the mutation_info df and plots the MIC fc related to each mutation in df, vs the number of samples"""


    # get a df with only info about the drug
    df = df[df.drug==drug]

    # define the interesting genes
    if genes=="all": genes = sorted(set(df.gene))

    # get only a df with the genes of interest
    df = df[df.gene.isin(genes)]

    # get only a df with the mutations that are in less than max_freq
    df["mut_freq"] = df.n_samples_mut / df.n_samples_total
    df = df[df.mut_freq<=max_freq]

    # init fig
    fig = plt.figure(figsize=figsize)


    # define marks
    gene_to_color = cglab_gene_to_color
    mut_annot_to_marker = {"":"o", "*":"*", "~":"^", "=":"v", "H":"X"}

    fig = plt.figure(figsize=figsize)

    # plot a swarmplot
    ax = get_seaborn_swarmplot_with_marker_and_color("n_samples_mut", "MIC_fc_median", df, "gene", "mut_annotation", gene_to_color, mut_annot_to_marker, figsize=figsize)

    # add horizontal lines
    for y in [-1, 0, 1]: plt.axhline(y, color="gray", linestyle="--", linewidth=.8)

    # change legend
    ax.legend(bbox_to_anchor=(1.01, 1), loc=2, borderaxespad=0.)
    ax.set_title(drug)

    # define the filename
    filename = "%s_%s_%s.pdf"%(fileprefix, drug, "".join(genes))
    fig.savefig(filename, bbox_inches='tight')

    

def get_barplot(x, y, hue, df, filename, palette=None, rotation_xticks=75, figsize=(5,3), remove_spaces=True):

    """Gets a barplot"""

    fig = plt.figure(figsize=figsize)

    # skip non0
    if remove_spaces is True: df = df[df[y]!=0]

    # get the barplot
    ax = sns.barplot(x=x, y=y, hue=hue, data=df, palette=palette)

    # rotate
    for item in ax.get_xticklabels(): item.set_rotation(rotation_xticks)

    # change legend
    ax.legend(bbox_to_anchor=(1.01, 1), loc=2, borderaxespad=0.)

    # define the filename
    fig.savefig(filename, bbox_inches='tight')

def get_df_resistance_attributed_to_genes_df(metadata_df, vars_df, treefile):

    """This function gets a df where each row is a combination of gene and type_resistance. It includes the number of events that are associated to a mutation in the gene"""

    # copy dfs
    metadata_df = cp.deepcopy(metadata_df)

    # keep only clinical isolates and add the MIC data
    metadata_df = metadata_df[(metadata_df.type=="clinical")].set_index("Run", drop=False)

    # initialzie the final df data
    data_dict_all = {}

    # go through each drug
    for drug in ["echinocandin", "azole"]:
    #for drug in ["azole"]:
        print(drug)

        # get the mic_f
        mic_f = "%s_MIC_FCtoBp"%drug

        # get only those samples that are interesting
        interesting_samples = set(metadata_df[~pd.isna(metadata_df[mic_f])].Run)

        # load the tree with the interesting samples
        tree = Tree(treefile)
        tree.prune(interesting_samples)

        # keep only interesting samples
        vars_df = vars_df[vars_df.SRR.isin(interesting_samples)]

        # get mutations df. The samples are 
        mutations_df = get_sample_to_gene_to_mutation_set_df(vars_df, list(interesting_samples))

        # define the mutations that are in >0.75% of samples
        gene_to_uninteresting_mutations = {}
        for gene in mutations_df.columns:

            # define all mutations
            all_mutations = set.union(*mutations_df[gene])

            # map each mutation to the fraction of samples with it
            mutation_to_fraction_samples = {m : sum(mutations_df[gene].apply(lambda muts: m in muts)) / len(mutations_df) for m in all_mutations}

            gene_to_uninteresting_mutations[gene] = {mut for mut, fract in mutation_to_fraction_samples.items() if fract>=0.75}

        # define each type of samples
        resistant_samples = {s for s in interesting_samples if metadata_df.loc[s, "%s_resistant"%drug] is True}
        strongly_resistant_samples = {s for s in interesting_samples if metadata_df.loc[s, "%s_resistant"%drug] is True and metadata_df.loc[s, mic_f]>=3}
        susceptible_samples = interesting_samples.difference(resistant_samples)

        print("There are %i/%i/%i (total=%i) resistant/strongly resistant/susceptible for %s"%(len(resistant_samples), len(strongly_resistant_samples), len(susceptible_samples), len(interesting_samples), drug))

        # go through each type of samples
        for type_sample, samples in [("resistant", resistant_samples), ("strongly resistant", strongly_resistant_samples), ("susceptible", susceptible_samples)]:

            # define the other samples to which to compare
            if type_sample=="resistant": other_samples = susceptible_samples
            elif type_sample=="strongly resistant": other_samples = susceptible_samples
            elif type_sample=="susceptible": other_samples = resistant_samples
            else: raise ValueError("%s is not valid"%type_sample)

            # map each sample to the closest two of a different cathegoy
            sample_to_closestOtherSamples = get_sample_to_closest_samples(samples, other_samples, tree, nsamples=2)

            # initialize the samples that have been attributed to mutations
            samples_with_mutation_related = set()

            # go throughe each gene
            for Ig, gene in enumerate(mutations_df.columns): 

                # initialize a df_g
                df_g = pd.DataFrame()

                # define the uninteresting mutations
                uninteresting_mutations = gene_to_uninteresting_mutations[gene]

                # get the mutations_series
                df_g["mutations_sample"] = mutations_df.loc[samples, gene].apply(lambda muts: muts.difference(uninteresting_mutations))

                # add the other samples
                df_g["otherSamples"] = [sample_to_closestOtherSamples[g] for g in df_g.index]

                """
                # add the mutations of the other samples
                df_g["mutations_otherSamples"] = df_g.apply(lambda r: set.union(*[mutations_df.loc[osample, gene] for osample in r["otherSamples"]]).difference(uninteresting_mutations), axis=1)
                # add whether there is a new mutation
                df_g["has_new_mutation"] = df_g.apply(lambda r: r["mutations_sample"]!=r["mutations_otherSamples"], axis=1)
                """

                df_g["has_new_mutation"] = df_g.apply(lambda r: all([r["mutations_sample"]!=(mutations_df.loc[osample, gene].difference(uninteresting_mutations)) for osample in r["otherSamples"]]), axis=1)

                # keep data
                data_dict = {}
                data_dict["drug"] = drug
                data_dict["type_sample"] = type_sample
                data_dict["gene"] = gene


                data_dict["samples_mut"] = set(df_g[df_g["has_new_mutation"]].index)
                data_dict["n_samples_mut"] = sum(df_g["has_new_mutation"])
                data_dict["n_samples_total"] = len(samples)

                data_dict["pct_samples_withChange"] = data_dict["n_samples_mut"]/data_dict["n_samples_total"]

                data_dict_all["%s_%s_%s"%(drug, type_sample, gene)] = data_dict

                # keep samples
                samples_with_mutation_related.update(set(df_g[df_g.has_new_mutation].index))

            # add data for remaining 
            unassociated_samples = samples.difference(samples_with_mutation_related)

            data_dict = {}
            data_dict["drug"] = drug
            data_dict["type_sample"] = type_sample
            data_dict["gene"] = "none"

            data_dict["samples_mut"] = unassociated_samples
            data_dict["n_samples_mut"] = len(unassociated_samples)
            data_dict["n_samples_total"] = len(samples)

            data_dict["pct_samples_withChange"] = data_dict["n_samples_mut"]/data_dict["n_samples_total"]

            data_dict_all["%s_%s_%s"%(drug, type_sample, "none")] = data_dict



    # get as df
    df_final = pd.DataFrame(data_dict_all).transpose()

    # add compound fields
    df_final["type_resistance"] = df_final.drug + "-" + df_final.type_sample

    return df_final


def get_resistance_attributed_to_genes_df_mostLikelyCause(df):

    """Returns a df that has type-resistance and the percentage of samples attributable to the most important genes"""

    # map each drug to the interesting genes (sorted by importance)
    drug_to_genes = {"azole":["PDR1", "ERG11", "CDR1", "ERG3", "ERG4", "UPC2A", "Scer_CNE1", "FKS1", "FKS2"],
                 "echinocandin": ["FKS1", "FKS2", "ERG3", "ERG4", "Scer_CNE1", "UPC2A", "PDR1", "CDR1", "ERG11"]}


    data_dict_all = {}
    for type_resistance in {"azole-resistant", "azole-strongly resistant", "echinocandin-resistant", "echinocandin-strongly resistant"}:

        # get the drug
        drug = type_resistance.split("-")[0]

        # get the df
        df_r = df[df.type_resistance==type_resistance]  

        # define all samples
        all_samples = set.union(*df_r.samples_mut)

        # get the already found genes
        already_associated_samples = set()

        # go through each gene
        for gene in drug_to_genes[drug]:

            # get the df
            df_g = df_r[df_r.gene==gene]
            if len(df_g)>1: raise ValueError("can't be >1")
            elif len(df_g)==0: continue
            gene_series = df_g.iloc[0]

            # get the samples mut interesting (only those already associated)
            samples_with_gene_mut = gene_series["samples_mut"].difference(already_associated_samples)

            # keep them
            already_associated_samples.update(samples_with_gene_mut)

            # get the number of samples associated to this 
            data_dict = {}
            data_dict["type_resistance"] = type_resistance
            data_dict["gene"] = gene
            data_dict["nsamples_mut"] = len(samples_with_gene_mut)
            data_dict["nsamples_total"] = len(all_samples)

            data_dict_all["%s_%s"%(type_resistance, gene)] = data_dict

            # break if all genes have been covered
            if already_associated_samples==all_samples: break

        # add remaining
        data_dict = {}
        data_dict["type_resistance"] = type_resistance
        data_dict["gene"] = "none"
        data_dict["nsamples_mut"] = len(all_samples.difference(already_associated_samples))
        data_dict["nsamples_total"] = len(all_samples)

        data_dict_all["%s_%s"%(type_resistance, "none")] = data_dict

    df_final = pd.DataFrame(data_dict_all).transpose()

    # add the percentage
    df_final["fraction_samples_mut"] = df_final.nsamples_mut / df_final.nsamples_total

    return df_final
  


def get_FKSmutations_df_only_sanger(FKS_muts_df_file):

    """Takes the df with FKS mutations and returns the df"""

    ####### get the FKS for all samples from Ewa's #########

    df = pd.read_csv(FKS_muts_df_file, sep=";")
    wrong_cols =  list(df.keys())[4:17] # these are fitness-related stuff

    # rename
    df = df[[x for x in df.columns if x not in wrong_cols]].rename(columns={"Drug": "condition", "Replicate":"replicate_exp_evol", "Strain":"strain", "Clade":"clade"})

    # change things
    df["strain"] = df["strain"].apply(lambda x: x.replace("P352", "P35"))
    df["condition"] = df["condition"].apply(lambda x: x.replace("AF", "ANIFLZ"))

    # keep only non-WT
    df = df[df.condition!="WT"]

    # change 
    df["sampleID"] = df.strain + "_" + df.replicate_exp_evol + "_" + df.condition

    # get the fks1 and fks2 cols
    fks1_cols = list(df.keys()[4:25])
    fks2_cols = [x for x in list(df.keys()[25:]) if x!="sampleID"]
    for f in (fks1_cols + fks2_cols): df[f] = df[f].apply(str)


    ##########  correct the format and put as in the mutations table ##########
    df_fks1 = df.set_index("sampleID", drop=False)[fks1_cols]
    df_fks2 = df.set_index("sampleID", drop=False)[fks2_cols]

    def get_formatted_mutation(mut):

        """Takes a mutation and formats it in the consistent version"""

        # PTC
        if mut.endswith("*"):
            pos = str(int(mut[1:-1]))
            ref = mut[0] 
            alt = "*"
            type_mut = "PTC"
            type_molecule = "p"

        # deletion
        elif mut.endswith("-"):
            pos = str(int(mut[1:-1]))
            ref = mut[0] 
            alt = "-"
            type_mut = "del"
            type_molecule = "p"


        # frameshift
        elif mut.endswith("fs"):
            pos = str(int(mut[1:-2]))
            ref = mut[0] 
            alt = "X"
            type_mut = "FS"
            type_molecule = "p"

        # miss
        else: 
            pos = str(int(mut[1:-1]))
            ref = mut[0] 
            alt = mut[-1]
            type_mut = "mis"
            type_molecule = "p"

        mut_formatted = "%s|%s.%s|%s/%s"%(type_mut, type_molecule, pos, ref, alt)
        return mut_formatted


    def get_formatted_mutations_fks(sample, df_fks):

        """Takes a sample and returns the fomatted mutations"""

        # get mutations as set
        set_mutations = set(df_fks.loc[sample].values).difference({'0'})

        # remove the A650* from FKS1 in (it was strange in the dendrogram)
        if sample=="CST109_1F_ANI": set_mutations = set_mutations.difference({"A650*"})

        # if empty
        if set_mutations==set(): return ""
        else: return "\n".join(sorted([get_formatted_mutation(mut) for mut in set_mutations]))


    df_fks_formatted = pd.DataFrame()
    df_fks_formatted["sampleID"] = df.sampleID
    print("adding fks1")
    df_fks_formatted["FKS1"] = df.sampleID.apply(lambda s: get_formatted_mutations_fks(s, df_fks1))
    print("adding fks2")
    df_fks_formatted["FKS2"] = df.sampleID.apply(lambda s: get_formatted_mutations_fks(s, df_fks2))
    df_fks_formatted[(df_fks_formatted.FKS1!="") | (df_fks_formatted.FKS2!="")]

    df_fks_formatted = df_fks_formatted.transpose()


    # get to excel
    #excel_file = "%s.formatted.xlsx"%FKS_muts_df_file
    #print("writing %s"%excel_file)
    #df_fks_formatted.to_excel(excel_file)


    ###########################################################################

    return df, fks1_cols, fks2_cols


def get_FKSmutations_sanger_one_row_per_sample_and_variant(FKS_muts_df_file):

    """Takes a file with the FKS mutations and returns the df that has var_as_str, sampleID and final_name"""

    # load df
    fks_mutations_df_sanger, fks1_cols, fks2_cols = get_FKSmutations_df_only_sanger(FKS_muts_df_file)

    important_fields = ["sampleID", "final_name", "Consequence", "Protein_position", "Amino_acids", "CDS_position", "Codons"]

    # init df
    dict_data = {}
    Irow = 0

    # go through each FKS
    for final_name, cols in [("FKS1", fks1_cols), ("FKS2", fks2_cols)]:

        # go through each sample
        for sampleID in set(fks_mutations_df_sanger.sampleID):

            # get the row
            df = fks_mutations_df_sanger[fks_mutations_df_sanger.sampleID==sampleID][cols]
            if len(df)!=1: raise ValueError("df should be 1")
            row = df.iloc[0]
            row = row[row!="0"]

            # go through each variant and annotate
            for position, var in dict(row).items():

                # stop codon
                if var.endswith("*"):

                    Consequence = "stop_gained"
                    Protein_position = str(position)
                    ref = var[0]
                    alt = var[-1]

                # deletion or FS
                elif var.endswith("-") or var.endswith("fs"):

                    if var=="F625-" and final_name=="FKS1": 

                        Consequence = "inframe_deletion"
                        Protein_position = "623-624"
                        ref = "YF"
                        alt = "Y"


                    elif var=="F659-" and final_name=="FKS2": 

                        Consequence = "inframe_deletion"
                        Protein_position = "657-658"
                        ref = "YF"
                        alt = "Y"

                    elif var=="L628fs" and final_name=="FKS1":

                        Consequence = "frameshift_variant"
                        Protein_position = "628"
                        ref = "L"
                        alt = "X"

                    elif var=="I670fs" and final_name=="FKS2":

                        Consequence = "frameshift_variant"
                        Protein_position = "670"
                        ref = "I"
                        alt = "X"

                    else:

                        print(final_name, sampleID, position, var)
                        raise ValueError("%s is not valid"%var)  

                # missense
                else:

                    Consequence = "missense_variant"
                    Protein_position = str(position)
                    ref = var[0]
                    alt = var[-1]

                # keep
                Amino_acids = "%s/%s"%(ref, alt)
                dict_data[Irow] = {"sampleID":sampleID, "final_name":final_name, "Consequence":Consequence, "Protein_position":Protein_position, "Amino_acids":Amino_acids, "CDS_position":"0", "Codons":"-" }

                Irow += 1


    df_vars = pd.DataFrame(dict_data).transpose()

    return df_vars

              
def plot_distribution_fractionOverlapping_mutations(inVitroEvol_vs_clinics_mutations, samples_WGS_inVitro, inVitroEvol_mutations, filename, rotation_xticks=75, figsize=(6,3)):

    """This function plots a violinplot for each gene. The values represent the fraction of mutations that can be found in the other half of samples, in a given resample of the data"""

    # get a df with the data for several re-samplings
    df_resampling_file = "%s.resampling_df.tab"%filename

    interesting_genes = ["CDR1", "ERG11", "ERG3", "ERG4", "FKS1", "FKS2", "PDR1", "Scer_CNE1"]

    if file_is_empty(df_resampling_file):
    #if True:

        # initialize
        dict_data = {}
        Idata = 0

        # go through each gene
        for gene in interesting_genes:

            # define the samples
            if gene in {"FKS1", "FKS2"}: all_samples = set(inVitroEvol_mutations.index)
            else: all_samples = set(inVitroEvol_mutations.index).intersection(samples_WGS_inVitro)

            # go through several fractions of samples
            for fraction_samples in [0.1, 0.2, 0.3, 0.5]:
                print(gene, fraction_samples)

                nsamples_resample = int(len(all_samples)*fraction_samples)

                # go through each sample
                for Isample in range(0, 1000):

                    # define the samples
                    query_samples = set(random.sample(all_samples, nsamples_resample))
                    non_query_samples = all_samples.difference(query_samples)
                    subject_samples = set(random.sample(non_query_samples, nsamples_resample))

                    # define the mutatoins in each set of samples
                    query_mutations = set.union(*inVitroEvol_mutations.loc[query_samples, gene])
                    subject_mutations = set.union(*inVitroEvol_mutations.loc[subject_samples, gene])
                    query_mutations_in_subject = query_mutations.intersection(subject_mutations)

                    # calculate the fraction
                    if len(query_mutations)==0: fraction_overlapping_muts = 0.0
                    else: fraction_overlapping_muts = len(query_mutations_in_subject)/len(query_mutations)

                    # keep
                    dict_data[Idata] = {"gene":gene, "Isample":Isample, "fraction_muts_found":fraction_overlapping_muts, "fraction_samples":fraction_samples}
                    Idata += 1

        # save
        df_resampling = pd.DataFrame(dict_data).transpose()
        df_resampling.to_csv(df_resampling_file, sep="\t", header=True, index=False)

    df_resampling = pd.read_csv(df_resampling_file, sep="\t")

    # define the fraction of mutations found in clinical isolates
    df = inVitroEvol_vs_clinics_mutations.set_index(["gene", "type mut"])

    gene_to_fractMutsInClinicalIsolates = {gene : df.loc[(gene, "in clinical isolates")]["n mutations"]/df.loc[(gene, "not in clinical isolates")]["n mutations"] for gene in interesting_genes}


    ######## PLOT ########

    # get the plot
    fig = plt.figure(figsize=figsize)
    #ax = sns.violinplot(x="gene", y="fraction_muts_found", data=df_resampling, hue="fraction_samples", palette="rocket_r")
    ax = sns.boxplot(x="gene", y="fraction_muts_found", data=df_resampling, hue="fraction_samples", palette="rocket_r")

    # add the fractMutsInClinicalIsolates as a line
    for ticklabel in ax.get_xmajorticklabels():

        gene = ticklabel.get_text()
        xpos = ticklabel.get_position()[0]
        ypos = gene_to_fractMutsInClinicalIsolates[gene]

        plt.plot([xpos-0.5, xpos+0.5], [ypos, ypos], linewidth=.9, linestyle="--", color="black")

    # rotate axes
    for item in fig.axes[0].get_xticklabels(): item.set_rotation(rotation_xticks)

    # set legend
    ax.legend(bbox_to_anchor=(1.01, 1), loc=2, borderaxespad=0.)



    fig.savefig(filename, bbox_inches='tight')

def get_chr_to_len(genome, replace=False):

    chr_to_len_file = "%s.chr_to_len.py"%genome
    chr_to_len_file_tmp = "%s.tmp"%chr_to_len_file

    if file_is_empty(chr_to_len_file) or replace is True:

        remove_file(chr_to_len_file_tmp)

        # define chromosome_to_length for a genome
        chr_to_len = {seq.id: len(seq.seq) for seq in SeqIO.parse(genome, "fasta")}

        # save
        save_object(chr_to_len, chr_to_len_file_tmp)
        os.rename(chr_to_len_file_tmp, chr_to_len_file)

    else: chr_to_len = load_object(chr_to_len_file)

    return chr_to_len

def get_taxid2name(taxIDs):

    """Takes an iterable of taxIDs and returns a dict mapping each of them to the scientific name"""

    taxIDs = list(taxIDs)

    ncbi = NCBITaxa()
    taxid2name = ncbi.get_taxid_translator(taxIDs)

    return taxid2name
    
def get_allWGS_runInfo_fromSRA_forTaxIDs(fileprefix, taxIDs, reference_genome, replace=False, min_coverage=30):

    """This function tages all the SRRs that have WGS for the required taxIDs"""

    SRA_runInfo_df_file = "%s.SRA_runInfo_df.py"%fileprefix

    if file_is_empty(SRA_runInfo_df_file) or replace is True:

        # define the WGS fastq filters
        WGS_filters = '("biomol dna"[Properties] AND "strategy wgs"[Properties] AND "library layout paired"[Properties] AND "platform illumina"[Properties] AND "strategy wgs"[Properties] OR "strategy wga"[Properties] OR "strategy wcs"[Properties] OR "strategy clone"[Properties] OR "strategy finishing"[Properties] OR "strategy validation"[Properties])'


        # get the name of these taxIDs
        taxid2name = get_taxid2name(taxIDs)

        # define the esearch query
        organism_filters = " OR ".join(['"%s"[orgn:__txid%i]'%(name.split()[0], taxID) for taxID, name in taxid2name.items()])
        esearch_query = "(%s) AND %s"%(organism_filters, WGS_filters) 

        # get esearch
        efetch_outfile = "%s.efetch_output.txt"%fileprefix

        columns_efetch = "Run,ReleaseDate,LoadDate,spots,bases,spots_with_mates,avgLength,size_MB,AssemblyName,download_path,Experiment,LibraryName,LibraryStrategy,LibrarySelection,LibrarySource,LibraryLayout,InsertSize,InsertDev,Platform,Model,SRAStudy,BioProject,Study_Pubmed_id,ProjectID,Sample,BioSample,SampleType,TaxID,ScientificName,SampleName,g1k_pop_code,source,g1k_analysis_group,Subject_ID,Sex,Disease,Tumor,Affection_Status,Analyte_Type,Histological_Type,Body_Site,CenterName,Submission,dbgap_study_accession,Consent,RunHash,ReadHash".split(",")

        # if there are no runs, it will run an error
        esearch_efetch_stderr = "%s.generating.stderr"%efetch_outfile
        print("Querying the SRA database. This will throw an error if there are no results. The stderr is in %s"%esearch_efetch_stderr)
        run_cmd("esearch -db sra -query '%s' | efetch -db sra --format runinfo | egrep -v '^Run' | egrep 'https' > %s 2>%s"%(esearch_query, efetch_outfile, esearch_efetch_stderr))
        remove_file(esearch_efetch_stderr)

        SRA_runInfo_df = pd.read_csv(efetch_outfile, sep=",", header=None, names=columns_efetch)

        save_object(SRA_runInfo_df, SRA_runInfo_df_file)

    else: SRA_runInfo_df = load_object(SRA_runInfo_df_file)


    # define the expected coverage
    length_genome = sum(get_chr_to_len(reference_genome).values())
    SRA_runInfo_df["expected_coverage"] = SRA_runInfo_df.apply(lambda r: (r["spots_with_mates"]*r["avgLength"])/length_genome,axis=1)

    # plot the number of spots
    filename = "%s.distribution_parameters.pdf"%fileprefix
    fig = plt.figure(figsize=(5,13))
    for I, field in enumerate(["spots", "spots_with_mates", "avgLength", "InsertSize", "size_MB", "expected_coverage"]):
        ax = plt.subplot(6, 1, I+1)
        sns.distplot(SRA_runInfo_df[field], kde=False, rug=True)
        ax.set_xlabel(field)

    fig.tight_layout()  # otherwise the right y-label is slightly 
    fig.savefig(filename, bbox_inches='tight');
    plt.close(fig)

    # keep only those that have at least a coverage of min_coverage    
    SRA_runInfo_df = SRA_runInfo_df[SRA_runInfo_df["expected_coverage"]>=min_coverage]

    for field in ["AssemblyName", "SampleType", "TaxID"]: pass

    print("There are %i SRRs ready to use with at least %ix coverage"%(len(SRA_runInfo_df), min_coverage))

    return SRA_runInfo_df

def get_empiric_resisrance_breakpoint(metadata_df, drug, species, plot=True):

    """This function takes a drug and species and returns the breakpoint, based on the metadata_df (already of that species) 
    
    For some species it will return NaN because the bp can't be established.

    With some exceptions, we'll define as d
    """
    
    # get the df for the drug
    df = metadata_df[~pd.isna(metadata_df["%s_MIC"%drug])]

    # there should be at least 10 samples with the drug to get some estimate
    if len(df)<=10: breakpoint = np.nan
    
    else:

        # init plot
        if plot is True: 
            sns.distplot(df["%s_MIC"%drug], kde=False, hist=True)
            plt.title(species)

        # Candida albicas
        if species=="Candida_albicans":

            # in CAS we can just set the percentile
            if drug=="CAS": breakpoint = np.percentile(df["CAS_MIC"], 90)

        # add breakpoint
        if plot is True: plt.axvline(breakpoint, color="k", linestyle="--")
        

    return breakpoint

def get_metadata_df_with_cladeID(metadata_df, manually_curated_data):

    """Adds the clade ID from one study in each species"""

    print("adding clade ID")

    # define general things (mappings between the strain and the cladeIDs)
    Calbicans_strain_to_cladeID = {strain : str(cladeID) for strain, cladeID in dict(pd.read_excel("%s/SuppTableS7_Ropars2018_Calbicans.xlsx"%manually_curated_data).set_index("Strain")["Cluster"]).items() if cladeID!="NC"} # it has 181 strains

    Cglabrata_strain_to_cladeID = {sampleName : str(cglab_strain_to_clade[strain]) for sampleName, strain in dict(get_fitness_and_susceptibility_dfs_Carrete2017(manually_curated_data).set_index("SampleName")["strain"]).items()} # Laia's paper

    Cauris_cladeNameToCladeID = {"I":"1", "II":"2", "III":"3", "IV":"4"}
    Cauris_srr_to_cladeID = dict(pd.read_excel("%s/Chow2020_TableS1_strainsUsed.xlsx"%manually_curated_data, header=1).rename(columns={"FCZ* (AFST)":"FLC", "AMB* (AFST)":"AMB", "MCF* (AFST)":"MIF", "SRA Run":"SRR"}).set_index("SRR")["Clade"].map(Cauris_cladeNameToCladeID)) # Chow 20202 (300 strains)

    df_Obrien2021 = pd.read_excel("%s/TableS4_OBrien2021_Ctropicalis.xlsx"%manually_curated_data) # 77 strains clustered by PCA
    Ctropicalis_strain_to_cladeID = {}
    for I, r in df_Obrien2021.iterrows(): Ctropicalis_strain_to_cladeID = {**Ctropicalis_strain_to_cladeID, **{strain.lstrip().rstrip() : str(r["Cluster number"]) for strain in str(r["Isolates in cluster"]).split(",")}}

    Corthopsilosis_strain_to_cladeID = {'CAS08-0151': '3', 'CAS09-1540': '3', 'CAS09-1799': '1', 'CAS10-1825': '3', 'CAS08-0185': '3', 'CAS08-282': '4', 'CAS08-0320': '4', 'CP25': '3', 'CP47': '3', 'CP85': '1', 'CP124': '4', 'CP125': '2', 'CP185': '2', 'CP269': '2', 'CP287': np.nan, 'CP288': '3', 'CP289': '1', 'CP296': '2', 'CP331': '2', 'CP344': '3', 'CAS08-0498': '4', 'CAS08-0504': '2', 'CAS08-0599': '3', 'CAS09-0748': '4', 'CAS09-0831': '2', 'B-8274': '3', 'B-8323': '2'} # this is from "https://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1006404", which has the data in an image

    # define a function that gets the clade ID
    def get_cladeID_from_r(r):

        # some species have no clade ID
        if r.species_name in {"Candida_metapsilosis", "Candida_parapsilosis"}: return np.nan

        elif r.species_name=="Candida_albicans":

            if r.BioProject=="PRJNA432884" and r.strain in Calbicans_strain_to_cladeID: return Calbicans_strain_to_cladeID[r.strain] 
            else: return np.nan

        elif r.species_name=="Candida_glabrata":

            if r.BioProject=="PRJNA361477": return Cglabrata_strain_to_cladeID[r.strain]
            else: return np.nan

        elif r.species_name=="Candida_auris":

            if r.Run in Cauris_srr_to_cladeID: return Cauris_srr_to_cladeID[r.Run]
            else: return np.nan

        elif r.species_name=="Candida_tropicalis":

            if r.BioProject=="PRJNA604451" and r.strain in Ctropicalis_strain_to_cladeID: return Ctropicalis_strain_to_cladeID[r.strain]
            else: return np.nan

        elif r.species_name=="Candida_orthopsilosis":

            if r.BioProject=="PRJNA322245": return Corthopsilosis_strain_to_cladeID[r.strain]
            else: return np.nan

        else: raise ValueError("%s has not been considered"%r.species_name)

    metadata_df["cladeID_previousPaper"] = metadata_df.apply(get_cladeID_from_r, axis=1)

    # print stats
    for species in set(metadata_df.species_name): 
        df_s = metadata_df[metadata_df.species_name==species]
        print("There are %i/%i samples with cladeID in %s"%(sum(~pd.isna(df_s.cladeID_previousPaper)), len(df_s), species))

    return metadata_df

def get_tree_with_cladeID_from_supportAndBranchLength(tree, min_fraction_branchLen, min_branch_suport):

    """Takes a tree and returns it with the cladeID"""

    tree = cp.deepcopy(tree)

    # change the support of the children of the root
    for root_child in tree.get_children(): root_child.support=100

    # define the minimum branch length as the min_fraction_branchLen*(maximum distance to the root)
    max_dist_btw_leaves = max(map(lambda l: l.get_farthest_node()[1], tree.get_leaves()))
    min_branchLen = max_dist_btw_leaves*min_fraction_branchLen

    # define branches that could be clades (potential_clade)
    for n in tree.traverse():

        if n.is_leaf(): n.potential_clade = False
        elif not n.is_leaf() and n.support>=min_branch_suport and n.dist>=min_branchLen: n.potential_clade = True
        else: n.potential_clade = False

    # define the clades as those branches that could be potential clades and have no internal branch that could be it as well.
    current_clade = 1
    type_traverse = "preorder"
    for n in tree.traverse(type_traverse):

        # if you find a clade, define all the leafs of the node to that clade
        if n.potential_clade is True and all([child.potential_clade is False for child in n.traverse() if child!=n]): 

            # map all the children to this clade
            n.cladeID = current_clade
            for child in n.traverse(): child.cladeID = current_clade
            current_clade+=1

    # add missing clade IDs 
    for n in tree.traverse(type_traverse):

        # if the clade ID has not been assigned, set to nan
        if "cladeID" not in set(n.__dict__): n.cladeID = np.nan

    return tree

def plots_investigate_best_min_fraction_branchLen(tree, min_branch_suport, plots_prefix, best_min_fraction_branchLen, title_prefix, expected_nclades, linspace_range=(0.001, 0.2, 20)):

    """Plots the effect of different min_fraction_branchLen on the fraction of unassigned samples and the number of clades"""

    ######### get data ###########
    data_dict = {}; I=0
    for min_fraction_branchLen in (list(np.linspace(linspace_range[0], linspace_range[1], linspace_range[2])) + [best_min_fraction_branchLen]):
    #for min_fraction_branchLen in (list(np.linspace(0.0, 0.006, 30)) + [best_min_fraction_branchLen]): # C. albicans exploring a finer range

        print(min_fraction_branchLen)

        # get the tree with the clade IDs
        tree_withCladeIDs = get_tree_with_cladeID_from_supportAndBranchLength(tree, min_fraction_branchLen, min_branch_suport)
        leaf_to_clade = pd.Series({l.name : l.cladeID for l in tree_withCladeIDs.get_leaves()})

        fraction_samples_clade = 1-(sum(pd.isna(leaf_to_clade))/len(leaf_to_clade))
        nclades = len(set(leaf_to_clade[~pd.isna(leaf_to_clade)].values))

        data_dict[I] = {"min_fraction_branchLen":min_fraction_branchLen, "fraction_samples_clade":fraction_samples_clade, "nclades":nclades}; I+=1

    df_plot = pd.DataFrame(data_dict).transpose().sort_values(by="min_fraction_branchLen")

    print(df_plot)

    ##############################


    #### plot #####

    # define the title to include the selected number of clades and the numbers of samples with some clade
    selected_r = df_plot[df_plot.min_fraction_branchLen==best_min_fraction_branchLen].iloc[0]
    nsamples = len(tree.get_leaf_names())
    nsamples_with_clade = int(nsamples*selected_r.fraction_samples_clade)
    title = "%s\n#clades=%i; %i/%i (%.2f%s) samples with clade"%(title_prefix, int(selected_r.nclades), nsamples_with_clade, nsamples, selected_r.fraction_samples_clade*100, "%")

    # add plot
    ax = df_plot.plot(x="min_fraction_branchLen", y="fraction_samples_clade", legend=False, color="b")
    
    # add line with all
    plt.axhline(1.0, color="k", linestyle="--")

    # switch to ax2
    ax2 = ax.twinx()
    df_plot.plot(x="min_fraction_branchLen", y="nclades", ax=ax2, legend=False, color="r", linewidth=2)
    ax.set_title(title)

    # add best_min_fraction_branchLen
    plt.axvline(best_min_fraction_branchLen, color="k", linestyle="--")
    if expected_nclades!=0: plt.axhline(expected_nclades, color="k", linestyle="--")

    # change the axis
    ax.set_ylabel("fraction samples with clade", color="b")
    ax.spines['left'].set_color('b')
    ax.tick_params(axis='y', colors='b')

    ax2.set_ylabel("total # clades", color="r")
    ax2.spines['right'].set_color('r')
    ax2.tick_params(axis='y', colors='r')

    plt.show()

    # save
    fig = ax.get_figure()
    filename = "%s_choosingBestTshd.pdf"%plots_prefix
    fig.savefig(filename, bbox_inches="tight")

    print("saving %s"%filename)

    ###############


def get_metadata_df_with_cladeID_definedBy_branchSupports_and_diversity(metadata_df, species_to_tree, df_pairwise_SNP_distances, PlotsDir, min_branch_suport=95, make_plots=True, threads=4):

    """Gets a df with cladeID as defined by branch supports and branche length of the clade."""

    # define the plots dor
    plots_dir = "%s/investigate_Best_min_fraction_branchLen"%PlotsDir; make_folder(plots_dir)

    # init dict
    species_to_sampleID_to_cladeID = {}

    print("adding clade ID")
    for species in species_to_tree.keys():
        print(species)

        #if species!="Candida_orthopsilosis": continue

        # get the species tree
        tree = cp.deepcopy(species_to_tree[species])

        # define the expected number of clades
        species_to_expectedNclades = metadata_df.groupby("species_name").apply(lambda df_s: len(set(df_s[~pd.isna(df_s.cladeID_previousPaper)].cladeID_previousPaper)))

        # define the best species_to_min_fraction_branchLen (this was chosen from plots_investigate_best_min_fraction_branchLen below)
        best_min_fraction_branchLen = species_to_min_fraction_branchLen[species]

        # get the tree with the optimal min_fraction_branchLen, according to plots_investigate_best_min_fraction_branchLen
        tree = get_tree_with_cladeID_from_supportAndBranchLength(tree, best_min_fraction_branchLen, min_branch_suport)

        # map each leaf to the clade
        leaf_to_clade = {l.name : l.cladeID for l in tree.get_leaves()}
        species_to_sampleID_to_cladeID[species] = leaf_to_clade

        # map each sample to the previous clade
        metadata_df_s = metadata_df[metadata_df.species_name==species]
        sample_to_cladePreviousPaper = dict(metadata_df_s.set_index("sampleID").cladeID_previousPaper.apply(str))
        cladePreviousPaper_to_samples = dict(metadata_df[(metadata_df.species_name==species) & ~(pd.isna(metadata_df.cladeID_previousPaper))].groupby("cladeID_previousPaper").apply(lambda df_c: sorted(set(df_c.sampleID))))
        cladePreviousPaper_to_cladesOfSamples = {cprev : sorted([leaf_to_clade[s] for s in samples]) for cprev, samples in cladePreviousPaper_to_samples.items()}

        # plot the tree
        if make_plots is True:

            for color_tree in ["clades", "black"]:

                tree_plot = cp.deepcopy(tree)
                nclades = len(set([x for x in leaf_to_clade.values() if not pd.isna(x)]))

                print(nclades)

                if nclades<=10: palette = "tab10"
                else: palette = "tab20"
                clade_to_color = get_value_to_color(list(range(1, nclades+1)), palette=palette, n=nclades, type_color="hex")[0]

                # define the samples that are representative of a clade
                sample_to_clade_noNaNs = {l:c for l,c in leaf_to_clade.items() if not pd.isna(c)}
                tree_onlyClades = cp.deepcopy(tree_plot); tree_onlyClades.prune(set(sample_to_clade_noNaNs), preserve_branch_length=True)
                clade_to_representativeSamples = get_clade_to_representative_samples_from_tree(tree_onlyClades, sample_to_clade_noNaNs, threads, nrepresentative_samples=5)

                representative_samples_of_clade = set(make_flat_listOflists(clade_to_representativeSamples.values()))

                # map to color
                def get_mostfrequent_element_in_list(list_obj): return sorted([(x, list_obj.count(x)) for x in list_obj if not(pd.isna(x))], key=(lambda y: y[1]))[-1][0]

                cladePreviousPaper_to_color = {cprev : clade_to_color[get_mostfrequent_element_in_list(clades)] for cprev, clades in cladePreviousPaper_to_cladesOfSamples.items()}

                for n in tree_plot.traverse():

                    if species in {"Candida_albicans", "Candida_glabrata", "Candida_auris", "Candida_tropicalis"}: bwidth = 18
                    elif species in {"Candida_metapsilosis", "Candida_parapsilosis", "Candida_orthopsilosis"}: bwidth = 8

                    nst = NodeStyle()
                    nst["hz_line_width"] = bwidth
                    nst["vt_line_width"] = bwidth
                    nst["size"] = 0


                    if color_tree=="clades":

                        if pd.isna(n.cladeID): branch_color = "gray"
                        else: branch_color = clade_to_color[n.cladeID]

                    elif color_tree=="black": branch_color = "gray"

                    nst["hz_line_color"] = branch_color
                    nst["vt_line_color"] = branch_color
                    n.set_style(nst)

                    if n.is_leaf() and color_tree=="clades": 

                        # add the defined clade
                        if pd.isna(leaf_to_clade[n.name]): 
                            color_leaf = "gray"; label = ""
                        else: 
                            color_leaf = clade_to_color[leaf_to_clade[n.name]]; label = str(n.cladeID)
                        n.add_face(RectFace(100, 20, fgcolor="black", bgcolor=color_leaf, label={"text":label, "color":"black"}), position="aligned", column=0)

                        # add the previous clade
                        col_representative = 1
                        if int(n.name) not in sciName_to_badSamples[species]:
                            clade_previous = sample_to_cladePreviousPaper[n.name]

                            if clade_previous!="nan": 

                                n.add_face(RectFace(100, 20, fgcolor="black", bgcolor=cladePreviousPaper_to_color[clade_previous], label={"text":clade_previous, "color":"black"}), position="aligned", column=1)

                                col_representative = 2

                        # add whether it is representative of the clade
                        """
                        if n.name in representative_samples_of_clade: n.add_face(RectFace(50, 20, fgcolor="black", bgcolor=color_leaf, label={"text":"R", "color":"black"}), position="aligned", column=col_representative)
                        """


                ts = TreeStyle()
                ts.show_branch_length = False
                ts.show_branch_support = False
                ts.show_leaf_name = True
                
                # cicular orientation
                ts.mode = "c"
                ts.root_opening_factor = 1
                ts.arc_start = -180 # 0 degrees = 3 o'clock
                ts.arc_span = 180    
                
                #tree_plot.show(tree_style=ts)
                tree_plot.render(file_name="%s/%s_%s_tree.pdf"%(plots_dir, species, color_tree), tree_style=ts)

    # add to the metadata df
    print("adding cladeID to metadata_df")
    metadata_df["cladeID_Tree_and_BranchLen"] = metadata_df[["species_name", "sampleID"]].apply(lambda r: species_to_sampleID_to_cladeID[r.species_name][r.sampleID], axis=1)

    return metadata_df

def get_tree_withcladeID_clonal(tree, snps_kb_series, clonal_threshold_SNPs_kb):

    """Adds the cladeID to the tree"""

    # keep tree
    tree = cp.deepcopy(tree)


    # check that all the comparisons are there
    all_comparisons = set(itertools.product(tree.get_leaf_names(), tree.get_leaf_names()))
    if set(snps_kb_series.index)!=all_comparisons: raise ValueError("all comparisons from the tree should be in snps_kb_series")

    # define the minimal branch support to define a clade
    min_branch_suport = 95

    # define the set of compared samples that could be part of the same clonal group. Samples that are more far away can't be part of the same clonal group
    compared_samples_same_clonal_clade = set(snps_kb_series[snps_kb_series<=clonal_threshold_SNPs_kb].index)

    # change the support of the children of the root
    for root_child in tree.get_children(): root_child.support=100

    # define a function that, given a node, calculates whether 
    def get_node_can_be_clonal_clade(n): 
        sorted_leafs = n.get_leaf_names()
        all_combinations_leafs = set(itertools.product(sorted_leafs, sorted_leafs))
        combinatiions_not_sample_clonal_clade = all_combinations_leafs.difference(compared_samples_same_clonal_clade)
        if len(combinatiions_not_sample_clonal_clade)==0: return True
        else: return False 

    # define branches that could be clades (potential_clade)
    for n in tree.traverse():

        if n.is_leaf(): n.potential_clade = False
        elif not n.is_leaf() and n.support>=min_branch_suport and get_node_can_be_clonal_clade(n): n.potential_clade = True
        else: n.potential_clade = False

    # define the clades as those branches that could be potential clades and have no higher order branches that could be it
    current_clade = 1
    type_traverse = "postorder" # 1) Traverse the left subtree , 2) Traverse the right subtree, 3) Visit the root
    for n in tree.traverse(type_traverse):

        # if a node is a potential clade, only set it as a clade if there is not an upper node that could be a clade
        if n.potential_clade is True and all([a.potential_clade is False for a in n.get_ancestors() if a!=n]): 

            # map all the children to this clade
            n.cladeID = current_clade
            for child in n.traverse(): child.cladeID = current_clade
            current_clade+=1

    # add missing clade IDs 
    for n in tree.traverse(type_traverse):

        # if the clade ID has not been assigned, set to nan
        if "cladeID" not in set(n.__dict__): n.cladeID = np.nan

    return tree


def get_metadata_df_with_cladeID_clonal(metadata_df, species_to_tree, df_pairwise_SNP_distances_all, PlotsDir, threads=4):

    """adds a field (cladeID_clonal), which groups together samples that are all close (below a certain threshold). """

    #species_to_tree = {s:t for s,t in species_to_tree.items() if s=="Candida_parapsilosis"}

    print("running get_metadata_df_with_cladeID_clonal")

    # map each species to the snps_kb_series
    species_to_snps_kb_series_file = "%s/get_metadata_df_with_cladeID_clonal_species_to_snps_kb_series.py"%PlotsDir
    if file_is_empty(species_to_snps_kb_series_file):

        species_to_snps_kb_series = {}

        for species in species_to_tree.keys():

            # set the pairwise SNP distance
            fields_df = ["query_sampleID", "target_sampleID", "expected_vars_per_bp"]
            df_pairwise_SNP_distances = df_pairwise_SNP_distances_all[df_pairwise_SNP_distances_all.species==species][fields_df]
            print(len(df_pairwise_SNP_distances))
            df_pairwise_SNP_distances = df_pairwise_SNP_distances.append(df_pairwise_SNP_distances.rename(columns={"query_sampleID":"target_sampleID", "target_sampleID":"query_sampleID"}))
            df_pairwise_SNP_distances["sorted_tuple_comparison"] = df_pairwise_SNP_distances[["query_sampleID", "target_sampleID"]].applymap(str).apply(tuple, axis=1)
            df_pairwise_SNP_distances["SNPs_kb"] = df_pairwise_SNP_distances.expected_vars_per_bp * 1000
            species_to_snps_kb_series[species] = df_pairwise_SNP_distances[["sorted_tuple_comparison", "SNPs_kb"]].set_index("sorted_tuple_comparison").SNPs_kb

        save_object(species_to_snps_kb_series, species_to_snps_kb_series_file)

    species_to_snps_kb_series = load_object(species_to_snps_kb_series_file)

    # benchmark the threshold
    all_species = [s for s in sorted_species_byPhylogeny if s in species_to_tree]

    print("creating df plot")
    df_plot_file = "%s/get_metadata_df_with_cladeID_clonal_df_plot.py"%PlotsDir
    if file_is_empty(df_plot_file):

        df_plot = pd.DataFrame()
        for Is, species in enumerate(all_species):
            print(species)

            for clonal_threshold_SNPs_kb in [0.1, 0.5, 1, 2, 3, 4, 5, 6, 7, 8, 9,  10]:

                # get tree and map each sample to clade
                tree_tshd = get_tree_withcladeID_clonal(species_to_tree[species], species_to_snps_kb_series[species], clonal_threshold_SNPs_kb)
                sample_to_clade_tshd = pd.Series({l.name : l.cladeID for l in tree_tshd.get_leaves()})
                nclades = len(set(sample_to_clade_tshd[~pd.isna(sample_to_clade_tshd)]))
                fraction_samples_no_clade = sum(pd.isna(sample_to_clade_tshd))/len(sample_to_clade_tshd)


                df_plot = df_plot.append(pd.DataFrame({0:{"nclades":nclades, "nsamples":len(sample_to_clade_tshd), "fraction_samples_no_clade":fraction_samples_no_clade, "species":species, "clonal_threshold_SNPs_kb":clonal_threshold_SNPs_kb}}).transpose()).reset_index(drop=True)

        save_object(df_plot, df_plot_file)

    df_plot = load_object(df_plot_file)

    for f in ["clonal_threshold_SNPs_kb", "fraction_samples_no_clade", "nclades"]: df_plot[f] = df_plot[f].apply(float)

    print("plotting")
    for yfield in ["fraction_samples_no_clade", "nclades"]:

        fig = plt.figure()
        ax = sns.lineplot(data=df_plot, x="clonal_threshold_SNPs_kb", y=yfield, hue="species", style="species", markers=True, dashes=False)
        ax.legend(loc='upper left', bbox_to_anchor=(1.05, 0.5))
        if yfield=="nclades": ax.set_yscale("log")

    # keep results for different thresholds
    for tshd in [1, 2, 3, 4]:

        # init df
        species_to_sampleID_to_cladeID = {}

        for species in all_species:
            print(species, tshd)

            # get the tree and the clade mapping
            tree = get_tree_withcladeID_clonal(species_to_tree[species], species_to_snps_kb_series[species], tshd)

            # map each leaf to the clade
            leaf_to_clade = {l.name : l.cladeID for l in tree.get_leaves()}
            species_to_sampleID_to_cladeID[species] = leaf_to_clade

            ####### PLOT CLADE ASSIGNATION #########

            # map each clade to a color
            nclades = len(set([x for x in leaf_to_clade.values() if not pd.isna(x)]))
            clade_to_color = get_value_to_color(list(range(1, nclades+1)), palette="tab10", n=nclades, type_color="hex")[0]

            # go through each node
            for n in tree.traverse():

                # define the width
                if species in {"Candida_albicans", "Candida_glabrata", "Candida_auris", "Candida_tropicalis"}: 
                    bwidth = 18
                    fontsize = 30

                elif species in {"Candida_metapsilosis", "Candida_parapsilosis", "Candida_orthopsilosis"}: 
                    bwidth = 8
                    fontsize = 12

                # define the color
                if pd.isna(n.cladeID): 
                    branch_color = "gray"
                    bwidth = bwidth*0.25

                else: branch_color = clade_to_color[n.cladeID]

                nst = NodeStyle()
                nst["hz_line_width"] = bwidth
                nst["vt_line_width"] = bwidth
                nst["size"] = 0
                nst["hz_line_color"] = branch_color
                nst["vt_line_color"] = branch_color
                n.set_style(nst)


            ts = TreeStyle()
            ts.show_branch_length = False
            ts.show_branch_support = False
            ts.show_leaf_name = False

            # add title
            ts.title.add_face(TextFace("%s max pairwise SNPs/kb=%s"%(species, tshd), bold=False, fsize=fontsize), column=0)

            
            # cicular orientation
            ts.mode = "c"
            ts.root_opening_factor = 1
            ts.arc_start = -180 # 0 degrees = 3 o'clock
            ts.arc_span = 180    
            
            plots_dir = "%s/trees_clonal_clades"%PlotsDir; make_folder(plots_dir)
            tree.render(file_name="%s/%s_tshdSNPsKb=%s_tree.pdf"%(plots_dir, species, tshd), tree_style=ts)

 
            ########################################


        # add the data to metadata df
        metadata_df["cladeID_clonal_tshdSNPsKb_%s"%tshd] = metadata_df[["species_name", "sampleID"]].apply(lambda r: species_to_sampleID_to_cladeID[r.species_name][r.sampleID], axis=1)


    return metadata_df

def get_df_metadata_with_resistance_and_others(metadata_df, manually_curated_data, species_to_srr_to_sampleID, DataDir, admixture_Qs_df, ProcessedDataDir, df_pairwise_SNP_distances, species_to_tree, PlotsDir, df_coverage_per_gene, replace=False, make_plots=True, threads=4):

    """This functions takes a df_metadata and defines the resistance state of each sample, also for the combination of azoles and echinocandins. The breakpoints are extracted from https://www.eucast.org/fileadmin/src/media/PDFs/EUCAST_files/AFST/Clinical_breakpoints/AFST_BP_v10.0_200204.pdf"""

    # define final file
    metadata_df_withAdds_file = "%s/metadata_df_with_resistance_and_others.py"%ProcessedDataDir


    if file_is_empty(metadata_df_withAdds_file) or replace is True:
        print("running get_df_metadata_with_resistance_and_others")

        ######### DEFINE VARIABLES #########

        # define the classes of drugs
        azoles = ["FLC", "POS", "VRC", "IVZ", "ITR", "KET", "CLZ", "MIZ"]
        echinocandins = ["MIF", "ANI", "CAS"]
        other_drugs = ["AMB", "UCA", "BVN", "5FC", "TRB"]
        all_drugs = set(azoles + echinocandins + other_drugs)

        drug_to_name = {'KET': 'ketoconazole', 'FLC': 'fluconazole', 'MIF': 'micafungin', 'CAS': 'caspofungin', 'UCA': 'undecanoic acid', 'MIZ': 'miconazole', 'ITR': 'itraconazole', 'AMB': 'amphotericin B', 'POS': 'posaconazole', 'ANI': 'anidulafungin', 'BVN': 'beauvericin', '5FC': '5-flucytosine', 'TRB': 'terbinafine', 'CLZ': 'clotriminazole', 'VRC': 'voriconazole', 'IVZ': 'isavuconazole'}

        # load the EUCAST breakpoints
        df_eucast_bps = pd.read_excel("%s/Eucast_MIC_breakpoints_perSpecies.xlsx"%manually_curated_data).set_index("species").applymap(float)

        # print the bioprojects that have some MIC measured for each drug and species:
        """
        for drug in all_drugs:
            for species in sorted(set(metadata_df.species_name)):
                mic_f = "%s_MIC"%drug
                df = metadata_df[(metadata_df.species_name==species) & ~(pd.isna(metadata_df[mic_f])) & (metadata_df.type=="clinical")]
                if len(df)>0 and pd.isna(df_eucast_bps.loc[species, drug]): 
                    bp_to_nsamples = dict(df.groupby("BioProject").apply(len))
                    sorted_bioprojects = list(reversed(sorted(bp_to_nsamples.keys(), key=(lambda x: bp_to_nsamples[x]))))
                    print("\n%s-%s: %s"%(species, drug, ", ".join(sorted_bioprojects[0:4])))
                    #print("%s-%s"%(drug_to_name[drug], species))
        """

        # merge EUCAST and manual MICs
        df_eucast_bps.columns = [c.split("_")[0] for c in df_eucast_bps.columns]
        df_manual_MICs = pd.read_excel("%s/ManualMIC_breakpoints.xlsx"%manually_curated_data).pivot(index="species", columns="drug", values="breakpoint").applymap(float)

        species_to_drug_to_breakpoint = {}
        for species in sorted(set(metadata_df.species_name)):
            for drug in all_drugs:

                # define bps
                bp_eucast = df_eucast_bps.loc[species, drug]
                if drug in df_manual_MICs.columns and species in df_manual_MICs.index: bp_manual = df_manual_MICs.loc[species, drug]
                else: bp_manual = np.nan

                if not pd.isna(bp_eucast): bp = bp_eucast
                elif not pd.isna(bp_manual): bp = bp_manual
                else: continue

                species_to_drug_to_breakpoint.setdefault(species, {}).setdefault(drug, bp)

        ####################################

        ####### ADD RESISTANCE ##########

        # go through each drug
        def get_resistance(r, drug):

            # get the resistance and susceptibilities
            if not pd.isna(r.resistance): resistance_drugs = {x for x in r.resistance.split("+") if x in all_drugs}
            else: resistance_drugs = set()

            if not pd.isna(r.susceptibility): susceptible_drugs = {x for x in r.susceptibility.split("+") if x in all_drugs}
            else: susceptible_drugs = set()

            # define whether the susceptibility and resistance
            if drug in resistance_drugs and drug in susceptible_drugs: raise ValueError("the same drug can't be in R and S")
            
            if drug in resistance_drugs: reported_resistance = True
            else: reported_resistance = False

            if drug in susceptible_drugs: reported_susceptibility = True
            else: reported_susceptibility = False

            # get the MIC
            mic = r["%s_MIC"%drug]

            if pd.isna(mic) and reported_resistance is False and reported_susceptibility is False: return np.nan
            else:

                # if there is a mic, prioritize it
                if not pd.isna(mic):

                    # get the resistance breakpoint
                    bp = species_to_drug_to_breakpoint[r["species_name"]][drug]
                    if mic>=(2*bp): return "R"
                    elif mic<=(0.5*bp): return "S"
                    else: return "I"

                elif reported_resistance is True: return "R"
                elif reported_susceptibility is True: return "S"
                else: raise ValueError("something went wrong in %s"%r)

        for drug in all_drugs: metadata_df["%s_resistance"%drug]  = metadata_df.apply(get_resistance, drug=drug, axis=1)

        # add the resistance to the classes of drugs
        def get_susceptibility_several_drugs(r, drugs, type_resistance, any_or_all):

            # get all booleans
            all_resistance_booleans = [r["%s_resistance"%d]==type_resistance for d in drugs if not pd.isna(r["%s_resistance"%d])]

            if len(all_resistance_booleans)==0: return np.nan
            else: return any_or_all(all_resistance_booleans)

        for type_resistance in ["R", "I", "S"]:
            for any_or_all_fn, any_or_all_name in [(any, "any"), (all, "all")]:
                for drugs, drugs_name in [(echinocandins, "echinocandin"), (azoles, "azole"), (["AMB"], "ampB"), (["5FC"], "5FC")]:

                    metadata_df["%s_%s_%s"%(any_or_all_name, drugs_name, type_resistance)] = metadata_df.apply(get_susceptibility_several_drugs, drugs=drugs,  type_resistance=type_resistance, any_or_all=any_or_all_fn, axis=1)
       
        #################################

        ########### ADD OTHER THINGS ##############

        # add the number of timepoints available in each patient with timepoints
        def get_clinical_patient_ID(r):
            if pd.isna(r.patient_ID) or r.type!="clinical": return np.nan
            else: return r.species_name + "_" + r.BioProject + "_" + r.patient_ID
        metadata_df["clinical_patient_ID"] = metadata_df.apply(get_clinical_patient_ID, axis=1)

        patient_to_n_timepoints = metadata_df[~(pd.isna(metadata_df.clinical_patient_ID)) & ~(pd.isna(metadata_df.timepoint))][["clinical_patient_ID", "timepoint"]].drop_duplicates().groupby("clinical_patient_ID").apply(len)
        metadata_df["ntimepoints_per_patient"] = metadata_df.clinical_patient_ID.map(patient_to_n_timepoints)
        print("number of samples with at least two isolates from the same patient", Counter(metadata_df[metadata_df.ntimepoints_per_patient>1].species_name))


        # add the clade ID from previous papers
        metadata_df = get_metadata_df_with_cladeID(metadata_df, manually_curated_data)

        # remove bad srrs
        metadata_df = metadata_df[~(metadata_df.Run.isin(srrs_to_remove))]

        # remove srrs that are not in the species_to_srr_to_sampleID
        all_srrs_withSampleID = set.union(*[set(srr_to_sampleID) for srr_to_sampleID in species_to_srr_to_sampleID.values()])
        metadata_df = metadata_df[metadata_df.Run.isin(all_srrs_withSampleID)]

        # add the numeric sampleID
        metadata_df["sampleID"] = metadata_df.apply(lambda r: species_to_srr_to_sampleID[r.species_name][r.Run], axis=1)

        # remove bad samples
        sciName_to_badSamplesStrs = {sciName : {str(x) for x in bad_samples} for sciName, bad_samples in sciName_to_badSamples.items()}
        metadata_df = metadata_df[metadata_df.apply(lambda r: str(r.sampleID) not in sciName_to_badSamplesStrs[r.species_name], axis=1)]

        # add clade ID based on clonal clusters
        metadata_df = get_metadata_df_with_cladeID_clonal(metadata_df, species_to_tree, df_pairwise_SNP_distances, PlotsDir, threads=threads)

        # add the cladeID as defined by branch supports and diversity
        metadata_df = get_metadata_df_with_cladeID_definedBy_branchSupports_and_diversity(metadata_df, species_to_tree, df_pairwise_SNP_distances, PlotsDir, make_plots=make_plots, threads=threads)

        # add the popID for different K values
        metadata_df  = get_metadata_df_with_popID_variousKs(metadata_df, admixture_Qs_df, species_to_tree, PlotsDir, make_plots=make_plots)

        ###########################################

        # add the fraction of genes with >95% covered
        print("adding fraction genes covered")
        df_coverage_per_gene["sampleID"] = df_coverage_per_gene.sampleID.apply(str)
        species_to_ngenes = df_coverage_per_gene.reset_index(drop=True)[["species", "ID"]].drop_duplicates().groupby("species").apply(len)
        metadata_df["ngenes_species"] = metadata_df.species_name.map(species_to_ngenes); check_no_nans_series(metadata_df["ngenes_species"])

        ngenes_covered_df = pd.DataFrame({"ngenes_covered_above95pct" : df_coverage_per_gene.reset_index(drop=True)[["species", "sampleID", "percentcovered_1"]].groupby(["species", "sampleID"]).apply(lambda df_s: sum(df_s.percentcovered_1>=95) )}).reset_index().rename(columns={"species":"species_name"})
        metadata_df = metadata_df.merge(ngenes_covered_df, on=["species_name", "sampleID"], validate="one_to_one", how="left")
        metadata_df["fraction_genes_covered_above95pct"] = metadata_df.ngenes_covered_above95pct / metadata_df.ngenes_species

        # add the coverage per gene
        print("adding median coverage per gene")
        median_coverage_df = pd.DataFrame({"median_median_reads_per_gene" : df_coverage_per_gene.reset_index(drop=True)[["species", "sampleID", "median_reads_per_gene"]].groupby(["species", "sampleID"]).apply(lambda df_s: np.median(df_s.median_reads_per_gene))}).reset_index().rename(columns={"species":"species_name"})
        metadata_df = metadata_df.merge(median_coverage_df, on=["species_name", "sampleID"], validate="one_to_one", how="left")

        # checks
        check_no_nans_series(metadata_df.median_median_reads_per_gene)
        check_no_nans_series(metadata_df.fraction_genes_covered_above95pct)
        if any(metadata_df.fraction_genes_covered_above95pct>1) or any(metadata_df.fraction_genes_covered_above95pct==0): raise ValueError("fraction_genes_covered_above95pct should be btw 0 and 1")

        # save
        save_object(metadata_df, metadata_df_withAdds_file)

    metadata_df = load_object(metadata_df_withAdds_file)

    return metadata_df

def get_dir(filename): return "/".join(filename.split("/")[0:-1])

def get_file(filename): return filename.split("/")[-1]


def download_srr_with_prefetch(srr, SRRfile, replace=False):

    """This function downloads an srr file for an srr if not already done"""

    # define the downloading dir
    downloading_dir = get_dir(SRRfile)

    # make the downloading dir
    make_folder(downloading_dir)

    # define the std files
    prefetch_std = "%s.std.txt"%SRRfile
    prefetch_std_copy = "%s.copy"%prefetch_std

    # try a couple of times
    for Itry in range(2):

        if file_is_empty(SRRfile) or replace is True:
            print("running prefetch for %s"%srr)

            # remove the locks of previous runs
            for file in ["%s/%s"%(downloading_dir, f) for f in os.listdir(downloading_dir) if ".lock" in f or ".tmp." in f]: remove_file(file)

            # remove the actual srr
            remove_file(SRRfile)

            # run prefetch
            print("running prefetch. The std can be found in %s"%prefetch_std)
            try: run_cmd("prefetch -o %s --max-size 500G --progress 1 %s > %s 2>&1"%(SRRfile, srr, prefetch_std))
            except: print("prefetch did not work for %s"%srr)

            # test that the std of prefetch states that there are no unresolved dependencies
            std_lines = open(prefetch_std, "r").readlines()
            successful_download = any(["was downloaded successfully" in l for l in std_lines])
            no_dependencies_left = any(["has 0 unresolved dependencies" in l for l in std_lines])
            has_dependencies_line = any(["unresolved dependencies" in l for l in std_lines])

            if not successful_download or (has_dependencies_line and not no_dependencies_left): 
                run_cmd("cp %s %s"%(prefetch_std, prefetch_std_copy))
                print("prefetch did not work for %s. Test the log in %s"%(srr, prefetch_std_copy))
                remove_file(SRRfile)

    # check that the prefetch works 
    if file_is_empty(SRRfile): 
        raise ValueError("prefetch did not work for %s"%srr)

    remove_file(prefetch_std)
    remove_file(prefetch_std_copy)

    return SRRfile


def generate_jobarray_file(jobs_filename, name):
    
    """ 
    This function takes a jobs filename and replaces it creating the necessary STD files. These will be set to be run in a cluster environment
    """

    # define the stddir
    outdir = get_dir(jobs_filename)
    stddir = "%s/STDfiles"%outdir; 
    delete_folder(stddir)
    make_folder(stddir)

    # remove previous rst files
    name_jobs_filename = get_file(jobs_filename)
    for file in os.listdir(get_dir(jobs_filename)): 

        if file.startswith("%s-"%name_jobs_filename) and file.endswith(".rst"): 
            remove_file("%s/%s"%(get_dir(jobs_filename), file))

    # rewrite the jobs_filename so that each std goes to a different file
    std_perJob_prefix = "%s/%s"%(stddir, name)
    jobs_filename_lines = ["%s > %s.%i.out 2>&1"%(l.strip(), std_perJob_prefix, I+1) for I, l in enumerate(open(jobs_filename, "r").readlines())]

    # redefine the parent dir to be in MN
    jobs_filename_lines = [c.replace(ParentDir, "/gpfs/projects/bsc40/mschikora") for c in jobs_filename_lines]

    # rewrite
    open(jobs_filename, "w").write("\n".join(jobs_filename_lines))

    print("You need to successfully run all jobs in %s to continue"%jobs_filename)

   


def run_jobarray_file_Nord3_greasy(jobs_filename, name, time="12:00:00", queue="bsc_ls", threads_per_job=4, RAM_per_thread=1800, nodes=4, submit=True):

    """
    This function takes a jobs filename and creates a jobscript with args (which is a list of lines to be written to the jobs cript). It works in Nord3 for greasy    

    """

    # define dirs
    outdir = get_dir(jobs_filename)
    stddir = "%s/STDfiles"%outdir; 

    delete_folder(stddir); make_folder(stddir)

    # define the std files
    greasy_logfile = "%s/%s_greasy.log"%(stddir, name)
    stderr_prefix = "%s/%s_stderr"%(stddir, name)
    stdout_prefix = "%s/%s_stdout"%(stddir, name)

    # define the job script
    jobs_filename_run = "%s.run"%jobs_filename

    # define the number of jobs in the job array
    njobs = len(open(jobs_filename, "r").readlines())

    # change the time
    time = ":".join(time.split(":")[0:2])

    # define the number of jobs per node (each node has 16 threads)
    threads_per_node = 16
    if (threads_per_node%threads_per_job)!=0: raise ValueError("the 16/threads_per_job should be an integer number")
    jobs_per_node = int(threads_per_node/threads_per_job)

    # define the number of nodes to be running at once
    jobs_ran_at_once = nodes*jobs_per_node


    # define the RAM per job
    RAM_per_job = threads_per_job*RAM_per_thread

    # define the arguments
    arguments = [ "#!/bin/sh",
                  "#BSUB -J greasy_%s"%name,
                  "#BSUB -eo %s-%sJ.err"%(stderr_prefix, "%"),
                  "#BSUB -oo %s-%sJ.out"%(stdout_prefix, "%"),
                  "#BSUB -cwd %s"%outdir,
                  "#BSUB -W %s"%time,
                  "#BSUB -q %s"%queue,
                  "#BSUB -n %i"%jobs_ran_at_once, # the number of jobs to be running at once
                  '#BSUB -R "span[ptile=%i]"'%jobs_per_node, # the number of jobs per node
                  "#BSUB -M %i"%RAM_per_job, # the ram for each job in Mb
                  "#BSUB -x",
                  "",
                  "module load greasy",
                  "export OMP_NUM_THREADS=%i"%threads_per_job,
                  "export GREASY_LOGFILE=%s;"%(greasy_logfile),
                  "echo 'running pipeline';",
                  "greasy %s"%jobs_filename
                ]

    # replace by the mn
    arguments = [a.replace(ParentDir, "/gpfs/projects/bsc40/mschikora") for a in arguments] # this is only for mschikora

    # define and write the run filename
    with open(jobs_filename_run, "w") as fd: fd.write("\n".join(arguments))
    
    # run in cluster if specified
    if submit is True: run_cmd("bsub < %s"%jobs_filename_run)


def run_jobarray_file_Nord3(jobs_filename, name, time="12:00:00", queue="bsc_ls", threads_per_job=4, RAM_per_thread=1800, max_njobs_to_run=1000, submit=True):

    """Runs jobs_filename in Nord3"""

   # define dirs
    outdir = get_dir(jobs_filename)
    stddir = "%s/STDfiles"%outdir; 

    delete_folder(stddir); make_folder(stddir)

    # define the std files
    stderr_file = "%s/%s_stderr.txt"%(stddir, name)
    stdout_file = "%s/%s_stdout.txt"%(stddir, name)

    # define the job script
    jobs_filename_run = "%s.run"%jobs_filename

    # define the number of jobs in the job array
    njobs = len(open(jobs_filename, "r").readlines())

    # change the tome
    time = ":".join(time.split(":")[0:2])

    # define the number of jobs to run at once, maximum 1000
    njobs_to_run = min([max_njobs_to_run, njobs])

    arguments =  ["#!/bin/sh",
                  "#BSUB -e  %s"%stderr_file,
                  "#BSUB -o %s"%stdout_file,
                  "#BSUB -cwd %s"%outdir,
                  "#BSUB -W %s"%time,
                  "#BSUB -q %s"%queue,
                  "#BSUB -n %i"%threads_per_job, # the number of processes
                  "#BSUB -M %i"%RAM_per_thread, # the ram per thread in Mb
                  "#BSUB -J %s[1-%i]"%(name, njobs_to_run), # the job array
                  "",
                  "cmdFile=%s/command.$LSB_JOBINDEX &&"%stddir, # define the command file
                  "head -n $LSB_JOBINDEX %s | tail -n 1 > $cmdFile &&"%jobs_filename, # create cmdFile
                  'bash $cmdFile && echo "$cmdFile finished well"' # execute it
                  ]

    # '#BSUB -R "span[ptile=16]"', "export OMP_NUM_THREADS=16"
    # replace by the mn
    arguments = [a.replace(ParentDir, "/gpfs/projects/bsc40/mschikora") for a in arguments]
    
    # define and write the run filename
    with open(jobs_filename_run, "w") as fd: fd.write("\n".join(arguments))
    
    # run in cluster if specified
    print("Submiting jobfile to the cluster from stddir %s"%(stddir))
    if submit is True: run_cmd("bsub < %s"%jobs_filename_run)

def run_jobarray_file_MN4_greasy_ListJobFiles(jobs_filenames_list, name, time="12:00:00", queue="bsc_ls", threads_per_job=4, nodes=1, submit=True):

    """
    This function takes a list of jobs filenames jobs filename and creates a jobscript with args (which is a list of lines to be written to the jobs cript). It works in MN4 for greasy    

    """

    # define the first jobs_filename
    first_jobs_filename = jobs_filenames_list[0]

    # define dirs
    outdir = get_dir(first_jobs_filename)
    stddir = "%s/STDfiles"%outdir; 
    delete_folder(stddir); make_folder(stddir)
    os.chdir(outdir)

    # define the job script
    jobs_filename_run = "%s/jobs.%s.run"%(outdir, name)

    # define the std files of all the jobs
    stderr_file = "%s/%s_stderr.txt"%(stddir, name)
    stdout_file = "%s/%s_stdout.txt"%(stddir, name)

    # define the number of jobs in the job array (this is based on the first job)
    njobs = int("".join([x for x in str(subprocess.check_output("wc -l %s"%first_jobs_filename, shell=True)).split()[0] if x.isdigit()])) + 1

    # define the requested nodes. Each node has 48 threads
    max_nodes = max([int((njobs*threads_per_job)/48), 1])
    requested_nodes = min([nodes, max_nodes])

    # define the number of tasks
    max_ntasks = int((requested_nodes*48)/threads_per_job)
    ntasks = min([njobs, max_ntasks])


    # init the general arguments
    arguments = [ "#!/bin/sh",
                  "#SBATCH --error=%s"%stderr_file,
                  "#SBATCH --output=%s"%stdout_file,
                  "#SBATCH --job-name=%s"%name, 
                  "#SBATCH --get-user-env",
                  #"#SBATCH --workdir=%s"%outdir,
                  "#SBATCH --time=%s"%time,
                  "#SBATCH --qos=%s"%queue,
                  "#SBATCH --cpus-per-task=%i"%threads_per_job,
                  "#SBATCH --ntasks=%i"%ntasks,
                  "",
                  "module load greasy"]

    # add the jobs_filenames_list
    for Ichunk, jobs_filename in enumerate(jobs_filenames_list):

        # define the log file
        greasy_logfile = "%s/%s_greasy.chunk%i.log"%(stddir, name, Ichunk+1)

        # add the run of this chunk
        arguments.append("export GREASY_LOGFILE=%s && greasy %s"%(greasy_logfile, jobs_filename))

    # replace by the run in MN
    arguments = [a.replace(ParentDir, "/gpfs/projects/bsc40/mschikora") for a in arguments]

    # define and write the run filename
    with open(jobs_filename_run, "w") as fd: fd.write("\n".join(arguments))
    
    # run in cluster if specified
    if submit is True: run_cmd("sbatch %s"%jobs_filename_run)



def run_jobarray_file_MN4_greasy(jobs_filename, name, time="12:00:00", queue="bsc_ls", threads_per_job=4, nodes=1, submit=True):

    """
    This function takes a jobs filename and creates a jobscript with args (which is a list of lines to be written to the jobs cript). It works in MN4 for greasy    

    """

    # define dirs
    outdir = get_dir(jobs_filename)
    stddir = "%s/STDfiles"%outdir; 
    delete_folder(stddir); make_folder(stddir)
    os.chdir(outdir)

    # define the std files
    greasy_logfile = "%s/%s_greasy.log"%(stddir, name)
    stderr_file = "%s/%s_stderr.txt"%(stddir, name)
    stdout_file = "%s/%s_stdout.txt"%(stddir, name)

    # define the job script
    jobs_filename_run = "%s.run"%jobs_filename

    # define the number of jobs in the job array
    njobs = int("".join([x for x in str(subprocess.check_output("wc -l %s"%jobs_filename, shell=True)).split()[0] if x.isdigit()]))

    # define the requested nodes. Each node has 48 threads
    max_nodes = max([int((njobs*threads_per_job)/48), 1])
    requested_nodes = min([nodes, max_nodes])

    # define the number of tasks
    max_ntasks = int((requested_nodes*48)/threads_per_job)
    ntasks = min([njobs, max_ntasks])

    # define the arguments
    arguments = [ "#!/bin/sh",
                  "#SBATCH --error=%s"%stderr_file,
                  "#SBATCH --output=%s"%stdout_file,
                  "#SBATCH --job-name=%s"%name, 
                  "#SBATCH --get-user-env",
                  #"#SBATCH --workdir=%s"%outdir,
                  "#SBATCH --time=%s"%time,
                  "#SBATCH --qos=%s"%queue,
                  "#SBATCH --cpus-per-task=%i"%threads_per_job,
                  "#SBATCH --ntasks=%i"%ntasks,
                  "",
                  "module load greasy",
                  "export GREASY_LOGFILE=%s;"%(greasy_logfile),
                  "echo 'running pipeline';",
                  "greasy %s"%jobs_filename
                ]

    # replace by the run in MN
    arguments = [a.replace(ParentDir, "/gpfs/projects/bsc40/mschikora") for a in arguments]

    # define and write the run filename
    with open(jobs_filename_run, "w") as fd: fd.write("\n".join(arguments))
    
    # run in cluster if specified
    if submit is True: run_cmd("sbatch %s"%jobs_filename_run)




def run_jobarray_file_CTE_KNL_greasy(jobs_filename, name, time="12:00:00", queue="bsc_ls", threads_per_job=4, nodes=1, submit=True):

    """
    This function takes a jobs filename and creates a jobscript with args (which is a list of lines to be written to the jobs cript). It works in CTE-KNL for greasy. Each node has 64 cores. We can run 256 cores, so 4 nodes at max.

    """

    # define dirs
    outdir = get_dir(jobs_filename)
    stddir = "%s/STDfiles"%outdir; 

    delete_folder(stddir); make_folder(stddir)

    # define the std files
    greasy_logfile = "%s/%s_greasy.log"%(stddir, name)
    stderr_file = "%s/%s_stderr.txt"%(stddir, name)
    stdout_file = "%s/%s_stdout.txt"%(stddir, name)

    # define the job script
    jobs_filename_run = "%s.run"%jobs_filename

    # define the number of jobs in the job array
    njobs = len(open(jobs_filename, "r").readlines())

    # define the requested nodes. Each node has 64 threads
    max_nodes = max([int((njobs*threads_per_job)/64), 1])
    requested_nodes = min([nodes, max_nodes])

    # define the number of tasks
    max_ntasks = int((requested_nodes*64)/threads_per_job)
    ntasks = min([njobs, max_ntasks])


    # define the arguments
    arguments = [ "#!/bin/sh",
                  "#SBATCH --partition=knl",
                  "#SBATCH --error=%s"%stderr_file,
                  "#SBATCH --output=%s"%stdout_file,
                  "#SBATCH --job-name=%s"%name, 
                  "#SBATCH --get-user-env",
                  "#SBATCH --workdir=%s"%outdir,
                  "#SBATCH --time=%s"%time,
                  "#SBATCH --qos=%s"%queue,
                  "#SBATCH --cpus-per-task=%i"%threads_per_job,
                  "#SBATCH --ntasks=%i"%ntasks,
                  "",
                  "module load intel impi GREASY/2.1.2.1",
                  "export OMP_NUM_THREADS=%i"%threads_per_job, # USE_SIMPLE_THREADED_LEVEL3=1
                  "export GREASY_LOGFILE=%s;"%(greasy_logfile),
                  "echo 'running pipeline';",
                  "greasy %s"%jobs_filename
                ]

    # replace by the run in MN
    arguments = [a.replace(ParentDir, "/gpfs/projects/bsc40/mschikora") for a in arguments]

    # define and write the run filename
    with open(jobs_filename_run, "w") as fd: fd.write("\n".join(arguments))
    
    # run in cluster if specified
    if submit is True: run_cmd("sbatch %s"%jobs_filename_run)



def get_coverage_df_for_sorted_bam(sorted_bam):

    """Returns a coverage df from a sorted bam"""

    # define the dir
    coverage_file_dir = "%s.calculating_windowcoverage"%sorted_bam

    # define the files    
    coverage_files = ["%s/%s"%(coverage_file_dir, f) for f in os.listdir(coverage_file_dir) if f.startswith("coverage_windows_") and f.endswith("bp.tab")]
    if len(coverage_files)!=1: 

        print(coverage_files)
        print("WARNING: There should be only one coverage file dir")

    df = pd.read_csv(coverage_files[0], sep="\t")

    return df

def get_coverage_df_several_srrs(srr_to_sorted_bam, filename, taxID, sciName, replace=False):

    """Generates a coverage df and saves it into filename"""

    if file_is_empty(filename) or replace is True:

        coverage_dict_all = {}
        for srr, sorted_bam in srr_to_sorted_bam.items():

            # get the coverage df
            df_coverage = get_coverage_df_for_sorted_bam(sorted_bam)

            # add to dict
            coverage_dict_all[srr] = {"taxID":taxID, "sciName":sciName, "srr":srr, "mean_coverage":np.mean(df_coverage.mediancov_1), "pct_covered":np.mean(df_coverage.percentcovered_1)}

        # get df
        df_coverage_stats = pd.DataFrame(coverage_dict_all).transpose()

        # save
        save_df_as_tab(df_coverage_stats, filename)

    df_coverage_stats = get_tab_as_df_or_empty_df(filename)

    return df_coverage_stats



def run_cmd_anyEnv(cmd, env):

    """This function runs a cmd with a given env"""

    # define the cmds
    CondaDir = "/gpfs/projects/bsc40/mschikora/anaconda3"
    SOURCE_CONDA_CMD = "source %s/etc/profile.d/conda.sh"%CondaDir
    cmd_prefix = "%s && conda activate %s &&"%(SOURCE_CONDA_CMD, env)

    # define the running
    cmd_to_run = "%s %s"%(cmd_prefix, cmd)

    # run
    out_stat = os.system(cmd_to_run) 
    if out_stat!=0: raise ValueError("\n%s\n did not finish correctly. Out status: %i"%(cmd_to_run, out_stat))


def get_repeat_maskerDF_from_perSVade(reference_genome, perSVade_dir, threads=4, replace=False):

    """Gets the repeat masker df from perSVade"""

    # define the table 
    repeats_table_file = "%s.repeats.tab"%reference_genome

    # run perSVade to get repeats

    if file_is_empty(repeats_table_file) or replace is True: 

        cmds = ["import sys",
                "sys.path.append(%s)"%perSVade_dir,
                "import sv_functions as fun",
                "fun.get_repeat_maskerDF('%s', threads=%i, replace=True)"%(reference_genome, threads)]

        cmd = ";".join(cmds)

        run_cmd_anyEnv(cmd, "perSVade_env")


    return repeats_table_file

def load_df_coverage_per_gene(CurDir, filename, species_to_srr_to_sampleID, replace=False):

    """Loads the coverage per gene for all C. mine datasets"""

    if file_is_empty(filename) or replace is True:

        # init df
        df_coverage_all = pd.DataFrame()

        for taxID, sciName in taxID_to_sciName.items():
            print(sciName)

            # get the df of the coverage and append
            df_coverage = pd.read_csv("%s/data/%s_%i/integrated_varcalls/merged_coverage.tab"%(CurDir, sciName, taxID), sep="\t")
            df_coverage["species"] = sciName

            # keep only some fields
            df_coverage = df_coverage[["species", "ID", "sampleID", "chromosome", "start", "end", "median_reads_per_gene", "relative_coverage", "percentcovered_1"]]
    
            df_coverage_all = df_coverage_all.append(df_coverage)

        # keep only a few cols
        df_coverage_all = df_coverage_all.sort_values(by=["species", "chromosome", "start", "end"])

        # save
        print("saving")
        save_object(df_coverage_all, filename)

    print("loading coverage")
    df_coverage_all = load_object(filename)

    return df_coverage_all

def plot_relative_coverage_per_gene(df_coverage, species_to_ref_genome, filename, figsize=(10, 10), srrs_to_outline=set()):

    """Makes subplots for each species of the coverage per gene"""

    all_species = sorted(set(df_coverage.species))
    fig = plt.figure(figsize=figsize)

    for Is, species in enumerate(all_species):
        print(species)

        # init subplot
        ax = plt.subplot(len(all_species), 1, Is+1)

        # get the df of the species
        df = df_coverage.loc[species]

        # get the df of the species and gDNA
        df = df[df.chromosome!=species_to_mtChromosome[species]]

        # get srrs that have a simley face effect
        #print("calculate smiley")
        chrom_to_len = get_chr_to_len(species_to_ref_genome[species])
        #print("There are %i chromosomes"%len(chrom_to_len))

        df["dist_btw_start_and_endChrom"] =  df.chromosome.map(chrom_to_len) - df.start
        df["raw_distance_to_telomere"] = df[["start", "dist_btw_start_and_endChrom"]].apply(min, axis=1)

        def get_df_srr_has_smiley(df_srr): 
            r_spearman, p_spearman = stats.spearmanr(df_srr.raw_distance_to_telomere, df_srr.relative_coverage, nan_policy="raise")
            return (r_spearman<-0.1 and p_spearman<0.05)

        srr_to_isSmiley = df.groupby("srr").apply(get_df_srr_has_smiley)

        # keep only srrs with smiley
        srrs_with_smiley = set(srr_to_isSmiley[srr_to_isSmiley].index)
        print("There are %i/%i srrs with smiley"%(len(srrs_with_smiley), len(srr_to_isSmiley)))
        if len(df)>0:

            # add the offset
            all_chromosomes = sorted(set(df.chromosome))

            chrom_to_Xoffset = {}
            current_offset = 0
            for chrom in all_chromosomes:
                chrom_to_Xoffset[chrom] = current_offset
                current_offset += chrom_to_len[chrom] + 10000

            df["Xoffset"] = df.chromosome.map(chrom_to_Xoffset)
            df["plot_xpos"] = df.start + df.Xoffset


            # define the y
            df["relative_coverage_smooth"] = df.relative_coverage.rolling(50).median()

            # make one line plot for each chromosome
            df_plot = df[df.srr.isin(srrs_with_smiley)]
            for chrom in all_chromosomes: sns.lineplot(x="plot_xpos", y="relative_coverage_smooth", data=df_plot[df_plot.chromosome==chrom], hue="srr", linewidth=.3)

            # add the data for srrs_to_outline
            for srr_outline in srrs_to_outline:
                if srr_outline not in set(df.srr): continue
                df_srr = df[(df.srr==srr_outline)]
            
                for chrom in all_chromosomes: sns.lineplot(x="plot_xpos", y="relative_coverage_smooth", data=df_srr[df_srr.chromosome==chrom], linewidth=2.8, hue="srr", palette={srr_outline:"black"})


            # add a vline at each chromosome
            for xoffset in chrom_to_Xoffset.values(): plt.axvline(xoffset, color="k", linestyle="--", linewidth=1.3)
            for y in [1, 2]: plt.axhline(y, color="gray", linestyle="--", linewidth=1.3)

            # remove legend
            ax.get_legend().remove()

        # add titles
        ax.set_title("%s (%i chromosomes, %i samples)"%(species, len(chrom_to_len), len(srrs_with_smiley)))
        ax.set_ylabel("rel. coverage")
        if Is==(len(all_species)-1): ax.set_xlabel("position in the genome (bp)")
        else: ax.set_xlabel("")

    fig.tight_layout()  # otherwise the right y-label is slightly 
    fig.savefig(filename, bbox_inches='tight');
    #plt.close(fig)

def get_mosdepth_coverage_for_positions(sorted_bam, coverage_file, positions_df, chrom_field, replace=False, threads=threads):

    """Gets the coverage of the positions_df (1-based) into coverage_file from sorted_bam with mosdepth"""

    if file_is_empty(coverage_file) or replace is True:

        print("getting coverage into %s"%coverage_file)

        # define the prefix
        fileprefix = "%s.MDout"%coverage_file

        # get the positions bed
        windows_file = "%s.windows.tab"%fileprefix
        positions_df["end_POS"] = positions_df.POS+1
        positions_df[[chrom_field, "POS", "end_POS"]].to_csv(windows_file, sep="\t", header=False, index=False)

        # run mosdepth
        mosdepth_std = "%s.std"%fileprefix
        run_cmd("mosdepth --threads %i --by %s --no-per-base --fast-mode  %s %s > %s 2>&1"%(threads, windows_file, fileprefix, sorted_bam, mosdepth_std)) # mosdepth does not look at internal cigar operations or correct mate overlaps (recommended for most use-cases). It is also faster

        # remove all files that don't contribute to coverage
        for sufix in ["mosdepth.global.dist.txt", "mosdepth.region.dist.txt", "mosdepth.summary.txt", "regions.bed.gz.csi", "std", "windows.tab"]: remove_file("%s.%s"%(fileprefix, sufix))

        # unzip
        run_cmd("gunzip %s.regions.bed.gz"%fileprefix)

        # keep the unzipped as the coverage file
        os.rename("%s.regions.bed"%fileprefix, coverage_file)

    # load
    df_coverage = pd.read_csv(coverage_file, sep="\t", names=[chrom_field, "POS", "POS_plus1", "coverage"])

    return df_coverage

def get_coverage_per_positions_from_vars_df(vars_df, coverage_df_file, outdir, df_inputs, threads):

    """Generates a df written in coverage_df_file with the coverage of all positions in vars_df. df_inputs should have sorted bam"""

    if file_is_empty(coverage_df_file):
        print("getting coverage per position")

        # define a unique df
        positions_df = vars_df[["#CHROM", "POS"]].drop_duplicates().sort_values(by=["#CHROM", "POS"])

        # run mosdepth to get the coverage for each sample
        all_samples = sorted(set(df_inputs.sampleID))

        # define the mosdepth_outdir
        coverage_files_dir = "%s/coverage_files"%outdir; make_folder(coverage_files_dir)

        # rename the input
        df_inputs = df_inputs.set_index("sampleID")

        # run mosdepth in parallel
        inputs_fn = []
        for sampleID in all_samples:

            # define the sorted bam
            sorted_bam = df_inputs.loc[sampleID, "sorted_bam"]
            if file_is_empty(sorted_bam): raise ValueError("%s is not found"%sorted_bam)

            # define the coverage file
            coverage_file = "%s/%s.coverage.%s.tab"%(coverage_files_dir, get_file(coverage_df_file), sampleID)

            # get the inputs
            inputs_fn.append((sorted_bam, coverage_file, cp.deepcopy(positions_df), "#CHROM", False, 1))

        # run the function in parallel
        print("running get_mosdepth_coverage_for_positions in parallel")
        with multiproc.Pool(threads) as pool:

            list_coverage_dfs = pool.starmap(get_mosdepth_coverage_for_positions, inputs_fn)
            pool.close()
            pool.terminate()

        # add to positions_df
        print("integrating")
        for Is, coverage_df_raw in enumerate(list_coverage_dfs):

            # define vars
            sampleID = all_samples[Is]
            coverage_df = coverage_df_raw.rename(columns={"coverage":sampleID})[["#CHROM", "POS", sampleID]]

            # add to positions_df
            positions_df = positions_df.merge(coverage_df, on=["#CHROM", "POS"], validate="one_to_one")

        # clean
        delete_folder(coverage_files_dir)

        # save
        print("saving")
        save_df_as_tab(positions_df, coverage_df_file)


    # load
    positions_df = get_tab_as_df_or_empty_df(coverage_df_file)

    return positions_df

def generate_multifasta_correct_positions_for_tree(vars_df, positions_df, multifasta_correct_SNPs, df_inputs, min_coverage_pos=15):

    """Takes a variants df and a df with the coverage and generates a multifasta with aligned variable positions that have a coverage of at least 15 in all of them."""

    if file_is_empty(multifasta_correct_SNPs):
        print("getting multifasta of the correct SNPs")

        # define vars
        all_samples = sorted(set(df_inputs.sampleID))

        # keep the positions that have at least min_coverage_pos reads covering in all samples
        print("getting correct positions")
        positions_df = positions_df[(positions_df[all_samples]>=min_coverage_pos).apply(all, axis=1)]
        print("There are %i positions with coverage >%i in all samples"%(len(positions_df), min_coverage_pos))

        # keep the vars df that are in that positions
        positions_df = positions_df[["#CHROM", "POS"]]
        positions_df["chrom_and_pos"] = positions_df["#CHROM"] + "_" + positions_df.POS.apply(str)
        vars_df["chrom_and_pos"] =  vars_df["#CHROM"] + "_" + vars_df.POS.apply(str)
        vars_df = vars_df[vars_df.chrom_and_pos.isin(set(positions_df.chrom_and_pos))].set_index("sampleID")

        # map each chrom_pos to the reference
        chromPos_to_ref = dict(vars_df[["chrom_and_pos", "REF"]].drop_duplicates().set_index("chrom_and_pos")["REF"])
        positions_df["REF"] = positions_df.chrom_and_pos.apply(lambda x: chromPos_to_ref[x])
        actg = {"A", "C", "T", "G"}
        all_samples_withVars = set(vars_df.index)

        # add the nucelotide that is in each position for each sample
        for I, sampleID in enumerate(all_samples): 
            print(I, sampleID)

            # merge so that the positions without SNPs have NaNs
            if sampleID in all_samples_withVars: positions_df = positions_df.merge(vars_df.loc[{sampleID}][["chrom_and_pos", "ALT"]], on="chrom_and_pos", how="left", validate="one_to_one").rename(columns={"ALT":sampleID})

            else: positions_df[sampleID] = np.nan

            # correct, so that the NaNs get the reference allele
            def get_corrected_nt(r):
                if pd.isna(r[sampleID]): return r["REF"]
                else: return r[sampleID]
            positions_df[sampleID] = positions_df.apply(get_corrected_nt, axis=1)

            # debug
            if any(~positions_df[sampleID].isin(actg)): raise ValueError("all positions should be ACTG")

        # sort by chromosome and position
        positions_df = positions_df.sort_values(by=["#CHROM", "POS"])

        # generate the multifasta and write
        all_records = [SeqRecord(Seq("".join(positions_df[sampleID].values)), id=sampleID, name="", description="") for sampleID in all_samples]
        multifasta_correct_SNPs_tmp = "%s.tmp"%multifasta_correct_SNPs
        SeqIO.write(all_records, multifasta_correct_SNPs_tmp, "fasta")
        os.rename(multifasta_correct_SNPs_tmp, multifasta_correct_SNPs)

def get_series_ALTsequence(sampleID, positions_df, snps_df, pickRandomHetSNPs):

    """This function takes a positions_df and the samples with snps and returns a series where the index is the chrom_and_pos and the value is an ACTG alternative allele. snps_df should have [["chrom_and_pos", "ALT"]] """

    # report
    print("working on sample %s"%sampleID)
    
    # redefine snps_df to keep one random heterozygous SNP
    if pickRandomHetSNPs is True:

        # define the number of het SNPs
        n_hetSNPs = sum(snps_df.common_GT=="0/1")
        n_nonHet_SNPs = sum(snps_df.common_GT!="0/1")

        # get the SNPs so that heterozygous SNPs get randomly kept
        def get_boolean_for_GT(gt):
            if gt=="1/1": return True
            elif gt=="0/1": return bool(random.getrandbits(1))
            else: raise ValueError("GT not defined: %s"%gt) 

        snps_df = snps_df[snps_df.common_GT.apply(get_boolean_for_GT)]

        # print the fraction of heterozygous SNPs kept
        print("You kept %i/%i het SNPs and %i/%i other SNPs in sample %s"%(sum(snps_df.common_GT=="0/1"), n_hetSNPs, sum(snps_df.common_GT!="0/1"), n_nonHet_SNPs, sampleID))

    # debug
    if len(snps_df)==0: raise ValueError("there are no snps in %s"%sampleID)

    # keep only the first SNP of each position if it is multiallelic
    snps_df = snps_df.sort_values(by=["chrom_and_pos", "ALT"]).drop_duplicates(subset=["chrom_and_pos"], keep="first")

    # check that the positions are unique
    if len(snps_df)!=len(set(snps_df.chrom_and_pos)): raise ValueError("there are some repeated snps_df.chrom_and_pos")

    # define the series so that the positions without SNPs have NaNs
    initial_len_positions_df = len(positions_df)
    positions_df = positions_df.merge(snps_df[["chrom_and_pos", "ALT"]], on="chrom_and_pos", how="left").rename(columns={"ALT":sampleID})

    # debug merge
    if len(positions_df)!=initial_len_positions_df: raise ValueError("some missing positions in positions_df")

    # report
    print("there are %i/%i positions with snps in sample %s"%(sum(~pd.isna(positions_df[sampleID])), len(positions_df), sampleID))

    # correct, so that the NaNs get the reference allele
    def get_corrected_nt(r):
        if pd.isna(r[sampleID]): return r["REF"]
        else: return r[sampleID]
    positions_df[sampleID] = positions_df.apply(get_corrected_nt, axis=1)

    # debug
    actg = {"A", "C", "T", "G"}
    if any(~positions_df[sampleID].isin(actg)): raise ValueError("all positions should be ACTG")

    return positions_df.set_index("chrom_and_pos")[sampleID]

def generate_multifasta_from_snps_df(snps_df, multifasta_correct_SNPs, all_samples, threads=4, pickRandomHetSNPs=False, generate_one_aln_each_chrom=False):

    """Takes a snps df and a df with SNPs and generates a multifasta of them."""

    # define a df that will have all the seqs
    positions_df_file = "%s.positions_df.py"%multifasta_correct_SNPs

    if file_is_empty(multifasta_correct_SNPs):
        print("getting multifasta of the correct SNPs on %i threads"%threads)

        # check that all the samples are in all_samples
        strange_samples = set(snps_df.index).difference(set(all_samples))
        if len(strange_samples)>0: raise ValueError("there are some undefined samples in snps_df: %s"%strange_samples)

        # define a positions_df with all the positions with some SNP
        positions_df = snps_df[["#CHROM", "POS"]].drop_duplicates().sort_values(by=["#CHROM", "POS"])
        positions_df["chrom_and_pos"] = positions_df["#CHROM"] + "_" + positions_df.POS.apply(str)

        # map each chrom_pos to the reference
        snps_df["chrom_and_pos"] =  snps_df["#CHROM"] + "_" + snps_df.POS.apply(str)
        chromPos_to_ref = dict(snps_df[["chrom_and_pos", "REF"]].drop_duplicates().set_index("chrom_and_pos")["REF"])
        positions_df["REF"] = positions_df.chrom_and_pos.apply(lambda x: chromPos_to_ref[x])

        # define samples with vars
        all_samples_withVars = set(snps_df.index)

        # get list of ALT sequences in parallel
        snps_df = snps_df[["chrom_and_pos", "ALT", "common_GT"]]
        inputs_fn = [(sampleID, positions_df, snps_df.loc[{sampleID}], pickRandomHetSNPs) for sampleID in all_samples if sampleID in all_samples_withVars]

        print("getting SNPs series")
        if threads>1:

            with multiproc.Pool(threads) as pool:

                list_ALTseries = pool.starmap(get_series_ALTsequence, inputs_fn)
                pool.close()
                pool.terminate()

        else: list_ALTseries = list(map(lambda x: get_series_ALTsequence(x[0], x[1], x[2], x[3]), inputs_fn))

        # add the alts to positions_df
        print("adding series to positions df")
        positions_df = positions_df.sort_values(by=["#CHROM", "POS"]).set_index("chrom_and_pos", drop=False)
        for series in list_ALTseries: positions_df[series.name] = series

        # add the samples with no vars
        for s in set(all_samples).difference(all_samples_withVars): positions_df[s] = positions_df.REF

        # write the positions of the fasta
        save_object(positions_df, positions_df_file)

        # save the alignment for each chromosome
        if generate_one_aln_each_chrom is True:

            for chrom in set(positions_df["#CHROM"]):
                print("saving multifasta for %s"%chrom)

                positions_df_chrom = positions_df[positions_df["#CHROM"]==chrom]
                all_records = [SeqRecord(Seq("".join(positions_df_chrom[sampleID].values)), id=str(sampleID), name="", description="") for sampleID in all_samples]
                
                SeqIO.write(all_records, "%s.%s.fasta"%(multifasta_correct_SNPs, chrom), "fasta")

        # generate the multifasta and write
        print("saving records of the concatenated alignment")
        all_records = [SeqRecord(Seq("".join(positions_df[sampleID].values)), id=str(sampleID), name="", description="") for sampleID in all_samples]
        multifasta_correct_SNPs_tmp = "%s.tmp"%multifasta_correct_SNPs
        SeqIO.write(all_records, multifasta_correct_SNPs_tmp, "fasta")
        os.rename(multifasta_correct_SNPs_tmp, multifasta_correct_SNPs)

    return positions_df_file

def get_multifasta_onlyVariableSites(positions_df_file, multifasta, sorted_samples, replace=False):

    """Gets a df positions, such as returned by generate_multifasta_from_snps_df, and writes multifasta """

    if file_is_empty(multifasta) or replace is True:

        # load the positions df and sort
        print("loading positions df")
        positions_df = load_object(positions_df_file)

        # sort 
        positions_df = positions_df.sort_values(by=["#CHROM", "POS"])

        # keep only variable positions
        total_npositions = len(positions_df)
        positions_df["n_different_nucleotides"] = positions_df[sorted_samples].apply(pd.unique, axis=1).apply(len)
        positions_df = positions_df[positions_df.n_different_nucleotides>1]
        print("there are %i/%i variable sites in the alignment"%(len(positions_df), total_npositions))

        # generate the multifasta and write
        print("saving records of the concatenated alignment only for variable sites")
        all_records = [SeqRecord(Seq("".join(positions_df[sampleID].values)), id=str(sampleID), name="", description="") for sampleID in sorted_samples]
        multifasta_tmp = "%s.tmp"%multifasta
        SeqIO.write(all_records, multifasta_tmp, "fasta")
        os.rename(multifasta_tmp, multifasta)

        del positions_df

def create_sequence_dict(genome, replace=False):

    """Takes a fasta and generates the reference dict"""

    rstrip = genome.split(".")[-1]
    dictionary = "%sdict"%(genome.rstrip(rstrip)); tmp_dictionary = "%s.tmp"%dictionary

    if file_is_empty(dictionary) or replace is True:

        # remove any previously created tmp_file
        remove_file(tmp_dictionary)

        # define the std
        dictionary_std = "%s.generating.std"%dictionary
        print("Creating picard dictionary. The std is in %s"%dictionary_std)

        run_cmd("picard CreateSequenceDictionary R=%s O=%s TRUNCATE_NAMES_AT_WHITESPACE=true > %s 2>&1"%(genome, tmp_dictionary, dictionary_std), env="perSVade_env_picard_env") 

        remove_file(dictionary_std)  
        os.rename(tmp_dictionary , dictionary)


def generate_fasta_with_subset_positions(input_fasta, output_fasta, positions_bed):

    """Takes a genome and keeps only the positions in positions_bed"""

    if file_is_empty(output_fasta):
        print("generating subset positions")

        # get the fasta of each position
        fasta_all_positions = "%s.all_positions.fasta"%output_fasta
        run_cmd("bedtools getfasta -fi %s -bed %s > %s"%(input_fasta, positions_bed, fasta_all_positions), env="perSVade_env")

        # rewrite so that there is only the chromosome name as ID
        def get_seq_with_CorrectID(seq): 
            seq.id = seq.id.split(":")[0]
            seq.name = ""
            seq.description = ""
            return seq

        all_records = list(map(get_seq_with_CorrectID, SeqIO.parse(fasta_all_positions, "fasta")))
        SeqIO.write(all_records, fasta_all_positions, "fasta")

        # collapse records of the same type
        output_fasta_tmp = "%s.tmp"%output_fasta
        cmd_get_fasta_with_positions = 'cat %s | paste - - | datamash -g 1 collapse 2 | tr -d "," | tr "\t" "\n" > %s'%(fasta_all_positions, output_fasta_tmp)
        run_cmd(cmd_get_fasta_with_positions)

        # check that all the chromosomes are there as expected
        chroms_input = set(get_chr_to_len(input_fasta))
        chroms_output_tmp = set(get_chr_to_len(output_fasta_tmp))

        if chroms_input!=chroms_output_tmp: raise ValueError("ERROR in %s\n. chroms_input is %s and chroms_output_tmp is %s. They should be the same"%(output_fasta_tmp, chroms_input, chroms_output_tmp))

        # check that the file is not empty
        if file_is_empty(output_fasta_tmp): raise ValueError("output_fasta_tmp can't be empty")

        remove_file(fasta_all_positions)
        os.rename(output_fasta_tmp, output_fasta)

def get_alternative_genome_FastaAlternateReferenceMaker(sampleID, snps_df, reference_genome, tmpdir, pickRandomHetSNPs, positions_bed):

    """Takes a df with SNPs and generates a multifasta with the genome changed, only positions in intervals_file"""

    # define files
    vcf_file = "%s/sample_%s_variants.vcf"%(tmpdir, sampleID)
    alternative_fasta = "%s.transformed_seq.fasta"%vcf_file

    # get the alternative genome
    if file_is_empty(alternative_fasta):

        if len(snps_df)==0: rsync_file(reference_genome, alternative_fasta)

        else:

            # remove randomly half of the hetero SNPs if indicated
            if pickRandomHetSNPs is True:

                # define the number of het SNPs
                n_hetSNPs = sum(snps_df.common_GT=="0/1")
                n_nonHet_SNPs = sum(snps_df.common_GT!="0/1")

                # get the SNPs so that heterozygous SNPs get randomly kept
                def get_boolean_for_GT(gt):
                    if gt in {"1/1", "1"}: return True
                    elif gt in {"0/1"}: return bool(random.getrandbits(1))
                    else: raise ValueError("GT not defined: %s"%gt) 

                snps_df = snps_df[snps_df.common_GT.apply(get_boolean_for_GT)]

                # print the fraction of heterozygous SNPs kept
                print("You kept %i/%i het SNPs and %i/%i other SNPs in sample %s"%(sum(snps_df.common_GT=="0/1"), n_hetSNPs, sum(snps_df.common_GT!="0/1"), n_nonHet_SNPs, sampleID))

            # add vcf fields
            vcf_fields = ["#CHROM", "POS", "ID", "REF", "ALT", "QUAL", "FILTER", "INFO", "FORMAT", "DATA"]
            snps_df["ID"] = snps_df["#Uploaded_variation"]
            snps_df["QUAL"] = 1000
            snps_df["FILTER"] = "PASS"
            snps_df["INFO"] = "."
            snps_df["FORMAT"] = "GT"
            snps_df["DATA"] = 1

            def get_upper(x): return x.upper()
            snps_df["REF"] = snps_df.REF.apply(get_upper)
            snps_df["ALT"] = snps_df.ALT.apply(get_upper)

            # write to vcf
            vcf_df = snps_df[vcf_fields].drop_duplicates().sort_values(by=["#CHROM", "POS"])
            vcf_lines = vcf_df.to_csv(sep="\t", header=False, index=False)
            header_lines = "\n".join(["##fileformat=VCFv4.2", "\t".join(vcf_fields)])
            open(vcf_file, "w").write(header_lines + "\n" + vcf_lines)

            # index vcf
            vcf_file_gz, vcf_file_gx_tbi = get_bgzip_and_and_tabix_vcf_file(vcf_file, replace=True)

            # get the alternative genome        
            alternative_fasta_tmp = "%s.tmp.fasta"%alternative_fasta
            make_alternative_fasta_std = "%s.generating.std"%alternative_fasta

            print("getting alternative genome. STD in %s"%make_alternative_fasta_std)
            run_cmd("gatk FastaAlternateReferenceMaker -R %s -O %s -V %s > %s 2>&1"%(reference_genome, alternative_fasta_tmp, vcf_file_gz, make_alternative_fasta_std), env="perSVade_env")

            # check that the fasta is correct and change the names
            print("changing names and testing that everything went well")
            chrom_to_refSeq = {seq.id : seq for seq in SeqIO.parse(reference_genome, "fasta")}
            chrom_to_len = get_chr_to_len(reference_genome)

            all_records = []
            all_new_chroms = set()
            for seq in SeqIO.parse(alternative_fasta_tmp, "fasta"):

                # change the seqID
                seq.id = seq.description.split()[1].split(":")[0]
                seq.name = ""
                seq.description = ""

                # keep
                all_new_chroms.add(seq.id)
                all_records.append(seq)

                # check that the length is the same
                if len(seq)!=chrom_to_len[seq.id]: raise ValueError("there are different lengths in chrom %s"%(seq.id))

                # check that the variant positions match those of the SNPs
                alternative_seq = pd.Series(list(seq.seq), index=range(1, len(seq)+1))
                ref_seq = pd.Series(list(chrom_to_refSeq[seq.id].seq), index=range(1, len(seq)+1))

                real_variable_positions = set(ref_seq[alternative_seq!=ref_seq].index)
                expected_variable_positions = set(snps_df[snps_df["#CHROM"]==(seq.id)].POS)

                strange_real_variable_positions = real_variable_positions.difference(expected_variable_positions)
                if len(strange_real_variable_positions)>0: 

                    # define a df with the strange_real_variable_positions, keeping the ones were the REF is ACGT
                    df_seqs = pd.DataFrame({"REF":ref_seq.loc[strange_real_variable_positions], "ALT":alternative_seq.loc[strange_real_variable_positions]}).sort_values(by=["REF", "ALT"])

                    df_seqs = df_seqs[df_seqs.REF.isin({"A", "C", "T", "G"})]

                    # if there are some of these positons, raise ERROR
                    if len(df_seqs)>0:

                        print("ERROR: There are %i unexpected variable positions in chromosome %s. These are the sequences found in these positions:\n%s"%(len(df_seqs), seq.id, df_seqs))

                        raise ValueError("there are unexpected variable positions")

                missing_expected_variable_positions = expected_variable_positions.difference(real_variable_positions)
                if len(missing_expected_variable_positions)>0: raise ValueError("There are some variable positions that are missing %s: %s"%(seq.id, missing_expected_variable_positions))

            # check that the defined chromosomes are correct
            if all_new_chroms!=set(chrom_to_len): raise ValueError("there are some unexpected chromosomes")

            # write
            SeqIO.write(all_records, alternative_fasta_tmp, "fasta")

            # clean
            remove_file(make_alternative_fasta_std)
            os.rename(alternative_fasta_tmp, alternative_fasta)
        
    # get a subset of the alternative genome in positions_bed
    alternative_fasta_subset = "%s.onlySubsetPositions.fasta"%alternative_fasta
    generate_fasta_with_subset_positions(alternative_fasta, alternative_fasta_subset, positions_bed)

    print("the alternative multifasta generation for sample %s went well"%sampleID)

    return alternative_fasta_subset

def generate_multifasta_from_snps_df_file_FastaAlternateReferenceMaker(snps_df_file, reference_genome, multifasta, sorted_samples, threads=4, expected_GTs={"1/1", "0/1"}, pickRandomHetSNPs=False):

    """Generates a multifasta with the concatenated reference genome (whole sequence) and the snps file. This fasta contains only positions that have some SNP in snps_df_file."""

    if file_is_empty(multifasta):
        print("generatig multifasta")

        ######### FILTER SNPS #########

        # define a tmpdir
        tmpdir = "%s.generating"%multifasta
        #delete_folder(tmpdir)
        make_folder(tmpdir)

        # index the genome
        index_genome(reference_genome)
        create_sequence_dict(reference_genome)

        # load snps
        print("loading SNPs")
        snps_df = load_object(snps_df_file)

        # define a bed with the positions with snps
        snp_positions_df = snps_df[["#CHROM", "POS"]].sort_values(by=["#CHROM", "POS"]).drop_duplicates()
        snp_positions_bed = "%s/SNP_positions.bed"%tmpdir
        snp_positions_df["start"] = snp_positions_df.POS-1
        snp_positions_df[["#CHROM", "start", "POS"]].to_csv(snp_positions_bed, sep="\t", header=False, index=False)

        # add fields
        snps_df["sampleID"] = snps_df.sampleID.apply(int)
        snps_df = snps_df.set_index("sampleID", drop=False)

        # check that all the samples are as expected
        strange_samples = set(snps_df.index).difference(set(sorted_samples))
        if len(strange_samples)>0: raise ValueError("there are some undefined samples in snps_df: %s"%strange_samples)

        # check that the genotypes make sense
        strange_GTs = set(snps_df.common_GT).difference(expected_GTs)
        if len(strange_GTs)>0: raise ValueError("There are unexpected GTs: %s"%strange_GTs)

        # check the GTs
        all_GTs = set(snps_df.common_GT)
        if len(all_GTs)==1: print("WARNING: There is only one type of GTs: %s"%all_GTs)

        ##############

        ########## GET MULIFASTA #######

        # keep important fields
        snps_df = snps_df[["#Uploaded_variation", "#CHROM", "POS", "REF", "ALT", "common_GT"]]

        # get list of individual fasta sequences transformed to include the SNPs
        inputs_fn = [(sampleID, snps_df.loc[{sampleID}], reference_genome, tmpdir, pickRandomHetSNPs, snp_positions_bed) for sampleID in sorted_samples]

        if threads>1:

            with multiproc.Pool(threads) as pool:

                list_alternative_fastaFiles = pool.starmap(get_alternative_genome_FastaAlternateReferenceMaker, inputs_fn)
                pool.close()
                pool.terminate()

        else: list_alternative_fastaFiles = list(map(lambda x: get_alternative_genome_FastaAlternateReferenceMaker(x[0], x[1], x[2], x[3], x[4], x[5]), inputs_fn))

        sampleID_to_altGenome = dict(zip(sorted_samples, list_alternative_fastaFiles))

        ################################

        ######### CONCATENATE MULTIFASTA BY CHROMS ##########

        # define the sorted chroms
        sorted_chroms = sorted(set(get_chr_to_len(reference_genome)))

        # define a dict that maps all the records
        sample_to_chrom_to_seq = {sampleID : {seq.id : seq for seq in SeqIO.parse(altGenome, "fasta")}   for sampleID, altGenome in sampleID_to_altGenome.items()}

        # generate one fasta for each chrom with all the samples
        print("generating one sample for each chromosome")
        for chrom in sorted_chroms:
            print(chrom)

            all_records = []
            all_lengths = set()
            for sampleID in sorted_samples:

                seq = sample_to_chrom_to_seq[sampleID][chrom]
                seq.id = str(sampleID)
                seq.name = ""
                seq.description = ""
                all_records.append(seq)
                all_lengths.add(len(seq.seq))

            # check and write
            if len(all_lengths)!=1: raise ValueError("there are strange lengths")
            SeqIO.write(all_records, "%s.%s.fasta"%(multifasta, chrom), "fasta")

        # generate one single multifasta with all concatenated chromosomes
        print("generating one single multifasta with all records")
        all_records = []
        all_lengths = set()

        for sampleID in sorted_samples:
            print(sampleID)

            # init a seq record
            str_seq = "".join([str(sample_to_chrom_to_seq[sampleID][chrom].seq) for chrom in sorted_chroms])
            concatenated_seq = SeqRecord(Seq(str_seq), id=str(sampleID), name="", description="")

            all_records.append(concatenated_seq)
            all_lengths.add(len(concatenated_seq.seq))

        if all_lengths!={len(snp_positions_df)}: raise ValueError("the length of the final alignment has less positions than expected")
        multifasta_tmp = "%s.tmp"%multifasta
        SeqIO.write(all_records, multifasta_tmp, "fasta")


        #####################################################

        # clean
        delete_folder(tmpdir)
        os.rename(multifasta_tmp, multifasta)


def run_iqtree_many_models(multifasta, outfileprefix, replace=False, threads=4, memGb=25):

    """This function runs iqtree (default) testing all models for the multifasta."""
     
    final_file = "%s.iqtree"%outfileprefix

    if file_is_empty(final_file) or replace is True:

        print("running IQTREE to get tree into %s"%outfileprefix)
        run_cmd("iqtree -s %s -pre %s --mem %iG -m TEST -T AUTO -B 1000"%(multifasta, outfileprefix, memGb)) # 1000 ultra-fast bootstrap

    # return the unrooted tree
    return "%s.treefile"%outfileprefix

def get_variant_calling_filenames(Cmine_dir, filename, replace=False):

    """gets a df with all the variant calling files"""

    # define vars
    interesting_taxIDs = [5480, 273372, 273371, 5482] # hide the haploids and C. albicans
    #interesting_taxIDs = [5476, 5480, 273372, 273371, 5482, 5478, 498019]

    min_coverage = 40
    min_pct_covered = 90

    # init dict
    dict_data = {}; I=0

    # go through each taxID
    for taxID in interesting_taxIDs:

        # define foders
        sciName = taxID_to_sciName[taxID]
        taxID_dir = "%s/data/%s_%i"%(Cmine_dir, sciName, taxID)
        varCall_dir = "%s/varCall_output"%taxID_dir
        ploidy = taxID_to_ploidy[taxID]

        # get the df for which we should have coverage
        coverage_df = get_tab_as_df_or_empty_df("%s/coverage_all_srrs.tab"%taxID_dir)
        coverage_df_filt = coverage_df[(coverage_df.mean_coverage>=min_coverage) & (coverage_df.pct_covered>=min_pct_covered)]

        # go through each sample
        for Isrr, srr in enumerate(coverage_df_filt.srr):
            

            """
            if ploidy==1: multialt_vcf = "%s/%s/smallVars_CNV_output/variants_atLeast2PASS_ploidy1.vcf"%(varCall_dir, srr)
            elif ploidy==2: multialt_vcf = "%s/%s/smallVars_CNV_output/variants_atLeast2PASS_ploidy2.withMultiAlt.vcf"%(varCall_dir, srr)
            """

            # get the ploidy 2 var call
            ploidy2_variant_calling = "%s/%s/smallVars_CNV_output/variant_calling_ploidy2.tab"%(varCall_dir, srr)


            # keep in the dict
            dict_data[I] = {"species":sciName, "srr":srr, "ploidy2_variant_calling":ploidy2_variant_calling}
            I+=1

    # get as df
    df_files = pd.DataFrame(dict_data).transpose()

    # check that all files exist
    for f in ["ploidy2_variant_calling"]: 
        for file in df_files[f]:
            if file_is_empty(file): raise ValueError("%s should exist"%file)

    return df_files


def get_diploid_hetero_filt_vars(df_files, ProcessedDataDir, replace=False):

    """This function returns a df with the called diploid heterozygous variants"""

    filename = "%s/df_heterozygous_vars.tab"%ProcessedDataDir

    if file_is_empty(filename) or replace is True:
        print("getting diploid heterozygous variants")

        def get_vars_df_one_sample(r):
            print("loading df %i"%r.name)
            
            # load as df
            vars_df_s = get_tab_as_df_or_empty_df(r.ploidy2_variant_calling)

            # filter high quality SNPs and nuclear
            vars_df_s = vars_df_s[(vars_df_s.mean_fractionReadsCov_PASS_algs>=0.25) & (vars_df_s.NPASS>=2) & (vars_df_s.ISSNP)  & (vars_df_s.common_GT=="0/1")]

            # add the sample
            vars_df_s["species"] = r.species
            vars_df_s["srr"] = r.srr

            return vars_df_s[["species", "srr", "#CHROM", "POS", "REF", "ALT"]] 

        # get as df and save
        vars_df = pd.concat(list(df_files.apply(get_vars_df_one_sample, axis=1)))
        save_df_as_tab(vars_df, filename)

    df_hetero_vars = get_tab_as_df_or_empty_df(filename)
    return df_hetero_vars



def get_filtered_small_vars_df(small_vars, small_vars_annot, small_vars_filt_file, replace=False, min_AF_haploid=0.9, min_AF_diploidHetero=0.25, min_NPASS=2):

    """Gets the small vars data and returns the filtered ones. We will return the small vars filtered"""

    if file_is_empty(small_vars_filt_file) or replace is True:
        print("getting filtered small vars")

        # debug: keep only the fist three samples
        #first_samples = set(sorted(set(small_vars.sampleID))[0:3])
        #small_vars = small_vars[small_vars.sampleID.isin(first_samples)]

        # keep only some fields
        small_vars = small_vars[['#Uploaded_variation', '#CHROM', 'POS', 'REF', 'ALT', 'INREPEATS', 'ISSNP', 'common_GT', 'mean_DP', 'mean_fractionReadsCov_PASS_algs', 'relative_CN', 'sampleID', 'calling_ploidy', 'NPASS']]

        # add the number of variants that are found in each
        """
        def get_Samples_from_df_variant(df_v): return set(df_v.sampleID) 
        var_to_nsamples = small_vars[["#Uploaded_variation", "sampleID"]].groupby("#Uploaded_variation").apply(get_Samples_from_df_variant).apply(len)
        
        small_vars["nsamples_with_var"] = small_vars["#Uploaded_variation"].map(var_to_nsamples)
        if any(pd.isna(small_vars["nsamples_with_var"])): raise ValueError("there should be no NaNs in nsamples_with_var")

        """

        # add fileds that indicate whether it is a correct haploid or a correct diploid
        small_vars["is_correct_haploid"] = (small_vars.calling_ploidy==1) & (small_vars.mean_fractionReadsCov_PASS_algs>=min_AF_haploid)
        small_vars["is_correct_diploid"] = (small_vars.calling_ploidy==2) & (small_vars.mean_fractionReadsCov_PASS_algs>=min_AF_diploidHetero)
        
        # get the filtered vars
        small_vars_filt = small_vars[(small_vars.NPASS>=min_NPASS) & ((small_vars.is_correct_haploid) | (small_vars.is_correct_diploid))]

        print("There are %i/%i vars kept after filtering "%(len(small_vars_filt), len(small_vars)))

        # remove unnecessary fields
        for f in ["is_correct_haploid", "is_correct_diploid", "NPASS"]: small_vars_filt.drop(f, inplace=True, axis=1)

        # add whether it is protein alterring
        protein_alterring_vars = set(small_vars_annot[small_vars_annot.is_protein_altering]["#Uploaded_variation"]).intersection(small_vars_filt["#Uploaded_variation"])
        small_vars_filt["is_protein_altering_inSomeGenes"] = small_vars_filt["#Uploaded_variation"].isin(protein_alterring_vars)

        print("writing")
        save_object(small_vars_filt, small_vars_filt_file)


def get_tuple_from_string(x):

    elements = [y for y in re.split("\(|\)|,", x) if y!=""]

    return tuple(elements)

def get_as_list(x, fn):

    """Returns a tuple if it is not one already"""

    if type(x)==tuple: return [fn(y) for y in x]
    elif type(x)==str and x.startswith("(") and x.endswith(")"): return [fn(y) for y in get_tuple_from_string(x)]
    else: return [fn(x)]



def get_is_correct_coverage_deletion_or_duplication_from_SV_CNV_r(r, max_relative_coverage_deletion=0.1, min_relative_coverage_duplication=1.7):

    """Takes a row of SV_CNV and returns if the duplication or deletion is correct """

    type_CNV = r.ID.split("|")[0]

    # debug
    if type_CNV in {"coverageDEL", "coverageDUP"} and pd.isna(r.INFO_RELCOVERAGE): raise ValueError("INFO_RELCOVERAGE can't be NaN")

    if type_CNV=="coverageDEL": return r["INFO_RELCOVERAGE"]<=max_relative_coverage_deletion
    elif type_CNV=="coverageDUP": return r["INFO_RELCOVERAGE"]>=min_relative_coverage_duplication
    else: return False

def get_filtered_SV_CNV_df(SV_CNV, SV_CNV_annot, SV_CNV_filt_file, replace=False, min_minAF=0.25, min_maxAF=0.9, min_lenCNV=600, max_relative_CN_deletion=0.0, min_relative_CN_duplication=2.0, max_relative_coverage_deletion=0.1, min_relative_coverage_duplication=1.7):

    """Gets the CNV data and returns them filtered. It also adds some fields."""

    if file_is_empty(SV_CNV_filt_file) or replace is True:
        print("running get_filtered_SV_CNV_df")

        ########### GENERATE varIDs_df ###########

        # get a df where each SV is one actual varID
        varIDs_df = SV_CNV[["INFO_variantID", "sampleID"]].drop_duplicates()
        varIDs_df["variantID"] = varIDs_df.INFO_variantID
        varIDs_df["varID_and_sampleID"] = varIDs_df.INFO_variantID + "_" + varIDs_df.sampleID.apply(str)
        SV_CNV["varID_and_sampleID"] = SV_CNV.INFO_variantID + "_" + SV_CNV.sampleID.apply(str)
        varIDs_df.index = list(range(0, len(varIDs_df)))

        # debug
        if len(set(varIDs_df["varID_and_sampleID"]))!=len(varIDs_df): raise ValueError("varID and sampleID are not unique")

        # add type of SV
        print("adding SV type")

        def get_typeVar_from_varID(varID): 
            if varID.startswith("coverage"): return "coverageCNV"
            else: return "SV"

        varIDs_df["type_var"] = varIDs_df.INFO_variantID.apply(get_typeVar_from_varID)

        # add the length of the coverageCNV
        print("adding var length")

        def get_SV_CNV_variantID_is_coverage(x): return x.startswith("coverage")
        varIDs_df = varIDs_df.merge(SV_CNV[SV_CNV.INFO_variantID.apply(get_SV_CNV_variantID_is_coverage)][["varID_and_sampleID", "POS", "INFO_END"]].drop_duplicates(), on="varID_and_sampleID", how="left")

        def get_length_var_from_r(r):
            if r.type_var=="coverageCNV": return r["INFO_END"] - r["POS"]
            else: return np.nan

        varIDs_df["length_var"] = varIDs_df.apply(get_length_var_from_r, axis=1)
        if any((varIDs_df.type_var=="coverageCNV") & pd.isna(varIDs_df.length_var)): raise ValueError("There can't be NaNs in length_var")

        # add the minAF and maxAF
        print("adding maxAF and minAF")

        def get_listAFs_from_SV_CNV_varIDsampleID(df): return list(df["INFO_BREAKEND_real_AF"].apply(get_as_list, fn=float).apply(max))
        varIDsampleID_to_AFlist = dict(SV_CNV[["varID_and_sampleID", "INFO_BREAKEND_real_AF"]].drop_duplicates().groupby("varID_and_sampleID").apply(get_listAFs_from_SV_CNV_varIDsampleID))

        varIDs_df["maxAF"] = varIDs_df.varID_and_sampleID.map(varIDsampleID_to_AFlist).apply(max)
        varIDs_df["minAF"] = varIDs_df.varID_and_sampleID.map(varIDsampleID_to_AFlist).apply(min)

        for f in ["maxAF", "minAF"]:
            if any((varIDs_df.type_var=="SV") & pd.isna(varIDs_df[f])): raise ValueError("There can't be NaNs in %s for SVs"%f)

        ##########################################

        # add to the df
        print("merging dfs")
        SV_CNV_merged = SV_CNV.merge(varIDs_df[["variantID", "sampleID", "maxAF", "minAF", "type_var", "length_var"]], how="left", left_on=["INFO_variantID", "sampleID"], right_on=["variantID", "sampleID"], validate="many_to_one")

        # get the filtered SV_CNV
        idx_correct_SVs = (SV_CNV_merged.type_var=="SV") & (SV_CNV_merged.maxAF>=min_maxAF) & (SV_CNV_merged.minAF>=min_minAF)

        idx_correct_SVs_coverageCNV = (SV_CNV_merged.type_var=="coverageCNV") & (SV_CNV_merged.length_var>=min_lenCNV) & ((SV_CNV_merged.INFO_merged_relative_CN<=max_relative_CN_deletion) | (SV_CNV_merged.INFO_merged_relative_CN>=min_relative_CN_duplication)) & (SV_CNV_merged.apply(get_is_correct_coverage_deletion_or_duplication_from_SV_CNV_r, max_relative_coverage_deletion=max_relative_coverage_deletion, min_relative_coverage_duplication=min_relative_coverage_duplication, axis=1))

        SV_CNV_filt = SV_CNV_merged[(idx_correct_SVs) | (idx_correct_SVs_coverageCNV)]

        # add whether it is transcript disrupting in some cases
        print("add whether it is transcript disrupting")
        transcript_disrupting_vars = set(SV_CNV_annot[SV_CNV_annot.is_transcript_disrupting]["#Uploaded_variation"])
        SV_CNV_filt["is_transcript_disrupting_inSomeGenes"] = SV_CNV_filt.ID.isin(transcript_disrupting_vars)

        save_object(SV_CNV_filt, SV_CNV_filt_file)
        print("SV df generated")

    # load dfs
    SV_CNV_filt = load_object(SV_CNV_filt_file)

    return SV_CNV_filt



def print_numbers_resistant_strains(metadata_df):

    """This function takes the metadata df and prints the numbers of resistant isolates"""

    azoles = ["FLC", "POS", "VRC", "IVZ", "ITR", "KET", "CLZ", "MIZ"]
    echinocandins = ["MIF", "ANI", "CAS"]
    other_drugs = ["AMB", "UCA", "BVN", "5FC", "TRB"]
    all_drugs = set(azoles + echinocandins + other_drugs)

    for species in set(metadata_df.species_name):
        
        # get df
        df = metadata_df[metadata_df.species_name==species]
        n_samples = len(df)
        
        # get df for clinical isolates 
        df = df[(df.type.isin({"clinical", "inmouse_evol_clone"}))]
        n_clinical = len(df)
        
        # go through each drug class and print how many R and S isolates are there
        """
        for drugs, drugs_name in [(echinocandins, "echinocandin"), (azoles, "azole"), (["AMB"], "ampB"), (["5FC"], "5FC")] :
            nResistant = sum(df["any_%s_R"%drugs_name]==True)
            nIntermediate = sum(df["any_%s_I"%drugs_name]==True)
            nSusceptible = sum(df["all_%s_S"%drugs_name]==True)
            
            if nResistant>0: print("in %s, there are %i/%i clinical isolates. For %s, there are %i/%i susceptible to all/resistant to any samples."%(species, n_clinical, n_samples, drugs_name, nSusceptible, nResistant))
            
        """
        for drug in all_drugs:
            df_d = df[~pd.isna(df["%s_resistance"%drug])]
            if len(df_d)==0: continue
            nResistant = sum(df_d["%s_resistance"%drug]=="R")
            nSusceptible = sum(df_d["%s_resistance"%drug]=="S")

            if nResistant>2: print("in %s, there are %i/%i clinical isolates. For %s, there are %i/%i susceptible/resistant samples."%(species, n_clinical, n_samples, drug, nSusceptible, nResistant))


def print_MIC_distributions(metadata_df):

    """This prints the MIC distributions"""

    #mic_f = "KET_MIC"; species = "Candida_tropicalis"; breakpoint=5
    #mic_f = "CAS_MIC"; species = "Candida_tropicalis"; breakpoint=0.5
    #mic_f = "CAS_MIC"; species = "Candida_glabrata"; breakpoint=1
    #mic_f = "TRB_MIC"; species = "Candida_auris"; breakpoint=1
    #mic_f = "ITR_MIC"; species = "Candida_auris"; breakpoint=0.25
    #mic_f = "ITR_MIC"; species = "Candida_glabrata"; breakpoint=2
    mic_f = "VRC_MIC"; species = "Candida_auris"; breakpoint=1
    mic_f = "VRC_MIC"; species = "Candida_glabrata"; breakpoint=1
    mic_f = "POS_MIC"; species = "Candida_auris"; breakpoint=0.06
    mic_f = "POS_MIC"; species = "Candida_glabrata"; breakpoint=1
    mic_f = "5FC_MIC"; species = "Candida_albicans"; breakpoint=0.5
    mic_f = "5FC_MIC"; species = "Candida_auris"; breakpoint=0.5
    mic_f = "5FC_MIC"; species = "Candida_glabrata"; breakpoint=0.5
    mic_f = "5FC_MIC"; species = "Candida_parapsilosis"; breakpoint=0.5
    mic_f = "MIZ_MIC"; species = "Candida_auris"; breakpoint=0.5
    mic_f = "ANI_MIC"; species = "Candida_auris"; breakpoint=1
    mic_f = "ANI_MIC"; species = "Candida_parapsilosis"; breakpoint=2
    mic_f = "CAS_MIC"; species = "Candida_albicans"; breakpoint=0.5
    mic_f = "CAS_MIC"; species = "Candida_parapsilosis"; breakpoint=0.5
    mic_f = "IVZ_MIC"; species = "Candida_auris"; breakpoint=0.5
    mic_f = "IVZ_MIC"; species = "Candida_glabrata"; breakpoint=0.5
    mic_f = "MIZ_MIC"; species = "Candida_albicans"; breakpoint=1
    mic_f = "BVN_MIC"; species = "Candida_albicans"; breakpoint=40

    df = metadata_df[(~pd.isna(metadata_df[mic_f])) & (metadata_df.species_name==species)]
    ax = sns.distplot(df[mic_f], bins=50, kde=False)
    ax.set_xscale("linear")
    plt.axvline(breakpoint, color="k", linestyle="--")
    #ax.set_xlim([0,3])
    ax.set_title(species)


def get_minimal_small_vars_df(DataDir, ProcessedDataDir):

    """Loads the minimal set of variants"""

    small_vars_file = "%s/minimal_small_vars.py"%ProcessedDataDir

    if any([file_is_empty(f) for f in [small_vars_file]]):
        print("getting minimal small vars")

        all_small_vars = pd.DataFrame()

        for taxID, sciName in taxID_to_sciName.items():
            print(sciName)

            # get the dir
            integrated_varcallsDir = "%s/%s_%i/integrated_varcalls"%(DataDir, sciName, taxID)
            #if sciName!="Candida_metapsilosis": continue

            # keep small vars
            small_vars = load_object("%s/smallVars_filt.py"%integrated_varcallsDir)[['#Uploaded_variation', '#CHROM', 'POS', 'REF', 'ALT', 'ISSNP', 'common_GT', 'relative_CN', 'sampleID', 'calling_ploidy', 'is_protein_altering_inSomeGenes']]

            small_vars["species_name"] = sciName
            all_small_vars = all_small_vars.append(small_vars)

        # save
        print("saving")
        save_object(all_small_vars, small_vars_file)

    # load
    print("loading")
    all_small_vars = load_object(small_vars_file).set_index("species_name", drop=False)

    return all_small_vars


def get_correct_tree_midpointRooted(treefile, min_support=0):

    """Gets a tree with midpointRooting"""


    # load
    tree = Tree(treefile)

    # get the midpoint rooted tree
    midpoint =  tree.get_midpoint_outgroup()
    tree.set_outgroup(midpoint)

    # collapse low support bracnhes
    for n in tree.traverse():

        # for internal nodes, if the support is below the minimum, set each of the children as children of the parent
        if n.is_leaf() is False and len(n.get_ancestors())>0 and n.support<min_support: n.delete()

    return tree


def get_correct_tree(tree, min_support=0):

    """Gets a tree with min_support"""

    # collapse low support bracnhes
    for n in tree.traverse():

        # for internal nodes, if the support is below the minimum, set each of the children as children of the parent
        if n.is_leaf() is False and len(n.get_ancestors())>0 and n.support<min_support: n.dist = 0.0

    return tree


def index_genome(genome):

    """Takes a fasta and generates a <genome>.fai file"""

    # index the genome of interest if not already done
    if file_is_empty("%s.fai"%genome): 

        faidx_std = "%s.indexing.std"%genome
        #print("running faidx. The std is in %s"%faidx_std)
        run_cmd("samtools faidx %s > %s 2>&1"%(genome, faidx_std), env="perSVade_env")
        remove_file(faidx_std)


def get_windows_for_reference_genome(reference_genome, window_size):

    """This function generates a window file for the reference genome"""

    windows_file = "%s.windows%ibp.bed"%(reference_genome, window_size)
    if file_is_empty(windows_file):

        # index the genome
        index_genome(reference_genome)

        # get the windows
        windows_file_stderr = "%s.generating.stderr"%windows_file
        windows_file_tmp = "%s.tmp"%windows_file

        #print("genearting windows_file. The stderr is in %s"%windows_file_stderr)
        run_cmd("bedtools makewindows -g %s.fai -w %i > %s 2>%s"%(reference_genome, window_size, windows_file_tmp, windows_file_stderr), env="perSVade_env")

        remove_file(windows_file_stderr)

        # sort by chrom and pos
        colnames = ["chrom", "start", "end"]
        df_windows = pd.read_csv(windows_file_tmp, sep="\t", header=None, names=colnames).sort_values(by=colnames)
        df_windows[colnames].to_csv(windows_file_tmp, sep="\t", header=False, index=False)

        # clean
        os.rename(windows_file_tmp, windows_file)

    return windows_file


def get_series_nSNPs(sampleID, df_SNPs, windows_bed, tmpdir):

    """This function returns a series where the index is each region in windows_bed and the content is the number of SNPs that are there in df_SNPs """

    print("working on sample %s"%sampleID)

    # define the bedmap outfile
    bedmap_outdir = "%s/mapping_SNPs_sample_%s.out"%(tmpdir, sampleID)
    if file_is_empty(bedmap_outdir):
        print("running bedmap into %s"%bedmap_outdir)

        # create the bed with the SNPs
        snps_bed =  "%s/SNPs_sample_%s.bed"%(tmpdir, sampleID)
        df_SNPs = df_SNPs.sort_values(by=["#CHROM", "START", "POS"])
        df_SNPs["snp_id"] = list(range(len(df_SNPs)))
        df_SNPs[["#CHROM", "START", "POS", "snp_id"]].to_csv(snps_bed, sep="\t", index=False, header=False)

        # run bedmap to get the SNPs that overlap with windows_bed
        bedmap_outdir_tmp = "%s.tmp"%bedmap_outdir
        run_cmd("bedmap --bp-ovr 1 --echo-map-id --delim '\t' %s %s > %s"%(windows_bed, snps_bed, bedmap_outdir_tmp), env="perSVade_env")

        os.rename(bedmap_outdir_tmp, bedmap_outdir)

    # get the number of SNPs per window
    df_windows = pd.read_csv(windows_bed, sep="\t", header=None, names=["#CHROM", "START", "END"])
    df_windows["window_ID"] = df_windows["#CHROM"] + "|||" + df_windows.START.apply(str) + "|||" + df_windows.END.apply(str)

    def get_snpIDs_in_line(l): return set(l.split(";")).difference({"", "\n"})
    df_windows["snps_set"] = list(map(get_snpIDs_in_line, open(bedmap_outdir, "r")))
    df_windows["nsnps"] = df_windows.snps_set.apply(len)

    # return the series with the  number of SNPs
    final_series  = df_windows.set_index("window_ID").nsnps
    return final_series

def get_series_CNVdensity_one_sample(sampleID, df_CNVs, windows_bed, tmpdir):

    """This function returns something similar to get_series_nSNPs but for CNVs """

    print("working on sample %s"%sampleID)

    # load the windows df
    df_windows = pd.read_csv(windows_bed, sep="\t", header=None, names=["#CHROM", "START", "END"])
    df_windows["window_ID"] = df_windows["#CHROM"] + "|||" + df_windows.START.apply(str) + "|||" + df_windows.END.apply(str)
    df_windows = df_windows.reset_index(drop=True)

    # define the bedmap outfile
    bedmap_outdir = "%s/mapping_CNVs_sample_%s.out"%(tmpdir, sampleID)
    if file_is_empty(bedmap_outdir):
        print("running bedmap into %s"%bedmap_outdir)

        # add the copy number
        df_CNVs["CN"] = df_CNVs.INFO_SVTYPE.map({"TDUP":2.0, "DUP":2.0, "DEL":0.0})
        if any(pd.isna(df_CNVs.CN)): raise ValueError("There can't be NaNs in CN")

        # create the bed with the SNPs
        cnvs_bed =  "%s/CNVs_sample_%s.bed"%(tmpdir, sampleID)
        df_CNVs["START"] = df_CNVs.POS-1
        df_CNVs = df_CNVs.sort_values(by=["#CHROM", "START", "INFO_END"])
        df_CNVs["cnv_ID"] = ["CN_%i"%x for x in range(len(df_CNVs))]

        df_CNVs[["#CHROM", "START", "INFO_END", "cnv_ID", "CN"]].to_csv(cnvs_bed, sep="\t", index=False, header=False)

        # add the CN=1 regions
        cn1_bed = "%s.CN1regions.bed"%cnvs_bed
        run_cmd("bedtools subtract -a %s -b %s > %s"%(windows_bed, cnvs_bed, cn1_bed), env="perSVade_env")

        df_CN1 = pd.read_csv(cn1_bed, sep="\t", names=["#CHROM", "START", "INFO_END"])
        df_CN1["cnv_ID"] = ["CN1_%i"%x for x in range(len(df_CN1))]
        df_CN1["CN"] = 1.0

        df_CNVs = df_CNVs.append(df_CN1).sort_values(by=["#CHROM", "START", "INFO_END"])[["#CHROM", "START", "INFO_END", "cnv_ID", "CN"]].to_csv(cnvs_bed, sep="\t", index=False, header=False)

        # run bedmap to get the SNPs that overlap with windows_bed
        bedmap_outdir_tmp = "%s.tmp"%bedmap_outdir
        run_cmd("bedmap --bp-ovr 1 --wmean --delim '\t' %s %s > %s"%(windows_bed, cnvs_bed, bedmap_outdir_tmp), env="perSVade_env")

        os.rename(bedmap_outdir_tmp, bedmap_outdir)

    # add the mean_relative_CN
    df_windows["mean_relative_CN"] = pd.read_csv(bedmap_outdir, sep="\t", names=["mean_relative_CN"],  na_values=["NAN", "nan", "NaN"], keep_default_na=False)["mean_relative_CN"]
    df_windows = df_windows.set_index("window_ID")

    if any(pd.isna(df_windows.mean_relative_CN)): raise ValueError("Can't be Nans")

    return df_windows.mean_relative_CN

def get_CNVdensity_df(SV_CNV_filt_file, ProcessedDataDir, threads, reference_genome, window_size, species, expected_samples=None):


    """This function is like get_SNPdensity_df but for CNVs. It returns, for each window of the genome, a relative coverage value that goes from 0 to 2 depending on the fraction of deletions and duplications (including CNVs and SVs with CNV) in each region"""

    # define a prefix
    prefix = "%s_w=%i"%(species, window_size)

    # define file
    df_square_file = "%s/df_square_CNV_density_per_window_%s.py"%(ProcessedDataDir, prefix)

    if file_is_empty(df_square_file):
        print("getting SV density")

        # get a bed with the windows
        windows_bed = get_windows_for_reference_genome(reference_genome, window_size)

        # load the CNVs
        if type(SV_CNV_filt_file)==str: df = load_object(SV_CNV_filt_file)
        else: df = SV_CNV_filt_file # assume it is a df
        df = df[df.INFO_SVTYPE.isin({'TDUP', 'DEL', 'DUP'})]

        # keep only the non-redundant vars
        df = df[["sampleID", "#CHROM", "POS", "INFO_END", "INFO_SVTYPE"]].sort_values(by=["sampleID", "#CHROM", "POS"]).drop_duplicates().set_index("sampleID")
        for f in ["POS", "INFO_END"]: df[f] = df[f].apply(int)

        if any(df.INFO_END<=df.POS): raise ValueError("END should be after POS")

        # get the SNP density in parallel
        tmpdir = "%s/CNV_density_%s_bedFiles"%(ProcessedDataDir, prefix); make_folder(tmpdir)
        all_samples = sorted(set(df.index))
        inputs_fn = [(s, df.loc[{s}], windows_bed, tmpdir) for s in all_samples]

        # parallel
        with  multiproc.Pool(threads) as pool:

            list_series_CNVnumbers = pool.starmap(get_series_CNVdensity_one_sample, inputs_fn)
            pool.close()
            pool.terminate()


        # map
        #list_series_CNVnumbers = list(map(lambda x: get_series_CNVdensity_one_sample(x[0], x[1], x[2], x[3]), inputs_fn))

        df_square = pd.DataFrame(list_series_CNVnumbers)
        df_square.index = [str(x) for x in all_samples]

        # if you want some samples, set all to 0
        if expected_samples is not None:

            missing_samples = set(expected_samples).difference(set(df_square.index))
            if len(missing_samples)>0: 
                
                print("WARNING: There are %i samples with no SNPs"%len(missing_samples))
                for sampleID in missing_samples: df_square = df_square.append(pd.DataFrame({c : {sampleID: 0} for c in df_square.columns}))

        # write
        delete_folder(tmpdir)
        save_object(df_square, df_square_file)

    # load df 
    df_square = load_object(df_square_file)
    df_square.index = [str(x) for x in df_square.index]

    return df_square



def get_SVdensity_df(SV_CNV_filt_file, ProcessedDataDir, threads, reference_genome, window_size, species,  expected_samples=None):

    """This function is like get_SNPdensity_df but it returns the number of breakends found in each window"""
    
    # define a prefix
    prefix = "%s_w=%i"%(species, window_size)

    # define file
    df_square_file = "%s/df_square_SV_breakends_per_window_%s.py"%(ProcessedDataDir, prefix)

    if file_is_empty(df_square_file):
        print("getting SV density")

        # get a bed with the windows
        windows_bed = get_windows_for_reference_genome(reference_genome, window_size)

        # load the SVs
        df = load_object(SV_CNV_filt_file)
        df = df[df.type_var=="SV"]

        # get two dfs, one with POS and one with END (this will include all the breakends)
        df_POS = df[["sampleID", "#CHROM", "POS", "variantID_across_samples"]]
        df_END = df[~pd.isna(df.INFO_END)][["sampleID", "#CHROM", "INFO_END", "variantID_across_samples"]].rename(columns={"INFO_END":"POS"})

        # append them
        df = df_POS.append(df_END)
        df["POS"] = df.POS.apply(int)
        if any(pd.isna(df.POS)): raise ValueError("There can't be NaNs in POS")

        # sort by sample and position, and get the sample
        df = df[["sampleID", "#CHROM", "POS", "variantID_across_samples"]].sort_values(by=["sampleID", "#CHROM", "POS"]).drop_duplicates().set_index("sampleID")
        df["START"] = df.POS-1


        # get the SNP density in parallel
        tmpdir = "%s/SVs_positions_%s_bedFiles"%(ProcessedDataDir, prefix); make_folder(tmpdir)
        all_samples = sorted(set(df.index))
        inputs_fn = [(s, df.loc[{s}], windows_bed, tmpdir) for s in all_samples]

        with  multiproc.Pool(threads) as pool:

            list_series_SNPnumbers = pool.starmap(get_series_nSNPs, inputs_fn)
            pool.close()
            pool.terminate()

        df_square = pd.DataFrame(list_series_SNPnumbers)
        df_square.index = [str(x) for x in all_samples]

        # if you want some samples, set all to 0
        if expected_samples is not None:

            missing_samples = set(expected_samples).difference(set(df_square.index))
            if len(missing_samples)>0: 
                
                print("WARNING: There are %i samples with no SNPs"%len(missing_samples))
                for sampleID in missing_samples: df_square = df_square.append(pd.DataFrame({c : {sampleID: 0} for c in df_square.columns}))

        # write
        delete_folder(tmpdir)
        save_object(df_square, df_square_file)

    # load df 
    df_square = load_object(df_square_file)
    df_square.index = [str(x) for x in df_square.index]

    return df_square

    
def get_SNPdensity_df(small_vars_filt_file, ProcessedDataDir, interesting_GTs, ploidy, threads, reference_genome, window_size, species, discardRepeats=False, discardDuplicatedRegions=False, onlyTrulyHetero=False, expected_samples=None):

    """This function gets a df with the number of SNPs per window for df (small_vars for some) species """
    
    # define prefix
    prefix = "%s_w=%i_%s_p=%i"%(species, window_size, "-".join(sorted([gt.replace("/", "_") for gt in interesting_GTs])), ploidy)

    if discardRepeats is True: prefix += "_notInRepeats"
    if discardDuplicatedRegions is True: prefix += "_notInDupRegions"
    if onlyTrulyHetero is True: prefix += "_onlyTrulyHetero"

    # get a square df with SNP densisties
    df_square_file = "%s/df_square_SNPs_per_window_%s.py"%(ProcessedDataDir, prefix)
    if file_is_empty(df_square_file):
        print("getting SNP density ")

        # get a bed with the windows
        windows_bed = get_windows_for_reference_genome(reference_genome, window_size)

        # load the small variants filtered file
        df = load_object(small_vars_filt_file)

        # get the interesting SNPs and DF
        df = df[(df.calling_ploidy==ploidy) & (df.common_GT.isin(interesting_GTs)) & (df.ISSNP)]

        # filter each of the additional options
        if discardRepeats is True: df = df[~(df.INREPEATS)]
        if discardDuplicatedRegions is True: df = df[(df.relative_CN<=1)]
        if onlyTrulyHetero is True: df = df[(df.mean_fractionReadsCov_PASS_algs>=0.25) & (df.mean_fractionReadsCov_PASS_algs<=0.75)]

        # sort by sample and position, and get the sample
        df = df[["sampleID", "#CHROM", "POS", "#Uploaded_variation"]].sort_values(by=["sampleID", "#CHROM", "POS"]).drop_duplicates().set_index("sampleID")
        df["START"] = df.POS-1

        # get the SNP density in parallel
        tmpdir = "%s/SNPs_positions_%s_bedFiles"%(ProcessedDataDir, prefix); make_folder(tmpdir)
        all_samples = sorted(set(df.index))
        inputs_fn = [(s, df.loc[{s}], windows_bed, tmpdir) for s in all_samples]

        with  multiproc.Pool(threads) as pool:

            list_series_SNPnumbers = pool.starmap(get_series_nSNPs, inputs_fn)
            pool.close()
            pool.terminate()

        #list_series_SNPnumbers = list(map(lambda x: get_series_nSNPs(x[0], x[1], x[2], x[3]), inputs_fn))
        df_square = pd.DataFrame(list_series_SNPnumbers)
        df_square.index = [str(x) for x in all_samples]

        # if you want some samples, set all to 0
        if expected_samples is not None:

            missing_samples = set(expected_samples).difference(set(df_square.index))
            if len(missing_samples)>0: 
                
                print("WARNING: There are %i samples with no SNPs"%len(missing_samples))
                for sampleID in missing_samples: df_square = df_square.append(pd.DataFrame({c : {sampleID: 0} for c in df_square.columns}))

        # write
        delete_folder(tmpdir)
        save_object(df_square, df_square_file)

    # load df 
    df_square = load_object(df_square_file)
    df_square.index = [str(x) for x in df_square.index]

    return df_square

def get_uniqueVals_df(df): return set.union(*[set(df[col]) for col in df.columns])

def plot_distribution_ratio_homo_hetero(DataDir, species_to_ref_genome, species_to_tree, PlotsDir, ProcessedDataDir, ploidy=2, threads=4, window_size=10000):

    """This function plots the distribution of hetero/homo SNPs per windows of the genome"""

    # map each species to the min homo SNPs per window
    species_to_minNhomoSNPs = {'Candida_parapsilosis': 4, 'Candida_metapsilosis': 20, 'Candida_orthopsilosis': 20, 'Candida_tropicalis': 10, 'Candida_glabrata': 20, "Candida_albicans":20, "Candida_auris":20}

    # map each species to the minimum number of windows to calculate the ratios
    species_to_minNwindows = {'Candida_parapsilosis': 5, 'Candida_metapsilosis': 20, 'Candida_orthopsilosis': 20, 'Candida_tropicalis': 20, 'Candida_glabrata': 20, "Candida_albicans":20, "Candida_auris":20}

    # init fig
    nrows = 7
    if run_in_cluster is False: fig = plt.figure(figsize=(5, nrows*2.6))

    # go through each species
    for Is, (taxID, species) in enumerate(taxID_to_sciName.items()):
        print(species)

        #if species=="Candida_glabrata": continue

        ############ GET RAW DATA ############

        # define things of this species
        reference_genome = species_to_ref_genome[species]
        tree = cp.deepcopy(species_to_tree[species])
        sorted_samples = tree.get_leaf_names()
        small_vars_filt_file = "%s/%s_%i/integrated_varcalls/smallVars_filt.py"%(DataDir, species, taxID)

        # get the homo and hetero densities
        df_homo_SNPs = get_SNPdensity_df(small_vars_filt_file, ProcessedDataDir, {"1/1"}, ploidy, threads, reference_genome, window_size, species, discardRepeats=True, discardDuplicatedRegions=True, expected_samples=sorted_samples).loc[sorted_samples]    

        df_hetero_SNPs = get_SNPdensity_df(small_vars_filt_file, ProcessedDataDir, {"0/1"}, ploidy, threads, reference_genome, window_size, species, discardRepeats=True, discardDuplicatedRegions=True, onlyTrulyHetero=True, expected_samples=sorted_samples).loc[sorted_samples]

        ######################################

        ######## GET THE PLOT DF ############

        df_plot_file = "%s/%s_%ibp_p%i_data_ratioHeteroHomoSNPs.py"%(ProcessedDataDir, species, window_size, ploidy)
        if file_is_empty(df_plot_file):

            df_plot = pd.DataFrame()

            for sampleID in sorted_samples:
                print(sampleID)

                # keep only windows that have >=species_to_minNhomoSNPs homo SNPs
                correct_windows = [wID for wID in df_homo_SNPs.keys() if df_homo_SNPs.loc[sampleID, wID]>=species_to_minNhomoSNPs[species]]

                # keep only samples that have more than species_to_minNwindows windows
                if len(correct_windows)<species_to_minNwindows[species]: continue

                # keep the ratios
                log_ratio = np.log2((df_hetero_SNPs.loc[sampleID, correct_windows]+1)/(df_homo_SNPs.loc[sampleID, correct_windows]+1))
                ratio = (df_hetero_SNPs.loc[sampleID, correct_windows])/(df_homo_SNPs.loc[sampleID, correct_windows])

                # get df
                df = pd.DataFrame({"ratio_het_homo":ratio, "log_ratio_het_homo":log_ratio}).reset_index(drop=True)
                df["sampleID"] = sampleID

                # keep
                df_plot = df_plot.append(df)

            save_object(df_plot, df_plot_file)

        df_plot = load_object(df_plot_file).set_index("sampleID", drop=False)

        if run_in_cluster is True: continue

        #####################################

        ######### PLOT #########
        print("plotting")

        ax = plt.subplot(nrows, 1, Is+1)
        for sampleID in set(df_plot.index): sns.distplot(df_plot.loc[{sampleID}, "log_ratio_het_homo"], rug=False, hist=False)

        ax.set_xlabel("log2 (# hetero SNPs / # homo SNPs)")
        ax.set_ylabel("samples density")
        ax.set_title("C. %s. windows >%i homo SNPs. Samples >%i windows (%i/%i)"%(species.split("_")[1], species_to_minNhomoSNPs[species], species_to_minNwindows[species], len(set(df_plot.index)), len(sorted_samples)))

        ax.set_xlim([-15, 10]) # this is for the log_ratio

    ########################

    if run_in_cluster is True: return

    print("saving")
    filename = "%s/distribution_ratio_homo_hetero_SNPs.pdf"%(PlotsDir)
    fig.tight_layout()  # otherwise the right y-label is slightly 
    fig.savefig(filename, bbox_inches='tight');
    #plt.close(fig)

    
def plot_SNP_density_and_coverage_on_tree(DataDir, df_coverage_per_gene, species_to_ref_genome, species_to_tree, PlotsDir, ProcessedDataDir, window_size=10000, ploidy=2, threads=4, show_tree=True, sorted_typeVars=["breakpoints", "CNVs"]):

    """This function plots in a df the density of SNPs for window_size's of the genome in a heatmap """

    print("running plot_SNP_density_and_coverage_on_tree")

    # go through each species
    for taxID, species in taxID_to_sciName.items():
        print(species)

        #if species!="Candida_metapsilosis": continue

        ############ GET DATA ############

        # define things of this species
        reference_genome = species_to_ref_genome[species]
        tree = cp.deepcopy(species_to_tree[species])
        sorted_samples = tree.get_leaf_names()
        small_vars_filt_file = "%s/%s_%i/integrated_varcalls/smallVars_filt.py"%(DataDir, species, taxID)
        SV_CNV_filt_file = "%s/%s_%i/integrated_varcalls/SV_CNV_filt.py"%(DataDir, species, taxID)

        # define the CNV density
        df_all_CNVs = get_CNVdensity_df(SV_CNV_filt_file, ProcessedDataDir, threads, reference_genome, window_size, species, expected_samples=sorted_samples)

        # define all the SV density
        df_all_SVs = get_SVdensity_df(SV_CNV_filt_file, ProcessedDataDir, threads, reference_genome, window_size, species, expected_samples=sorted_samples)

        # define all the SNPs
        df_all_SNPs = get_SNPdensity_df(small_vars_filt_file, ProcessedDataDir, {"0/1", "1/1"}, ploidy, threads, reference_genome, window_size, species).loc[sorted_samples]


        # get the snp density of homozygous and heterozygous SNPs
        df_hetero_SNPs = get_SNPdensity_df(small_vars_filt_file, ProcessedDataDir, {"0/1"}, ploidy, threads, reference_genome, window_size, species).loc[sorted_samples]
        df_homo_SNPs = get_SNPdensity_df(small_vars_filt_file, ProcessedDataDir, {"1/1"}, ploidy, threads, reference_genome, window_size, species).loc[sorted_samples]    

        # define the hetero SNPs that are in nonduplicated regions
        df_really_hetero_SNPs = get_SNPdensity_df(small_vars_filt_file, ProcessedDataDir, {"0/1"}, ploidy, threads, reference_genome, window_size, species, discardRepeats=True, discardDuplicatedRegions=True, onlyTrulyHetero=True, expected_samples=sorted_samples).loc[sorted_samples]

        # continue if you are running on the cluster
        if run_in_cluster is True: continue

        # get the ratio df
        df_HeteroVsHomo_SNPs = np.log2(df_hetero_SNPs.applymap(lambda c: c+1)/df_homo_SNPs.applymap(lambda c: c+1)).loc[sorted_samples]

        # get the coverage per windows of the genome in the same format
        df_cov = df_coverage_per_gene[df_coverage_per_gene.species==species]
        df_cov["windowID"] = df_cov.chromosome + "|||" + df_cov.start.apply(str) + "|||" + df_cov.end.apply(str)
        df_cov = df_cov.drop_duplicates(subset=["sampleID", "windowID"]).sort_values(by=["sampleID", "windowID"])
        df_cov["sampleID"] = df_cov.sampleID.apply(str)
        df_coverage = df_cov.pivot(index="sampleID", columns="windowID", values="relative_coverage").applymap(float).loc[sorted_samples]

        ##################################

        ####### PLOT TREE #######

        # int tree style
        ts = TreeStyle()

        # internal tree parameters
        for n in tree.traverse():

            # set the linewidth according to the support
            nst = NodeStyle()
            nst["hz_line_type"] = 0
            nst["vt_line_type"] = 0
            nst["size"] = 0

            if n.support>=90 or n.is_leaf() is True: 
                nst["hz_line_width"] = 6
                nst["vt_line_width"] = 6

            else:
                nst["hz_line_width"] = 1
                nst["vt_line_width"] = 1

            n.set_style(nst)

        # go through each column
        colI = 1

        # define parms
        bar_width =  600
        bar_heigth = 20

        # map each chromosome to a color
        all_chroms = sorted(set(df_cov.chromosome).union({c.split("|||")[0] for c in df_homo_SNPs.columns}))
        chrom_to_color = get_value_to_color(all_chroms, palette="tab20", type_color="hex")[0]

        # map the graphics
        typeVar_to_graphics = {"real_hetero_SNPs" : (df_really_hetero_SNPs, "# real hetero SNPs", "Spectral", None, None, None),
                               "hetero_SNPs" : (df_hetero_SNPs, "# hetero SNPs", "Spectral", None, None, None), 
                               "homo_SNPs" : (df_homo_SNPs, "# homo SNPs", "Spectral", None, None, None),
                               "hetero_vs_homo" : (df_HeteroVsHomo_SNPs, "log2(hetero /homo) SNPs", "vlag", -5, 0, 5),
                               "relative_coverage" : (df_coverage, "relative coverage", "vlag", 0, 1, 2),
                               "breakpoints": (df_all_SVs, "# SVs", "rocket_r", None, None, None),
                               "CNVs": (df_all_CNVs, "relative CN", "coolwarm", 0, 1, 2)}
        
        for typeVar in sorted_typeVars:

            # get the grpahical values
            df_heatmap, cbarLabel, cbar, min_v, center_v, max_v = typeVar_to_graphics[typeVar]

            print(species, cbarLabel)

            # define all the values
            all_vals = pd.Series(sorted(get_uniqueVals_df(df_heatmap)))
            all_vals_notUnique = np.array(make_flat_listOflists(df_heatmap.values))

            # inf the minimum vals are not defined, set the percentile 90 as the maximum and the minimum as the minimym
            if min_v is None: min_v = min(all_vals)
            if max_v is None: max_v = np.percentile(all_vals_notUnique[all_vals_notUnique>0], 90)

            # reorder according to chroms
            df_heatmap = df_heatmap[reversed(sorted(df_heatmap.columns, key=(lambda x: x.split("|||")[0])))]

            # filter dfs
            all_vals = all_vals[all_vals>=min_v]
            all_vals = all_vals[all_vals<=max_v]

            # get the palette
            value_to_color, palette =  get_value_to_color(all_vals, palette=cbar, type_color="hex", center=center_v)


            # add missing vals
            missing_vals = get_uniqueVals_df(df_heatmap).difference(set(value_to_color))
            for val in missing_vals:
                if val>max_v: value_to_color[val] = value_to_color[max(all_vals)]
                elif val<min_v: value_to_color[val] = value_to_color[min(all_vals)]

            # for each leave add a heatmap through the stacked colorbar
            percents = [100/len(df_heatmap.columns)]*(len(df_heatmap.columns))
            for l in tree.get_leaves():
                colors = list(df_heatmap.loc[l.name].map(value_to_color))
                l.add_face(StackedBarFace(percents, bar_width, bar_heigth, colors=colors, line_color=None), position="aligned", column=colI)

            # add the header as a text
            nameF = TextFace("chromosome", fsize=10, bold=True); nameF.rotation = 0
            nameF.vt_align = 0
            ts.aligned_header.add_face(nameF, column=colI) 

            # add a colorbar for the chromosomes
            colors_chroms = [chrom_to_color[x.split("|||")[0]] for x in df_heatmap.columns]
            ts.aligned_header.add_face(StackedBarFace(percents, bar_width, bar_heigth, colors=colors_chroms, line_color=None), column=colI)
            ts.aligned_header.add_face(RectFace(5, 5, fgcolor="white", bgcolor="white"), column=colI)

            # add as footer a colorbar
            ts.aligned_foot.add_face(RectFace(5, 5, fgcolor="white", bgcolor="white"), column=colI)
            ts.aligned_foot.add_face(TextFace(cbarLabel, fsize=10, bold=True), column=colI) 

            percents_colorBar = [100/len(palette)]*len(palette)
            colors_colorBar = list(palette.values())
            ts.aligned_foot.add_face(StackedBarFace(percents_colorBar, 100, 20, colors=colors_colorBar, line_color=None), column=colI)

            # add the ticks
            min_val, center_val, max_val = np.linspace(min(palette), max(palette), 3)
            ts.aligned_foot.add_face(TextFace("%.1f  %.1f %.1f"%(min_val, center_val, max_val), fsize=10, bold=True), column=colI)

            # update colI
            colI+=1

            # add blank
            l.add_face(RectFace(20, 20, fgcolor="white", bgcolor="white"), position="aligned", column=colI); colI+=1

        # final tree style
        ts.show_branch_length = False
        ts.show_branch_support = False
        ts.show_leaf_name = True
        ts.draw_guiding_lines = 2 # 0=solid, 1=dashed, 2=dotted.

        # show or write
        if run_in_cluster is False and show_tree is True: 
            tree.show(tree_style=ts)
            continue

        # write
        plots_dir_trees = "%s/SNP_coverage_density"%PlotsDir; make_folder(plots_dir_trees)
        filename = "%s/%s_%s.pdf"%(plots_dir_trees, species, "_".join(sorted_typeVars))

        print("writing %s"%filename)
        tree.render(file_name=filename, tree_style=ts)
        
        #########################


def plot_clades_based_on_branchSupport(species_to_tree, PlotsDir, ProcessedDataDir, show_tree=False, min_support=95) :

    """This function plots the tree and shows clades based on branch support"""

     # go through each species
    for taxID, species in taxID_to_sciName.items():
        print(species)

        if species!="Candida_metapsilosis": continue

        # define things of this species
        tree = cp.deepcopy(species_to_tree[species])
        sorted_samples = tree.get_leaf_names()

        # int tree style
        ts = TreeStyle()

        # internal tree parameters
        nclades = 0
        for n in tree.traverse():

            # set the linewidth according to the support
            nst = NodeStyle()
            nst["hz_line_type"] = 0
            nst["vt_line_type"] = 0
            nst["size"] = 0

            if n.support>=min_support and n.is_leaf() is False: 
                nst["hz_line_width"] = 6
                nst["vt_line_width"] = .1
                nst["hz_line_color"] = "red"
                nst["vt_line_color"] = "gray"

                nclades+=1

            else:
                nst["hz_line_width"] = .1
                nst["vt_line_width"] = .1
                nst["hz_line_color"] = "gray"
                nst["vt_line_color"] = "gray"

            n.set_style(nst)

        print("There are %i clades"%nclades)

        # final tree style
        ts.show_branch_length = False
        ts.show_branch_support = False
        ts.show_leaf_name = False
        ts.draw_guiding_lines = 2 # 0=solid, 1=dashed, 2=dotted.

        # show or write
        if run_in_cluster is False and show_tree is True: 
            tree.show(tree_style=ts)

 
def get_SV_df_oneVarPerRow_df(SV_CNV_filt_file, SV_df_oneVarPerRow_file, only_unique_SVs=True, threads=4, var_types={"SV"}, replace=False, bad_samples=set()):

    """This function returns a SV_CNV df where each row is one variant with two regions as a bedpe. If only_unique_SVs is True, this function will return all the uniuque IDs by 

    var_types can be SV or coverageCNV """

    if file_is_empty(SV_df_oneVarPerRow_file) or replace is True: 
        print("running get_SV_CNV_oneVarPerRow_df")

        # load SV df
        SVdf_all = load_object(SV_CNV_filt_file)
        SVdf_all = SVdf_all[~(SVdf_all.sampleID.apply(str).isin(bad_samples))]
        SVdf = SVdf_all[SVdf_all.type_var.isin(var_types)].sort_values(["variantID_across_samples"]).drop_duplicates(subset=["variantID_across_samples"])[["INFO_variantID", "variantID_across_samples"]]

        # add the progress var
        SVdf["var_idx"] = list(range(len(SVdf)))

        # add the bedpe fields
        def add_bedpe_r_to_SVdf_r(r):

            if (r.var_idx%100)==0: print("already traversed %.4f%s vars"%((r.var_idx/len(SVdf))*100, "%"))

            varID = r.INFO_variantID
            svtype = varID.split("|")[0]

            # undetermined chroms
            if svtype.endswith("like"):

                regionA, regionB = varID.split("|")[1].split("-")
                chrA, startA = regionA.split(":")
                chrB, startB = regionB.split(":")

                endA = int(startA)+1
                endB = int(startB)+1

                final_svtype = "unclassified_breakpoint"

            # simple svtypes 
            elif svtype in {"DEL", "TDUP", "INV"}:

                chrA = chrB = varID.split("|")[1].split(":")[0]
                startA, endB = varID.split("|")[1].split(":")[1].split("-")

                endA = int(startA)+1
                startB = int(endB)-1

                final_svtype = {"DEL":"deletion", "TDUP":"tandem_duplication", "INV":"inversion"}[svtype]

            # insertions 
            elif svtype=="INS":

                regionA, regionB = varID.split("|")[1:-1]

                chrA = regionA.split(":")[0]
                chrB = regionB.split(":")[0]

                startA, endA = regionA.split(":")[1].split("-")
                startB = int(regionB.split(":")[1])
                endB = startB+1

                final_svtype = "%s_insertion"%(varID.split("|")[-1])

            # translocations
            elif svtype=="TRA":

                # define the regions as in the variant
                regionA, regionB = varID.split("|")[1].split("<>")

                # in chrA, the end has the breakpoint
                chrA, positionsA = regionA.split(":")
                startA = int(positionsA.split("-")[1])
                endA = startA+1

                # in chrB, the orientation defines the breakpoint
                chrB, positionsB = regionB.split(":")
                sv_startB, sv_endB = positionsB.split("-")
                
                if int(sv_startB)==0: 
                    startB = int(sv_endB)
            
                else: 
                    startB = int(sv_startB)

                endB = startB+1

                final_svtype = "bal_translocation"

            # complex inverted translocations/duplications
            elif svtype in {"CVD", "IVT"}:

                regionA, regionB = varID.split("|")[1:]

                chrA = regionA.split(":")[0]
                chrB = regionB.split(":")[0]

                startA, endA = regionA.split(":")[1].split("-")
                startB = int(regionB.split(":")[1])
                endB = startB+1

                final_svtype = {"CVD":"inverted_copyPaste_insertion", "IVT":"inverted_cutPaste_insertion"}[svtype]

            # complex inverted intrachromosomal translocation (encoded like copy paste insertions)
            elif svtype=="CVT":


                regionA, regionB = varID.split("|")[1:]
                chrA, endA = regionA.split(":")
                chrB, endB = regionB.split(":")

                startA = int(endA)-1
                startB = int(endB)-1

                final_svtype = "inverted_translocation"

            # coverage elements
            elif svtype in {"coverageDEL", "coverageDUP"}:

                chromosome, region = varID.split("|")[1].split(":")
                startA, startB = region.split("-")

                chrA = chrB = chromosome
            
                endA = int(startA)+1
                endB = int(startB)+1

                final_svtype = {"coverageDEL":"coverage_deletion", "coverageDUP":"coverage_duplication"}[svtype]

            else: raise ValueError("%s not considered. %s"%(svtype, varID))

            r["chrA"] = chrA
            r["chrB"] = chrB
            r["startA"] = int(startA)
            r["startB"] = int(startB)
            r["endA"] = int(endA)
            r["endB"] = int(endB)
            r["svtype"] = final_svtype

            # debug
            if r.startA<0 or r.startB<0 or r.endA<0 or r.endB<0: 
                print(r.INFO_variantID)
                raise ValueError("There are <0 coordinates\n: %s"%r)

            return r

        SVdf = SVdf.apply(add_bedpe_r_to_SVdf_r, axis=1)

        # if you did not specify to drop all duplicates, keep each sample when necessary
        if only_unique_SVs is False: 

            # merge the df so that each sample appears more than once
            SV_df_all_nodups = SVdf_all[SVdf_all.type_var.isin(var_types)][["variantID_across_samples", "sampleID"]].drop_duplicates()
            SVdf = SV_df_all_nodups.merge(SVdf, on="variantID_across_samples", how="left", validate="many_to_one")

            if len(SV_df_all_nodups)!=len(SVdf): raise ValueError("The SVdf changed the size")
            if any(pd.isna(SVdf.svtype)): raise ValueError("There can't be NaNs")

        # save file
        save_object(SVdf, SV_df_oneVarPerRow_file)

    SVdf = load_object(SV_df_oneVarPerRow_file)

    return SVdf

import matplotlib.colors as mcolors

def get_matplotlib_color_as_hex(c):

    """Takes a matplotlib color and returns a hex"""

    # find the rgb
    if c in mcolors.BASE_COLORS: return mcolors.rgb2hex(mcolors.BASE_COLORS[c])
    elif c in mcolors.CSS4_COLORS: return mcolors.CSS4_COLORS[c]
    else: raise ValueError("invalid color %s"%c)


def get_shortChrom_from_chrName(chrName, species):

    """It gets the short chromosome for a given species"""

    # define the mtDNA
    taxID = sciName_to_taxID[species]
    if chrName in taxID_to_mtChromosome[taxID]: return "MT"

    # other chroms
    species_to_lambda_fn = {"Candida_parapsilosis":(lambda x: str(int(x.split(".")[0][-2:]))),
                            "Candida_albicans":(lambda x: str(x.split("chr")[1][0])),
                            "Candida_glabrata":(lambda x: x.split("_")[0][-1]),
                            "Candida_metapsilosis":(lambda x: str(int(x.split("scaffold")[1]))),
                            "Candida_orthopsilosis":(lambda x: x.split("_")[1].split(".")[0][-3:]),
                            "Candida_tropicalis":(lambda x: x.split("_")[1].split(".")[0][-2:]),
                            "Candida_auris":(lambda x: x.split(".")[0][-2:])}

                            
    return species_to_lambda_fn[species](chrName)

def get_sorted_chromosomes_from_interchromosomal_rearrangements(chrom_to_len, SV_df):

    """It gets a list with the sorted chromosomes according to the clustering of the numbers of chromosomes with rearrangements in SV_df"""

    # define a series with the pairs of chromosomes
    pairs_chrAB_series = SV_df[SV_df.chrA!=SV_df.chrB][["chrA", "chrB"]].apply(lambda r: set(r.values), axis=1)

    # map each chrom to the number of rearrangements (pseudocounted)
    chrom_to_nrearrangements = {c : sum(pairs_chrAB_series.apply(lambda x: c in x))+1 for c in chrom_to_len}

    # create the distance between each pair of chromosomes
    chrA_to_chrB_to_distanceInRearrangements = {chrA : {chrB : sum(pairs_chrAB_series==({chrA, chrB}))/max([chrom_to_nrearrangements[chrA], chrom_to_nrearrangements[chrB]]) for chrB in set(chrom_to_len)} for chrA in set(chrom_to_len)}

    df_pairwise_distances = pd.DataFrame(chrA_to_chrB_to_distanceInRearrangements)

    # clustermap to get the closest chromosomes
    cm = sns.clustermap(df_pairwise_distances, row_cluster=True, col_cluster=True)
    sorted_chromosomes = [t.get_text() for t in cm.ax_heatmap.get_xticklabels()]
    plt.close()

    if set(sorted_chromosomes)!=set(chrom_to_len): raise ValueError("not all chromosomes were considered")

    return sorted_chromosomes

def generate_biplot_SVs_one_species(SV_df, filename, svtype_to_color, svtype_to_marker, reference_genome, species):

    """This function generates filename with a biplot with the SVs in SV_df"""

    # define the chromosomes offset
    chrom_to_len = get_chr_to_len(reference_genome)
    sorted_chroms = sorted(chrom_to_len)
    genome_size = sum(chrom_to_len.values())

    chrom_to_Xoffset = {}
    current_offset = 0
    offset_val = int(genome_size/80)
    for chrom in sorted_chroms:
        chrom_to_Xoffset[chrom] = current_offset
        current_offset += chrom_to_len[chrom] + offset_val

    # define graphics
    chrom_to_color = get_value_to_color(sorted_chroms, palette="tab20", type_color="hex")[0]
    size_plot = chrom_to_Xoffset[sorted_chroms[-1]] + chrom_to_len[sorted_chroms[-1]] 
    width_rect = size_plot/20
    resolution = size_plot/50
    alpha_marker = .5

    # init fig
    fig = plt.figure(figsize=(10, 10))

    # add the plot of the 
    plt.plot([0, size_plot], [0, size_plot], color="gray", linestyle="--", linewidth=.8)
    ax = fig.axes[0]

    # add one rectangle for each chromosome and a grid with the chromosome boundaries
    for chrom in sorted_chroms:

        # define graphics about chrom
        offset = chrom_to_Xoffset[chrom]
        color = chrom_to_color[chrom]
        short_name =  get_shortChrom_from_chrName(chrom, species)

        # add lines for the chromosome boundaries
        for pos in [offset, offset+chrom_to_len[chrom]]:
            plt.plot([0, size_plot], [pos, pos], color=color, linewidth=.7, linestyle="-")
            plt.plot([pos , pos], [0, size_plot], color=color, linewidth=.7, linestyle="-")

        # add the horizontal patch and label
        rect = mpatches.Rectangle((offset, -width_rect), chrom_to_len[chrom], width_rect, linewidth=1, edgecolor='black', facecolor=color)
        ax.add_patch(rect)

        ax.text(offset+(chrom_to_len[chrom]/2), -(2*width_rect), short_name, color=color, weight="bold")

        # add the vertical pathc and label
        rect = mpatches.Rectangle((-width_rect, offset), width_rect, chrom_to_len[chrom], linewidth=1, edgecolor='black', facecolor=color)
        ax.add_patch(rect)

        ax.text(-(2*width_rect), offset+(chrom_to_len[chrom]/2), short_name, color=color, weight="bold")



    # add the variants
    print("adding the variants")
    for I, r in cp.deepcopy(SV_df).iterrows(): 

        # progress
        if (r.var_idx%100)==0: print("already traversed %.4f%s vars"%((r.var_idx/len(SV_df))*100, "%"))


        # for balanced translocations, reverse the vals
        if r.svtype=="bal_translocation":

            chrB = r.chrA
            startB = r.startA
            endB = r.endA
            chrA = r.chrB
            startA = r.startB
            endA = r.endB

            r.chrA, r.startA, r.endA = chrA, startA, endA
            r.chrB, r.startB, r.endB = chrB, startB, endB


        # define the number of points
        lenA = r.endA-r.startA
        npoints = int(lenA/resolution)+1


        # define the xs and ys
        xs = np.linspace(r.startA+chrom_to_Xoffset[r.chrA], r.endA+chrom_to_Xoffset[r.chrA], npoints)
        ys = [r.startB+chrom_to_Xoffset[r.chrB]]*npoints

        # plot a scatterplot with the points
        plt.scatter(xs, ys, s=10, edgecolor=svtype_to_color[r.svtype], facecolor=svtype_to_color[r.svtype], linewidth=1.5, marker=svtype_to_marker[r.svtype], alpha=alpha_marker)

        # if there are more than one point, add a line
        if npoints>1: plt.plot(xs, ys, color=svtype_to_color[r.svtype], linewidth=2, alpha=alpha_marker)


    # add the legend

    interchromosomal_events = ["bal_translocation", "inverted_cutPaste_insertion"]
    intra_and_inter_events = ["unclassified_breakpoint", "copyPaste_insertion", "cutPaste_insertion"]
    intrachromosomal_events = ["inverted_copyPaste_insertion", "inverted_translocation", "inversion", "tandem_duplication", "deletion"]

    legend_elements = [("white", "inter-chromosome:", "o")] + [(svtype_to_color[e], e, svtype_to_marker[e]) for e in interchromosomal_events] + [("white", "inter/intra-chromosome:", "o")] + [(svtype_to_color[e], e, svtype_to_marker[e]) for e in intra_and_inter_events] + [("white", "intra-chromosome:", "o")] + [(svtype_to_color[e], e, svtype_to_marker[e]) for e in intrachromosomal_events]

    legend_elements = [Line2D([0], [0], markeredgecolor=c, markerfacecolor=c, lw=0, markeredgewidth=2, label=l, alpha=alpha_marker, marker=m, c=c, linestyle="", markersize=10) for c,l,m in legend_elements]
    ax.legend(handles=legend_elements, loc='right', bbox_to_anchor=(1.4, 0.5))

    # add final graphics
    ax.set_title(species)
    
    for spine in ["bottom", "top", "right", "left"]: ax.spines[spine].set_color('white')
    ax.set_xticks([])
    ax.set_xticklabels([])
    ax.set_yticks([])
    ax.set_yticklabels([])

    ax.set_xlabel("SV origin")
    ax.set_ylabel("SV destination")

    #plt.axis('off')

    # remove the axes
    
    # save
    print("saving")
    fig.savefig(filename, bbox_inches='tight');


    #print("plot generated")
    #sys.exit(0)

def get_float_or_int(x):

    """Returns a float or an int"""

    if float(int(x))==float(x): return int(x)
    else: return float(x)

def plot_histograms_rows_in_species_vatypes_in_cols(df_vars, sorted_vartypes, xfield, filename, vartype_field, logscale_x=True, logscale_y=True, pseudocount=0, xlabel="", input_xticks=None, input_yticks=None, extended_xrange=0.1, ylabel="number variants", species_to_vartype_to_xlines=None, add_xlim=[0,0], add_ylim=[0,0], xlim=None, ylim=None, title="", size_multiplier=0.8, text_nelements_above_some_x_min_x=None, text_nelements_above_some_x_text_coords=None):

    """This function takes a df with variants and plots a subplot where the rows are species and the columns are different svtypes """

    df_vars = cp.deepcopy(df_vars)

    ############### DEFINE GRAPHICS #############

    vartype_to_color = {
                       # interchromosomal events
                       'bal_translocation':'lightcoral', # tomato
                       'inverted_cutPaste_insertion':'green',

                       # both inter and intrachromosomal
                       'unclassified_breakpoint':'gray',
                       'copyPaste_insertion':'blue',
                       'cutPaste_insertion':'magenta',

                       # intrachromosomal
                       'inverted_copyPaste_insertion':'orange',
                       'inverted_translocation':'olive',
                       'inversion':'black',
                       'tandem_duplication':'cyan',
                       'deletion':'red',
                       "coverage_deletion":"crimson",
                       "coverage_duplication":"dodgerblue",

                       # generalized vars
                       "SV": "navy",
                       "coverageCNV":"cyan",

                       # small vars
                       "SNP":"red",
                       "IN/DEL":"magenta",

                       # selection vars
                       "if_INDEL":"magenta",
                       "DEL": "grey",
                       "DUP": "navy"
                       }


    vartype_to_shortName = {
                       # interchromosomal events
                       'bal_translocation':'tra', # tomato
                       'inverted_cutPaste_insertion':'cut-paste\n(inv)',

                       # both inter and intrachromosomal
                       'unclassified_breakpoint':'unknown',
                       'copyPaste_insertion':'copy-paste\nins',
                       'cutPaste_insertion':'cut-paste\nins',

                       # intrachromosomal
                       'inverted_copyPaste_insertion':'copy-paste\n(inv)',
                       'inverted_translocation':'tra (inv)',
                       'inversion':'inv',
                       'tandem_duplication':'tdup',
                       'deletion':'del',
                       "coverage_deletion":"del\n(cov)",
                       "coverage_duplication":"dup\n(cov)",

                       # generalized vars
                       "SV": "SV",
                       "coverageCNV":"coverageCNV",

                       # small vars
                       "SNP":"SNP",
                       "IN/DEL":"IN/DEL",
                       
                       # selection vars
                       "if_INDEL":"if_INDEL",
                       "DEL": "DEL",
                       "DUP": "DUP"

                       }


    sorted_species = sorted_species_byPhylogeny

    #############################################

    # redefine as log the x
    if logscale_x is True: 
        
        # redefine the values of the pseudocount
        df_vars[xfield] = np.log10(df_vars[xfield] + pseudocount)
        
        # redefine the species_to_vartype_to_xlines
        if species_to_vartype_to_xlines is not None: species_to_vartype_to_xlines = {species : {vartype : [np.log10(x+pseudocount) for x in xlines] for vartype, xlines in vartype_to_xlines.items()} for species, vartype_to_xlines in species_to_vartype_to_xlines.items()}

    # define the range
    xrange_hist = (min(df_vars[xfield])-extended_xrange, max(df_vars[xfield])+extended_xrange)

    ###### PLOT FIG #####

    ####### FIND THE FIG LIMS #########
    print("calculating plot limits")

    all_ylims = []
    all_xlims = []

    # get the lims for all plots
    for Is, species in enumerate(sorted_species):
        for Iv, vartype in enumerate(sorted_vartypes):

            # get the df of this species and vatyype
            df_plot = df_vars[(df_vars.species==species) & (df_vars[vartype_field]==vartype)]

            # add the seaborn distplot to get the axis
            if len(df_plot)>0: 

                ax = sns.distplot(df_plot[xfield], kde=False, rug=False, hist=True, hist_kws={"color":vartype_to_color[vartype], "linewidth":2, "alpha":1.0, "range":xrange_hist}, bins=25)

                all_ylims += list(ax.get_ylim())
                all_xlims += list(ax.get_xlim())
                plt.close()

        if xlim is None: xlim = [min(all_xlims)-add_xlim[0], max(all_xlims)+add_xlim[1]]
        if ylim is None: ylim = [min(all_ylims)-add_ylim[0], max(all_ylims)+add_ylim[1]]

    ###################################
    print("plotting distribution")

    # init figure
    nrows = len(sorted_species)
    ncols = len(sorted_vartypes)
    fig = plt.figure(figsize=(ncols*size_multiplier, nrows*size_multiplier)); Iplot = 1
        
    # iterate thrpugh rows and then cols
    for Is, species in enumerate(sorted_species):
        for Iv, vartype in enumerate(sorted_vartypes):
            print(species, vartype)

            # get the df of this species and vatyype
            df_plot = df_vars[(df_vars.species==species) & (df_vars[vartype_field]==vartype)]

            # init subplot
            ax = plt.subplot(nrows, ncols, Iplot); Iplot+=1

            # add the seaborn distplot
            if len(df_plot)>0: 

                sns.distplot(df_plot[xfield], kde=False, rug=False, hist=True, hist_kws={"color":vartype_to_color[vartype], "linewidth":2, "alpha":1.0, "range":xrange_hist}, bins=25)

                # add the lines
                if species_to_vartype_to_xlines is not None: 
                    for x in species_to_vartype_to_xlines[species][vartype]: plt.axvline(x, color="k", linestyle="--", linewidth=.9)

                # add text of the number of elements that are above some x
                if text_nelements_above_some_x_min_x is not None: plt.text(text_nelements_above_some_x_text_coords[0], text_nelements_above_some_x_text_coords[1], str(sum(df_plot[xfield]>=text_nelements_above_some_x_min_x)), color=vartype_to_color[vartype])



            # set the ylabel
            if Iv==0 and Is==int(len(sorted_species)/2): 
                ax.set_ylabel("%s\n%s"%(ylabel, species_to_shortName[species]))
            elif Iv==0: ax.set_ylabel(species_to_shortName[species])
            else: 
                ax.set_ylabel("")
                
            # set the xlim the same for all
            ax.set_xlim(xrange_hist)



            # set title
            if Is==0 and Iv==0: ax.set_title("%s\n%s"%(title, vartype_to_shortName[vartype]), rotation=90)
            elif Is==0: ax.set_title(vartype_to_shortName[vartype], rotation=90) # rotation=90

            # set the xlabel
            if species==sorted_species[-1] and vartype==sorted_vartypes[int(len(sorted_vartypes)/2)]: ax.set_xlabel(xlabel)
            else: ax.set_xlabel("")


            # set the yaxis to be log scale if demanded
            if logscale_y is True and len(df_plot)>0: ax.set_yscale('log', nonposy='clip')

            # set the limits
            ax.set_ylim(ylim)
            ax.set_xlim(xlim)

            # add the yticks and labels
            if logscale_y is True:

                if input_yticks is None: yticks = [int(y) for y in [1e1, 1e2, 1e3, 1e4, 1e5, 1e6] if y<=ylim[1]]
                else: yticks = input_yticks

                ax.set_yticks(yticks)
                ax.set_yticklabels([str(get_float_or_int(y)) for y in yticks])

            # add the xticks if log
            if logscale_x is True:

                if input_xticks is None: xticks = [x for x in range(-3, 10) if x<=xlim[1] and x>=xlim[0]]
                else: xticks = [np.log10(x) for x in input_xticks]
                ax.set_xticks(xticks)
                ax.set_xticklabels([str(get_float_or_int(10**x)) for x in xticks], rotation=90)


            # remove the yticklabels when neccesary
            if Iv!=0: ax.set_yticklabels([]) 
            if species!=sorted_species[-1]: ax.set_xticklabels([])

            # add a text
            xtext = xlim[0]+((xlim[1]-xlim[0])/2)

            if logscale_y is False: ytext = ylim[0]+((ylim[1]-ylim[0])/2)
            else: ytext = ylim[0]+((ylim[1]-ylim[0])/2)

            if len(df_plot)==0: plt.text(xtext, ytext, "NA", color=vartype_to_color[vartype],  ha='center', va='center', fontsize=11) # fontweight="bold"

    plt.subplots_adjust(wspace=0.05, hspace=0.05)
    plt.show()
    print("saving %s"%filename)
    fig.savefig(filename, bbox_inches="tight")
    plt.close(fig)


    #####################


def get_df_MAF_from_df_vars(df_vars, nsamples, variant_f="#Uploaded_variation", var_type_f="var_type"):

    """This function takes a df vars where each line is one variants and it returns a df withe the minor allele frequencies. As we don't have GTs for SV dfs"""

    # map each variant to the MAF
    def get_sum_AF_from_GT_from_df_var(df_var): return sum(df_var.AF_from_GT)
    variant_to_MAF = df_vars[[variant_f, "sampleID", "AF_from_GT"]].drop_duplicates().groupby(variant_f).apply(get_sum_AF_from_GT_from_df_var)/nsamples

    # keep only variants that are in less than 0.5 of the samples
    variant_to_MAF = variant_to_MAF[variant_to_MAF<=0.5]


    # create the df with the numbers of variants
    df_MAF = pd.DataFrame({"MAF":variant_to_MAF}).reset_index(drop=False)

    variant_to_type= dict(df_vars[[variant_f, var_type_f]].drop_duplicates().set_index(variant_f)[var_type_f])
    df_MAF["var_type"] = df_MAF[variant_f].map(variant_to_type)
    if any(pd.isna(df_MAF.var_type)): raise ValueError("There can't be NaNs")

    return df_MAF

def plot_variants_frequency_distribution(DataDir, ProcessedDataDir, PlotsDir, species_to_tree):

    """This function will plot the distribution of different SVtypes"""

    ####### GET DF SVs ########

    df_varFrequencies_SVs = pd.DataFrame()

    # go through each species
    for taxID, species in taxID_to_sciName.items():
        print(species)

        # debug
        #if species!="Candida_parapsilosis": continue

        # load a df where each row is one unique SV, all samples present
        taxID_dir = "%s/%s_%i"%(DataDir, species, taxID)
        SV_CNV_filt_file = "%s/integrated_varcalls/SV_CNV_filt.py"%(taxID_dir)
        SV_df_oneVarPerRow_file = "%s/%s_SVs_CNVs_oneVarPerRow_allSamples.py"%(ProcessedDataDir, species)
        #remove_file(SV_df_oneVarPerRow_file); continue
        SV_df = get_SV_df_oneVarPerRow_df(SV_CNV_filt_file, SV_df_oneVarPerRow_file, only_unique_SVs=False, threads=threads, var_types={"SV", "coverageCNV"})

        # set the fact that all SVs have AF=1
        SV_df["AF_from_GT"] = 1.0

        # get the df with the minor allele freq
        nsamples = len(set(species_to_tree[species].get_leaf_names()))
        df_MAF = get_df_MAF_from_df_vars(SV_df, nsamples, variant_f="variantID_across_samples", var_type_f="svtype")

        # add the species
        df_MAF["species"] = species

        # keep
        df_varFrequencies_SVs = df_varFrequencies_SVs.append(df_MAF)

    #######################

    ########### GET DF VAR FREQUENCIES SMALL VARS ###########

    df_varFrequencies_smallVars_file = "%s/small_varFrequencies_df.py"%ProcessedDataDir
    if file_is_empty(df_varFrequencies_smallVars_file):
        print("getting df_varFrequencies_smallVars_file")

        # init df
        df_varFrequencies_smallVars = pd.DataFrame()

        # go through each species
        for taxID, species in taxID_to_sciName.items():
            print(species)

            #if species!="Candida_auris": continue

            # load the df
            print("loading df")
            df_vars = load_object("%s/%s_%i/integrated_varcalls/smallVars_filt.py"%(DataDir, species, taxID))[["#Uploaded_variation", "sampleID", "ISSNP", "common_GT", "calling_ploidy"]]

            # get the df with the correct ploidy and some GT called
            print("getting the MAFs")
            df_vars = df_vars[(df_vars.calling_ploidy==taxID_to_ploidy[taxID]) & (df_vars.common_GT!=".")]

            # add the af_from_GT
            df_vars["common_GT"] = df_vars.common_GT.apply(str)
            strange_GTs = set(df_vars.common_GT).difference({"1", "0/1", "1/1"})
            if len(strange_GTs)>0: raise ValueError("There are strange gts: %s"%strange_GTs)

            df_vars["AF_from_GT"] = df_vars.common_GT.map({"1":1.0, "0/1":0.5, "1/1":1.0})
            if any(pd.isna(df_vars.AF_from_GT)): raise ValueError("There can't be NaNs")

            # add the type_var
            df_vars["type_var"] = df_vars.ISSNP.map({True:"SNP", False:"IN/DEL"})
            if any(pd.isna(df_vars.type_var)): raise ValueError("There can't be NaNs")

            # get the df with the minor allele freq
            nsamples = len(set(species_to_tree[species].get_leaf_names()))
            df_MAF = get_df_MAF_from_df_vars(df_vars, nsamples, variant_f="#Uploaded_variation", var_type_f="type_var")

            # add the species
            df_MAF["species"] = species

            # keep
            df_varFrequencies_smallVars = df_varFrequencies_smallVars.append(df_MAF)

        # save
        save_object(df_varFrequencies_smallVars, df_varFrequencies_smallVars_file)

    # load
    df_varFrequencies_smallVars = load_object(df_varFrequencies_smallVars_file)

    #########################################################

    ####### PLOT DISTPLOT ########

    print("plotting distplot")

    sorted_svtypes = ["deletion", "tandem_duplication", "inversion", "copyPaste_insertion", "cutPaste_insertion", "bal_translocation", "inverted_copyPaste_insertion", "inverted_translocation", "inverted_cutPaste_insertion", "unclassified_breakpoint", "coverage_deletion", "coverage_duplication"]

    sorted_species = ["Candida_glabrata", "Candida_auris", "Candida_albicans", "Candida_tropicalis", "Candida_metapsilosis", "Candida_orthopsilosis", "Candida_parapsilosis"]


    sorted_smallVarTypes = ["SNP", "IN/DEL"]

    # define the var iterables
    var_iterables = [("small vars", df_varFrequencies_smallVars, sorted_smallVarTypes, [0.01, 0.1, 0.5], [100, 1000, 10000]),
                     ("SVs", df_varFrequencies_SVs, sorted_svtypes, [0.01, 0.1, 0.5], [10, 100, 1000])]


    # define the minimum AF
    #min_AF = min([min(df_varFrequencies_smallVars.MAF), min(df_varFrequencies_SVs.MAF)])
    #max_AF = max([max(df_varFrequencies_smallVars.MAF), max(df_varFrequencies_SVs.MAF)])

    # get one set of histograms for each plot
    for Iv, (var_type, df_MAF, sorted_typeVars, xticks, yticks)  in enumerate(var_iterables):
        print(var_type)


        filename = "%s/MAF_distribution_%s.pdf"%(PlotsDir, var_type)
        plot_histograms_rows_in_species_vatypes_in_cols(df_MAF, sorted_typeVars, "MAF", filename, "var_type", logscale_x=True, logscale_y=True, pseudocount=0, xlabel="Minor Allele Frequency", input_xticks=xticks, input_yticks=yticks, extended_xrange=0.1)



    ##############################

def plot_SVs_len_distribution(DataDir, PlotsDir, ProcessedDataDir):

    """Gets plots with the SV length distribution for all unique variants, only for those SVs where it makes sense to consider length"""

    ########## GET THE DF ##########
    df_lengths_SVs = pd.DataFrame()

    # go through each species
    for taxID, species in taxID_to_sciName.items():
        print(species)

        # debug
        #if species=="Candida_albicans": continue
        #if species!="Candida_parapsilosis": continue
        #if species!="Candida_auris": continue

        # load a df where each row is one unique SV
        SV_CNV_filt_file = "%s/%s_%i/integrated_varcalls/SV_CNV_filt.py"%(DataDir, species, taxID)
        SV_df_oneVarPerRow_file = "%s/%s_SVs_CNVs_oneVarPerRow_onlyVars.py"%(ProcessedDataDir, species)
        #remove_file(SV_df_oneVarPerRow_file); continue
        SV_df = get_SV_df_oneVarPerRow_df(SV_CNV_filt_file, SV_df_oneVarPerRow_file, only_unique_SVs=True, var_types={"SV", "coverageCNV"})

        # keep the SVtypes where len calculation makes sense
        SV_df = SV_df[SV_df.svtype.isin({"inverted_cutPaste_insertion", "copyPaste_insertion", "cutPaste_insertion", "inverted_copyPaste_insertion", "inverted_translocation", "inversion", "tandem_duplication", "deletion", "coverage_deletion", "coverage_duplication"})]

        # add different lengths
        SV_df["lenA"] = SV_df.endA-SV_df.startA
        SV_df["lenA_to_B"] = SV_df.endB-SV_df.startA

        # map each svtype to the length field
        svtype_to_lenField = {"inverted_cutPaste_insertion":"lenA", "copyPaste_insertion":"lenA","cutPaste_insertion":"lenA","inverted_copyPaste_insertion":"lenA","inverted_translocation":"lenA_to_B","inversion":"lenA_to_B","tandem_duplication":"lenA_to_B","deletion":"lenA_to_B", "coverage_deletion":"lenA_to_B", "coverage_duplication":"lenA_to_B"}
        SV_df["len_field"] = SV_df.svtype.map(svtype_to_lenField)

        def get_len_event(r): return r[r.len_field]
        SV_df["len_event"] = SV_df[["len_field", "lenA", "lenA_to_B"]].apply(get_len_event, axis=1)

        # keep df
        SV_df["species"] = species
        df_lengths_SVs = df_lengths_SVs.append(SV_df)

    ################################

    # plot histogram
    filename = "%s/SV_length_distribution.pdf"%PlotsDir
    sorted_svtypes = ["deletion", "tandem_duplication", "inversion", "copyPaste_insertion", "cutPaste_insertion", "inverted_copyPaste_insertion", "inverted_translocation", "inverted_cutPaste_insertion", "coverage_deletion", "coverage_duplication"]

    #sorted_svtypes = ["deletion", "tandem_duplication", "inversion", "copyPaste_insertion", "cutPaste_insertion", "bal_translocation", "inverted_copyPaste_insertion", "inverted_translocation", "inverted_cutPaste_insertion", "unclassified_breakpoint"]

    plot_histograms_rows_in_species_vatypes_in_cols(df_lengths_SVs, sorted_svtypes, "len_event", filename, "svtype", logscale_x=True, logscale_y=True, pseudocount=0, xlabel="length event (bp)", input_xticks=[100, 10000, 1000000])




def get_repeat_maskerDF_providedRepeatFamilies(reference_genome, repeat_families, threads=4, replace=False, skip_lowComplexity_and_BacterialRepeatAnnotation=True, repeats_table_file=None):

    """gets the repeat masker outfile as a pandas df. The repeatmasker locations are 1-based (https://bedops.readthedocs.io/en/latest/content/reference/file-management/conversion/rmsk2bed.html). It uses the repeat_families as fasta."""

    # define the table 
    if repeats_table_file is None: repeats_table_file = "%s.repeats.tab"%reference_genome
    repeats_table_file_tmp = "%s.tmp"%repeats_table_file

    if file_is_empty(repeats_table_file) or replace is True:
        print("running RepeatMasker into %s"%repeats_table_file)

        ############ RUN REPEAT MASKER WITH THE PROVIDED FAMILIES ###########

        # define the outdir
        repeat_masker_outdir = "%s.repeat_masker_outdir"%reference_genome; delete_folder(repeat_masker_outdir); make_folder(repeat_masker_outdir)
        repeat_masker_std = "%s/repeatMasker.std"%repeat_masker_outdir

        # run repeat masker 
        repeat_masker_outfile = "%s/%s.out"%(repeat_masker_outdir, get_file(reference_genome))
        if file_is_empty(repeat_masker_outfile): 

            repeatMasker_cmd = "cd %s && RepeatMasker -pa %i -dir %s -poly -html -gff"%(repeat_masker_outdir, threads, repeat_masker_outdir)

            if skip_lowComplexity_and_BacterialRepeatAnnotation is True: repeatMasker_cmd += " -no_is -nolow"
            if repeat_families is not None: repeatMasker_cmd += " -lib %s"%repeat_families

            repeatMasker_cmd += " %s > %s 2>&1"%(reference_genome, repeat_masker_std)

            print("running:\n%s\n"%repeatMasker_cmd)
            run_cmd(repeatMasker_cmd, env="perSVade_env_RepeatMasker_env")

        ######################################################################

        ##### INTEHRATE INTO DF ######

        # define the header
        header = ["SW_score", "perc_div", "perc_del", "perc_ins", "chromosome", "begin_repeat", "end_repeat", "left_repeat", "strand", "repeat", "type", "position_inRepeat_begin", "position_inRepeat_end", "left_positionINrepeat",  "IDrepeat"]
        function_map_type = [int, float, float, float, str, int, int, str, str, str, str, str, str, str, int]

        # check if there are repetitive elements
        if open(repeat_masker_outfile, "r").readlines()[0].startswith("There were no repetitive sequences detected in"): df = pd.DataFrame(columns=header)

        else:

            # initialize header
            dict_repeat_masker = {}

            # go line by line
            for IDline, line in enumerate(open(repeat_masker_outfile, "r").readlines()):

                # debug lines
                split_line = line.strip().split()
                if len(split_line)==0 or split_line[0] in {"SW", "score"}: continue
                if split_line[-1]=="*": del split_line[-1]

                # keep
                line_content = [function_map_type[I](content) for I, content in enumerate(split_line)]
                dict_repeat_masker[IDline] = dict(zip(header, line_content))

            # get as df
            df = pd.DataFrame(dict_repeat_masker).transpose().sort_values(by=["chromosome", "begin_repeat", "end_repeat"])

            # get df
            df = df.drop_duplicates(subset=[x for x in df.keys() if x not in {"IDrepeat"}], keep="first")
            df["IDrepeat"] = list(range(1, len(df)+1))
            df.index = list(range(1, len(df)+1))

        # write
        df.to_csv(repeats_table_file_tmp, sep="\t", header=True, index=False)
        os.rename(repeats_table_file_tmp, repeats_table_file)

        ##############################

    # load df
    df = pd.read_csv(repeats_table_file, sep="\t")

    return df

def get_intersection_sets(setA, setB): return setA.intersection(setB)

def get_similarity_between_repeatFamilies_one_to_another(df_repeats, from_fam, to_fam):

    """This function returns the similarity between repeats from from_fam to to_fam. It takes all the repeats predicted as 'to_fam'. It calculates how many of these repeats are overlapping a prediction of 'from_fam'."""

    # define a df with the repeats that have 'to_fam' as the query repeat
    df_repeats_to_fam = df_repeats.loc[{to_fam}]
    if len(df_repeats)==0: raise ValueError("to_fam %s was never predicted"%to_fam)

    # define the set of rowIDs that are related to from_fam
    unique_rowIDS_from_fam = set(df_repeats.loc[{from_fam}].unique_rowID)

    # calculate the fraction of to_fam rows that are overlapped by some from_fam repeat
    similarity_score = sum(df_repeats_to_fam.overlapping_unique_rowIDs.apply(get_intersection_sets, setB=unique_rowIDS_from_fam).apply(len)>0)/len(df_repeats_to_fam)
    if similarity_score>1: raise ValueError("similarity score can't be above 0")

    return similarity_score



def get_similarity_between_repeatFamilies(df_repeats, famA, famB):

    """This function takes two repeat families and returns the similarity between them (according to the fraction of overlapping matches). This is a number between 0 and 1. It is the mean between famA_to_famB and famB_to_famA"""

    iterable_fams = [(famA, famB), (famB, famA)]
    return np.mean([get_similarity_between_repeatFamilies_one_to_another(df_repeats, from_fam, to_fam) for from_fam, to_fam in iterable_fams])

def get_IDmapped_repeat_families(DataDir, ProcessedDataDir, PlotsDir, species_to_ref_genome, threads=4, replace=False, plots=False):

    """This function defines the ID mapping between orthologous repeat families of different species. It returns a dict with repeatName_to_clusterID"""


    ######### RUN REPEAT MASKER WITH THE FAMILIES OF OTHER SPECIES ############

    # define an outdir
    outdir_crossRepeatMasker = "%s/cross_repeat_masker"%ProcessedDataDir; make_folder(outdir_crossRepeatMasker)
    df_repeatsAll_file = "%s/df_repeatsAll.py"%outdir_crossRepeatMasker

    if file_is_empty(df_repeatsAll_file) or replace is True:

        df_repeatsAll = pd.DataFrame()

        for species_query, refGenome_query in species_to_ref_genome.items():
            for species_subject, refGenome_subject in species_to_ref_genome.items():
                print("running %s on %s"%(species_query, species_subject))

                # define the query species families
                query_species_repeatFamilies = "%s.repeat_modeler_outdir/reference_genome.fasta-families.fa"%refGenome_query

                # define the outdir and add the files there
                outdir_repeatmasker = "%s/%s_Families_on_%s_Genome"%(outdir_crossRepeatMasker, species_query, species_subject); make_folder(outdir_repeatmasker)
                linked_refGenome_subject = "%s/subject_ref_genome.fasta"%outdir_repeatmasker
                linked_query_species_repeatFamilies = "%s/query_families.fasta"%outdir_repeatmasker

                soft_link_files(refGenome_subject, linked_refGenome_subject)
                soft_link_files(query_species_repeatFamilies, linked_query_species_repeatFamilies)

                # run the repeat masker using the query families 
                df_repeats = get_repeat_maskerDF_providedRepeatFamilies(linked_refGenome_subject, linked_query_species_repeatFamilies, threads=threads, replace=replace)

                # keep
                df_repeats["query_families_species"] = species_query
                df_repeats["subject_genome_species"] = species_subject
                df_repeatsAll = df_repeatsAll.append(df_repeats)

        # save
        save_object(df_repeatsAll, df_repeatsAll_file)

    df_repeatsAll = load_object(df_repeatsAll_file).reset_index(drop=True)

    # add some unique repeatIDs, which maps 
    df_repeatsAll["repeat_family"] = df_repeatsAll["repeat"] + "#" + df_repeatsAll.query_families_species

    ###########################################################################

    ######### SIMILARITY BTW REPEATS #############

    # define the file
    pairwise_similarity_df_file = "%s/pairwise_similarity_df.py"%outdir_crossRepeatMasker

    if file_is_empty(pairwise_similarity_df_file) or replace is True:
        print("calculating pairwise similarity")

        # add fields for bed
        df_repeatsAll["unique_chromosome"] = df_repeatsAll.chromosome + "_" + df_repeatsAll.subject_genome_species
        df_repeatsAll["begin_repeat_0based"] = df_repeatsAll.begin_repeat-1

        # sort
        df_repeatsAll = df_repeatsAll.sort_values(by=["unique_chromosome", "begin_repeat_0based", "end_repeat"]).reset_index(drop=True)

        # add  unique ID
        df_repeatsAll["unique_rowID"] = list(range(len(df_repeatsAll)))

        # run bedmap to define repeats that are similar (they should overlap by >50% reciprocally)
        bedmap_outfile = "%s/bedmap_outfile.txt"%outdir_crossRepeatMasker
        if file_is_empty(bedmap_outfile) or replace is True:
            print("running bedmap")

            # write bed
            repeats_bed = "%s/all_repeats.bed"%outdir_crossRepeatMasker
            df_repeatsAll[["unique_chromosome", "begin_repeat_0based", "end_repeat", "unique_rowID"]].to_csv(repeats_bed, sep="\t", header=False, index=False)

            # run bedmap to macth
            bedmap_outfile_tmp = "%s.tmp"%bedmap_outfile
            run_cmd("bedmap --fraction-both 0.5 --echo-map-id --delim '\t' %s > %s"%(repeats_bed, bedmap_outfile_tmp), env="perSVade_env")
            os.rename(bedmap_outfile_tmp, bedmap_outfile)

        # add the overlapping unique map IDs
        df_repeatsAll["overlapping_unique_rowIDs"] = [{int(x) for x in l.strip().split(";")} for l in open(bedmap_outfile, "r").readlines()]

        # cross mapping dict
        sorted_repeat_families = sorted(set(df_repeatsAll.repeat_family))
        print("There are %i fams"%len(sorted_repeat_families))
        df_repeatsAll_minimal = df_repeatsAll.set_index("repeat_family")[["unique_rowID", "overlapping_unique_rowIDs"]]

        paiwise_combinations_families = list(itertools.combinations(sorted_repeat_families, 2)) + [(x,x) for x in sorted_repeat_families]
        pairwise_similarity_df = pd.DataFrame({"pairwise_similarity" : dict(zip(paiwise_combinations_families , map(lambda x: get_similarity_between_repeatFamilies(df_repeatsAll_minimal, x[0], x[1]),  paiwise_combinations_families) ))})

        pairwise_similarity_df["familyA"] = pairwise_similarity_df.index.get_level_values(0)
        pairwise_similarity_df["familyB"] = pairwise_similarity_df.index.get_level_values(1)
        pairwise_similarity_df = pairwise_similarity_df.reset_index(drop=True)

        # save
        save_object(pairwise_similarity_df, pairwise_similarity_df_file)

    pairwise_similarity_df_all = load_object(pairwise_similarity_df_file)

    #########################################

    ########### PLOT THE PAIRWISE SIMILARITIES ##########

    if plots is True:

        # one plot for different types of families
        for type_df in ["all_vars", "only_families_with_homologs"]:

            # define the pairwise_similarity_df
            if type_df=="all_vars": 

                pairwise_similarity_df = pairwise_similarity_df_all
                figsize = (12,12)
                annot = None

            else: 

                # get the comparisons where there is a family with some relatives
                pairwise_similarity_df_similarClusters = pairwise_similarity_df_all[(pairwise_similarity_df_all.pairwise_similarity>=0.5) & (pairwise_similarity_df_all.familyA!=pairwise_similarity_df_all.familyB)]

                familes_similar = set(pairwise_similarity_df_similarClusters.familyA).union(pairwise_similarity_df_similarClusters.familyB)

                pairwise_similarity_df = pairwise_similarity_df_all[(pairwise_similarity_df_all.familyA.isin(familes_similar)) & (pairwise_similarity_df_all.familyB.isin(familes_similar))]

                figsize = (9,9)
                annot = True

            # get the complementary df
            pairwise_similarity_df_complement = pairwise_similarity_df.rename(columns={"familyA":"familyB", "familyB":"familyA"})

            # get a df square
            df_square = pairwise_similarity_df.append(pairwise_similarity_df_complement).drop_duplicates(subset=["familyA", "familyB"]).pivot(index="familyA", columns="familyB", values="pairwise_similarity")
            sorted_families = sorted(set(df_square.index))
            df_square = df_square.loc[sorted_families, sorted_families]

            # get the row colors df
            colors_df = pd.DataFrame({"species": {f : species_to_color[f.split("#")[-1]] for f in sorted_families}}).loc[sorted_families]

            # plot the clustermap
            cm = sns.clustermap(df_square, row_cluster=True, col_cluster=True, col_colors=colors_df, row_colors=colors_df, figsize=figsize, square=True, annot=annot, annot_kws={"size": 6.5})
            #cm.ax_heatmap.set_xticklabels(list(cm.ax_heatmap.get_xticklabels()), fontsize=6)
            cm.ax_col_dendrogram.set_title(type_df)

            cm.savefig("%s/pairwise_similarity_repeats_%s.pdf"%(PlotsDir, type_df))

            plt.show()

    # define clusters (those that are linked by pairwise links of 50%)
    list_clusters = []

    for I,r in pairwise_similarity_df_all[(pairwise_similarity_df_all.pairwise_similarity>=0.5) & (pairwise_similarity_df_all.familyA!=pairwise_similarity_df_all.familyB)].iterrows():

        # define family
        set_families = set([r.familyA, r.familyB])
        is_in_existingCluster = False

        # add to an existing cluster
        for existing_cluster in list_clusters:
            if len(set_families.intersection(existing_cluster))>0:
                existing_cluster.update(set_families)
                is_in_existingCluster = True
                break

        # create new cluster if necessary
        if is_in_existingCluster is False: list_clusters.append(set_families)

    list_clusters = sorted([tuple(sorted(x)) for x in list_clusters])

    #####################################################


    ####### DEFINE THE FINAL DICT ########

    # # check that all repeat families are in the expected repeats
    all_cluster_families = set(df_repeatsAll.repeat_family)
    cluster_families_in_cluster = set(make_flat_listOflists(list_clusters))
    strange_cluster_families_in_cluster = cluster_families_in_cluster.difference(all_cluster_families)
    if len(strange_cluster_families_in_cluster)>0: raise ValueError("there are some strange families in cluster_families_in_cluster: %s"%strange_cluster_families_in_cluster)

    # define the dict that maps to the cluster
    family_to_clusterID = {}

    for Icluster, families in enumerate(list_clusters):

        # define a prefix that depends on whether there are more than one species in the cluster
        species_in_families = {x.split("#")[1] for x in families}
        if len(species_in_families)>1: species_prefix = "multiSpecies"
        else: species_prefix = "singleSpecies"

        # keep the clustes
        for f in families: family_to_clusterID[f] = "%s_cluster%i"%(species_prefix, Icluster)

    # add for the undefined families
    sorted_unassinged_families = sorted(all_cluster_families.difference(cluster_families_in_cluster))
    for f in sorted_unassinged_families:

        Icluster+=1
        family_to_clusterID[f] = "singleton_cluster%i"%Icluster

    ######################################


    ####### RUN ORTHOFINDER FOR ALL FAMILIES ########

    # this did not yield acceptable results (no single orthologous group), so we don't use it

    """
    # define outdir
    output_dir = "%s/repeats_families_orthofinder_outdir"%ProcessedDataDir;  

    if os.path.isdir(output_dir): orthogroups_file = "%s/Orthogroups/Orthogroups.txt"%(os.listdir(output_dir)[0])
    else: orthogroups_file = ""

    if file_is_empty(orthogroups_file) or replace is True:

        # prepare the data for orthofinder running
        input_dir = "%s/repeats_families_from_repeatModeller"%ProcessedDataDir
        delete_folder(input_dir)
        make_folder(input_dir)
        for species, reference_genome in species_to_ref_genome.items():
            print(species)

            # move the families to input dir
            repeatFamilies = "%s.repeat_modeler_outdir/reference_genome.fasta-families.fa"%reference_genome

            # get into input_dir
            dest_repeatFamilies = "%s/%s.fasta"%(input_dir, species)
            seq_records = [SeqRecord(seq.seq, id="%s#%s"%(seq.id, species), name="", description="") for seq in SeqIO.parse(repeatFamilies, "fasta")]
            SeqIO.write(seq_records, dest_repeatFamilies, "fasta")

        # run orthofinder
        delete_folder(output_dir)
        print("running orthofinder")
        run_cmd("orthofinder -f %s -d -o %s -t %i -M dendroblast -S blast"%(input_dir, output_dir, threads))

        print("orthofinder was ran into %s"%output_dir)
    """
    #################################################

    return family_to_clusterID


def get_repeats_df_all_species(DataDir, ProcessedDataDir, PlotsDir, species_to_ref_genome, threads=4, replace=False):

    """Gets a df with the repeats in a way that is formated equally for all species. It acts like a report of the families that might be similar."""

    # define the file
    df_repeats_all_file = "%s/df_repeats_all.py"%ProcessedDataDir
    if file_is_empty(df_repeats_all_file) or replace is True:


        ########## DEFINE A DF WITH ALL THE REPEATS ########

        # generate a df that has the RepeatMasker repeats for all species
        df_repeats_all = pd.DataFrame()

        for species, reference_genome in species_to_ref_genome.items():

                # get the df with the repeats and save
                df_repeats = pd.read_csv("%s.repeats.tab"%reference_genome, sep="\t")
                df_repeats["species"] = species

                df_repeats_all = df_repeats_all.append(df_repeats)

        # add fields
        df_repeats_all["unique_repeat_family"] = df_repeats_all["repeat"] + "#" + df_repeats_all.species

        # get a dict that maps each repeatModller-based repeat ID to a cluster ID (like orthologs)
        repeatModellerRepeatID_to_clusterID = get_IDmapped_repeat_families(DataDir, ProcessedDataDir, PlotsDir, species_to_ref_genome, threads=threads, replace=replace)

        def get_clusterID_repeatModellerRepeatID(x):
            if x in repeatModellerRepeatID_to_clusterID: return repeatModellerRepeatID_to_clusterID[x]
            else: return "no_cluster"

        df_repeats_all["clusterID_repeatModeller"] = df_repeats_all.unique_repeat_family.apply(get_clusterID_repeatModellerRepeatID)

        ####################################################

        ########## REMOVE REDUNDANT REPEATS ###########

        # create bed for bedmap
        df_repeats_all = df_repeats_all.sort_values(by=["species", "chromosome", "begin_repeat", "end_repeat"])
        df_repeats_all["unique_rowID"] = list(range(len(df_repeats_all)))
        df_repeats_all["unique_chromosome"] = df_repeats_all.species + "#" + df_repeats_all.chromosome
        df_repeats_all["begin_repeat_0based"] = df_repeats_all.begin_repeat-1

        # run bedmap to match
        bedmap_outfile = "%s/matching_repeats.txt"%ProcessedDataDir
        if file_is_empty(bedmap_outfile) or replace is True or True:

            repeats_bed = "%s/all_repeats.bed"%ProcessedDataDir
            df_repeats_all[["unique_chromosome", "begin_repeat_0based", "end_repeat", "unique_rowID"]].to_csv(repeats_bed, sep="\t", header=False, index=False)
            
            print("running bedmap")
            bedmap_outfile_tmp = "%s.tmp"%bedmap_outfile
            run_cmd("bedmap --fraction-both 0.95 --echo-map-id --delim '\t' %s > %s"%(repeats_bed, bedmap_outfile_tmp), env="perSVade_env")
            os.rename(bedmap_outfile_tmp, bedmap_outfile)

        df_repeats_all["unique_rowID_not_overlapping"] = [",".join(sorted(l.strip().split(";"))) for l in open(bedmap_outfile, "r").readlines()]
        initial_len_df_repeats_all = len(df_repeats_all)

        # re-sort the df_repeats_all so that the 'unknown' repeats go last. In addition, for repeats of the same type and that are overlapping I will keep the one with the longest match. Finally drop the duplicates keeping one row for each  unique_rowID_not_overlapping

        def get_number_from_type(x):
            if x=="Unknown": return 1
            else: return 0
        df_repeats_all["numberBy_type"] = df_repeats_all["type"].apply(get_number_from_type)

        df_repeats_all["length_match"] = df_repeats_all.end_repeat-df_repeats_all.begin_repeat
        if any(df_repeats_all.length_match<=0): raise ValueError("there was some strange repeats")
        df_repeats_all["inverse_length_match"] = 1/df_repeats_all["length_match"]

        # sort
        df_repeats_all = df_repeats_all.sort_values(by=["species", "chromosome", "unique_rowID_not_overlapping", "numberBy_type", "inverse_length_match"])

        # print duplicates (debug)
        #print(df_repeats_all[df_repeats_all.duplicated(subset=["unique_rowID_not_overlapping"], keep=False)][["unique_rowID_not_overlapping", "length_match", "type"]])

        # drop duplicates
        df_repeats_all = df_repeats_all.drop_duplicates(subset=["unique_rowID_not_overlapping"], keep="first")

        print("you preserved %i/%i repeat annotations after drop_duplicates"%(len(df_repeats_all), initial_len_df_repeats_all))

        ###############################################

        # save
        print("saving repeats")
        save_object(df_repeats_all, df_repeats_all_file)

    return load_object(df_repeats_all_file)


def get_SV_CNV_df_with_pctSVregion_covered_by_eachRepeatFamily(SV_CNV_df, repeats_df_SVs, fileprefix):

    """This function adds the % of each SV region covered by each family of repeats from repeats_df_SVs"""

    print("running get_SV_CNV_df_with_pctSVregion_covered_by_eachRepeatFamily")

    # add the repeat family
    def get_repeat_family(r):
        if r.type!="Unknown" or r.clusterID_repeatModeller=="no_cluster" or r.clusterID_repeatModeller.startswith("singleton_"): return r.type.split("/")[0]
        else: return "%s_cluster"%(r.clusterID_repeatModeller.split("_")[0])

    repeats_df_SVs["repeat_family"] = repeats_df_SVs.apply(get_repeat_family, axis=1)

    # get all the repeat families
    all_repeat_families = sorted(set(repeats_df_SVs.repeat_family))

    # define the vars that don't have repeats
    SV_CNV_df_withSVregion = SV_CNV_df[~pd.isna(SV_CNV_df.SVregion_chromosome)] 
    vars_without_repeats = set(SV_CNV_df_withSVregion.variantID_across_samples).difference(set(repeats_df_SVs.chromosome))

    # add a set with the affected positions of each repeat
    def get_range_altered_positions(r): return range(r.begin_repeat-1, r.end_repeat)
    repeats_df_SVs["altered_positions"] = repeats_df_SVs[["begin_repeat", "end_repeat"]].apply(get_range_altered_positions, axis=1).apply(set)

    # map each repeat family to the percentage covered of each SV
    def get_len_set_altered_positions_by_repeatFamily(x):

        df_var, repeat_family = x

        if repeat_family in df_var.index: return len(set.union(*df_var.loc[{repeat_family}, "altered_positions"]))
        else: return 0

    def get_repeatFamily_to_nBasesCovered(df_var):

        # get all the repeats
        rfamily_to_nbases = pd.Series(dict(zip(all_repeat_families , map(get_len_set_altered_positions_by_repeatFamily, [(df_var, repeat_family) for repeat_family in all_repeat_families]))))

        return rfamily_to_nbases

    nBases_covered_df = repeats_df_SVs[["repeat_family", "chromosome", "altered_positions"]].set_index("repeat_family").groupby("chromosome").apply(get_repeatFamily_to_nBasesCovered) # this is grouping by variantID
    nBases_covered_df["varID"] = nBases_covered_df.index

    # add the length of each var
    varID_to_length = dict(SV_CNV_df.set_index("variantID_across_samples")["len_event"])
    nBases_covered_df["length_var"] = nBases_covered_df.varID.map(varID_to_length)
    if any(pd.isna(nBases_covered_df.length_var)): raise ValueError("length var can't be Nan")

    # add to SV_CNV_df
    for rfamily in all_repeat_families:
        rfamily_f = "repeat#%s#pctCoverage"%rfamily
        rfamily_to_rcoverage = dict((nBases_covered_df[rfamily]/nBases_covered_df.length_var)*100)

        # add the vars with no repeat
        for varID in vars_without_repeats: rfamily_to_rcoverage[varID] = 0.0

        # add to df
        SV_CNV_df[rfamily_f] = SV_CNV_df.variantID_across_samples.map(rfamily_to_rcoverage)
        if any(pd.isna(SV_CNV_df[~pd.isna(SV_CNV_df.SVregion_chromosome)][rfamily_f])): raise ValueError("there should not be Nans")

    return SV_CNV_df

def get_SV_CNV_df_withRepeat_overlapping_SVregions(SV_CNV_df, repeats_df_refGenome, outdir, reference_genome, threads, replace):

    """Gets a df with the unique SVs and adds a column with the highest overlapping repeat (as predicted from RepeatMasker)"""

    # get the initial length of the df
    initial_len_SV_CNV_df = len(SV_CNV_df)

    # create a fasta file with the regions under SV
    chrom_to_seq = {seq.id:seq for seq in SeqIO.parse(reference_genome, "fasta")}
    list_sequences_regions_underSV = [SeqRecord(Seq(str(chrom_to_seq[r.SVregion_chromosome][int(r.SVregion_start):int(r.SVregion_end)].seq).upper()), id=r.variantID_across_samples, description="", name="") for I,r in SV_CNV_df[~pd.isna(SV_CNV_df.SVregion_chromosome)].iterrows()]

    sequences_regions_underSV = "%s/sequences_with_SVs.fasta"%outdir
    SeqIO.write(list_sequences_regions_underSV, sequences_regions_underSV, "fasta")

    # run repeat masker with the default configuration or just annotating the novel families
    repeats_df_default = get_repeat_maskerDF_providedRepeatFamilies(sequences_regions_underSV, None, threads=threads, replace=replace, skip_lowComplexity_and_BacterialRepeatAnnotation=False, repeats_table_file="%s.repeats_default.tab"%sequences_regions_underSV)

    repeatModeller_repeatFamilies = "%s.repeat_modeler_outdir/reference_genome.fasta-families.fa"%reference_genome
    repeats_df_unknownFamilies = get_repeat_maskerDF_providedRepeatFamilies(sequences_regions_underSV, repeatModeller_repeatFamilies, threads=threads, replace=replace, skip_lowComplexity_and_BacterialRepeatAnnotation=True, repeats_table_file="%s.repeats_unkownFamilies.tab"%sequences_regions_underSV)

    # merge bothe repeats
    repeats_df_SVs = repeats_df_default.append(repeats_df_unknownFamilies)

    # add the fields from the reference genome repeats
    fields_repeats_refGenome = ["repeat", "clusterID_repeatModeller"]
    repeats_df_SVs = repeats_df_SVs.merge(repeats_df_refGenome[fields_repeats_refGenome].drop_duplicates(), on="repeat", how="left", validate="many_to_one")

    def get_corrected_clusterID_repeatModeller(x):
        if pd.isna(x): return "no_cluster"
        else: return x

    repeats_df_SVs["clusterID_repeatModeller"] = repeats_df_SVs["clusterID_repeatModeller"].apply(get_corrected_clusterID_repeatModeller)

    # add the length of the repeat
    repeats_df_SVs["length_repeat"] = repeats_df_SVs.end_repeat - repeats_df_SVs.begin_repeat # the length of the regions predicted as repeat

    # add the coverage of the SV region by each repeat family
    fileprefix = "%s/running_get_SV_CNV_df_with_pctSVregion_covered_by_eachRepeatFamily"%outdir
    SV_CNV_df = get_SV_CNV_df_with_pctSVregion_covered_by_eachRepeatFamily(SV_CNV_df, repeats_df_SVs, fileprefix)

    # map each SV (encoded in th chromosome) to the repeat with the longest overlap
    repeats_df_SVs = repeats_df_SVs.sort_values(by=["chromosome", "length_repeat", "perc_div", "perc_ins", "perc_del", "SW_score"], ascending=[False, False, True, True, True, False]).drop_duplicates(subset=["chromosome"], keep="first")

    # rename to make sure that it is the SVregion overlap
    repeats_df_SVs = repeats_df_SVs.rename(columns={c : "BestRepeatSVregion_%s"%c for c in repeats_df_SVs.keys()})

    # add the repeats (relevant field) to the SV_CNV_df, so that there are no duplicates
    SV_CNV_df = SV_CNV_df.merge(repeats_df_SVs, left_on="variantID_across_samples", right_on="BestRepeatSVregion_chromosome", how="left", validate="one_to_one")
    if len(SV_CNV_df)!=initial_len_SV_CNV_df: raise ValueError("The length changed")

    return SV_CNV_df


def get_bedID_to_overlappingRepeats(df_bed, repeats_df, fileprefix):

    """It takes a bed df and a repeats_df. It returns a dict with each ID of the bed mapped to a set with the repeat families"""

    # create the bed file (sorted) of the interesting regions
    df_bed = df_bed.sort_values(by=["chromosome", "start", "end"])
    regions_bed = "%s.query_file.bed"%fileprefix
    df_bed[["chromosome", "start", "end", "ID"]].to_csv(regions_bed, sep="\t", index=False, header=False)

    # add the repeat family to the repeats_df
    def get_repeat_family(r):
        if r.type!="Unknown" or r.clusterID_repeatModeller=="no_cluster" or r.clusterID_repeatModeller.startswith("singleton_"): return r.type.split("/")[0]
        else: return "%s_cluster"%(r.clusterID_repeatModeller.split("_")[0])

    repeats_df["repeat_family"] = repeats_df.apply(get_repeat_family, axis=1)

    # create a bed file with the repeats
    repeats_df["start_repeat"] = repeats_df.begin_repeat-1
    repeats_bed_fields = ["chromosome", "start_repeat", "end_repeat", "repeat_family"]
    repeats_bed_df = repeats_df[repeats_bed_fields].sort_values(by=["chromosome", "start_repeat", "end_repeat"])

    repeats_bed = "%s.repeats_file.bed"%fileprefix
    repeats_bed_df[repeats_bed_fields].to_csv(repeats_bed, sep="\t", index=False, header=False)

    # run bedmap to match the repeats to the bed
    bedmap_outfile = "%s.bedmap.out"%fileprefix
    run_cmd("bedmap --range 0 --echo-map-id --delim '\t' %s %s > %s"%(regions_bed, repeats_bed, bedmap_outfile), env="perSVade_env")

    # add to the bed
    def get_overlapping_repeats_set(x): return set(x.strip().split(";")).difference({""})
    df_bed["overlapping_repeats_set"] = list(map(get_overlapping_repeats_set , open(bedmap_outfile, "r").readlines()))

    return dict(df_bed.set_index("ID").overlapping_repeats_set)

def get_SV_CNV_df_with_allRepeats_overlapping_SVbreakpoints_or_CNVregions(SV_CNV_df, repeats_df, fileprefix="./"):

    """It tajes a SV_CNV_df and adds a set with the repeats that overlap any of the breakpoints of the SV. For SVs we will consider only the boundaries. For CNVs we will consider the whole region"""

    print("running get_SV_CNV_df_with_allRepeats_overlapping_SVbreakpoints_or_CNVregions")

    # generate a df that has all the SV regions to be interrogated for repeats overlaps
    def get_df_breakpointRegion_bed_from_SV_CNV_df_r(r):

        # for CNVs, set all the region
        if r.type_var=="coverageCNV": 
            chroms = [r.chrA]
            starts = [r.startA]
            ends = [r.endB]

        # for SVs that have a very small A region
        elif r.type_var=="SV" and r.lenA==1:
            chroms = [r.chrA, r.chrB]
            starts = [r.startA, r.startB]
            ends = [r.endA, r.endB]

        # SVs that have a large A region (there are 3 regions)
        elif r.type_var=="SV" and r.lenA>1:
            chroms = [r.chrA, r.chrA, r.chrB]
            starts = [r.startA, r.endA-1, r.startB]
            ends = [r.startA+1, r.endA, r.endB]

        # make the bed
        df_bed = pd.DataFrame({"chromosome":chroms, "start":starts, "end":ends})
        df_bed["ID"] = ["%s#%i"%(r.variantID_across_samples, Ic) for Ic in range(len(chroms))]

        return df_bed

    df_regions_breakpoints = pd.concat(SV_CNV_df.apply(get_df_breakpointRegion_bed_from_SV_CNV_df_r, axis=1).values).reset_index(drop=True)
    if any(df_regions_breakpoints.start>=df_regions_breakpoints.end): raise ValueError("start should be < end")

    # map each region (a part of an SV) to the overlapping repeats
    bedID_to_overlappingRepeats = get_bedID_to_overlappingRepeats(df_regions_breakpoints, repeats_df, fileprefix)
    df_regions_breakpoints["overlapping_repeats_set"] = df_regions_breakpoints.ID.map(bedID_to_overlappingRepeats)
    if any(pd.isna(df_regions_breakpoints.overlapping_repeats_set)): raise ValueError("no NAns allowed in overlapping_repeats_set")

    # add to the SV_CNV_df
    df_regions_breakpoints["varID"] = df_regions_breakpoints.ID.apply(lambda x: x.split("#")[0])
    varID_to_overlapping_repeats_set = dict(df_regions_breakpoints.groupby("varID").apply(lambda df_var: set.union(*df_var.overlapping_repeats_set)))
    SV_CNV_df["all_overlapping_repeats_aroundBreakpoints"] = SV_CNV_df.variantID_across_samples.map(varID_to_overlapping_repeats_set)
    if any(pd.isna(SV_CNV_df.all_overlapping_repeats_aroundBreakpoints)): raise ValueError("no NAns allowed in overlapping_repeats_set")

    return SV_CNV_df

def get_SV_CNV_df_with_repeats_overlapping_breakpoints(SV_CNV_df, repeats_df, outdir, SV_breakpointInfo, taxID_dir, threads, replace):

    """This function adds a column to SV_CNV_df called RepeatsBreakpoints, which indicates names of a repeats (from repeats_df.repeat) that overlap both breakends in some of the breakpoints related to that SV in any of the samples. Each breakpoint will be assigned to only one repeat, the one with the highest overlap"""

    ######## MAP EACH BREAKPOINT TO A FAMILY OF REPEATS ##########

    # get a df with the bedpe of the important breakpoints (those in SV_breakpointInfo) 
    df_bedpe = get_tab_as_df_or_empty_df("%s/integrated_varcalls/integrated_bedpe_df.tab"%(taxID_dir))
    df_bedpe["unique_breakpointID"] = df_bedpe.name + "_" + df_bedpe.sampleID.apply(int).apply(str)

    all_SVbreakpoints = set(make_flat_listOflists(SV_CNV_df[SV_CNV_df.type_var=="SV"].list_breakpoints))
    strange_SVbreakpoints = all_SVbreakpoints.difference(set(df_bedpe.unique_breakpointID))
    if len(strange_SVbreakpoints)>0: raise ValueError("There are %i/%i unexpected bps: %s"%(len(strange_SVbreakpoints), len(all_SVbreakpoints), strange_SVbreakpoints))

    df_bedpe = df_bedpe[df_bedpe.unique_breakpointID.isin(all_SVbreakpoints)]
    df_breakpoints_SV = df_bedpe[["unique_breakpointID", "chrom1", "start1", "end1", "chrom2", "start2", "end2"]]

    # define a bedpe for the CNVs (one breakpoint each CNV in SV_CNV_df)
    df_breakpoints_CNV = SV_CNV_df[SV_CNV_df.type_var=="coverageCNV"][["variantID_across_samples", "chrA", "startA", "endA", "chrB", "startB", "endB"]].rename(columns={"variantID_across_samples":"unique_breakpointID", "chrA":"chrom1", "startA":"start1", "endA":"end1", "chrB":"chrom2", "startB":"start2", "endB":"end2"})

    # merge
    df_breakpoints = df_breakpoints_SV.append(df_breakpoints_CNV)
    df_breakpoints["ID_1"] = df_breakpoints.unique_breakpointID + "#1"
    df_breakpoints["ID_2"] = df_breakpoints.unique_breakpointID + "#2"

    # define a bed with each breakend
    df_bed_1 = df_breakpoints[["chrom1", "start1", "end1", "ID_1"]].rename(columns={"chrom1":"chromosome", "start1":"start", "end1":"end", "ID_1":"ID"})
    df_bed_2 = df_breakpoints[["chrom2", "start2", "end2", "ID_2"]].rename(columns={"chrom2":"chromosome", "start2":"start", "end2":"end", "ID_2":"ID"})
    df_bed_breakends = (df_bed_1.append(df_bed_2)).sort_values(by=["chromosome", "start", "end"])

    bed_breakends = "%s/all_breakends_locations.bed"%outdir
    df_bed_breakends.to_csv(bed_breakends, sep="\t", header=False, index=False)

    # define a bed with the repeats
    repeats_df["begin_repeat_0based"] = repeats_df.begin_repeat-1
    repeats_df = repeats_df.sort_values(by=["chromosome", "begin_repeat_0based", "end_repeat"])
    repeats_df["unique_repeatID"] = list(range(len(repeats_df)))

    bed_repeats = "%s/all_repeats_refGenome.bed"%outdir
    repeats_df[["chromosome", "begin_repeat_0based", "end_repeat", "unique_repeatID"]].to_csv(bed_repeats, sep="\t", header=False, index=False)

    # run bedmap of how each breakend is matched by some repeat
    bedmap_outfile = "%s/mapping_refGenomeRepeats_on_all_Breakends.txt"%outdir
    if file_is_empty(bedmap_outfile) or replace is True or True:
        print("running bedmap")

        bedmap_outfile_tmp = "%s.tmp"%bedmap_outfile
        run_cmd("bedmap --range 100 --echo-map-id --delim '\t' %s %s > %s"%(bed_breakends, bed_repeats, bedmap_outfile_tmp), env="perSVade_env")
        os.rename(bedmap_outfile_tmp, bedmap_outfile)

    # add the overlapping repeats to df_bed_breakends
    print("running get_overlapping_repeatIDs_from_l")
    def get_overlapping_repeatIDs_from_l(l): return {int(x) for x in set(l.strip().split(";")).difference({""})}
    df_bed_breakends["overlapping_repeatIDs"] = list(map(get_overlapping_repeatIDs_from_l , open(bedmap_outfile, "r").readlines()))

    # convert the numeric sets to the actual famillies
    print("running get_overlapping_repeatIDs_Families_from_overlapping_repeatIDs")
    repeatID_to_repeatFamily = dict(repeats_df.set_index("unique_repeatID")["repeat"])
    def get_overlapping_repeatIDs_Families_from_overlapping_repeatIDs(x): return set(map(lambda y: repeatID_to_repeatFamily[y], x))
    df_bed_breakends["overlapping_repeatIDs_Families"] = df_bed_breakends.overlapping_repeatIDs.apply(get_overlapping_repeatIDs_Families_from_overlapping_repeatIDs)

    # add to the df of breakpoints the repeats that overlap each breakend
    print("merging dfs")
    df_bed_breakends_merge = df_bed_breakends[["ID", "overlapping_repeatIDs", "overlapping_repeatIDs_Families"]]
    df_breakpoints = df_breakpoints.merge(df_bed_breakends_merge, left_on="ID_1", right_on="ID", how="left", validate="one_to_one").rename(columns={"ID":"ID_bend1", "overlapping_repeatIDs":"overlapping_repeatIDs_bend1", "overlapping_repeatIDs_Families":"overlapping_repeatIDs_Families_bend1"})
    df_breakpoints = df_breakpoints.merge(df_bed_breakends_merge, left_on="ID_2", right_on="ID", how="left", validate="one_to_one").rename(columns={"ID":"ID_bend2", "overlapping_repeatIDs":"overlapping_repeatIDs_bend2", "overlapping_repeatIDs_Families":"overlapping_repeatIDs_Families_bend2"})

    # map each repeat to a score, related to the divergence. We will prioritize repeats that have a higher score (identity)
    repeats_df["divergence_score"] = 1-(repeats_df[["perc_div", "perc_ins", "perc_del"]].apply(np.mean, axis=1)/100)
    repeatID_to_score = dict(repeats_df.set_index("unique_repeatID").divergence_score)

    # add the best family among the overlapping ones
    print("getting the repeat that could best explain each breakend")
    def get_overlapping_repeat_from_df_breakpoints_r(r):

        # define the overlapping families
        overlapping_families = r["overlapping_repeatIDs_Families_bend1"].intersection(r["overlapping_repeatIDs_Families_bend2"])

        # depdending on the number of families that could explain this repeat get more or less
        if len(overlapping_families)==0: return ""
        elif len(overlapping_families)==1: return next(iter(overlapping_families))
        else:

            # map each family to a score that comes from the mean scores of the repeats that form each family
            family_to_meanScore = pd.Series({fam : np.mean([max([repeatID_to_score[repeatID] for repeatID in r["overlapping_repeatIDs_bend%i"%bendID] if repeatID_to_repeatFamily[repeatID]==fam]) for bendID in [1, 2]]) for fam in overlapping_families})

            # get the family that has the highest score
            return family_to_meanScore[family_to_meanScore==max(family_to_meanScore)].index[0]

    df_breakpoints["overlapping_repeat"] = df_breakpoints.apply(get_overlapping_repeat_from_df_breakpoints_r, axis=1)
    print("There are %i/%i breakpoints that can be explained by some repeat"%(sum(df_breakpoints.overlapping_repeat!=""), len(df_breakpoints)))


    #######################################################################

    ########## ADD THE OVERLAPPING REPEATS TO EACH OF THE SVs ###########
    print("adding to the SV_CNV_df")

    # debug the fact that there are breakpoints
    if any(pd.isna(SV_CNV_df["list_breakpoints"])): raise ValueError("There can't be NaNs in list_breakpoints")

    # add the repeats that overlap each SV
    bpID_to_repeat = dict(df_breakpoints.set_index("unique_breakpointID")["overlapping_repeat"])
    def get_RepeatsAroundBreakpoints(x): return ([bpID_to_repeat[bpID] for bpID in x])
    SV_CNV_df["RepeatsAroundBreakpoints"] = SV_CNV_df.list_breakpoints.apply(get_RepeatsAroundBreakpoints)

    ######################################################################

    return SV_CNV_df

def get_breakpointID_to_homologyLength(SV_breakpointInfo, SV_CNV_df, field_homology):

    """This function returns a dict mapping each breakpoint to the homology from field_homology"""


    ####### MAP BREAKENDS TO HOMLENS ######

    #filter
    SV_breakpointInfo = SV_breakpointInfo[SV_breakpointInfo.type_var=="SV"]

    # add the breakends as list
    if any(SV_breakpointInfo.INFO_BREAKENDIDs.apply(len)!=SV_breakpointInfo[field_homology].apply(len)): raise ValueError("the breakends don't have always the same lengths")

    SV_breakpointInfo["nbends"] = SV_breakpointInfo.INFO_BREAKENDIDs.apply(len)
    if any(SV_breakpointInfo.nbends<1): raise ValueError("some breakends make no sense")

    # init the the mapping with the single breakend cols
    SV_breakpointInfo["first_bendID"] = SV_breakpointInfo.INFO_BREAKENDIDs.apply(lambda x: x[0])
    SV_breakpointInfo["first_homLen"] = SV_breakpointInfo[field_homology].apply(lambda x: x[0])

    def get_homLen_forBend(df_bend):
        all_lens = set(df_bend.first_homLen)
        if len(all_lens)!=1: raise ValueError("%s is not valid"%df_bend)
        return next(iter(all_lens))

    bend_to_homLen = dict(SV_breakpointInfo[SV_breakpointInfo.nbends==1].groupby("first_bendID").apply(get_homLen_forBend))

    # add the the brekends for SVs where you have more than 1 breakpoint
    for I, r in SV_breakpointInfo[SV_breakpointInfo.nbends>1].iterrows():

        # ierate through each brekend
        for bend, homLen in dict(zip(r.INFO_BREAKENDIDs, r[field_homology])).items():

            if bend not in bend_to_homLen: bend_to_homLen[bend] = homLen
            elif homLen!=(bend_to_homLen[bend]): raise ValueError("a bend has different homlens")

    bend_to_homLen = pd.Series(bend_to_homLen)

    #######################################

    ######## MAP EACH BREAKPOINT TO THE HOMLEN THROUGH THE BREAKENDS #########

    # init df
    df_homLen = pd.DataFrame({"homLen":bend_to_homLen, "bend":bend_to_homLen.index}).reset_index(drop=True)

    # add breakpoint ID
    def get_bpID_from_bendID(x): return "%so_%s"%("_".join(x.split("_")[0:-1])[:-1], x.split("_")[-1])
    df_homLen["bpID"] = df_homLen.bend.apply(get_bpID_from_bendID)
    df_homLen = df_homLen[["bpID", "homLen"]].sort_values(by=["bpID", "homLen"]).drop_duplicates(subset=["bpID", "homLen"])

    # map each bpID to the hom length
    if len(df_homLen)!=len(set(df_homLen.bpID)): raise ValueError("There are some bpoints with >1 length microhomology")
    bpID_to_homLen_SV = dict(df_homLen.set_index("bpID").homLen)

    # add the homLens of the SV_CNV_df coverageCNV
    all_CNVids = sorted(set(SV_CNV_df[SV_CNV_df.type_var=="coverageCNV"].variantID_across_samples))
    bpID_to_homLen_CNV = dict(zip(all_CNVids, [0]*len(all_CNVids)))

    # merge
    bpID_to_homLen = {**bpID_to_homLen_SV, **bpID_to_homLen_CNV}

    ##########################################################################

    return bpID_to_homLen

def get_SV_CNV_df_with_homologyLength_aroundBreakpoints(SV_CNV_df, SV_breakpointInfo):

    """Takes the SV_CNV_df and returns it with fields that indicate the microhomology lengths of each of the breakpoints associated to each SV """
    print("running get_SV_CNV_df_with_homologyLength_aroundBreakpoints")

    # keep the important breakpoints from the SV_breakpointInfo data
    all_SVbreakpoints = set(make_flat_listOflists(SV_CNV_df[SV_CNV_df.type_var=="SV"].list_breakpoints))

    def get_set_x_intersection_set_y(set_x, set_y): return (set_x.intersection(set_y))
    SV_breakpointInfo = SV_breakpointInfo[SV_breakpointInfo.INFO_BREAKPOINTIDs.apply(set).apply(get_set_x_intersection_set_y, set_y=all_SVbreakpoints).apply(len)>0]

    # map each breakpoint to a homology length
    bpID_to_microHomologyLen = get_breakpointID_to_homologyLength(SV_breakpointInfo, SV_CNV_df, "INFO_BREAKEND_length_microHomology")
    bpID_to_inexactHomologyLen = get_breakpointID_to_homologyLength(SV_breakpointInfo, SV_CNV_df, "INFO_BREAKEND_length_inexactHomology")

    # add to the df
    def get_list_homLen(bpIDs, bpID_to_len): return ([bpID_to_len[bpID] for bpID in bpIDs])

    SV_CNV_df["micro_homology_length"] = SV_CNV_df.list_breakpoints.apply(get_list_homLen, bpID_to_len=bpID_to_microHomologyLen)
    SV_CNV_df["inexact_homology_length"] = SV_CNV_df.list_breakpoints.apply(get_list_homLen, bpID_to_len=bpID_to_inexactHomologyLen)

    return  SV_CNV_df

def get_unique_SV_CNV_df_with_metadata(repeats_df_all, DataDir, ProcessedDataDir, species_to_ref_genome, threads=4, replace=False):

    """ This function stacks the dataframes with the unique SVs for each species and adds metadata about the mechanisms of SV formation (i.e. type of overlapping repeats, whether any of the breakpoints are overlapping repeats and the microhomology lenghts of the implicated breakpoints) """

    # deifine file
    unique_SV_CNV_df_all_file = "%s/unique_SV_CNV_df_all_withMetadata.py"%ProcessedDataDir
    if file_is_empty(unique_SV_CNV_df_all_file) or replace is True:
        print("generating %s"%unique_SV_CNV_df_all_file)

        # init df
        unique_SV_CNV_df_all = pd.DataFrame()

        # go through each species
        for taxID, species in taxID_to_sciName.items():
            print(species)

            # debug
            #if species!="Candida_parapsilosis": continue

            # define things of this species
            reference_genome = species_to_ref_genome[species]

            ######## LOAD DF WITH THE UNIQUE VARIANTS AND ADD FIELDS ########
            print("loading SV_CNV_df")

            # load a df where each row is one unique SV
            taxID_dir = "%s/%s_%i"%(DataDir, species, taxID)
            SV_CNV_filt_file = "%s/integrated_varcalls/SV_CNV_filt.py"%(taxID_dir)
            SV_df_oneVarPerRow_file = "%s/%s_SVs_oneVarPerRow_onlyVars_SV_and_CNV_uniqueSVs.py"%(ProcessedDataDir, species)
            #remove_file(SV_df_oneVarPerRow_file); continue
            SV_CNV_df = get_SV_df_oneVarPerRow_df(SV_CNV_filt_file, SV_df_oneVarPerRow_file, only_unique_SVs=True, threads=threads, var_types={"SV", "coverageCNV"}, replace=replace, bad_samples=set(map(str, sciName_to_badSamples[species])))

            # add the type of var
            SV_CNV_df["type_var"] = SV_CNV_df.svtype.isin({"coverage_deletion", "coverage_duplication"}).map({True:"coverageCNV", False:"SV"})

            # add length
            SV_CNV_df["lenA"] = SV_CNV_df.endA-SV_CNV_df.startA
            SV_CNV_df["lenA_to_B"] = SV_CNV_df.endB-SV_CNV_df.startA
            SV_CNV_df["no_length"] = np.nan

            svtype_to_lenField = {"inverted_cutPaste_insertion":"lenA", "copyPaste_insertion":"lenA","cutPaste_insertion":"lenA","inverted_copyPaste_insertion":"lenA","inverted_translocation":"lenA_to_B","inversion":"lenA_to_B","tandem_duplication":"lenA_to_B","deletion":"lenA_to_B", "coverage_deletion":"lenA_to_B", "coverage_duplication":"lenA_to_B", "bal_translocation":"no_length", "unclassified_breakpoint":"no_length"}
            SV_CNV_df["len_field"] = SV_CNV_df.svtype.map(svtype_to_lenField)

            def get_len_event(r): return r[r.len_field]
            SV_CNV_df["len_event"] = SV_CNV_df[["len_field", "lenA", "lenA_to_B", "no_length"]].apply(get_len_event, axis=1)

            # add the region with SV (SVregion_chromosome, SVregion_start, SVregion_end)
            def get_r_with_fields_SVregion(r):

                if r.len_field=="lenA":
                    r["SVregion_chromosome"] = r.chrA
                    r["SVregion_start"] = r.startA
                    r["SVregion_end"] = r.endA

                elif r.len_field=="lenA_to_B":
                    r["SVregion_chromosome"] = r.chrA
                    r["SVregion_start"] = r.startA
                    r["SVregion_end"] = r.endB

                elif r.len_field=="no_length":
                    r["SVregion_chromosome"] = np.nan
                    r["SVregion_start"] = np.nan
                    r["SVregion_end"] = np.nan

                return r

            SV_CNV_df = SV_CNV_df.apply(get_r_with_fields_SVregion, axis=1)

            # add the  

            # get a df with the breakpoint info
            interesting_fields = ["type_var", "INFO_BREAKPOINTIDs", "INFO_BREAKEND_length_inexactHomology", "INFO_BREAKEND_length_microHomology", "sampleID", "variantID_across_samples", "INFO_variantID", "INFO_BREAKENDIDs"]
            SV_breakpointInfo = load_object(SV_CNV_filt_file)[interesting_fields]
            SV_breakpointInfo = SV_breakpointInfo[SV_breakpointInfo.type_var=="SV"].drop_duplicates()

            def get_correct_lenHomology(x):
                if x.startswith("("): return [int(y.replace("(","").replace(")","")) for y in x.split(",")]
                else: return [int(x)]

            for f in ["INFO_BREAKEND_length_inexactHomology", "INFO_BREAKEND_length_microHomology"]: SV_breakpointInfo[f] = SV_breakpointInfo[f].apply(get_correct_lenHomology)

            def get_set_sampleWise_INFO_BREAKPOINTIDs(r): return sorted({"%so_%s"%(x[:-1], r.sampleID) for x in r.INFO_BREAKENDIDs.split(",")})
            SV_breakpointInfo["INFO_BREAKPOINTIDs"] = SV_breakpointInfo.apply(get_set_sampleWise_INFO_BREAKPOINTIDs, axis=1)

            def get_ordered_sampleWise_INFO_BREAKENDIDs(r): return ["%s_%s"%(x, r.sampleID) for x in r.INFO_BREAKENDIDs.split(",")]
            SV_breakpointInfo["INFO_BREAKENDIDs"] = SV_breakpointInfo.apply(get_ordered_sampleWise_INFO_BREAKENDIDs, axis=1)


            for f in ["INFO_BREAKPOINTIDs", "INFO_BREAKEND_length_inexactHomology", "INFO_BREAKEND_length_microHomology"]:
                if any(SV_breakpointInfo[f].apply(len)<1): raise ValueError("There are some variants without correct lengths in %s"%f) # debug

            # add the breakpoints that are associated to each variant from the first sample in SV_breakpointInfo
            print("getting variantIDacrossSamples_to_listBreakPoints")
            def get_listBps_for_varID_firstSample(df_v): return sorted(set(make_flat_listOflists(df_v[df_v.sampleID==(df_v.sampleID.iloc[0])].INFO_BREAKPOINTIDs)))
            variantIDacrossSamples_to_listBreakPoints = dict(SV_breakpointInfo[["variantID_across_samples", "INFO_BREAKPOINTIDs", "sampleID"]].sort_values(by=["variantID_across_samples", "sampleID"]).groupby("variantID_across_samples").apply(get_listBps_for_varID_firstSample))

            SV_CNV_df["list_breakpoints"] = SV_CNV_df.variantID_across_samples.map(variantIDacrossSamples_to_listBreakPoints)
            if any(pd.isna(SV_CNV_df[SV_CNV_df.type_var=="SV"].list_breakpoints)): raise ValueError("There can't be NaNs")

            # get the list_breakpoints in a way that the coverageCNVs get the variantID_across_samples
            def get_list_breakpoints_also_in_CNVs(r):
                if r.type_var=="SV": return r.list_breakpoints
                else: return [r.variantID_across_samples]

            SV_CNV_df["list_breakpoints"] = SV_CNV_df.apply(get_list_breakpoints_also_in_CNVs, axis=1)

            #################################################################

            # load the repeats for this species
            repeats_df = repeats_df_all[repeats_df_all.species==species]

            # define an outdir for the species
            species_ProcessedDataDir = "%s/mapping_SVmechanisms_%s"%(ProcessedDataDir, species); make_folder(species_ProcessedDataDir)

            # add the repeats overlapping the SV regions 
            SV_CNV_df = get_SV_CNV_df_withRepeat_overlapping_SVregions(SV_CNV_df, repeats_df, species_ProcessedDataDir, species_to_ref_genome[species], threads, replace)

            # add the repeats overlapping the breakpoints
            SV_CNV_df = get_SV_CNV_df_with_repeats_overlapping_breakpoints(SV_CNV_df, repeats_df, species_ProcessedDataDir, SV_breakpointInfo, taxID_dir, threads, replace)

            # add the homology lengths around the breakpoints
            SV_CNV_df = get_SV_CNV_df_with_homologyLength_aroundBreakpoints(SV_CNV_df, SV_breakpointInfo)

            # add all possible repeats overlapping any of the breakpoint regions regions
            SV_CNV_df = get_SV_CNV_df_with_allRepeats_overlapping_SVbreakpoints_or_CNVregions(SV_CNV_df, repeats_df, fileprefix="%s/calculating_overlapSVs_allRepeats"%species_ProcessedDataDir)

            # keep
            SV_CNV_df["species"] = species
            unique_SV_CNV_df_all = unique_SV_CNV_df_all.append(SV_CNV_df)

        # save
        save_object(unique_SV_CNV_df_all, unique_SV_CNV_df_all_file)

    # load
    unique_SV_CNV_df_all = load_object(unique_SV_CNV_df_all_file).reset_index(drop=True)


    return unique_SV_CNV_df_all


def get_unique_small_vars_df_with_metadata(repeats_df_all, DataDir, ProcessedDataDir, species_to_ref_genome, threads=4, replace=False):

    """This function generates a df with the unique filtered small variants for each species and some metadata"""

    # deifine file
    unique_small_vars_df_all_file = "%s/unique_small_vars_df_all_withMetadata.py"%ProcessedDataDir
    if file_is_empty(unique_small_vars_df_all_file) or replace is True:
        print("generating %s"%unique_small_vars_df_all_file)

        # init df
        unique_small_vars_df_all = pd.DataFrame()

        # go through each species
        for taxID, species in taxID_to_sciName.items():
            print(species)

            # debug
            #if species!="Candida_parapsilosis": continue

            # define things of this species
            reference_genome = species_to_ref_genome[species]

            ######## LOAD DF WITH THE UNIQUE VARIANTS AND ADD FIELDS ########
            print("loading vars")

            # load a df where each row is one unique SV
            taxID_dir = "%s/%s_%i"%(DataDir, species, taxID)
            small_vars_df = load_object("%s/integrated_varcalls/smallVars_filt.py"%taxID_dir)

            # remove bad samples
            bad_samples = sciName_to_badSamples[species]
            small_vars_df = small_vars_df[~(small_vars_df.sampleID.apply(int).isin(bad_samples))]

            # keep variants of the relevant ploidy and only unique vas
            print("filtering vars")
            small_vars_df = small_vars_df[small_vars_df.calling_ploidy==taxID_to_ploidy[taxID]]
            small_vars_df = small_vars_df[["#Uploaded_variation", "#CHROM", "POS", "REF", "ALT", "ISSNP", "is_protein_altering_inSomeGenes"]].drop_duplicates().sort_values(by=["#CHROM", "POS", "#Uploaded_variation"]).reset_index(drop=True)

            # debugs
            if len(small_vars_df)!=len(set(small_vars_df["#Uploaded_variation"])): raise ValueError("vars are not unique")

            #################################################################

            # load the repeats df
            repeats_df = repeats_df_all[repeats_df_all.species==species]

            ######### MAP REPEATS TO VARIANTS #########
            print("mapping repeats")

            # define a df with the bed regions
            df_bed_variants = small_vars_df[["#CHROM", "POS", "#Uploaded_variation"]].rename(columns={"#CHROM":"chromosome", "POS":"end", "#Uploaded_variation":"ID"})
            df_bed_variants["start"] = df_bed_variants.end-1

            # map each variant to the overlapping repeats
            fileprefix = "%s/%s_mappingRepeats_smallVars"%(ProcessedDataDir, species)
            bedID_to_overlappingRepeats = get_bedID_to_overlappingRepeats(df_bed_variants, repeats_df, fileprefix)
            small_vars_df["overlapping_repeats_set"] = small_vars_df["#Uploaded_variation"].map(bedID_to_overlappingRepeats)
            if any(pd.isna(small_vars_df.overlapping_repeats_set)): raise ValueError("no NAns allowed in overlapping_repeats_set")

            ###########################################

            # keep
            small_vars_df["species"] = species
            unique_small_vars_df_all = unique_small_vars_df_all.append(small_vars_df)

        # save
        save_object(unique_small_vars_df_all, unique_small_vars_df_all_file)

    # load
    unique_small_vars_df_all = load_object(unique_small_vars_df_all_file).reset_index(drop=True)

    return unique_small_vars_df_all


def plot_location_unique_SVs_biplot(DataDir, species_to_ref_genome, PlotsDir, ProcessedDataDir, threads=4):

    """This function plots the location of all SVs in a sepcies"""

    # define things
    svtype_to_color = {
                       # interchromosomal events
                       'bal_translocation':'black', # tomato
                       'inverted_cutPaste_insertion':'olive',

                       # both inter and intrachromosomal
                       'unclassified_breakpoint':'gray',
                       'copyPaste_insertion':'blue',
                       'cutPaste_insertion':'magenta',

                       # intrachromosomal
                       'inverted_copyPaste_insertion':'chocolate',
                       'inverted_translocation':'olive',
                       'inversion':'black',
                       'tandem_duplication':'cyan',
                       'deletion':'red'}

    svtype_to_marker = {
                       # interchromosomal events
                       'bal_translocation':'s',
                       'inverted_cutPaste_insertion':'^',

                       # both inter and intrachromosomal
                       'unclassified_breakpoint':'o',
                       'copyPaste_insertion':'P',
                       'cutPaste_insertion':'o',

                       # intrachromosomal
                       'inverted_copyPaste_insertion':'P',
                       'inverted_translocation':'v',
                       'inversion':'^',
                       'tandem_duplication':'P',
                       'deletion':'X'}


 
    # map to hex
    svtype_to_hexColor = {s : get_matplotlib_color_as_hex(c) for s,c in svtype_to_color.items()}

    # go through each species
    for taxID, species in taxID_to_sciName.items():
        print(species)

        # debug
        #if species=="Candida_albicans": continue
        #if species!="Candida_parapsilosis": continue
        #if species!="Candida_auris": continue

        # define things of this species
        reference_genome = species_to_ref_genome[species]

        # load a df where each row is one unique SV
        SV_CNV_filt_file = "%s/%s_%i/integrated_varcalls/SV_CNV_filt.py"%(DataDir, species, taxID)
        SV_df_oneVarPerRow_file = "%s/%s_SVs_oneVarPerRow_onlyVars.py"%(ProcessedDataDir, species)
        #remove_file(SV_df_oneVarPerRow_file); continue
        SV_df = get_SV_df_oneVarPerRow_df(SV_CNV_filt_file, SV_df_oneVarPerRow_file, only_unique_SVs=True, threads=threads)

        # generate the plot
        plots_dir_biplots = "%s/SV_locations_biplot"%PlotsDir; make_folder(plots_dir_biplots)
        filename = "%s/%s.pdf"%(plots_dir_biplots, species)
        generate_biplot_SVs_one_species(SV_df, filename, svtype_to_color, svtype_to_marker, reference_genome, species)


    

def plot_Circos_unique_SVs(DataDir, species_to_ref_genome, PlotsDir, ProcessedDataDir, threads=4):

    """This function plots for each species a circos plot with the SVs defined (only SVs, not interchromosomal changes) """

    # import pyCircos
    sys.path.insert(0, "%s/software/pyCircos"%ParentDir)
    import pycircos

    svtype_to_color = {
                       # interchromosomal events
                       'bal_translocation':'red', # tomato
                       'inverted_cutPaste_insertion':'olive',

                       # both inter and intrachromosomal
                       'unclassified_breakpoint':'gray',
                       'copyPaste_insertion':'blue',
                       'cutPaste_insertion':'magenta',

                       # intrachromosomal
                       'inverted_copyPaste_insertion':'darkviolet',
                       'inverted_translocation':'olive',
                       'inversion':'black',
                       'tandem_duplication':'cyan',
                       'deletion':'red'}

    # define the sets of events
    interchromosomal_events = ["bal_translocation", "inverted_cutPaste_insertion"]
    intra_and_inter_events = ["unclassified_breakpoint", "copyPaste_insertion", "cutPaste_insertion"]
    intrachromosomal_events = ["inverted_copyPaste_insertion", "inverted_translocation", "inversion", "tandem_duplication", "deletion"]

    # map to hex
    svtype_to_hexColor = {s : get_matplotlib_color_as_hex(c) for s,c in svtype_to_color.items()}

    # go through each species
    for taxID, species in taxID_to_sciName.items():
        print(species)

        # debug
        #if species=="Candida_albicans": continue
        if species!="Candida_parapsilosis": continue
        #if species!="Candida_auris": continue

        # define things of this species
        reference_genome = species_to_ref_genome[species]

        # load a df where each row is one unique SV
        SV_CNV_filt_file = "%s/%s_%i/integrated_varcalls/SV_CNV_filt.py"%(DataDir, species, taxID)
        SV_df_oneVarPerRow_file = "%s/%s_SVs_oneVarPerRow_onlyVars.py"%(ProcessedDataDir, species)
        #remove_file(SV_df_oneVarPerRow_file); continue
        SV_df = get_SV_df_oneVarPerRow_df(SV_CNV_filt_file, SV_df_oneVarPerRow_file, only_unique_SVs=True, threads=threads)

        # define all chromosomes and the color
        chrom_to_len = get_chr_to_len(reference_genome)
        sorted_chroms =  get_sorted_chromosomes_from_interchromosomal_rearrangements(chrom_to_len, SV_df)
        chrom_to_color = get_value_to_color(sorted_chroms, palette="tab20", type_color="hex")[0]

        ####### GENERATE CIRCOS VARS #########

        # init fig
        gcircle = pycircos.Gcircle()

        # add the each chromosome
        for chrom in sorted_chroms: gcircle.add_locus(chrom, chrom_to_len[chrom], bottom=900, height=300, facecolor=chrom_to_color[chrom]) #name, length, bottom (0<=bottom<=1000), height (0<=bottom<=1000)

        gcircle.set_locus(figsize=(6, 6), lw=10) #Creat figure object

        # init a dict that will contain the graphics for each rectangle
        chrom_to_graphics = {}

        # add the labels and new rectangles
        for chrom_name, bar_obj in gcircle.__dict__["locus_dict"].items():

            # define the coordinates of the rect
            robject = bar_obj["bar"].get_children()[0]
            x, y = robject.get_xy()
            width = robject.get_width()
            height = robject.get_height()

            # add the label
            gcircle.ax.text(x+(width/2), y+(height/1.7), get_shortChrom_from_chrName(chrom_name, species), color=chrom_to_color[chrom_name], weight="bold")

            # define the internal chromosome graphics

            # add a new rectangle
            rect = mpatches.Rectangle((x, y-300), width, height/6, linewidth=1, edgecolor='black', facecolor=chrom_to_color[chrom_name])
            gcircle.ax.add_patch(rect)

        # define a minimum len
        min_len = int(sum(chrom_to_len.values())/900)
        #min_len = 10
        half_min_len = int(min_len/2)
        alpha_chords = 0.99

        # add each SV as a chord
        print("adding the chord diagrams")
        for I, r in cp.deepcopy(SV_df).iterrows(): 
            #print("adding sv %i"%I, r.svtype)

            # progress
            #if (r.var_idx%100)==0: print("already traversed %.4f%s vars"%((r.var_idx/len(SV_df))*100, "%"))

            # define the lengths
            lenA = r.endA-r.startA
            lenB = r.endB-r.startB

            # define the centers
            centerA = int(r.startA + (lenA/2))
            centerB = int(r.startB + (lenB/2))

            # redefine the starts if the var starts is too small
            if lenA<min_len: 
                r.startA = max([0, centerA - half_min_len])
                lenA = min_len

            if lenB<min_len: 
                r.startB = max([0, centerB - half_min_len])
                lenB = min_len

            # redefine the starts so that they are not positioned towards the end
            chrA_dif_endStart = chrom_to_len[r.chrA]-r.startA
            if chrA_dif_endStart<min_len: r.startA = max([0, chrom_to_len[r.chrA]-min_len])

            chrB_dif_endStart = chrom_to_len[r.chrB]-r.startB
            if chrB_dif_endStart<min_len: r.startB = max([0, chrom_to_len[r.chrB]-min_len])

            # define the ends based on the start and the length
            r.endA = min([chrom_to_len[r.chrA]-2, r.startA+lenA])
            r.endB = min([chrom_to_len[r.chrB]-2, r.startB+lenB])

            # define the bottom depending on the type of var
            if r.chrA==r.chrB: 
                bottom = 900
                center = 500
                
            else: 
                center = 0
                bottom = 600

            # debug
            #if (r.chrA==r.chrB) and ((r.startA>r.endA) or (r.startB>r.endB) or (r.startA>r.startB)): raise ValueError("invalid r: %s"%r) 

            #if (r.chrA==r.chrB) and (centerB-centerA)>(min_len*3): print(r.INFO_variantID)

            # add the plot
            gcircle.chord_plot([r.chrA, r.startA, r.endA], [r.chrB, r.startB, r.endB], bottom=bottom, color=svtype_to_hexColor[r.svtype], alpha=alpha_chords, center=center)

        # set title
        #gcircle.ax.set_title("C. %s interchromosomal"%(species.split("_")[1]))
        print("getting figure")

        # add a legend of the colors
        legend_elements = [("white", "C. %s"%species.split("_")[1]), ("white", ""), ("white", "inter-chromosome:")] + [(svtype_to_hexColor[e], e) for e in interchromosomal_events] + [("white", "inter/intra-chromosome:")] + [(svtype_to_hexColor[e], e) for e in intra_and_inter_events] + [("white", "intra-chromosome:")] + [(svtype_to_hexColor[e], e) for e in intrachromosomal_events]

        legend_elements = [Line2D([0], [0], color=c, lw=4, label=l, alpha=alpha_chords) for c,l in legend_elements]
        gcircle.ax.legend(handles=legend_elements, loc='right', bbox_to_anchor=(1.7, 0.3))

        # save
        plotsdir_circos = "%s/circos_SVs"%PlotsDir; make_folder(plotsdir_circos)
        filename = "%s/%s.pdf"%(plotsdir_circos, species)
        gcircle.save(file_name=filename, format="pdf")
        #plt.show()
        plt.close()
        #sys.exit(0)

        ############################################################


def get_SVregions_bed_df_from_SV_df_r(r):

    """Takes the row of SV_df and returns a bed with the regions under SV"""

    # deifine the length of the A region
    lenA = r.endA-r.startA
    if lenA<=0: raise ValueError("lenA is not valid")

    # interchromosomal events are returned as provided. Intrachromosomal events where the A is a regions are also returned as provided
    if (r.chrA!=r.chrB) or (lenA>1): df_bed = pd.DataFrame({"chromosome":[r.chrA, r.chrB], "start":[r.startA, r.startB], "end":[r.endA, r.endB]})

    # intrachromosomal events are defined based on the type of the coordinates
    else: df_bed = pd.DataFrame({"chromosome":[r.chrA], "start":[r.startA], "end":[r.endB]})

    # add general things
    if r.svtype=="unclassified_breakpoint": svtype = r.INFO_variantID.split("|")[0]
    elif r.svtype in {"bal_translocation", "inverted_cutPaste_insertion", "copyPaste_insertion", "cutPaste_insertion", "inverted_copyPaste_insertion", "inverted_translocation", "inversion", "tandem_duplication", "deletion"}: svtype = r.svtype
    else: raise ValueError("%s was not defined"%r.svtype)

    df_bed["svtype"] = svtype

    if r.chrA==r.chrB and lenA>1: 
        print(df_bed)
        sys.exit(0)

    return df_bed


def plot_SNP_density_ratioHomoHetero(small_vars, species_to_ref_genome, species_to_tree, PlotsDir, ProcessedDataDir, window_size=10000, threads=4, hm_min=None, hm_max=None):

    """This function plots in a df the density of SNPs for window_size's of the genome in a heatmap """

    # go through each species
    for species in sorted(set(small_vars.species_name)):
        print(species)

        # define things
        reference_genome = species_to_ref_genome[species]
        tree = species_to_tree[species]

        sorted_samples = tree.get_leaf_names()

        # get a square df with SNP densisties (adding 1 SNP pseudocount)
        df_square_hetero = load_object("%s/df_square_SNPs_per_window_%s_w=%i_0_1_p=2.py"%(ProcessedDataDir, species, window_size))
        df_square_hetero.index = [str(x) for x in df_square_hetero.index]
        df_square_hetero  = df_square_hetero.loc[sorted_samples].applymap(lambda c: c+1)

        df_square_homo = load_object("%s/df_square_SNPs_per_window_%s_w=%i_1_1_p=2.py"%(ProcessedDataDir, species, window_size))
        df_square_homo.index = [str(x) for x in df_square_homo.index]
        df_square_homo  = df_square_homo.loc[sorted_samples].applymap(lambda c: c+1)

        # get the df_square of the ration
        df_square_ratio = np.log2(df_square_hetero/df_square_homo)

        ######## PLOT ###########

        # define the col colors from the chromosomes
        chroms_series = pd.Series(df_square_ratio.columns, index=df_square_ratio.columns).apply(lambda x: x.split("_")[0])
        sorted_chroms = set(chroms_series.values)
        chrom_to_color, palette_chroms = get_value_to_color(sorted_chroms, palette="tab10")
        col_colors_df = pd.DataFrame({"chromosome":chroms_series.map(chrom_to_color)})

        # plot a clustermap
        cm = sns.clustermap(df_square_ratio, col_cluster=False, row_cluster=False, cbar_kws={'label': "log2(het/homo) SNPs"}, col_colors=col_colors_df, cmap="vlag", yticklabels=1, vmin=hm_min, vmax=hm_max)

        # edit colormap
        cm.ax_heatmap.set_xticklabels([])
        cm.ax_heatmap.set_yticklabels([])
        cm.ax_heatmap.set_xticks([])
        cm.ax_heatmap.set_xticks([])
        cm.ax_heatmap.set_ylabel("samples")
        cm.ax_heatmap.set_xlabel("")
        cm.ax_col_colors.set_title("C. %s"%(species.split("_")[1]))

        # adjust position
        height_hm = 0.01*len(df_square_ratio)
        hm_pos = cm.ax_heatmap.get_position()
        rowColors_pos = cm.ax_col_colors.get_position()
        cm.ax_heatmap.set_position([hm_pos.x0, rowColors_pos.y0-height_hm, hm_pos.width, height_hm])

        # write
        cm.savefig("%s/SNPdensity_window=%i_ratio.pdf"%(PlotsDir, window_size))

        #########################

def get_small_vars_with_atLeastSomeCoverageInAllPositions(small_vars, outdir, df_sorted_bams, min_coverage_pos, threads=4):

    """Takes a df with small vars (all SNPs) and returns the same only with those that are covered in all samples of df_sorted_bams"""

    # get a df that has the covergae for each position
    coverage_df_file = "%s/coverage_positions.tab"%outdir
    positions_df = get_coverage_per_positions_from_vars_df(small_vars, coverage_df_file, outdir, df_sorted_bams, threads=threads)

    # define all samples
    all_samples = [str(x) for x in sorted(set(df_sorted_bams.sampleID))]

    # keep the positions that have at least min_coverage_pos reads covering in all samples
    positions_df = positions_df[(positions_df[all_samples]>=min_coverage_pos).apply(all, axis=1)]
    print("There are %i positions with coverage >%i in all samples"%(len(positions_df), min_coverage_pos))

    # keep the vars df that are in that positions
    positions_df = positions_df[["#CHROM", "POS"]]
    positions_df["chrom_and_pos"] = positions_df["#CHROM"] + "_" + positions_df.POS.apply(str)
    small_vars["chrom_and_pos"] =  small_vars["#CHROM"] + "_" + small_vars.POS.apply(str)
    small_vars = small_vars[small_vars.chrom_and_pos.isin(set(positions_df.chrom_and_pos))]

    return small_vars

def get_small_vars_onlyBiallelic_SNPs(small_vars):

    """Takes some SNPs vars and keeps only those where ther is only one allele in a position (biallelic)"""

    # debug
    if not all(small_vars.ISSNP): raise ValueError("All should be SNPs")

    # add the chromosome and position
    small_vars["chrom_and_pos"] =  small_vars["#CHROM"] + "_" + small_vars.POS.apply(str)

    # define the biallelic positions
    def get_ALTs_chrANDpos(df_chrPos): return set(df_chrPos.ALT)
    ChromAndPos_to_is_biallelic = dict(small_vars[["chrom_and_pos", "ALT"]].groupby("chrom_and_pos").apply(get_ALTs_chrANDpos).apply(len)==1)

    small_vars["is_biallelic_pos"] = small_vars.chrom_and_pos.map(ChromAndPos_to_is_biallelic)
    if any(pd.isna(small_vars.is_biallelic_pos)): raise ValueError("There can't be NaNs in is_biallelic_pos")

    # keep df
    small_vars = small_vars[small_vars.is_biallelic_pos]

    return small_vars

def get_SNPs_df_with_biallelic_and_minimumCoverage_positions(small_vars, min_coverage_pos, threads, outdir, df_sorted_bams, diploid_samples, haploid_samples):

    """Takes a small vars df and returns only the SNPs that are biallelic and are covered in all samples"""

    snps_df_file = "%s/SNPs_df_onlyBiallelic_and_coverage_above_%ix.py"%(outdir, min_coverage_pos)
    if file_is_empty(snps_df_file):
        print("Getting SNPs that are biallelic and in positions covered in all samples")

        # redefine the GTs so that they are strings
        small_vars["common_GT"] = small_vars.common_GT.apply(str)

        # check that the common_GT is as expected
        strange_GTs = set(small_vars[~(small_vars.common_GT.isin({"0/0", "0/1", "1/1", "0", "1", "."}))].common_GT)
        if len(strange_GTs)>0: raise ValueError("There are some strange values in common_GT: %s"%strange_GTs)

        # get the initial length
        initial_len_small_vars_SNPs = sum(small_vars.ISSNP)

        # redefine the ref and alt
        def get_upper(x): return x.upper()
        small_vars["REF"] = small_vars.REF.apply(get_upper)
        small_vars["ALT"] = small_vars.ALT.apply(get_upper)
        acgt = {"A", "C", "G", "T"}

        # get actually called SNPs (and only those in non ambiguous positions)
        small_vars = small_vars[(small_vars.common_GT.isin({"0/1", "1/1", "1"})) & (small_vars.ISSNP) & (small_vars.REF.isin(acgt)) & (small_vars.ALT.isin(acgt))]

        # get SNPs that are haploid or diploid, depending on the diploid_samples, haploid_samples
        sampleID_to_callingPloidy = {**{ds:2 for ds in diploid_samples}, **{hs:1 for hs in haploid_samples}}
        small_vars["target_calling_ploidy"] = small_vars.sampleID.map(sampleID_to_callingPloidy)
        if any(pd.isna(small_vars.target_calling_ploidy)): raise ValueError("There should not be NaNs")

        small_vars = small_vars[small_vars.calling_ploidy==small_vars.target_calling_ploidy]


        # get a small variants df with all the required positions
        small_vars = get_small_vars_with_atLeastSomeCoverageInAllPositions(small_vars, outdir, df_sorted_bams, min_coverage_pos, threads=threads)

        # get biallelic SNPs
        small_vars = get_small_vars_onlyBiallelic_SNPs(small_vars)

        # flag if there is a big fraction of uncalculated SNPs
        fraction_SNPs_kept = len(small_vars)/initial_len_small_vars_SNPs
        if fraction_SNPs_kept<0.1: raise ValueError("There are few SNPs kept after filtering biallelic and regions with slow coverage: %.3f"%fraction_SNPs_kept)
        print("There are %.3f vars that are biallelic and have coverage in all positions"%fraction_SNPs_kept)

        # save
        save_object(small_vars, snps_df_file)

    small_vars = load_object(snps_df_file)

    return small_vars



def get_bgzip_and_and_tabix_vcf_file(file, replace=False):

    """Takes a vcf file and returns a tabixed and gzipped file"""

    # define files
    file_gz = "%s.gz"%file
    file_tmp_gz = "%s.tmp.gz"%file
    file_gz_tbi = "%s.gz.tbi"%file
    file_tmp_gz_tbi = "%s.tmp.gz.tbi"%file

    if file_is_empty(file_gz) or file_is_empty(file_gz_tbi) or replace is True:

        # bgzip
        bgzip_stderr = "%s.generating.stderr"%file_tmp_gz
        print("bgzipping. The stderr is in %s"%bgzip_stderr)
        run_cmd("bgzip -c %s > %s 2>%s"%(file, file_tmp_gz, bgzip_stderr), env="Candida_mine_env")

        # tabix
        tabix_std = "%s.tabixing.std"%file_tmp_gz
        print("tabix-ing. The std is in %s"%tabix_std)
        run_cmd("tabix -p vcf %s > %s 2>&1"%(file_tmp_gz, tabix_std), env="Candida_mine_env")

        # remove files
        remove_file(bgzip_stderr)
        remove_file(tabix_std)

        # rename
        os.rename(file_tmp_gz, file_gz)
        os.rename(file_tmp_gz_tbi, file_gz_tbi)

    return file_gz, file_gz_tbi

def get_df_and_header_from_vcf(vcf_file):

    """Takes a vcf file and returns the df and the header lines (as a list)"""

    vcf_strings_as_NaNs = ['', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', '1.#IND', '1.#QNAN', 'N/A', 'NULL', 'NaN', 'n/a', 'nan', 'null']

    # define the header
    header_lines = [line.strip() for line in open(vcf_file, "r", encoding='utf-8', errors='ignore') if line.startswith("##")]

    # get the vcf
    df = pd.read_csv(vcf_file, skiprows=len(header_lines), sep="\t", na_values=vcf_strings_as_NaNs, keep_default_na=False)

    return df, header_lines

 
def get_vcf_from_sns_df_for_plink(snps_df, outdir, diploid_samples, haploid_samples, threads=4):

    """This function takes a file with snps_df and generates a vcf with multiple SNPs and several samples. All are set as diploids"""

    merged_vcf = "%s/filtered_SNPs_merged.vcf"%outdir
    if file_is_empty(merged_vcf):
        print("getting %s"%merged_vcf)

        # define samples
        all_samples = diploid_samples.union(haploid_samples)

        # set numeric chromosome
        sorted_chroms = sorted(set(snps_df["#CHROM"]))
        chrom_to_numericChrom = dict(zip(sorted_chroms, list(range(1, len(sorted_chroms)+1))))
        snps_df["#CHROM"] = snps_df["#CHROM"].map(chrom_to_numericChrom)
        if any(pd.isna(snps_df["#CHROM"])): raise ValueError("there should not be NaNs")

        # add vcf fields
        snps_df["ID"] = snps_df["#Uploaded_variation"]
        snps_df["QUAL"] = 1000
        snps_df["FILTER"] = "PASS"
        snps_df["INFO"] = "."
        snps_df["FORMAT"] = "GT"

        # init the vcf df with the general positions
        vcf_df = snps_df[["#CHROM", "POS", "ID", "REF", "ALT", "QUAL", "FILTER", "INFO", "FORMAT"]].drop_duplicates().sort_values(by=["#CHROM", "POS"])
        initial_len_vcf_df = len(vcf_df)

        # check that it is only biallelic
        if len(set(vcf_df.ID))!=len(vcf_df): raise ValueError("Each ID should appear only once")

        # add one col for each df (the common sample)
        for Is, sampleID in enumerate(sorted(all_samples)):
            print("adding sample %i/%i"%(Is+1, len(all_samples)))

            # define the fields
            sample_field = "sample_%s"%sampleID

            # define the df of the sample
            df_s = snps_df[(snps_df.sampleID==sampleID)][["ID", "common_GT"]]

            # set the GT with the sampleID
            if sampleID in diploid_samples: df_s[sample_field] = df_s.common_GT
            elif sampleID in haploid_samples: df_s[sample_field] = "1/1"
            else: raise ValueError("%s is not valid"%sampleID)

            # add and check
            vcf_df = vcf_df.merge(df_s[["ID", sample_field]], on="ID", how="left", validate="one_to_one")
            if len(vcf_df)!=initial_len_vcf_df: raise ValueError("Something went wrong")

            # change the nans to 0/0
            def get_nan_to_noSNP(x):
                if pd.isna(x): return "0/0"
                else: return str(x)

            vcf_df[sample_field] = vcf_df[sample_field].apply(get_nan_to_noSNP)

        # write vcf
        print("writing")
        merged_vcf_tmp = "%s.tmp"%merged_vcf
        vcf_fields = list(vcf_df.keys())
        vcf_lines = vcf_df[vcf_fields].to_csv(sep="\t", header=False, index=False)
        header_lines = "\n".join(["##fileformat=VCFv4.2", "\t".join(vcf_fields)])
        open(merged_vcf_tmp, "w").write(header_lines + "\n" + vcf_lines)
        os.rename(merged_vcf_tmp, merged_vcf)


    return merged_vcf

def get_plink_files_from_biallelic_filtered_snps_df(snps_df, outdir, diploid_samples, haploid_samples, threads=4):
    
    """This function generates the .ped and other files for plink running. The diploids will be encoded as females and the haploids as males. I get the encoding from https://zzz.bwh.harvard.edu/plink/data.shtml. Each chromosome should be a number. All chromosomes are supposed to be the same"""

    ########## GENERATE .PED AND .MAP FILES #########
    plink_root = "filtered_SNPs"
    ped_file = "%s/%s.ped"%(outdir, plink_root)
    map_file = "%s/%s.map"%(outdir, plink_root)
    bim_file = "%s/%s.bim"%(outdir, plink_root)
    bed_file = "%s/%s.bed"%(outdir, plink_root)
    fam_file = "%s/%s.fam"%(outdir, plink_root)

    if any([file_is_empty(x) for x in [ped_file, map_file, bim_file, bed_file, fam_file]]):
        print("getting ped and map files with SNPs.")

        # change the directory
        os.chdir(outdir)

        # debug
        if len(diploid_samples.intersection(haploid_samples))>0: raise ValueError("There are some samples both haploid and diploid")

        # get a vcf file with all SNPs
        vcf_SNPs = get_vcf_from_sns_df_for_plink(snps_df, outdir, diploid_samples, haploid_samples, threads=threads)

        # get the raw .tmp and .map files. The .map is already correct, the .ped is not
        print("getting .ped file")
        tmp_prefix = "%s/%s_tmp"%(get_dir(ped_file), plink_root)
        ped_file_tmp = "%s.ped"%tmp_prefix
        map_file_tmp = "%s.map"%tmp_prefix
        run_cmd("vcftools --vcf %s --out %s --plink"%(vcf_SNPs, tmp_prefix))

        # edit the .ped to be correct
        print("correcting .ped")

        # get correct metadata (diploids are femailes, haploids are males)
        df_ped = pd.read_csv("%s.ped"%tmp_prefix, sep="\t", header=None, usecols=[0,1,2,3,4,5], names=["familyID", "sampleID", "fatherID", "motherID", "sex", "phenotype"])

        haploid_sample_to_sexID = {"sample_%s"%s : 1 for s in haploid_samples}
        diploid_sample_to_sexID = {"sample_%s"%s : 2 for s in diploid_samples}
        sample_to_sexID = {**haploid_sample_to_sexID, **diploid_sample_to_sexID}
        df_ped["sex"] = df_ped.sampleID.apply(lambda x: sample_to_sexID[x])

        ped_metadata_file = "%s.metadata.tab"%ped_file_tmp
        df_ped.to_csv(ped_metadata_file, sep="\t", index=False, header=False)

        # get the SNP data
        ped_snps_file = "%s.snps.tab"%ped_file_tmp
        run_cmd("cut -f7- %s > %s"%(ped_file_tmp, ped_snps_file))

        # merge
        run_cmd("paste %s %s > %s"%(ped_metadata_file, ped_snps_file, ped_file_tmp))

        # check that the ped and map where properly generated
        print("checking that files are OK (this already generates other files)")
        run_cmd("plink --file %s_tmp --noweb"%(plink_root))

        # clean
        for f in [ped_snps_file, ped_metadata_file, vcf_SNPs, "%s/plink.log"%outdir, "%s/%s_tmp.log"%(outdir, plink_root)]: remove_file(f)

        # rename
        os.rename(ped_file_tmp, ped_file)
        os.rename(map_file_tmp, map_file)
        os.rename("plink.bim", bim_file)
        os.rename("plink.bed", bed_file)
        os.rename("plink.fam", fam_file)

    #################################################

    return ped_file, map_file, bed_file, bim_file, fam_file

def get_fullpath(x):

    """Takes a path and substitutes it bu the full path"""

    if x.startswith("/"): return x
    elif x.startswith("."): path = "/".join(x.split("/")[1:])
    else: path = x

    return os.getcwd() + "/" + path

def get_plink_bed_file_with_filtered_SNPs_by_LD(bed_file, bim_file, fam_file, SNP_window_size=50, SNP_advance_size=10, max_r2=0.5):

    """gets a plink bed file removing SNPs that are in LD"""

    # define files
    bed_file_noLD = "%s.noLD.bed"%bed_file
    bim_file_noLD = "%s.noLD.bim"%bed_file
    fam_file_noLD = "%s.noLD.fam"%bed_file

    tmpdir = "%s.removingLDsnps"%bed_file

    # get the current directory
    curdir = get_fullpath(".")
    print(curdir)

    if any([file_is_empty(x) for x in  [bed_file_noLD, bim_file_noLD, fam_file_noLD]]):
        print("removing SNPs in LD")

        # make dir
        delete_folder(tmpdir); make_folder(tmpdir)

        # get a tmp dir
        os.chdir(tmpdir)

        # move the input int the tmpdir
        rsync_file(bed_file, "rawData.bed")
        rsync_file(bim_file, "rawData.bim")
        rsync_file(fam_file, "rawData.fam")

        #  target  for  removal  each  SNP  that  has  an R 2value  ofgreater  than  0.1  with  any  other  SNP  within  a  50-SNP  sliding  window  (advanced  by  10SNPs  each  time).
        run_cmd("plink --bfile rawData --indep-pairwise %i %i %.1f"%(SNP_window_size, SNP_advance_size, max_r2))

        #  copies  the  remaining  (untargetted)  SNPs  toprunedData.bed
        run_cmd("plink --bfile rawData --extract plink.prune.in --make-bed --out prunedData")

        # keep files
        os.rename("plink.log", "%s/prunning_LD.plink.log"%curdir)
        os.rename("prunedData.log", "%s/prunning_LD.prunedData.log"%curdir)

        os.rename("prunedData.bed", bed_file_noLD)
        os.rename("prunedData.bim", bim_file_noLD)
        os.rename("prunedData.fam", fam_file_noLD)

    # go back to the curdir and remove tmp
    os.chdir(curdir)
    delete_folder(tmpdir)
    
    return bed_file_noLD, bim_file_noLD, fam_file_noLD

def run_ADMIXTURE(bed_file, K, outdir, threads=4):

    """Infers the population structure with admixture from a .bed file for K"""

    # make the outdir
    curdir = get_fullpath(".")

    # define the final file
    root_plink = ".".join(get_file(bed_file).split(".")[0:-1])
    final_file = "%s/%s.%i.Q"%(outdir, root_plink, K)

    if file_is_empty(final_file):
        print("running admixture for K=%i"%K)

        tmpdir = "%s_tmpdir"%outdir
        delete_folder(outdir); delete_folder(tmpdir); make_folder(tmpdir)
        os.chdir(tmpdir)

        # run admixture (note that I set males to be haploid)
        run_cmd('admixture -B200 --cv=5 %s %i -j%i --haploid="male:*" > admixture_running.log 2>&1'%(bed_file, K, threads))

        os.chdir(curdir)
        os.rename(tmpdir, outdir)

def get_admixture_jobs_severalKvals(small_vars_file, diploid_samples, haploid_samples, outdir, df_sorted_bams, mitochondrial_chromosome, threads=4, min_coverage_pos=12, filterOutSNPs_withLD=True, Kvals=list(range(1,36)), threads_admixture=16):

    """This function calculates population struture using SNPs from small_vars (already filtered) that are biallelic (only referenece and one SNP), have >=min_coverage_pos for all samples and there is LD filtered. It returs a list of admixture jobs """

    # define all samples
    all_samples = diploid_samples.union(haploid_samples)
    df_sorted_bams = df_sorted_bams[df_sorted_bams.sampleID.isin(all_samples)]
    if len(df_sorted_bams)==0: raise ValueError("There are no samples")
    if len(df_sorted_bams)!=len(all_samples): raise ValueError("There are some samples with no bam specified")

    # define the bed file to run admixture on
    if filterOutSNPs_withLD is True: final_bed_file = "%s/filtered_SNPs.bed.noLD.bed"%outdir
    else: final_bed_file = "%s/filtered_SNPs.bed"%outdir

    # get the bed file
    if file_is_empty(final_bed_file):
        print("getting .bed file")

        # load the variants df
        print("loading variants")
        small_vars = load_object(small_vars_file)
        small_vars = small_vars[small_vars["#CHROM"]!=mitochondrial_chromosome]

        # make outdir
        make_folder(outdir)

        #missing_samples = set(small_vars.sampleID).difference(all_samples)
        #if len(missing_samples)>0: raise ValueError("There are some missing samples: %s"%missing_samples)

        # keep df of the important samples
        small_vars = small_vars[small_vars.sampleID.isin(all_samples)]
        if len(small_vars)==0: raise ValueError("there are no SNPs")

        print("calculating pop Structure for %i samples and %i threads"%(len(all_samples), threads))

        # get the SNPs in positions that are biallelic and are covered with min_coverage_pos in all samples
        snps_df = get_SNPs_df_with_biallelic_and_minimumCoverage_positions(small_vars, min_coverage_pos, threads, outdir, df_sorted_bams, diploid_samples, haploid_samples)

        # get the plink files for all SNPs
        ped_file, map_file, bed_file, bim_file, fam_file = get_plink_files_from_biallelic_filtered_snps_df(snps_df, outdir, diploid_samples, haploid_samples, threads=threads)

        # remove the SNPs under LD
        if filterOutSNPs_withLD is True: bed_file, bim_file, fam_file = get_plink_bed_file_with_filtered_SNPs_by_LD(bed_file, bim_file, fam_file)

        # check that the final_bed_file is the bed_file
        if final_bed_file!=bed_file: raise ValueError("bed_file is %s and final_bed_file is %s"%(bed_file, final_bed_file))


    # get one admixture cmd for each K value
    run_admixture_py = "%s/CandidaMine_data_generation/v1/run_admixture_one_K.py"%ParentDir
    all_cmds = []
    for K in Kvals:

        # don't try K values that are bigger than the number of samples
        if K>len(all_samples): continue

        outdir_K = "%s/admixture_K_%i"%(outdir, K)
        final_file = "%s/admixture_ran.txt"%outdir_K

        if file_is_empty(final_file): all_cmds.append("%s --outdir %s --bed_file %s --K %i --threads %i"%(run_admixture_py, outdir_K, final_bed_file, K, threads_admixture))

    return all_cmds

def plot_K_vs_CVerror(species_to_popStructureDir, PlotsDir, all_Ks=list(range(1,26))):

    """Plots the K vs the CV error for different species"""

    # define the df plot
    data_dict = {}; I=0
    for species, popStructureDir in species_to_popStructureDir.items():
        for K in all_Ks:
            log = "%s/admixture_K_%i/admixture_running.log"%(popStructureDir, K)
            if file_is_empty(log): raise ValueError("%s does not exist"%log)

            CVerror_lines = [float(l.strip().split()[-1]) for l in open(log, "r").readlines() if l.startswith("CV error")]
            if len(CVerror_lines)!=1: raise ValueError("there should only be one error line")

            data_dict[I] = {"species":species, "K":K, "5-fold CV error":CVerror_lines[0]}
            I+=1

    df_plot = pd.DataFrame(data_dict).transpose().sort_values(by=["species", "K"])

    # plot (only one species)
    fig, ax = plt.subplots()
    #ax = sns.lineplot(data=df_plot, x="K", y="5-fold CV error", hue="species", palette=species_to_color)
    ax.plot(df_plot.K, df_plot["5-fold CV error"], color="k", marker="o")
    plt.axhline(0.0, color="k", linestyle="--")
    #ax = sns.lineplot(data=df_plot, x="K", y="5-fold CV error")

    ax.set_xlabel("K")
    ax.set_ylabel("5-fold CV error")
    ax.set_xticks(all_Ks)
    ax.set_xticklabels(all_Ks)


def get_tree_with_cladeID_from_admixture(tree, metadata_df, K, min_support_clade):

    """This function gets a tree with the clade ID defined by K, making sure that the clades have stromg support and are monophyletic"""

    tree = cp.deepcopy(tree)

    # change the support of the children of the root
    for root_child in tree.get_children(): root_child.support=100

    # map each sample to the clade
    def get_correct_popID(popID):
        if pd.isna(popID): return np.nan
        else: return int(popID.split("Pop")[1])+1

    sampleID_to_popID = {s : get_correct_popID(p) for s,p in dict(metadata_df.set_index("sampleID")["popID_K=%i_all_samples"%(K)]).items()}

    
    # map each popID to the nodes that are monophyletic for this popID
    """
    for l in tree.get_leaves(): l.popID = sampleID_to_popID[l.name]
    def get_largest_node_of_list(list_nodes): return sorted(list_nodes, key=(lambda x: len(x.get_leaf_names())))[-1]
    popID_to_largestMonophyleticNode = {popID : get_largest_node_of_list(tree.get_monophyletic(values=[popID], target_attr="popID")) for popID in range(1, K+1)}

    # add the potential clade (only if they have a strong support) and their children
    for n in popID_to_largestMonophyleticNode.values(): 
        if n.support>=min_support_clade: n.potential_clade = True

    """
    
    for l in tree.get_leaves(): l.popID = sampleID_to_popID[l.name]
    for popID in range(1, K+1):

        # get all the monophyletic nodes, and only define as clades those that are monophyletic
        popID_nodes = list(tree.get_monophyletic(values=[popID], target_attr="popID"))
        if len(popID_nodes)==1: popID_nodes[0].potential_clade = True

    # add the potential clade to all the others
    for n in tree.traverse():
        if "potential_clade" not in set(n.__dict__): n.potential_clade = False

    # define the clades as those branches that could be potential clades and have no internal branch that could be it as well.
    current_clade = 1
    type_traverse = "preorder"
    for n in tree.traverse(type_traverse):

        # if you find a clade, define all the leafs of the node to that clade
        if n.potential_clade is True: 

            # map all the children to this clade
            n.cladeID = current_clade
            for child in n.traverse(): child.cladeID = current_clade
            current_clade+=1

    # add missing clade IDs 
    for n in tree.traverse(type_traverse):

        # if the clade ID has not been assigned, set to nan
        if "cladeID" not in set(n.__dict__): n.cladeID = np.nan

    return tree

def plots_investigate_bestK_admixture_clade_definition(tree, metadata_df, min_support_clade, maxK, plots_prefix, expectedNclades, bestK, species, min_Q_to_beAssigned):

    """This function takes different Ks until maxK and assigns clades with get_tree_with_cladeID_from_admixture. For each tree it will calculate the number of clades, the # samples without clade and the number of clades with only one sample """

    ######### get data ###########

    data_dict = {}; I=0
    for K in range(1, maxK+1):
        print(K)

        # get the tree with the clade IDs
        tree_withCladeIDs = get_tree_with_cladeID_from_admixture(tree, metadata_df, K, min_support_clade)
        leaf_to_clade = pd.Series({l.name : l.cladeID for l in tree_withCladeIDs.get_leaves()})
        clade_to_nleaves = pd.Series({cID : len(leaf_to_clade[leaf_to_clade==cID].index) for cID in set(leaf_to_clade[~pd.isna(leaf_to_clade)])})

        # calculate things and add to data
        fraction_samples_clade = 1-(sum(pd.isna(leaf_to_clade))/len(leaf_to_clade))

        if len(clade_to_nleaves)>0: fraction_clades_only1sample = sum(clade_to_nleaves[clade_to_nleaves==1])/len(clade_to_nleaves)
        else: fraction_clades_only1sample = 0.0

        data_dict[I] = {"K":int(K), "fraction_samples_clade":fraction_samples_clade, "nclades":len(clade_to_nleaves), "fraction_clades_only1sample":fraction_clades_only1sample}; I+=1

    df_plot = pd.DataFrame(data_dict).transpose().sort_values(by="K")

    ##############################

    #### plot #####

    # define the 
    selected_r = df_plot[df_plot.K==bestK].iloc[0]
    nsamples = len(tree.get_leaf_names())
    nsamples_with_clade = int(nsamples*selected_r.fraction_samples_clade)
    nclades_only1sample = int(selected_r.nclades*selected_r.fraction_clades_only1sample)


    title = "%s; Q>=%i to be assigned; clades with support>%i  \nbestK=%i; expectedClades=%i; nclades=%i\n%i/%i (%.2f%s) samples with clade\n%i/%i (%.2f%s) clades with 1 sample"%(species, min_Q_to_beAssigned, min_support_clade, bestK, expectedNclades, selected_r.nclades, nsamples_with_clade, nsamples, selected_r.fraction_samples_clade*100, "%", nclades_only1sample, selected_r.nclades, selected_r.fraction_clades_only1sample, "%")

    # add plot of the fraction_samples
    ax = df_plot.plot(x="K", y="fraction_samples_clade", legend=False, color="b", linewidth=2)
    ax = df_plot.plot(x="K", y="fraction_clades_only1sample", legend=False, color="b", linewidth=2, ax=ax, linestyle="--")

    # add line with all
    plt.axhline(1.0, color="k", linestyle="--")

    # switch to ax2
    ax2 = ax.twinx()
    df_plot.plot(x="K", y="nclades", ax=ax2, legend=False, color="r", linewidth=2)
    ax.set_title(title)

    # add best_min_fraction_branchLen
    plt.axvline(bestK, color="k", linestyle="--")
    if expectedNclades!=0: plt.axhline(expectedNclades, color="k", linestyle="--")

    # change the axis
    ax.set_ylabel("(-) fraction samples with clade\n(--) fraction clades with 1 sample", color="b")
    ax.spines['left'].set_color('b')
    ax.tick_params(axis='y', colors='b')

    ax2.set_ylabel("total # clades", color="r")
    ax2.spines['right'].set_color('r')
    ax2.tick_params(axis='y', colors='r')

    plt.show()

    # save
    fig = ax.get_figure()
    filename = "%s_choosingBestK.pdf"%plots_prefix
    fig.savefig(filename, bbox_inches="tight")

    print("saving %s"%filename)

    ###############
def get_metadata_df_one_species_cladeID_based_on_admixture(metadata_df, make_plots, maxK, tree, species, min_support_clade, plots_prefix, min_Q_to_beAssigned):

    """Takes a metadata df of one species and adds the cladeID based on the admixture runs"""

    thisneedsrefactoring

    ######################### GET TREE WITH CLADE #######################

    # define the expected number of clades
    expectedNclades = len(set(metadata_df[~pd.isna(metadata_df.cladeID_previousPaper)].cladeID_previousPaper))

    # define the best K
    species_to_bestK = {"Candida_albicans":9, "Candida_parapsilosis":6, "Candida_glabrata":14, "Candida_tropicalis":4}
    bestK = species_to_bestK[species]

    # evaluate different Ks

    if make_plots is True: plots_investigate_bestK_admixture_clade_definition(tree, metadata_df, min_support_clade, maxK, plots_prefix, expectedNclades, bestK, species, min_Q_to_beAssigned)


    # define the best clade
    tree = get_tree_with_cladeID_from_admixture(tree, metadata_df, bestK, min_support_clade)

    # map each leaf to the clade
    leaf_to_clade = {l.name : l.cladeID for l in tree.get_leaves()}

    # map each sample to the previous clade
    sample_to_cladePreviousPaper = dict(metadata_df.set_index("sampleID").cladeID_previousPaper.apply(str))

    #####################################################################

    ########### PLOT RESULTS ##########

    if make_plots is True:

        nclades = len(set([x for x in leaf_to_clade.values() if not pd.isna(x)]))

        if nclades<=10: palette = "tab10"
        else: palette = "tab20"
        clade_to_color = get_value_to_color(list(range(1, nclades+1)), palette=palette, n=nclades, type_color="hex")[0]

        for n in tree.traverse():

            nst = NodeStyle()
            nst["hz_line_width"] = 8
            nst["vt_line_width"] = 8
            nst["size"] = 0


            if pd.isna(n.cladeID): branch_color = "gray"
            else: branch_color = clade_to_color[n.cladeID]

            nst["hz_line_color"] = branch_color
            nst["vt_line_color"] = branch_color
            n.set_style(nst)

            if n.is_leaf(): 

                # add the defined clade
                if pd.isna(leaf_to_clade[n.name]): 
                    color_leaf = "gray"; label = ""
                else: 
                    color_leaf = clade_to_color[leaf_to_clade[n.name]]; label = str(n.cladeID)
                n.add_face(RectFace(600, 20, fgcolor="black", bgcolor=color_leaf, label={"text":label, "color":"black"}), position="aligned", column=0)

                # add the previous clade
                if int(n.name) not in sciName_to_badSamples[species]:
                    clade_previous = sample_to_cladePreviousPaper[n.name]

                    if clade_previous!="nan": n.add_face(RectFace(200, 20, fgcolor="black", bgcolor="white", label={"text":clade_previous, "color":"black"}), position="aligned", column=1)


        ts = TreeStyle()
        ts.show_branch_length = False
        ts.show_branch_support = False
        ts.show_leaf_name = True

        tree.show(tree_style=ts)

        stopaftertree

    ###################################

    print(expectedNclades)

    dajgfjghfda

    metadata_df.groupby("species_name").apply(lambda df_s: len(set(df_s[~pd.isna(df_s.cladeID_previousPaper)].cladeID_previousPaper)))







def get_metadata_df_with_popID_variousKs(metadata_df, admixture_Q_df, species_to_tree, PlotsDir, min_Q_to_beAssigned=75, make_plots=True, min_support_clade=95):

    """Returns the popID for several Ks. If the sample has >=min_Q_to_beAssigned of one q it is assigned there"""

    all_metadata_df = pd.DataFrame()

    print("adding popIDs to each samples")
    for species in sorted(set(metadata_df.species_name)):
        print(species)

        ######### ADD THE POPID OF EACH SAMPLE IN EACH K #########

        #if species!="Candida_tropicalis": continue # debug

        # get the dfs
        metadata_s = metadata_df[metadata_df.species_name==species]
        admixture_Q_s = admixture_Q_df[admixture_Q_df.species==species]

        # init the len
        initial_len_metadata_s = len(metadata_s)

        # go through each K and typePopStructureAnalysis
        for K in sorted(set(admixture_Q_s.K)):
            for typePopStructureAnalysis in sorted(set(admixture_Q_s.typePopStructureAnalysis)):

                # get the Q df for this species, K and typePopStructureAnalysis
                df_Q = admixture_Q_s[(admixture_Q_s.K==K) & (admixture_Q_s.typePopStructureAnalysis==typePopStructureAnalysis)]

                # do not consider Ks that are longer than the sample size
                if len(df_Q)==0: continue


                # add to the metadata_s the popX to which it belongs according to min_Q_to_beAssigned
                def get_best_popID_from_df_Q_row(r):

                    good_popIDs = list(r[r>=min_Q_to_beAssigned].keys())

                    if len(good_popIDs)>1: raise ValueError("There can't be more than one good_popIDs")
                    elif len(good_popIDs)==0: return np.nan
                    else: return  good_popIDs[0]

                field_popID = "popID_K=%i_%s"%(K, typePopStructureAnalysis)
                df_Q[field_popID] = df_Q[["Pop%i"%x for x in range(K)]].apply(get_best_popID_from_df_Q_row, axis=1)

                # add to metadata_s
                metadata_s = metadata_s.merge(df_Q[["sampleID", field_popID]], how="left", on="sampleID", validate="one_to_one")

                # checks
                if len(metadata_s)!=initial_len_metadata_s: raise ValueError("len changed")

                # check the samples that are NaNs in metadata_s and not in df_Q
                nans_metadata_s = set(metadata_s[pd.isna(metadata_s[field_popID])].sampleID)
                nans_df_Q = set(df_Q[pd.isna(df_Q[field_popID])].sampleID)
                samples_nans_in_metadata_s = nans_metadata_s.difference(nans_df_Q)

                outlayers_samples = {str(o) for o in sciName_to_outlayerNumericSampleIDs[species]}

                if len(samples_nans_in_metadata_s)>0 and not (typePopStructureAnalysis=="noOutlayerSamples" and outlayers_samples==samples_nans_in_metadata_s):

                    raise ValueError("some NanNs appeared in %s, K=%i, %s"%(species, K, typePopStructureAnalysis))

        ##########################################################

        # add the cladeID based on admixture
        #plots_prefix = "%s/investigate_bestK_cladeDefinition/%s"%(PlotsDir, species); make_folder(get_dir(plots_prefix))
        #metadata_s = get_metadata_df_one_species_cladeID_based_on_admixture(metadata_s, make_plots, max(set(admixture_Q_s.K)), cp.deepcopy(species_to_tree[species]), species, min_support_clade, plots_prefix, min_Q_to_beAssigned)

        # keep the metadata df
        all_metadata_df = all_metadata_df.append(metadata_s)

    return all_metadata_df


def get_admixture_dfs(DataDir, ProcessedDataDir):

    """This function generates an admixture df and others from DataDir """


    admixture_dfs_tuple_file = "%s/admixture_dfs_tuple.py"%ProcessedDataDir
    if file_is_empty(admixture_dfs_tuple_file):
        print("getting admixture dfs")

        # init dfs
        admixture_Q_df = pd.DataFrame() # Q vals
        admixture_Qse_df = pd.DataFrame() # the SE by bootstrapping of each of the Q vals
        admixture_Kstats_df = pd.DataFrame() # info for each K in each species
        Fst_divergence_df = pd.DataFrame() # the Fst pairwise distances betwen populations

        # go through each species
        for taxID, sciName in taxID_to_sciName.items():
            print(sciName)

            taxID_dir = "%s/%s_%i"%(DataDir, sciName, taxID)

            # go through all samples or outlayers
            for typePopStructureAnalysis, outdir_pop_structure in [("all_samples", "%s/pop_structure_inference_allSamples"%taxID_dir), ("noOutlayerSamples", "%s/pop_structure_inference"%taxID_dir)]:

                if not os.path.isdir(outdir_pop_structure): continue

                # get the samples df 
                df_samples = pd.read_csv("%s/filtered_SNPs.fam"%outdir_pop_structure, sep="\t", header=None, names=["famID", "sampleID", "father", "mother", "sexID", "phenotype"])

                df_samples["sampleID"] = df_samples.sampleID.apply(lambda x: x.split("_")[1])

                for K in range(1, 36):

                    if K>len(df_samples): continue

                    # define the outdir
                    outdir_K = "%s/admixture_K_%i"%(outdir_pop_structure, K)
                    log_file = "%s/admixture_running.log"%outdir_K


                    ################  K stats ##############

                    # cross validation error
                    CVerror = [float(l.split(": ")[1].strip()) for l in open(log_file, "r").readlines() if l.startswith("CV error")][0]

                    # number of iterations to converge
                    niterations = [int(l.split("Converged in ")[1].split()[0]) for l in open(log_file, "r").readlines() if l.startswith("Converged in")][0]

                    # log likelihood
                    logLikelihood = [float(l.split()[1].strip()) for l in open(log_file, "r").readlines() if l.startswith("Loglikelihood:")][0]

                    # add
                    admixture_Kstats_df = admixture_Kstats_df.append(pd.DataFrame({0 : {"CV_error":CVerror, "n_iterations_coverge":niterations, "log_likelihood":logLikelihood, "K":K, "species":sciName, "typePopStructureAnalysis":typePopStructureAnalysis}}).transpose())

                    ########################################

                    ################ Fst divergence ##################

                    if K>1:

                        # define a file with Fst divergences
                        header_line = [Il for Il, l in enumerate(open(log_file, "r").readlines()) if l.startswith("Fst divergences between estimated populations:")][0]+1

                        Fst_divergences_file = "%s.Fst_divergences.tab"%log_file
                        lines = [l for Il, l in enumerate(open(log_file, "r").readlines()) if Il==header_line or (l.startswith("Pop") and not l.startswith("Pop0"))]
                        open(Fst_divergences_file, "w").write("".join(lines))

                        # define the popIDs
                        colnames = ["Pop%i"%x for x in range(K-1)]
                        rownames = ["Pop%i"%x for x in range(1, K)]

                        # load df
                        df_square_Fst = pd.read_csv(Fst_divergences_file, sep="\t", index_col=0).loc[rownames, colnames]

                        # keep as pairwise comparison
                        data_dict = {}; I=0
                        for col in colnames:
                            for row in rownames:

                                Fst = df_square_Fst.loc[row, col]
                                if not pd.isna(Fst): data_dict[I] = {"comparisonID":"%s_vs_%s"%(col, row), "Fst":Fst, "K":K, "species":sciName, "typePopStructureAnalysis":typePopStructureAnalysis}; I+=1


                        Fst_divergence_df = Fst_divergence_df.append(pd.DataFrame(data_dict).transpose())

                    ##################################################

                
                    ####################### Q values ################

                    # get a df with the proportions of each population (the index is the sampleID)
                    df_Qs = pd.read_csv("%s/filtered_SNPs.bed.noLD.%i.Q"%(outdir_K, K), sep=" ", header=None, names=["Pop%i"%x for x in range(0, K)]).applymap(lambda x: x*100)

                    # add sample
                    df_Qs["sampleID"] = list(df_samples.sampleID)

                    # add others
                    df_Qs["K"] = K
                    df_Qs["species"] = sciName
                    df_Qs["typePopStructureAnalysis"] = typePopStructureAnalysis

                    admixture_Q_df = admixture_Q_df.append(df_Qs)

                    ##################################################

                    ######### Q error values ############

                    # get a df with the SE of the proportions of each population (the index is the sampleID)
                    df_Q_se = pd.read_csv("%s/filtered_SNPs.bed.noLD.%i.Q_se"%(outdir_K, K), sep=" ", header=None, names=["Pop%i"%x for x in range(0, K)]).applymap(lambda x: x*100)

                    # add sample
                    df_Q_se["sampleID"] = list(df_samples.sampleID)

                    # add others
                    df_Q_se["K"] = K
                    df_Q_se["species"] = sciName
                    df_Q_se["typePopStructureAnalysis"] = typePopStructureAnalysis

                    admixture_Qse_df = admixture_Qse_df.append(df_Q_se)


                    #####################################

        admixture_Q_df = admixture_Q_df.reset_index(drop=True)
        admixture_Kstats_df = admixture_Kstats_df.reset_index(drop=True)
        Fst_divergence_df = Fst_divergence_df.reset_index(drop=True)

        # save
        print("saving")
        save_object((admixture_Q_df, admixture_Kstats_df, Fst_divergence_df, admixture_Qse_df), admixture_dfs_tuple_file)

    # load
    admixture_Q_df, admixture_Kstats_df, Fst_divergence_df, admixture_Qse_df = load_object(admixture_dfs_tuple_file)


    return admixture_Q_df, admixture_Kstats_df, Fst_divergence_df, admixture_Qse_df


def plots_find_bestK(metadata_df, admixture_Kstats_df, Fst_divergence_df, PlotsDir, ProcessedDataDir, species_to_tree, admixture_Qs_se_df, admixture_Qs_df):

    """This generates for each species a plot that has two cols (with and without outlayers). The upper plot has a series of lineplots showing general values of the K. The lower plots have several distributions."""

    dir_plots = "%s/plots_to_select_bestK"%PlotsDir; make_folder(dir_plots)

    for species in sorted(set(metadata_df.species_name)):
        for typePopStructureAnalysis in sorted(set(Fst_divergence_df.typePopStructureAnalysis)):

            print(species, typePopStructureAnalysis)

            # define general dfs for this species
            Kstats_df = admixture_Kstats_df[(admixture_Kstats_df.species==species) & (admixture_Kstats_df.typePopStructureAnalysis==typePopStructureAnalysis)]
            Fst_df = Fst_divergence_df[(Fst_divergence_df.species==species) & (Fst_divergence_df.typePopStructureAnalysis==typePopStructureAnalysis)] 

            # continue if it does not exist
            if len(Kstats_df)==0: continue

            # define the metadata df 
            df = metadata_df[(metadata_df.species_name==species)]

            if typePopStructureAnalysis=="all_samples": all_samples = set(df.sampleID)
            elif typePopStructureAnalysis=="noOutlayerSamples": 

                outlayers_samples = {str(s) for s in sciName_to_outlayerNumericSampleIDs[species]}
                all_samples = set(df.sampleID).difference(outlayers_samples)

            df = df[(df.sampleID.isin(all_samples))]

            if len(df)!=len(all_samples): raise ValueError("There are missing samples")

            # define the previous numbers of K
            previous_K = len(set(df[~pd.isna(df.cladeID_previousPaper)].cladeID_previousPaper))

            ########## DEFINE A DF WITH THE LINEPLOTS ############

            # this df will have, for each K, the CV error, normalised number of iterations to convergence, normalised log likelihood and the fraction of samples that belong to some cluster with no admixture

            # add the normalised log likelihood (between 0 and 1)
            Kstats_df["positive_log_likelihood"] = Kstats_df.log_likelihood + max(abs(Kstats_df.log_likelihood))
            Kstats_df["norm. log(likelihood)"] = Kstats_df.positive_log_likelihood / max(Kstats_df.positive_log_likelihood)

            # add the normalised number of iterations
            Kstats_df["norm. # iterations"] = Kstats_df.n_iterations_coverge / max(Kstats_df.n_iterations_coverge)

            # add the normalised CV error
            if set(Kstats_df.CV_error)=={0.0}: Kstats_df["norm. CV error"] = 0.0
            else: Kstats_df["norm. CV error"] =  Kstats_df.CV_error / max(Kstats_df.CV_error)

            # add the number of unassigned samples
            Kstats_df["fraction unassigned samples"] = Kstats_df.K.apply(lambda k: sum(pd.isna(df["popID_K=%i_%s"%(k, typePopStructureAnalysis)])) ) / len(df)

            # define the df
            df_lineplot = pd.DataFrame()
            for f in ["norm. CV error", "norm. log(likelihood)", "norm. # iterations", "fraction unassigned samples"]:
                
                df_f = Kstats_df[["K", f]].rename(columns={f:"value"})
                df_f["K"] = df_f.K.apply(int)
                df_f["value"] = df_f.value.apply(float)
                df_f["type"] = f
                df_lineplot = df_lineplot.append(df_f)

            ######################################################


            ######### DEFINE A DF WITH THE DISTRIBUTIONS ############

            df_distplot_file = "%s/%s_%s_distplot_df_choosingK_withQse.py"%(ProcessedDataDir, species, typePopStructureAnalysis)
            if file_is_empty(df_distplot_file):

                print("calculating distplot df")

                # init with the Fst_df
                df_distplot = Fst_df[["K", "Fst"]].rename(columns={"Fst":"value"})
                df_distplot["type"] = "Fst between populations"

                # define the dfQ and dfQ SE
                df_Q_se = admixture_Qs_se_df[(admixture_Qs_se_df.species==species) & (admixture_Qs_se_df.typePopStructureAnalysis==typePopStructureAnalysis)]

                df_Q = admixture_Qs_df[(admixture_Qs_df.species==species) & (admixture_Qs_df.typePopStructureAnalysis==typePopStructureAnalysis)]

                # define the maximum SE for this species and type of analyses
                max_Q_se_vals = np.array(make_flat_listOflists(df_Q_se[["Pop%i"%(K-1) for K in sorted(set(Kstats_df.K))]].values))
                max_Q_se = max(max_Q_se_vals[~pd.isna(max_Q_se_vals)])

                if max_Q_se<=0: raise ValueError("%s can't be"%max_Q_se)

                # add things for each K
                for K in sorted(set(Kstats_df.K)):
                    print(K)

                    # map each sample to a popID
                    popID_f = "popID_K=%s_%s"%(K, typePopStructureAnalysis)
                    sampleID_to_popID = dict(df.set_index("sampleID")[popID_f])

                    def get_NaN_into_str(x):
                        if pd.isna(x): return "unassigned"
                        else: return x

                    sampleID_to_popID = {sampleID : get_NaN_into_str(popID) for sampleID, popID in sampleID_to_popID.items()}


                    ########### FOR EACH POPID AND CALL WITH >0.25 Q, GET THE DISTRIBUTION OF SEs ###########

                    # define the dfs
                    df_Q_se_K = df_Q_se[df_Q_se.K==K]
                    df_Q_K = df_Q[df_Q.K==K]

                    for popID in ["Pop%i"%x for x in range(K)]:

                        # get the samples that have some Q
                        samples_someQ_assigned = set(df_Q_K[df_Q_K[popID]>=0.25].sampleID)

                        # get the df of SEs that have these samples
                        df_Q_se_K_popID = df_Q_se_K[df_Q_se_K.sampleID.isin(samples_someQ_assigned)]

                        # add the normalised SE
                        df_Q_se_K_popID["value"] = df_Q_se_K_popID[popID]/max_Q_se

                        # add the names
                        df_Q_se_K_popID["type"] = "norm. Q SE (bootstrapping)"

                        # keep
                        df_distplot = df_distplot.append(df_Q_se_K_popID[["K", "type", "value"]]) 

                    #########################################################################################


                    ########### FOR EACH POP, GET THE FRACTION OF SAMPLES THAT ARE MONOPHYLETIC TO THE MAIN GROUP #######

                    # get the tree of the samples that have some populationID. Just consider not unassigned samples
                    tree = cp.deepcopy(species_to_tree[species])
                    interesting_samples = {s for s in set(df.sampleID) if sampleID_to_popID[s]!="unassigned"}
                    #interesting_samples = set(df.sampleID)
                    tree.prune(interesting_samples)

                    # add the popID 
                    for l in tree.get_leaves(): l.add_feature("popID", sampleID_to_popID[l.name]) 

                    # get the fraction of monophyletic samples of each popID
                    popID_to_fractionMonophyletic = {}
                    for popID in ["Pop%i"%x for x in range(K)]:

                        # define the len of all the monophyletic nodes
                        all_lens_monophyleticNodes = [len(n.get_leaf_names()) for n in tree.get_monophyletic(values=[popID], target_attr="popID")]

                        # get the ratio between the longest and all
                        if len(all_lens_monophyleticNodes)==0: fraction_monophyletic = 0.0
                        else: fraction_monophyletic = max(all_lens_monophyleticNodes)/sum(all_lens_monophyleticNodes)

                        # add
                        df_distplot = df_distplot.append(pd.DataFrame({0: {"K":K, "type":"fraction monophyletic samples", "value":fraction_monophyletic}}).transpose())

                    #####################################################################################################

                    ######### OVERLAP WITH PREVIOUS CALLS #######

                    # we take all the population IDs that overlap samples with some previously described cladeIDs. We calcualte, for each of these population IDs and samples with some previously assigned pop IDs, what is the overlap with the most overlapping cluster.

                    # define a df for all these calculations
                    df_overlap = df[~pd.isna(df.cladeID_previousPaper)]
                    if len(df_overlap)==0: continue

                    # map each popID to the samples
                    popID_to_samples = dict(df_overlap.groupby(popID_f).apply(lambda df_pop: set(df_pop.sampleID)))

                    # map each cladeID (previously detected) to the sample
                    cladeID_to_samples = dict(df_overlap.groupby("cladeID_previousPaper").apply(lambda df_clade: set(df_clade.sampleID)))

                    # define the interesting popIDs, sorted from the longest to the shortest
                    sorted_interesting_popIDs = list(reversed(sorted({sampleID_to_popID[s] for s in set(df_overlap.sampleID)}.difference({"unassigned"}), key=(lambda popID: len(popID_to_samples[popID])))))

                    # initialize the already picked clade IDs
                    already_picked_cladeIDs = set()

                    # go through each popID 
                    for popID in sorted_interesting_popIDs:

                        # define the target_cladeIDs (not already considered)
                        target_cladeIDs = set(cladeID_to_samples).difference(already_picked_cladeIDs)

                        # if all the clade IDs have been chosen, set as 0
                        if len(target_cladeIDs)==0:
                            Fvalue = 0.0
                            recall = 0.0
                            precision = 0.0

                        else:

                            # define the samples
                            popID_samples = popID_to_samples[popID]

                            # define the cladeID that is most similar (it has the highest overlap)
                            cladeID_to_nOverlapping = {cladeID : len(cladeID_to_samples[cladeID].intersection(popID_samples)) for cladeID in target_cladeIDs}

                            best_cladeID = sorted(target_cladeIDs, key=(lambda x: cladeID_to_nOverlapping[x]))[-1]

                            best_cladeID_samples = cladeID_to_samples[best_cladeID]

                            # keep for the next run
                            already_picked_cladeIDs.add(best_cladeID)

                            # define the types of samples, so that the best_cladeID_samples are like the "truth set"
                            TP = len(best_cladeID_samples.intersection(popID_samples))
                            FP = len(popID_samples.difference(best_cladeID_samples))
                            FN = len(best_cladeID_samples.difference(popID_samples))

                            if TP==0 and FP==0: precision = 0.0
                            else: precision = TP/(TP + FP)

                            recall = TP/(TP + FN)

                            if precision==0.0 or recall==0.0: Fvalue = 0.0
                            else: Fvalue = (2*precision*recall)/(precision+recall)

                        # add to the df
                        df_distplot = df_distplot.append(pd.DataFrame({0: {"K":K, "type":"overlap previous work (Fvalue)", "value":Fvalue}}).transpose())

                    #############################################
                    
                # define the K as a float
                df_distplot["K"] = df_distplot.K.apply(int)
                df_distplot["value"] = df_distplot.value.apply(float)

                # save
                save_object(df_distplot, df_distplot_file)

            df_distplot = load_object(df_distplot_file)

            #########################################################

            ######### PLOT ###########

            print("plotting")
            fig = plt.figure(figsize=(len(Kstats_df)*0.95, 5))

            # get the first subplot with the line plots
            ax = plt.subplot(2, 1, 1)

            ax = sns.lineplot(x="K", y="value", data=df_lineplot, hue="type", style="type", markers=True, dashes=False)
            ax.set_title("C. %s, %s"%(species.split("_")[1], typePopStructureAnalysis))
            if previous_K!=0: plt.axvline(previous_K, color="k", linestyle="--")
            ax.legend(bbox_to_anchor=(1.01, 1), loc=2, borderaxespad=0.)
            ax.set_xlim([1, len(Kstats_df)+1])
            ax.set_xticks(list(range(1, len(Kstats_df)+1)))
            ax.set_xlabel("")

            # get the second subplot with distriburions
            ax = plt.subplot(2, 1, 2)

            #ax = sns.swarmplot(data=df_distplot, x="K", y="value", hue="type", dodge=True, alpha=.85)
            ax = sns.stripplot(data=df_distplot, x="K", y="value", hue="type", dodge=True, alpha=.95)

            # violin with alpha
            ax = sns.violinplot(data=df_distplot, x="K", y="value", hue="type")
            for art in ax.get_children():
                if type(art)==matplotlib.collections.PolyCollection: art.set_alpha(0.7)

            if previous_K!=0: plt.axvline(previous_K-1, color="k", linestyle="--")

            ax.legend(bbox_to_anchor=(1.01, 1), loc=2, borderaxespad=0.)
            ax.set_xlim([0, len(Kstats_df)])
            ax.set_ylim([0,1])

            # save
            print("saving")
            filename = "%s/%s_%s.pdf"%(dir_plots, species, typePopStructureAnalysis)
            fig.savefig(filename, bbox_inches='tight')
            plt.close(fig)

            ##########################

def get_distance_otherPopID_to_popID(popID, popID_K, popID_T, other_popID, other_popID_K, other_popID_T, metadata_df):

    """This function calcuates the distance between two populations in two different admixture runs as the number of overlapping samples (according to metadata_df) divided by the number of samples in other_popID"""

    # get the series of each
    popID_samples = set(metadata_df[metadata_df["popID_K=%i_%s"%(popID_K, popID_T)]==popID].sampleID)
    other_popID_samples = set(metadata_df[metadata_df["popID_K=%i_%s"%(other_popID_K, other_popID_T)]==other_popID].sampleID)

    if len(other_popID_samples)==0: return 0
    else: return 1 - (len(popID_samples.intersection(other_popID_samples)) / len(other_popID_samples))

def get_K_and_typePopStructureAnalysis_to_popID_to_color(metadata_df, list_K_and_typePopStructureAnalysis, type_color="hex"):

    """This function takes a list of combinations of list_K_and_typePopStructureAnalysis and the Q values, and returns a popID_to_color for each combination, so that the colors that map to similar popIDs are the same in all combinations"""

    # map each K and typePopStructureAnalysis to the number of pops
    KandT_to_popIDs = {(K, T) : ["Pop%i"%x for x in range(K)] for K, T in list_K_and_typePopStructureAnalysis}

    # get the K and T with the highest number of popIDs
    maxPops_KandT = sorted(KandT_to_popIDs.keys(), key=(lambda x: len(KandT_to_popIDs[x])))[-1]
    maxK, maxType = maxPops_KandT

    # define all colors by the maxPops_KandT
    maxPops_KandT_popID_to_color = get_value_to_color(KandT_to_popIDs[maxPops_KandT], n=len(KandT_to_popIDs[maxPops_KandT]), type_color=type_color, palette="tab20")[0]

    # for each pop in each KandT, define the color from maxPops_KandT_popID_to_color from the closest popID and without repeating
    KandT_to_popID_to_color = {}

    for KandT in sorted(KandT_to_popIDs.keys()):
        popIDs = KandT_to_popIDs[KandT]

        # init a set of the items in maxPops_KandT_popID_to_color already taken
        already_taken_maxPops_KandT_popIDs = set()

        # define popID_to_samples
        popIDs_f = "popID_K=%i_%s"%(KandT[0], KandT[1])
        popID_to_nsamples = dict(metadata_df[~pd.isna(metadata_df[popIDs_f])].groupby(popIDs_f).apply(lambda df_pop: len(set(df_pop.sampleID))))

        # add missing popIDs
        for popID in set(popIDs).difference(set(popID_to_nsamples)): popID_to_nsamples[popID] = 0

        # sort according to the number of samples
        sorted_popIDs = list(reversed(sorted(popIDs, key=(lambda x: popID_to_nsamples[x]))))

        # go through each popID
        for popID in sorted_popIDs:

            # calculate the distance (according to the overlap of samples that are fully mapped to each population) between this popID and each of the popIDs in maxPops_KandT_popID_to_color

            otherPopID_to_distance = {otherPopID : get_distance_otherPopID_to_popID(popID, KandT[0], KandT[1], otherPopID, maxK, maxType, metadata_df) for otherPopID in set(maxPops_KandT_popID_to_color.keys()).difference(already_taken_maxPops_KandT_popIDs)}

            closest_otherPopID = sorted(set(otherPopID_to_distance.keys()), key=(lambda x: otherPopID_to_distance[x]))[0]

            # keep the closest_otherPopId
            already_taken_maxPops_KandT_popIDs.add(closest_otherPopID)

            # keep 
            KandT_to_popID_to_color.setdefault(KandT, {}).setdefault(popID, cp.deepcopy(maxPops_KandT_popID_to_color[closest_otherPopID]))

    return KandT_to_popID_to_color


def plot_trees_with_admixture(species_to_tree, species_to_K_and_typePopStructureAnalysis, PlotsDir, admixture_Qs_df, metadata_df):

    """This function plots a tree with the admixed proportions"""

    # map each population ID to a letter
    #popID_to_letter = dict(zip(popID_to_color.keys(), string.ascii_uppercase[0:len(popID_to_color)]))
    #letterPopID_to_color = {letter : popID_to_color[popID] for popID, letter in popID_to_letter.items()}

    # go through each species
    for species in species_to_K_and_typePopStructureAnalysis.keys():

        # define the interesting df
        admixture_Qs_df_all = pd.concat([admixture_Qs_df[(admixture_Qs_df.K==K) & (admixture_Qs_df.typePopStructureAnalysis==typePopStructureAnalysis) & (admixture_Qs_df.species==species)] for K, typePopStructureAnalysis in species_to_K_and_typePopStructureAnalysis[species]])

        # get the popID to color for each K_and_typePopStructureAnalysis
        K_and_typePopStructureAnalysis_to_popID_to_color = get_K_and_typePopStructureAnalysis_to_popID_to_color(metadata_df[metadata_df.species_name==species], species_to_K_and_typePopStructureAnalysis[species], type_color="hex")

        # get the tree (only samples with admixture)
        tree = cp.deepcopy(species_to_tree[species])
        tree.prune(set(admixture_Qs_df_all.sampleID))

        #### DEFINE INTERAL TREE PARMS ######
        for n in tree.traverse():

            # set the linewidth according to the support
            nst = NodeStyle()
            nst["hz_line_type"] = 0
            nst["vt_line_type"] = 0
            nst["size"] = 0

            if n.support>=90 or n.is_leaf() is True: 
                nst["hz_line_width"] = 3
                nst["vt_line_width"] = 3

            else:
                nst["hz_line_width"] = 1
                nst["vt_line_width"] = 1

            n.set_style(nst)

        #####################################

        # init the treestyle
        ts = TreeStyle()

        colI = 1
        for K, typePopStructureAnalysis in species_to_K_and_typePopStructureAnalysis[species]:
            print(species, K, typePopStructureAnalysis)


            # get the popID_to_color
            popID_to_color = K_and_typePopStructureAnalysis_to_popID_to_color[(K, typePopStructureAnalysis)]

            # get a df with the proportions of each population (the index is the sampleID)
            df_Qs = admixture_Qs_df[(admixture_Qs_df.species==species) & (admixture_Qs_df.K==K) & (admixture_Qs_df.typePopStructureAnalysis==typePopStructureAnalysis)].set_index("sampleID")
            df_Qs = df_Qs[["Pop%i"%x for x in range(K)]]

            # add a bar to each leave as a function of the colors
            for l in tree.get_leaves(): 

                # existing samples
                if l.name in df_Qs.index:

                    colors = [popID_to_color[k] for k in df_Qs.keys()]
                    percents = list(df_Qs.loc[l.name])

                else:

                    colors = ["white"]
                    percents = [100.00]

                l.add_face(StackedBarFace(percents, 1000, 20, colors=colors, line_color=None), position="aligned", column=colI)

            # add the header
            nameF = TextFace("K=%i, %s"%(K, typePopStructureAnalysis), fsize=10, bold=True); nameF.rotation = 0
            nameF.vt_align = 0
            ts.aligned_header.add_face(nameF, column=colI) 

            # update colID
            colI+=1

            # add blank
            l.add_face(RectFace(20, 20, fgcolor="white", bgcolor="white"), position="aligned", column=colI); colI+=1

        # final tree style
        ts.show_branch_length = False
        ts.show_branch_support = False
        ts.show_leaf_name = True
        ts.draw_guiding_lines = 2 # 0=solid, 1=dashed, 2=dotted.


        # show or write
        #tree.show(tree_style=ts)

        # write
        filename = "%s/%s_tree_withAdmixture.pdf"%(PlotsDir, species)
        tree.render(file_name=filename, tree_style=ts)


def rotate_image_and_left_align(image_path, rotation, fractionImg_leftTrim=0.4):

    """This function rotates an image and left aligns it"""

    # define the images
    rotated_image_path = "%s.rotated_%sdegrees.png"%(image_path, rotation) 

    # load the image as an object
    img = PILImage.open(image_path)

    # dimensions of the image
    width, height = img.size # dimensions in cm
    expand = max([width, height])

    # rotate
    img = img.rotate(rotation, expand=True)

    # crop trimming the left
    width, height = img.size   # Get dimensions
    left = width*fractionImg_leftTrim
    crop_box = (left, 0, width, height)
    img = img.crop(crop_box)

    # save
    img.save(rotated_image_path)
    return rotated_image_path

def plot_tree_with_triangular_pairwise_heatmap(tree, df_data, filename, species, zfield="", dpi=1000):

    """plots a tree with a triangular heatmap of pairwise differences"""

    print("running plot_tree_with_triangular_pairwise_heatmap")

    #### PLOT TREE AS FILE ######

    for n in tree.traverse():

        # set the line
        nst = NodeStyle()
        nst["hz_line_type"] = 0
        nst["vt_line_type"] = 0
        nst["size"] = 0
        nst["hz_line_width"] = 1
        nst["vt_line_width"] = 1
        n.set_style(nst)

        if n.is_leaf(): n.add_face(RectFace(1, 5, fgcolor="white", bgcolor="white"), position="aligned", column=0)

    # init the treestyle
    ts = TreeStyle()

    # final tree style
    ts.show_branch_length = False
    ts.show_branch_support = False
    ts.show_leaf_name = False
    ts.draw_guiding_lines = True # 0=solid, 1=dashed, 2=dotted.
    #ts.guiding_lines_type = 0
    #ts.extra_branch_line_type = 0
    #ts.complete_branch_lines_when_necessary = True


    # show 
    if filename is None: 
        tree.show(tree_style=ts)
        return 

    # write tree
    filename_tree = "%s.tree.png"%filename
    nleafs = len(tree.get_leaf_names())
    tree.render(file_name=filename_tree, tree_style=ts, dpi=500,  units='mm', h=nleafs*2) #w=20



    # crop out the bottom
    heigth_crop = {"Candida_auris":390, "Candida_metapsilosis":280, "Candida_albicans":390, "Candida_glabrata":390, "Candida_parapsilosis":340, "Candida_tropicalis":350, "Candida_orthopsilosis":300}[species]

    tree_image = PILImage.open(filename_tree)
    width, height = tree_image.size 
    crop_box = (0, 0, width, height-heigth_crop) # 300 is the lower part
    tree_image = tree_image.crop(crop_box)
    tree_image.save(filename_tree)

    ###############################

    ####### GET THE TRIANGULAR HEATMAP ########

    # get as a square df
    sorted_samples = [int(x) for x in tree.get_leaf_names()]
    df_plot = df_data[["query_sampleID", "target_sampleID", zfield]].pivot(index="query_sampleID", columns="target_sampleID", values=zfield)    

    if set(sorted_samples)!=set(df_plot.keys()): raise ValueError("df_plot.keys does not have all the samples")
    if set(sorted_samples)!=set(df_plot.columns): raise ValueError("df_plot.columns does not have all the samples")

    # check that there are no NaNs
    def raise_if_nan(x):
        if pd.isna(x): raise ValueError("there is a nan")
    df_plot.applymap(raise_if_nan)    

    # get the sorted df
    reversed_samples = list(reversed(sorted_samples))
    df_plot = df_plot[reversed_samples]

    sampleID_to_I = dict(zip(reversed_samples, range(len(reversed_samples))))
    df_plot["sampleID_I"] = [sampleID_to_I[s] for s in df_plot.index]
    df_plot = df_plot.sort_values(by="sampleID_I", ascending=True)
    df_plot = df_plot.drop("sampleID_I", axis=1)

    # plot as a heatmap
    mask = np.zeros_like(df_plot)
    mask[np.triu_indices_from(mask)] = True

    fig = plt.figure(figsize=(5,5))
    ax = sns.heatmap(df_plot, cbar_kws={"label":zfield}, mask=mask, vmax=1.0, vmin=0.0, xticklabels=True, yticklabels=True, cbar=False, cmap="rocket_r", center=0.5)

    # remove things
    ax.set_xticks([]); ax.set_xticklabels([]); ax.set_xlabel("")
    ax.set_yticks([]); ax.set_yticklabels([]); ax.set_ylabel("")

    # save figure
    filename_heatmap = "%s.heatmap.png"%filename
    fig.savefig(filename_heatmap, bbox_inches="tight", dpi=1000)

    # get rotated figure
    filename_heatmap = rotate_image_and_left_align(filename_heatmap, 134.2, fractionImg_leftTrim=0.50)
    #plt.close()

    #########################################

    ########## MERGE PLOTS ############

    from reportlab.pdfgen import canvas
    from reportlab.lib.pagesizes import letter

    # get the sizes
    tree_width, tree_height = PILImage.open(filename_tree).size

    offset_height_hm = tree_height*0.019 # 0.025
    heatmap_height = tree_height + offset_height_hm*2

    # define the width so that the squares are the same
    #size_hm_len = np.sqrt(tree_height/2)
    heatmap_width = tree_height*np.cos(45)

    # generate pdf
    c = canvas.Canvas(filename, pagesize=(heatmap_width+tree_width+1, heatmap_height))   # width height

    c.drawImage(filename_tree, 0,0+offset_height_hm, width=tree_width, height=tree_height) # x,y are the lower left


    c.drawImage(filename_heatmap, tree_width+1, 0, width=heatmap_width, height=heatmap_height, mask="auto")

    c.save() 

    thisneedsrefactoring
    ###################################




def plot_fraction_windows_pairwise_low_SNPdensity_on_tree(species_to_tree, df_pairwise_SNP_distances_perWindow_summaryStats, PlotsDir):

    """This function plots the pairwise differences in a triangular heatmap on a tree"""

    # go through each species
    for species in species_to_tree.keys():
        print(species)

        #if species!="Candida_metapsilosis": continue

        # get the tree
        tree = cp.deepcopy(species_to_tree[species])

        # get the df with the pairwise SNP densities
        df_data = df_pairwise_SNP_distances_perWindow_summaryStats[df_pairwise_SNP_distances_perWindow_summaryStats.species==species]

        # define the filename to plot
        plots_dir = "%s/fraction_lowDiversity_windows"%PlotsDir; make_folder(plots_dir)
        filename = "%s/%s.pdf"%(plots_dir, species)
        plot_tree_with_triangular_pairwise_heatmap(tree, df_data, filename, species, zfield="fraction_genome_pairwiseSNPdensity_below_0.001")



def soft_link_files(origin, target):

    """This function takes an origin file and makes it accessible through a link (target)"""

    if file_is_empty(target):

        # rename as full paths
        origin = get_fullpath(origin)
        target = get_fullpath(target)

        # check that the origin exists
        if file_is_empty(origin): raise ValueError("The origin %s should exist"%origin)

        # remove previous lisqnk
        try: run_cmd("rm %s > /dev/null 2>&1"%target)
        except: pass

        soft_linking_std = "%s.softlinking.std"%(target)
        print("softlinking. The std is in %s"%soft_linking_std)
        run_cmd("ln -s %s %s > %s 2>&1"%(origin, target, soft_linking_std))
        remove_file(soft_linking_std)

    # check that it worked
    if file_is_empty(target): raise ValueError("The target %s should exist"%target)


def run_MCA_on_plink_filtered_SNPs(plink_prefix, mds_file, mds_eigenvals_file, n_components):

    """This function creates an mds_file and and mds_eigenvals_file for some SNPs that are in the current dir after transforming them to plink  """

    ######### PREPARE DF FOR MCA ##############
    
    # transform the .bed into a vcf
    vcf_file = "%s.vcf"%plink_prefix
    vcf_df_file = "%s.df.py"%vcf_file

    if file_is_empty(vcf_df_file):

        print("transforming to vcf")
        run_cmd("plink --bfile %s --recode vcf-iid --out %s"%(plink_prefix, plink_prefix))

        # load vcf
        print("saving as .py")
        vcf_df = get_df_and_header_from_vcf(vcf_file)[0]

        # load the df as a vcf and rename the sample_* fields to 0 and 1 (heterozygous are set to 1)
        gt_to_number = {"0/0":"n", "0/1":"y", "1/1":"y"}
        sample_fields = [k for k in vcf_df.keys() if k.startswith("sample_")]

        print("adding GT for vcf")
        for f in sample_fields: 
            
            vcf_df[f] = vcf_df[f].map(gt_to_number)
            if any(pd.isna(vcf_df[f])): raise ValueError("There can't be NaNs")

        save_object(vcf_df, vcf_df_file)

    # load the vcf
    print("loading vcf")
    vcf_df = load_object(vcf_df_file).sort_values(by=["#CHROM", "POS"])

    # define the sample fields
    sample_fields = [k for k in vcf_df.keys() if k.startswith("sample_")]

    # define a df where the samples are the index and for each SNP there is one column for the ref and one column for the ALT
    vcf_df["snpID"] = list(map(lambda x: "snp%i"%x, range(len(vcf_df))))

    # keep df with important fields
    vcf_df = vcf_df.set_index("snpID")[sample_fields]

    # keep only snps that are in more than 1 sample
    print("filtering SNPs")
    vcf_df = vcf_df[(vcf_df=="y").apply(any, axis=1)].transpose()
    

    ###########################################

    ###### PERFORM MCA ########

    print("running MCA for a %ix%i matrix"%(len(vcf_df.index), len(vcf_df.columns)))

    # init mca
    mca_obj = prince.MCA(n_components=n_components, n_iter=3, copy=True, check_input=True, engine='auto', random_state=42)

    # fit to the SNPs
    mca_obj = mca_obj.fit(vcf_df)

    # get the explained variance into, mds_eigenvals_file
    factor_fields = ["F%i"%I for I in range(1, n_components+1)]
    df_explainedVariance = pd.DataFrame({"factorID" : factor_fields, "explained_var" : (pd.Series(mca_obj.explained_inertia_) / mca_obj.total_inertia_)*100})

    save_df_as_tab(df_explainedVariance, mds_eigenvals_file)

    # get the coordinates across each factor into mds_file
    print("getting column coordinates")
    df_coordinates = mca_obj.row_coordinates(vcf_df)

    df_coordinates.columns = factor_fields
    df_coordinates["sample_name"] = list(vcf_df.index)

    save_df_as_tab(df_coordinates, mds_file)

    ###########################

def plot_MDS_popStructure(DataDir, ProcessedDataDir, PlotsDir, species_to_K_and_typePopStructureAnalysis, metadata_df, threads=4, replace=False, type_MDS="MDS"):

    """This function gets the population structure as an MDS plot based on the same SNPs used for structure"""

    os.chdir(ParentDir)

    # go through each species
    for species in species_to_K_and_typePopStructureAnalysis.keys():

        ncols = len(species_to_K_and_typePopStructureAnalysis[species])
        fig = plt.figure(figsize=(ncols*4, 3))

        # get the popID to color for each K_and_typePopStructureAnalysis
        K_and_typePopStructureAnalysis_to_popID_to_color = get_K_and_typePopStructureAnalysis_to_popID_to_color(metadata_df[metadata_df.species_name==species], species_to_K_and_typePopStructureAnalysis[species], type_color="hex")

        # go through each K and type popstructure
        for Iplot, (K, typePopStructureAnalysis) in enumerate(K_and_typePopStructureAnalysis_to_popID_to_color.keys()):
            print(species, K, typePopStructureAnalysis)

            ############ GET A DF WITH THE MDS COORDINDATES ############

            # define the bed file (all SNPs)
            if typePopStructureAnalysis=="all_samples": popStructDir = "%s/%s_%i/pop_structure_inference_allSamples"%(DataDir, species, sciName_to_taxID[species])

            elif typePopStructureAnalysis=="noOutlayerSamples": popStructDir = "%s/%s_%i/pop_structure_inference"%(DataDir, species, sciName_to_taxID[species])

            # define the ncomponents from the fam file
            if type_MDS in {"MDS", "PCA"}: ncomponents = len(pd.read_csv("%s/filtered_SNPs.bed.noLD.fam"%popStructDir, sep=" ", header=None))
            elif type_MDS in {"MCA"}: ncomponents = 5

            # move the files to outdir
            outdir_plink = "%s/%s_popStructureAnalysis_%s_datasets_%s_%iComponents"%(ProcessedDataDir, species, typePopStructureAnalysis, type_MDS, ncomponents)

            # define files
            if type_MDS=="MDS":

                mds_file = "%s/plink.mds"%outdir_plink
                mds_eigenvals_file = "%s.eigvals"%mds_file

            elif type_MDS=="PCA":

                mds_file = "%s/plink.eigenvec"%outdir_plink
                mds_eigenvals_file = "%s/plink.eigenval"%outdir_plink

            elif type_MDS=="MCA":

                mds_file = "%s/mca_factors.tbl"%outdir_plink
                mds_eigenvals_file = "%s/mca.eigenval"%outdir_plink

            if file_is_empty(mds_file) or file_is_empty(mds_eigenvals_file) or replace is True:

                # delete folders
                delete_folder(outdir_plink)
                make_folder(outdir_plink)

                soft_link_files("%s/filtered_SNPs.bed.noLD.bed"%popStructDir, "%s/filtered_SNPs.bed"%outdir_plink)
                soft_link_files("%s/filtered_SNPs.bed.noLD.bim"%popStructDir, "%s/filtered_SNPs.bim"%outdir_plink)
                soft_link_files("%s/filtered_SNPs.bed.noLD.fam"%popStructDir, "%s/filtered_SNPs.fam"%outdir_plink)

                # move to the popStructDir
                CurDir = get_fullpath("./")
                os.chdir(outdir_plink)

                # get the mds plot
                print("getting coordinates in %s"%outdir_plink)

                # MDS and PCA are very similar
                if type_MDS=="MDS": run_cmd("plink --bfile filtered_SNPs --cluster --mds-plot %i eigvals --threads %i"%(ncomponents, threads))
                elif type_MDS=="PCA": run_cmd("plink --bfile filtered_SNPs --pca %i header tabs --threads %i"%(ncomponents, threads))
                
                # MCA has it's own way of wor
                elif type_MDS=="MCA": run_MCA_on_plink_filtered_SNPs("%s/filtered_SNPs"%outdir_plink, mds_file, mds_eigenvals_file, ncomponents)


                # return
                os.chdir(CurDir)


            ############################################################

            ######### PLOT ###########


            # define things depending on the type_MDS
            if type_MDS=="MDS":

                # get the df with the MDS components
                components_fields = ["C%i"%x for x in range(1, ncomponents+1)]
                df_mds = pd.read_csv(mds_file, skiprows=1, names=(["FID", "IID", "SOL"] + components_fields), sep= "\s+")
                df_mds["sampleID"] = df_mds.IID.apply(lambda x: x.split("_")[1])
                df_mds = df_mds[components_fields + ["sampleID"]]

                # load the df with the explained variance by each component
                df_egenvals = pd.read_csv(mds_eigenvals_file, sep="\t", header=None, names=["eigenvalue"])
                df_egenvals["componentID"] = components_fields
                df_egenvals = df_egenvals.set_index("componentID", drop=False)

                if any(df_egenvals.eigenvalue<=0): raise ValueError("There can't be negative eigenvalues")
                df_egenvals["explained_var"] = (df_egenvals.eigenvalue / sum(df_egenvals.eigenvalue))*100

                xfield = "C1"
                yfield = "C2"

            elif type_MDS=="PCA":

                # get a df with the mds components
                components_fields = ["PC%i"%x for x in range(1, ncomponents+1)]
                df_mds = pd.read_csv(mds_file, sep="\t")
                df_mds["sampleID"] = df_mds.IID.apply(lambda x: x.split("_")[1])
                df_mds = df_mds[components_fields + ["sampleID"]]

                # load the df with the explained variance by each component
                df_egenvals = pd.read_csv(mds_eigenvals_file, sep="\t", header=None, names=["eigenvalue"])
                df_egenvals["componentID"] = components_fields
                df_egenvals = df_egenvals.set_index("componentID", drop=False)

                df_egenvals["explained_var"] = (df_egenvals.eigenvalue / sum(df_egenvals.eigenvalue))*100

                xfield = "PC1"
                yfield = "PC2"


            elif type_MDS=="MCA":

                df_mds = pd.read_csv(mds_file, sep="\t")
                df_mds["sampleID"] = df_mds.sample_name.apply(lambda x: x.split("_")[1])
                components_fields = [x for x in df_mds.keys() if x.startswith("F")]

                df_egenvals = get_tab_as_df_or_empty_df(mds_eigenvals_file)
                df_egenvals = df_egenvals.set_index("factorID", drop=False)

                xfield = "F1"
                yfield = "F2"
                
            # add the info from metadata
            df_meta = metadata_df[metadata_df.species_name==species].rename(columns={"popID_K=%i_%s"%(K, typePopStructureAnalysis) : "popID"})[["sampleID", "popID"]]

            def get_NaNs_to_unassigned(x):
                if pd.isna(x): return "unassigned"
                else: return x

            df_meta["popID"] = df_meta.popID.apply(get_NaNs_to_unassigned)

            df_mds = df_mds.merge(df_meta, on="sampleID", how="left", validate="one_to_one")
            if any(pd.isna(df_mds.popID)): raise ValueError("There should not be NaNs")

            # init fig
            ax = plt.subplot(1, ncols, Iplot+1)
            
            # add the scatter
            popID_to_color = cp.deepcopy(K_and_typePopStructureAnalysis_to_popID_to_color[(K, typePopStructureAnalysis)])
            popID_to_color["unassigned"] = "gray"

            all_markers = "ov^<>spP*hXDd"*100
            markers_dict = {popID : all_markers[I]   for I, popID in  enumerate(sorted(popID_to_color))}
            markers_dict["unassigned"] = "$?$"

            missing_popIDs = {x for x in set(df_mds.popID) if x not in popID_to_color}
            if len(missing_popIDs)>0: raise ValueError("%s are missing"%missing_popIDs)

            # get the assigned samples
            
            #df_assigned = 
            #ax = sns.scatterplot(data=df_mds[df_mds.popID=="unassigned"], x=xfield, y=yfield, edgecolor="gray", marker="$?$", linewidth=.5, facecolor="white", alpha=.8)


            #ax = sns.scatterplot(data=df_mds[df_mds.popID!="unassigned"], x=xfield, y=yfield, hue="popID", style="popID", palette=popID_to_color, alpha=.8, markers=markers_dict, edgecolor="k", linewidth=.5)
            

            ax = sns.scatterplot(data=df_mds, x=xfield, y=yfield, style="popID", alpha=.95, markers=markers_dict, edgecolor=[popID_to_color[popID] for popID in df_mds.popID], linewidth=1.2, facecolor="white")


            ax.set_xlabel("%s (%.2f%s variance)"%(xfield, df_egenvals.loc[xfield, "explained_var"], "%"))
            ax.set_ylabel("%s (%.2f%s variance)"%(yfield, df_egenvals.loc[yfield, "explained_var"], "%"))

            ax.set_title("%s, C. %s, K=%i, %s"%(type_MDS, species.split("_")[1], K, typePopStructureAnalysis))

            ax.legend(bbox_to_anchor=(0, -0.3), loc=2, borderaxespad=0.)

            ##########################

        plt.subplots_adjust(wspace=0.3, hspace=0.2)

        plt.show()
        plt.close(fig)



def run_MCA_on_SV_CNV_df(SV_CNV_file, target_samples, outdir, n_components, replace=False, subset_type_var={"SV", "coverageCNV"}):

    """This function runs an MCA for the SVs in SV_CNV_file and target samples. It returns a df with the MCA components and the explained variation  """
    
    # define the outputs
    df_mcaCoords_file = "%s/df_mcaCoords_%s.py"%(outdir, "-".join(sorted(subset_type_var)))
    df_explained_var_file = "%s/df_explained_var_%s.py"%(outdir, "-".join(sorted(subset_type_var)))

    if file_is_empty(df_mcaCoords_file) or file_is_empty(df_explained_var_file) or replace is True:
        print("running run_MCA_on_SV_CNV_df")

        # get the df
        print("loading df")
        SV_CNV = load_object(SV_CNV_file)
        SV_CNV["sampleID"] = SV_CNV.sampleID.apply(str)

        #  filter by subset_type_var
        SV_CNV = SV_CNV[SV_CNV.type_var.isin(subset_type_var)]

        # keep the target samples
        SV_CNV = SV_CNV[SV_CNV.sampleID.isin(target_samples)]
        SV_CNV["variant_is_present"] = "y"
        SV_CNV = SV_CNV[["sampleID", "variantID_across_samples", "variant_is_present"]].drop_duplicates()

        # get d square df, where each row is a sample and each col is a df 
        print("generating square df")
        def convert_NaN_to_n(x):
            if pd.isna(x): return "n"
            else: return x

        SV_CNV_square = SV_CNV.pivot(index="sampleID", columns="variantID_across_samples", values="variant_is_present").applymap(convert_NaN_to_n)

        # add missing samples
        missing_samples = target_samples.difference(set(SV_CNV.sampleID))
        if len(missing_samples)>0: 
            print("There are missing samples: %s"%missing_samples)
            SV_CNV_square_missing = pd.DataFrame({s : {c : "n" for c in SV_CNV_square.columns} for s in sorted(missing_samples)}).transpose()
            print(SV_CNV_square_missing)

            thisIsNOtCOnsidered


        # filter to keep only variants that are varying in some positions
        SV_CNV_square = SV_CNV_square[SV_CNV_square.columns[(SV_CNV_square=="y").apply(any, axis=0)]]

        # run MCA
        print("running MCA for SVs %ix%i matrix"%(len(SV_CNV_square.index), len(SV_CNV_square.columns)))

        # init mca
        mca_obj = prince.MCA(n_components=n_components, n_iter=3, copy=True, check_input=True, engine='auto', random_state=42)

        # fit to the SVs
        mca_obj = mca_obj.fit(SV_CNV_square)

        # get the explained variance into
        factor_fields = ["F%i"%I for I in range(1, n_components+1)]
        df_explained_var = pd.DataFrame({"factorID" : factor_fields, "explained_var" : (pd.Series(mca_obj.explained_inertia_) / mca_obj.total_inertia_)*100})

        save_object(df_explained_var, df_explained_var_file)

        # get the coordinates across each factor into mds_file
        print("getting column coordinates")
        df_mcaCoords = mca_obj.row_coordinates(SV_CNV_square)

        df_mcaCoords.columns = factor_fields
        df_mcaCoords["sample_name"] = list(df_mcaCoords.index)

        # saving object
        save_object(df_mcaCoords, df_mcaCoords_file)

    # load dfs
    df_mcaCoords = load_object(df_mcaCoords_file)
    df_explained_var = load_object(df_explained_var_file)

    return df_mcaCoords, df_explained_var

def plot_MCA_SVs_popStructure(DataDir, ProcessedDataDir, PlotsDir, species_to_K_and_typePopStructureAnalysis, metadata_df, threads=4, replace=False, subset_type_var={"SV", "coverageCNV"}):

    """This function generates a plot like plot_MDS_popStructure, but taking the SVs and performing an MCA by presence/absence"""

   # go through each species
    for species in species_to_K_and_typePopStructureAnalysis.keys():

        ncols = len(species_to_K_and_typePopStructureAnalysis[species])
        fig = plt.figure(figsize=(ncols*4, 3))

        # get the popID to color for each K_and_typePopStructureAnalysis
        K_and_typePopStructureAnalysis_to_popID_to_color = get_K_and_typePopStructureAnalysis_to_popID_to_color(metadata_df[metadata_df.species_name==species], species_to_K_and_typePopStructureAnalysis[species], type_color="hex")

        # define the number of components for the MCA
        n_components = 5

        # go through each K and type popstructure
        for Iplot, (K, typePopStructureAnalysis) in enumerate(K_and_typePopStructureAnalysis_to_popID_to_color.keys()):
            print(species, K, typePopStructureAnalysis)

            ############ GET A DF WITH THE MCA COORDINDATES ############

            # define an outdir for the processed data
            outdir_processedData = "%s/%s_popStructureAnalysis_%s_datasets_MCA_%iComponents"%(ProcessedDataDir, species, typePopStructureAnalysis, n_components)

            # define the taget samples
            all_samples = set(metadata_df[metadata_df.species_name==species].sampleID)
            if typePopStructureAnalysis=="all_samples": target_samples = all_samples
            elif typePopStructureAnalysis=="noOutlayerSamples": target_samples = all_samples.difference({str(x) for x in sciName_to_outlayerNumericSampleIDs[species]})

            # define the SV_CNV_file 
            SV_CNV_file = "%s/%s_%i/integrated_varcalls/SV_CNV_filt.py"%(DataDir, species, sciName_to_taxID[species])
            df_mcaCoords, df_explained_var = run_MCA_on_SV_CNV_df(SV_CNV_file, target_samples, outdir_processedData, n_components, replace=replace, subset_type_var=subset_type_var)
            df_explained_var = df_explained_var.set_index("factorID")

            ############################################################

            #### PLOT RESULTS ########

            # add the info from metadata
            df_meta = metadata_df[metadata_df.species_name==species].rename(columns={"popID_K=%i_%s"%(K, typePopStructureAnalysis) : "popID"})[["sampleID", "popID"]]

            def get_NaNs_to_unassigned(x):
                if pd.isna(x): return "unassigned"
                else: return x

            df_meta["popID"] = df_meta.popID.apply(get_NaNs_to_unassigned)

            df_mcaCoords = df_mcaCoords.merge(df_meta, left_on="sample_name", right_on="sampleID", how="left", validate="one_to_one")
            if any(pd.isna(df_mcaCoords.popID)): raise ValueError("There should not be NaNs")

            # init fig
            ax = plt.subplot(1, ncols, Iplot+1)
            
            # add the scatter
            popID_to_color = cp.deepcopy(K_and_typePopStructureAnalysis_to_popID_to_color[(K, typePopStructureAnalysis)])
            popID_to_color["unassigned"] = "gray"

            all_markers = "ov^<>spP*hXDd"*100
            markers_dict = {popID : all_markers[I]   for I, popID in  enumerate(sorted(popID_to_color))}
            markers_dict["unassigned"] = "$?$"

            missing_popIDs = {x for x in set(df_mcaCoords.popID) if x not in popID_to_color}
            if len(missing_popIDs)>0: raise ValueError("%s are missing"%missing_popIDs)

            # get scatter
            ax = sns.scatterplot(data=df_mcaCoords, x="F1", y="F2", style="popID", alpha=.95, markers=markers_dict, edgecolor=[popID_to_color[popID] for popID in df_mcaCoords.popID], linewidth=1.2, facecolor="white")


            ax.set_xlabel("F1 (%.2f%s variance)"%(df_explained_var.loc["F1", "explained_var"], "%"))
            ax.set_ylabel("F2 (%.2f%s variance)"%(df_explained_var.loc["F2", "explained_var"], "%"))

            ax.set_title("SVs (%s) MCA, C. %s, K=%i, %s"%(", ".join(sorted(subset_type_var)), species.split("_")[1], K, typePopStructureAnalysis))

            ax.legend(bbox_to_anchor=(0, -0.3), loc=2, borderaxespad=0.)

            ##########################

        plt.subplots_adjust(wspace=0.3, hspace=0.2)

        plt.show()
        plt.close(fig)


def run_picard_ValidateSamFile(sorted_bam, reference_genome, replace=False):

    """This function reports how good a sorted bam is"""

    report_file = "%s.ValidateSamFile_output.txt"%sorted_bam

    if file_is_empty(report_file) or replace is True:
        print("running picard ValidateSamFile into %s"%report_file)

        report_file_tmp = "%s.tmp"%report_file
        try: run_cmd("picard ValidateSamFile I=%s R=%s MO=1000000000 > %s 2>&1"%(sorted_bam, reference_genome, report_file_tmp), env="perSVade_env_picard_env")

        except: 
            print("WARNING: There are errors in the .bam file") 
            
            # only keep if it finished well
            if any(["picard.sam.ValidateSamFile done" in l for l in open(report_file_tmp).readlines()]): os.rename(report_file_tmp, report_file)
            else: raise ValueError("picard ValidateSamFile did not end correctly. Check %s"%report_file_tmp)

    return report_file

def get_df_vars_with_Xoffset_position(df_vars, ref_genome, chrom_f="#CHROM", pos_f="POS", xoffset_f="xoffset_POS", offset_val=1):

    """Adds to df_vars vars the X offset position"""

    # calculate teh len
    chrom_to_len = get_chr_to_len(ref_genome)

    # add the offset
    all_chromosomes = sorted(set(df_vars[chrom_f]))

    chrom_to_Xoffset = {}
    current_offset = 0
    for chrom in all_chromosomes:
        chrom_to_Xoffset[chrom] = current_offset
        current_offset += chrom_to_len[chrom] + offset_val

    df_vars["Xoffset_chromosomePOS"] = df_vars[chrom_f].map(chrom_to_Xoffset)
    if any(pd.isna(df_vars["Xoffset_chromosomePOS"])): raise ValueError("There can't be NANS in Xoffset_chromosomePOS")

    df_vars[xoffset_f] = df_vars[pos_f] + df_vars.Xoffset_chromosomePOS

    return df_vars

def plot_diploid_AF_distribution_selected_samples(DataDir, species, ref_genome, PlotsDir, ProcessedDataDir, threads=4, diploid_samples=set(), haploid_samples=set()):

    """This function plots the AF for several samples"""

    ######### GET DATA ##########

    all_samples = {str(s) for s in  haploid_samples.union(diploid_samples)}
    df_vars_file = "%s/%s_samples_%s_filtVars_onlyPloidy2.py"%(ProcessedDataDir, species, "-".join(sorted(all_samples)))  

    if file_is_empty(df_vars_file):
        print("generating", df_vars_file)

        # load vars
        all_df_vars = load_object("%s/%s_%i/integrated_varcalls/smallVars_filt.py"%(DataDir, species, sciName_to_taxID[species]))

        # keep only some samples
        all_df_vars = all_df_vars[all_df_vars.sampleID.isin(all_samples)]
        all_df_vars = all_df_vars[all_df_vars.calling_ploidy==2]

        if len(all_df_vars)==0: 
            print(all_df_vars.sampleID, all_df_vars.sampleID.iloc[0], all_df_vars.calling_ploidy)
            raise ValueError("not valid all_df_vars")

        save_object(all_df_vars, df_vars_file)
        del all_df_vars

    print("loading data")
    df_vars = load_object(df_vars_file)
    df_vars["sampleID"] = df_vars.sampleID.apply(str)

    # keep only vars that are not around repeats and not in dup regions
    df_vars = df_vars[(df_vars.ISSNP) & ~(df_vars.INREPEATS) & (df_vars.relative_CN<=1.0)]

    # add the offset positions
    print("adding offset")
    df_vars = get_df_vars_with_Xoffset_position(df_vars, ref_genome, chrom_f="#CHROM", pos_f="POS", xoffset_f="xoffset_POS")

    #############################

    ######### PLOT  AFs ###########
    print("plotting")

    sorted_samples = [str(x) for x in (sorted(haploid_samples) + sorted(diploid_samples))]
    sampleID_to_color = get_value_to_color(sorted_samples, palette="tab10")[0]

    fig = plt.figure(figsize=(len(sorted_samples)*3, 3))
    for Is, sampleID in enumerate(sorted_samples):

        ax = plt.subplot(1, len(sorted_samples), Is+1)
        df_plot = df_vars[df_vars.sampleID==sampleID]

        ax = sns.scatterplot(data=df_plot, x="xoffset_POS", y="mean_fractionReadsCov_PASS_algs", hue="sampleID", alpha=0.1, palette=sampleID_to_color)

        ax.set_ylim([0, 1])
        plt.axhline(0.5, color="k", linestyle="--")

        # add line at the median of the 0/1 SNPs
        median_het_AF = np.median(df_plot[df_plot.common_GT=="0/1"].mean_fractionReadsCov_PASS_algs)
        plt.axhline(median_het_AF, color=sampleID_to_color[sampleID], linestyle="--")

        if Is!=0: ax.set_yticklabels([])
        if Is!=0: ax.set_ylabel("")

        ax.set_title("sample %s"%sampleID)

        ax.get_legend().remove()

    # space between subplots
    plt.subplots_adjust(wspace=0.01, hspace=0.0)

    ###############################


def get_sample_to_setVars_for_typeVar(taxID_dir, type_var, ploidy, expected_samples):

    """This function returns a sample_to_vars series for a type of variant"""

    if type_var=="homo SNP":
        df = load_object("%s/integrated_varcalls/smallVars_filt.py"%taxID_dir).rename(columns={"#Uploaded_variation":"unique_varID"})
        df = df[(df.calling_ploidy==2) & (df.ISSNP) & (df.common_GT=="1/1") & (df.mean_fractionReadsCov_PASS_algs>=0.9)]

    elif type_var=="hetero SNP":
        df = load_object("%s/integrated_varcalls/smallVars_filt.py"%taxID_dir).rename(columns={"#Uploaded_variation":"unique_varID"})
        df = df[(df.calling_ploidy==2) & (df.ISSNP) & (df.common_GT=="0/1") & (df.mean_fractionReadsCov_PASS_algs>=0.25)]

    elif type_var=="homo IN/DEL":
        df = load_object("%s/integrated_varcalls/smallVars_filt.py"%taxID_dir).rename(columns={"#Uploaded_variation":"unique_varID"})
        df = df[(df.calling_ploidy==2) & ~(df.ISSNP) & (df.common_GT=="1/1") & (df.mean_fractionReadsCov_PASS_algs>=0.9)]

    elif type_var=="hetero IN/DEL":
        df = load_object("%s/integrated_varcalls/smallVars_filt.py"%taxID_dir).rename(columns={"#Uploaded_variation":"unique_varID"})
        df = df[(df.calling_ploidy==2) & ~(df.ISSNP) & (df.common_GT=="0/1") & (df.mean_fractionReadsCov_PASS_algs>=0.25)]

    elif type_var=="SNP":
        df = load_object("%s/integrated_varcalls/smallVars_filt.py"%taxID_dir).rename(columns={"#Uploaded_variation":"unique_varID"})
        df = df[(df.calling_ploidy==ploidy) & (df.ISSNP)]

    elif type_var=="IN/DEL":
        df = load_object("%s/integrated_varcalls/smallVars_filt.py"%taxID_dir).rename(columns={"#Uploaded_variation":"unique_varID"})
        df = df[(df.calling_ploidy==ploidy) & ~(df.ISSNP)]

    elif type_var=="SV": 
        df = load_object("%s/integrated_varcalls/SV_CNV_filt.py"%taxID_dir).rename(columns={"variantID_across_samples":"unique_varID"})
        df = df[df.type_var=="SV"]

    elif type_var=="CNV":
        df = load_object("%s/integrated_varcalls/SV_CNV_filt.py"%taxID_dir).rename(columns={"variantID_across_samples":"unique_varID"})
        df = df[df.type_var=="coverageCNV"]


    else: raise ValueError("%s is not valid"%type_var)

    # get the numeric variantID
    all_vars = sorted(set(df.unique_varID))
    varID_to_numericID = dict(zip(all_vars, range(len(all_vars))))
    df["numeric_variantID"] = df.unique_varID.map(varID_to_numericID)
    if any(pd.isna(df.numeric_variantID)): raise ValueError("There can't be NaNs in numeric_variantID")

    # rename the sample as a string
    df["sampleID"] = df.sampleID.apply(str)

    # get the sample_to_vars
    def get_vars_from_df(df_s): return set(df_s.numeric_variantID)
    sample_to_vars = dict(df[["sampleID", "numeric_variantID"]].groupby("sampleID").apply(get_vars_from_df))

    # add the missing samples
    missing_samples = expected_samples.difference(set(sample_to_vars.keys()))
    if len(missing_samples)>0: 
        print("WARNING!!!! There are some samples with no vars: %s"%missing_samples)
        for s in missing_samples: sample_to_vars[s] = set()
    
    return sample_to_vars, varID_to_numericID

def get_non_intesecting_vars_get_originSample_to_targetSample_to_differentVars_len(Ic, ncombs, samples_tuple, sample_to_vars):

    """Takes two samples and return th elen of the intersecting vars"""

    if (Ic%1000)==0: print("processed %.4f%s vars"%((Ic/ncombs)*100, "%"))

    sampleA, sampleB = samples_tuple
    varsA = sample_to_vars[sampleA]
    varsB = sample_to_vars[sampleB]

    unique_varsA = varsA.difference(varsB)
    unique_varsB = varsB.difference(varsA)

    return len(unique_varsA.union(unique_varsB))

def get_originSample_to_targetSample_to_differentVars_len_parallel(sample_to_vars, threads):

    """
    generate a dict that has the vars that are different between each set of vars
    """

    print("running get_originSample_to_targetSample_to_differentVars_len")

    # init
    sorted_samples = sorted(sample_to_vars)
    all_combinations_samples = list(itertools.combinations(sorted_samples, 2))
    ncombs = len(all_combinations_samples)

    # process in chunks of all_combinations_samples
    originSample_to_targetSample_to_differentVars_len = {}
    chunk_size = int(threads*2)

    for Ichunk, chunk_all_combinations_samples in enumerate(chunks(all_combinations_samples, chunk_size)):
        print("chunk %i..."%Ichunk)
        sample_to_vars_chunk = {s : sample_to_vars[s] for s in set(make_flat_listOflists(chunk_all_combinations_samples))}

        inputs_fn = [(Ic+(Ichunk*chunk_size), ncombs, samples_tuple, sample_to_vars_chunk) for Ic, samples_tuple in enumerate(chunk_all_combinations_samples)]

        with  multiproc.Pool(threads) as pool:

            chunk_originSample_to_targetSample_to_differentVars_len = dict(zip(chunk_all_combinations_samples, pool.starmap(get_non_intesecting_vars_get_originSample_to_targetSample_to_differentVars_len, inputs_fn)))

            pool.close()
            pool.terminate()

        originSample_to_targetSample_to_differentVars_len = {**originSample_to_targetSample_to_differentVars_len, **chunk_originSample_to_targetSample_to_differentVars_len}

    return originSample_to_targetSample_to_differentVars_len

def get_originSample_to_targetSample_to_differentVars_len(sample_to_vars, threads):

    """
    generate a dict that has the vars that are different between each set of vars
    """

    print("running get_originSample_to_targetSample_to_differentVars_len")

    # init
    sorted_samples = sorted(sample_to_vars)
    all_combinations_samples = list(itertools.combinations(sorted_samples, 2))
    ncombs = len(all_combinations_samples)

    # get the dict
    inputs_fn = [(Ic, samples_tuple) for Ic, samples_tuple in enumerate(all_combinations_samples)]
    originSample_to_targetSample_to_differentVars_len = dict(zip(all_combinations_samples, map(lambda x: get_non_intesecting_vars_get_originSample_to_targetSample_to_differentVars_len(x[0], ncombs, x[1], sample_to_vars), inputs_fn)))

    return originSample_to_targetSample_to_differentVars_len

def get_df_pairwise_differences_oneSpecies_and_typeVar(taxID_dir, df_pairwiseDifferences_file, threads, type_var, ploidy, expected_samples):

    """This function takes data from one species and returns a df, written into df_pairwiseDifferences_file with the # vars per KB between each pair of samples """

    if file_is_empty(df_pairwiseDifferences_file): 
        print("generating %s"%df_pairwiseDifferences_file)

        # map each sample to the vars
        sample_to_vars, varID_to_numericID = get_sample_to_setVars_for_typeVar(taxID_dir, type_var, ploidy, expected_samples)

        # generate a dict that has the vars that are different between each set of vars
        originSample_to_targetSample_to_differentVars_len = get_originSample_to_targetSample_to_differentVars_len(sample_to_vars, threads)

        # get into a series of the length
        samples_AvsB_to_nvarsPerKb = pd.Series(originSample_to_targetSample_to_differentVars_len); del originSample_to_targetSample_to_differentVars_len
        df_pairwiseDifferences = pd.DataFrame({"diffent_positions":samples_AvsB_to_nvarsPerKb})

        df_pairwiseDifferences["origin_sample"] = df_pairwiseDifferences.index.get_level_values(0)
        df_pairwiseDifferences["target_sample"] = df_pairwiseDifferences.index.get_level_values(1)
        df_pairwiseDifferences = df_pairwiseDifferences.reset_index(drop=True)

        # add other combination of samples
        df_pairwiseDifferences_otherOrder = cp.deepcopy(df_pairwiseDifferences.rename(columns={"origin_sample":"target_sample", "target_sample":"origin_sample"}))
        df_pairwiseDifferences = df_pairwiseDifferences.append(df_pairwiseDifferences_otherOrder).reset_index(drop=True)

        # save
        save_object(df_pairwiseDifferences, df_pairwiseDifferences_file)

    df_pairwiseDifferences = load_object(df_pairwiseDifferences_file)

    return df_pairwiseDifferences


def get_non_intesecting_genes_get_originSample_to_targetSample_to_differentVars_len_ngenes(samples_tuple, sample_to_vars, var_to_genes):

    """This function is like get_non_intesecting_vars_get_originSample_to_targetSample_to_differentVars_len but returning the len of the different genes"""

    #if (Ic%10)==0: print("processed %.4f%s vars"%((Ic/ncombs)*100, "%"))

    # define the different vars
    sampleA, sampleB = samples_tuple
    varsA = sample_to_vars[sampleA]
    varsB = sample_to_vars[sampleB]

    unique_varsA = varsA.difference(varsB)
    unique_varsB = varsB.difference(varsA)

    different_vars = unique_varsA.union(unique_varsB)
    if len(different_vars)==0: return 0

    # define the number of changing genes
    nchanging_genes = len(set.union(*map(lambda x: var_to_genes[x], different_vars)))

    # get the changing_genes length
    return nchanging_genes

def chunks(l, n):
    
    """Yield successive n-sized chunks from a list l"""
    
    for i in range(0, len(l), n):
        yield l[i:i + n]


def get_subset_of_dict(dict_obj, sorted_subset_keys):

    """Takes a dict and some keys and returns the dict with that keys"""

    return dict(zip(sorted_subset_keys , map(lambda s: dict_obj[s]  , sorted_subset_keys)))

def get_originSample_to_targetSample_to_differentVars_len_ngenes(sample_to_vars, var_to_genes, threads):

    """ This function is similar to get_originSample_to_targetSample_to_differentVars_len, but getting the number of different genes instead of variants"""


    print("running get_originSample_to_targetSample_to_differentVars_len_ngenes")

    # init
    sorted_samples = sorted(sample_to_vars)
    all_combinations_samples = list(itertools.combinations(sorted_samples, 2))

    # init final df
    originSample_to_targetSample_to_differentGenes_len = {}

    # define the number of chunks
    chunk_len = 100
    nchunks = int(len(all_combinations_samples)/chunk_len)

    # iterate in chunks of 10 times the number of therads
    for Ichunk, chunk_all_combinations_samples in enumerate(chunks(all_combinations_samples, chunk_len)):
        print("working on chunk %i/%i"%(Ichunk, nchunks))

        # redefine sample_to_vars and var_to_genes so that you just have the samples and vars of this gene
        all_samples_chunk = sorted(set(make_flat_listOflists(chunk_all_combinations_samples)))
        sample_to_vars_chunk = get_subset_of_dict(sample_to_vars, all_samples_chunk)

        all_vars_chunk = sorted(set.union(*sample_to_vars_chunk.values()))
        var_to_genes_chunk = get_subset_of_dict(var_to_genes, all_vars_chunk)

        # run without parallel
        originSample_to_targetSample_to_differentGenes_len_chunk = dict(zip(chunk_all_combinations_samples, map(lambda samples_tuple: get_non_intesecting_genes_get_originSample_to_targetSample_to_differentVars_len_ngenes(samples_tuple, sample_to_vars_chunk, var_to_genes_chunk), chunk_all_combinations_samples)))
    
        # append 
        originSample_to_targetSample_to_differentGenes_len = {**originSample_to_targetSample_to_differentGenes_len, **originSample_to_targetSample_to_differentGenes_len_chunk}

    return originSample_to_targetSample_to_differentGenes_len


def get_df_pairwise_differences_oneSpecies_and_typeVar_nGenes(taxID_dir, df_pairwiseDifferences_file, threads, type_var, ploidy, expected_samples, type_comparison):

    """This function is similar to get_df_pairwise_differences_oneSpecies_and_typeVar, but it returns the number of affected genes"""


    if file_is_empty(df_pairwiseDifferences_file): 
        print("generating %s"%df_pairwiseDifferences_file)

        # map each sample to the vars affected 
        sample_to_vars, varID_to_numericID = get_sample_to_setVars_for_typeVar(taxID_dir, type_var, ploidy, expected_samples)

        ####### CREATE A DICT THAT MAPS EACH VAR TO A GENE ##########
        print("mapping each var to a gene")

        # load the variant annotation, depending on type_var and type_comparison
        if type_var in {"homo SNP", "hetero SNP", "homo IN/DEL", "hetero IN/DEL", "SNP", "IN/DEL"}:  var_annot_df = get_tab_as_df_or_empty_df("%s/integrated_varcalls/smallVars_annot.tab"%taxID_dir).rename(columns={"#Uploaded_variation":"unique_varID"})

        elif type_var in {"SV", "CNV"}: 

            # get the variant annotation
            var_annot_df = get_tab_as_df_or_empty_df("%s/integrated_varcalls/SV_CNV_annot.tab"%taxID_dir)

            # load the varcalling
            df_vars = load_object("%s/integrated_varcalls/SV_CNV_filt.py"%taxID_dir).rename(columns={"variantID_across_samples":"unique_varID"})

            if type_var=="SV": df_vars = df_vars[df_vars.type_var=="SV"]
            elif type_var=="CNV": df_vars = df_vars[df_vars.type_var=="coverageCNV"]
            ID_to_unique_varID = dict(df_vars[["ID", "unique_varID"]].drop_duplicates().set_index("ID").unique_varID)

            # add the "unique_varID" to the var_annot_df
            var_annot_df["unique_varID"] = var_annot_df["#Uploaded_variation"].map(ID_to_unique_varID)
            var_annot_df = var_annot_df[~(pd.isna(var_annot_df.unique_varID))]
            if len(var_annot_df)==0: raise ValueError("There can't be 0 annotated vars")

            # add whether it is protein altering
            var_annot_df["is_protein_altering"] = (var_annot_df.is_protein_coding_gene) & (var_annot_df.is_transcript_disrupting)

        # add the numeric variantID
        var_annot_df["numeric_variantID"] = var_annot_df.unique_varID.map(varID_to_numericID)
        if all(pd.isna(var_annot_df.numeric_variantID)): raise ValueError("not all the vars can NaN")

        # filter to keep only vars that are in some samples
        all_vars = set.union(*sample_to_vars.values())
        var_annot_df = var_annot_df[var_annot_df.numeric_variantID.isin(all_vars)]
        if any(pd.isna(var_annot_df.numeric_variantID)): raise ValueError("there can't be nans in vars")

        var_annot_df["numeric_variantID"] = var_annot_df["numeric_variantID"].apply(int)

        # filter to keep only vars that make sense in the context of type_comparison
        if type_comparison=="number_genes_protAltering": var_annot_df = var_annot_df[(var_annot_df.Gene!="-") & (var_annot_df.is_protein_altering)]
        elif type_comparison=="number_genes_all": var_annot_df = var_annot_df[var_annot_df.Gene!="-"]

        # change gene name to strings
        var_annot_df["Gene"] = var_annot_df.Gene.apply(str)

        # map each var to a gene, so that it is numeric
        sorted_genes = sorted(set(var_annot_df.Gene))
        gene_to_numericID = dict(zip(sorted_genes, range(len(sorted_genes))))
        var_annot_df["numeric_geneID"] = var_annot_df.Gene.map(gene_to_numericID)
        if any(pd.isna(var_annot_df.numeric_geneID)): raise ValueError("there can't be NaNs")
        if any(pd.isna(var_annot_df.numeric_geneID)): raise ValueError("there can't be NaNs")

        def get_set_genes_from_df_v(df_v): return set(df_v.numeric_geneID)
        var_to_genes = dict(var_annot_df[["numeric_variantID", "numeric_geneID"]].drop_duplicates().groupby("numeric_variantID").apply(get_set_genes_from_df_v))

        # keep only vars that are in var_annot_df
        print("keeping only vars with functional alteration") # this is quite fast
        all_functional_vars = set(var_annot_df.numeric_variantID)
        sorted_samples = sorted(sample_to_vars.keys())

        def get_only_functional_var_for_sample(s): return sample_to_vars[s].intersection(all_functional_vars)
        sample_to_vars = dict(zip(sorted_samples, map(get_only_functional_var_for_sample , sorted_samples)))

        #############################################################

        # get the comparison in samples by genes as well
        originSample_to_targetSample_to_differentGenes_len = get_originSample_to_targetSample_to_differentVars_len_ngenes(sample_to_vars, var_to_genes, threads)

        # get into a series of the length
        samples_AvsB_to_nChangedGenes = pd.Series(originSample_to_targetSample_to_differentGenes_len); del originSample_to_targetSample_to_differentGenes_len
        df_pairwiseDifferences = pd.DataFrame({"different_genes":samples_AvsB_to_nChangedGenes})

        df_pairwiseDifferences["origin_sample"] = df_pairwiseDifferences.index.get_level_values(0)
        df_pairwiseDifferences["target_sample"] = df_pairwiseDifferences.index.get_level_values(1)
        df_pairwiseDifferences = df_pairwiseDifferences.reset_index(drop=True)

        # add other combination of samples
        df_pairwiseDifferences_otherOrder = cp.deepcopy(df_pairwiseDifferences.rename(columns={"origin_sample":"target_sample", "target_sample":"origin_sample"}))
        df_pairwiseDifferences = df_pairwiseDifferences.append(df_pairwiseDifferences_otherOrder).reset_index(drop=True)

        # save
        save_object(df_pairwiseDifferences, df_pairwiseDifferences_file)

    df_pairwiseDifferences = load_object(df_pairwiseDifferences_file)

    return df_pairwiseDifferences



def load_gff3_intoDF(gff_path, replace=False):

    """ Takes the path to a gff and loads it into a df"""

    gff_df_file = "%s.df.tab"%gff_path

    if file_is_empty(gff_df_file) or replace is True:

        # define the number of rows to skip
        gff_lines = "%s.gff_lines.gff"%gff_path
        run_cmd("egrep -v '^#' %s > %s"%(gff_path, gff_lines))

        # define the gff fields
        gff_fields = ["chromosome", "source", "feature", "start", "end", "blank1", "strand", "blank2", "annotation"]

        # load
        print("loading gff")
        gff = pd.read_csv(gff_lines, header=None, names=gff_fields, sep="\t")

        # set all possible annotations
        all_annotations = set.union(*[set([x.split("=")[0] for x in an.split(";")]) for an in gff.annotation])

        def list_to_str(list_x):

            if list_x==[]: return ""
            else: return list_x[0]
        
        for anno in all_annotations:

            # define the field 
            anno_field = "ANNOTATION_%s"%anno

            # add the normalQ annotation field
            gff[anno_field] = gff.annotation.apply(lambda x: list_to_str([a.split(anno)[1].lstrip("=") for a in x.split(";") if anno in a]))

            # add the Dbxref sepparated
            if anno=="Dbxref": 

                # get all the dbxref fields
                all_Dbxrefs = set.union(*[set([x.split(":")[0] for x in dbxref.split(",")]) for dbxref in gff[anno_field]]).difference({""})

                # go through each dbxref field and add it to the df
                for dbxref in all_Dbxrefs: 

                    gff["%s_%s"%(anno_field, dbxref)] = gff[anno_field].apply(lambda x: list_to_str([d.split(dbxref)[1].lstrip(":") for d in x.split(",") if dbxref in d]))

        # get the ID
        gff["ID"] = gff.ANNOTATION_ID
        gff["Parent"] = gff.ANNOTATION_Parent

        # change the ID so that all of the features are unique, add numbers for non-unique IDs
        gff["duplicated_ID"] = gff.duplicated(subset="ID", keep=False) # marks all the duplicated IDs with a True
        gff["numeric_idx"] = list(range(0, len(gff)))

        def getuniqueIDs_gff(row):

            """Takes a row and changes the IDs if they are not unique"""
            if row["duplicated_ID"] is False: return row["ID"]
            else: return "%s-%i"%(row["ID"], row["numeric_idx"])

        gff["ID"] = gff.apply(getuniqueIDs_gff, axis=1)

        # check that it is correct
        if len(gff)!=len(set(gff["ID"])): raise ValueError("IDs are not unique in the gff")

        # set the id as index
        gff = gff.set_index("ID", drop=False)

        # define all the IDs
        all_unique_IDs = set(gff.index)

        # add the upmost_parent (which should be the geneID)
        print("getting upmost parent")
        def get_utmost_parent(row, gff_df):

            """Takes a row and the gff_df and returns the highest parent. The index has to be the ID of the GFF"""

            # when you have found the parent it has no parent, so the ID is the upmost_parent
            if row["Parent"]=="": return row["ID"]

            # else you go to the parent
            else: 

                # define the ID of the parent
                if row["Parent"] in all_unique_IDs: parentID = row["Parent"] # normal

                elif sum(gff_df.ANNOTATION_ID==row["Parent"])>1: # split parents, take the first one
                    parentID = gff_df[gff_df.ANNOTATION_ID==row["Parent"]].ID.iloc[0]

                else: raise ValueError("%s is not a valid Parent ID"%(r["Parent"])) # errors

                # get the upmost parent from the parent
                return get_utmost_parent(gff_df.loc[parentID], gff_df)

        gff["upmost_parent"] = gff.apply(lambda row: get_utmost_parent(row, gff), axis=1)
        gff = gff.set_index("upmost_parent", drop=False)

        # add the type of upmost_parent
        df_upmost_parents = gff[gff.ID==gff.upmost_parent]

        # check that the upmost_parents are unique
        if len(df_upmost_parents)!=len(set(df_upmost_parents.ID)): raise ValueError("upmost parents are not unique")

        # map each upmost_parent to the feature
        upmost_parent_to_feature = dict(df_upmost_parents.set_index("upmost_parent")["feature"])

        # add the upmost_parent_feature to gff df
        gff["upmost_parent_feature"] = gff.upmost_parent.apply(lambda x: upmost_parent_to_feature[x])

        # write df
        gff_df_file_tmp = "%s.tmp"%gff_df_file
        gff.to_csv(gff_df_file_tmp, sep="\t", index=False, header=True)
        os.rename(gff_df_file_tmp, gff_df_file)


    # load
    gff = pd.read_csv(gff_df_file, sep="\t")

    return gff



def density_scatter( x , y, ax, sort = True, bins = 20, **kwargs )   :
    
    """
    Scatter plot colored by 2d histogram. From https://stackoverflow.com/questions/20105364/how-can-i-make-a-scatter-plot-colored-by-density-in-matplotlib
    """

    from matplotlib.colors import Normalize 
    from scipy.interpolate import interpn


    data , x_e, y_e = np.histogram2d( x, y, bins = bins, density = True )
    z = interpn( ( 0.5*(x_e[1:] + x_e[:-1]) , 0.5*(y_e[1:]+y_e[:-1]) ) , data , np.vstack([x,y]).T , method = "splinef2d", bounds_error = False)


    #To be sure to plot all data
    z[np.where(np.isnan(z))] = 0.0

    # Sort the points by density, so that the densest points are plotted last
    if sort :
        idx = z.argsort()
        x, y, z = x[idx], y[idx], z[idx]

    # get a value to color
    all_zs = sorted(set(z))
    val_to_color = get_value_to_color(all_zs, palette="rocket_r", n=bins, type_color="hex", center=None)[0]
    all_colors = list(map(lambda zval: val_to_color[zval], z))


    # make scatter
    ax.scatter( x, y, c=all_colors, **kwargs )

    #norm = Normalize(vmin = np.min(z), vmax = np.max(z))
    #cbar = fig.colorbar(matplotlib.ScalarMappable(norm = norm), ax=ax)
    #cbar.ax.set_ylabel('Density')

    return ax



def plot_correlation_btw_genetic_and_geographic_distance(metadata_df, PlotsDir, ProcessedDataDir, species_to_ref_genome, log=True):

    """Plots the correlation between genetic and geographic distance in all species."""


    ############ CREATE DF ###############

    # create df with pairwise distances
    df_pairwise_differences_all_file = "%s/df_pairwiseDifferences_all_genetic_and_geographic"%ProcessedDataDir
    if file_is_empty(df_pairwise_differences_all_file):
        print("getting df_pairwise_differences_all_file")

        # get metadata df with lat and lon 
        metadata_df = get_metadata_df_with_lat_and_lon(metadata_df)

        # init df 
        df_pairwise_differences_all = pd.DataFrame()

        # add each species
        for taxID, species in taxID_to_sciName.items():
            print(species)

            # get the clinical isolates with location place
            metadata_df_s = metadata_df[(metadata_df.species_name==species) & ~(pd.isna(metadata_df.latitude)) & (metadata_df.type=="clinical") & ~(metadata_df.sampleID.apply(int).isin(sciName_to_badSamples[species]))]
            metadata_df_s["sampleID"] = metadata_df_s.sampleID.apply(int)
            interesting_samples = set(metadata_df_s.sampleID)

            # add the latitude and longitude in radians
            pi_val = 22/7
            metadata_df_s["longitude_radians"] = metadata_df_s.longitude / (180/pi_val)
            metadata_df_s["latitude_radians"] = metadata_df_s.latitude / (180/pi_val)

            # get the df with pairwise SNP distances (generated by a previous run of plot_pairwise_diversity_different_typesVariants)
            df_pairwise_differences = load_object("%s/pairwise_diversity_datasets/SNP_%s_df_pairWiseDifferences.py"%(ProcessedDataDir, species))
            df_pairwise_differences["origin_sample"] = df_pairwise_differences.origin_sample.apply(int)
            df_pairwise_differences["target_sample"] = df_pairwise_differences.target_sample.apply(int)
            df_pairwise_differences = df_pairwise_differences[(df_pairwise_differences.origin_sample.isin(interesting_samples)) & (df_pairwise_differences.target_sample.isin(interesting_samples)) & (df_pairwise_differences.origin_sample!=df_pairwise_differences.target_sample) & (df_pairwise_differences.diffent_positions>0)]


            # dicard redundant comparisons
            df_pairwise_differences["uniqueID_comparison"] = df_pairwise_differences[["origin_sample", "target_sample"]].apply(sorted, axis=1).apply(tuple)
            df_pairwise_differences = df_pairwise_differences.drop_duplicates(subset="uniqueID_comparison")

            # add metadata about clades
            no_clade_samples = set(metadata_df_s[pd.isna(metadata_df_s.cladeID_Tree_and_BranchLen)].sampleID)
            sampleID_to_clade = dict(metadata_df_s[~(pd.isna(metadata_df_s.cladeID_Tree_and_BranchLen))].set_index("sampleID").cladeID_Tree_and_BranchLen.apply(int).apply(str))

            def get_clade(r):
                    
                # nan clades
                if r.origin_sample in no_clade_samples or r.target_sample in no_clade_samples: return "btw clades"

                # get clades
                origin_clade = sampleID_to_clade[r.origin_sample]
                target_clade = sampleID_to_clade[r.target_sample]

                if origin_clade==target_clade: return target_clade
                else: return "btw clades"

            df_pairwise_differences["clade"] = df_pairwise_differences.apply(get_clade, axis=1)

            # add the vars per kb
            genome_len = sum(get_chr_to_len(species_to_ref_genome[species]).values())
            df_pairwise_differences["vars/kb"] = (df_pairwise_differences.diffent_positions / genome_len)*1000

            # keep some fields to calculate pairwise differences
            df_radians = metadata_df_s[["longitude_radians", "latitude_radians", "sampleID"]].set_index("sampleID")

            # add the distance in KM
            print("adding km distance")
            df_pairwise_differences["distance (km)"] = df_pairwise_differences.apply(lambda r:  get_km_distance_btw_samples(r.origin_sample, r.target_sample, df_radians), axis=1)

            # keep
            print("saving")
            df_pairwise_differences["species"] = species
            df_pairwise_differences_all = df_pairwise_differences_all.append(df_pairwise_differences)


        # save
        print("saving")
        save_object(df_pairwise_differences_all, df_pairwise_differences_all_file)

    # load
    print("loading")
    df_pairwise_differences_all = load_object(df_pairwise_differences_all_file)
       
    ######################################


    ########## PLOT #######

    # define fielfs
    if log is True:
        xfield = "log_distance_km"; xlabel = "log(km distance)"; xposs = [0, 4]
        yfield = "log_vars_kb"; ylabel = "log(SNPs/kb)"; yposs = [-1, 0]

    else:
        xfield = "distance (km)"; xlabel = "km distance"; xposs = [0, 1e4]
        yfield = "vars/kb"; ylabel = "SNPs/kb"; yposs = [1e-1, 1]

    # create log files
    df_pairwise_differences_all["log_distance_km"] = np.log10(df_pairwise_differences_all["distance (km)"] + 1)
    df_pairwise_differences_all["log_vars_kb"] = np.log10(df_pairwise_differences_all["vars/kb"])

    xlim = [min(df_pairwise_differences_all[xfield])-0.3, max(df_pairwise_differences_all[xfield])+0.3]
    ylim = [min(df_pairwise_differences_all[yfield])-0.3, max(df_pairwise_differences_all[yfield])+0.3]


    # one plot for each species
    nrows = 1
    ncols = len(sorted_species_byPhylogeny)
    fig = plt.figure(figsize=(ncols*2.5, nrows*2.5)); I=1

    # add suplots
    for Ic, species in enumerate(sorted_species_byPhylogeny):
        print(species)

        # get df
        df_pairwise_differences = df_pairwise_differences_all[df_pairwise_differences_all.species==species].sort_values(by=[xfield, yfield])
        
        # make subplot
        ax = plt.subplot(nrows, ncols, I); I+=1

        # plot point density
        from scipy.stats import gaussian_kde
        x = df_pairwise_differences[xfield].values
        y = df_pairwise_differences[yfield].values

        # calculate the correlation
        r_spearman, p_spearman = stats.spearmanr(x, y, nan_policy="raise")


        # get density scatter
        density_scatter( x , y, ax, sort=True, bins=30, s=4)

        # add correlation line
        if p_spearman<0.05:

            #import statsmodels.nonparametric.smoothers_lowess as sm
            #y_fit = sm.lowess(y, x, frac=0.3, it=1)
            #x_fit, y_fit = sm.lowess(y, x, frac=0.3, it=5).transpose()
            #plt.plot(x_fit, y_fit, color="k", linestyle="--", linewidth=1.1)


            #rolling_y = pd.Series(y).rolling(int(len(y)/50)).median().values
            #rolling_y = pd.Series(y).rolling(int(len(y)/20)).mean().values
            

            polyfit_y = np.polyfit(x, y, 2)
            y_fit = np.poly1d(polyfit_y)(x)
            plt.plot(x, y_fit, color="k", linestyle="--", linewidth=1.1)

        # add lines
        for xpos in xposs: plt.axvline(xpos, linewidth=.7, color="gray", linestyle=":")
        for ypos in yposs: plt.axhline(ypos, linewidth=.7, color="gray", linestyle=":")


        #plt.scatter(df_c.log_distance_km, df_c.log_vars_kb, c=color, marker=marker, alpha=species_to_alpha[species], s=4)

        """

        # get the graphics 
        sorted_clades = list(map(str, sorted(set(metadata_df[(metadata_df.species_name==species) & ~(pd.isna(metadata_df.cladeID_Tree_and_BranchLen))].cladeID_Tree_and_BranchLen.apply(int)))))
        if len(sorted_clades)<=10: palette="tab10"
        else: palette="tab20"
        cladeID_to_color = get_value_to_color(sorted_clades, palette=palette, n=len(sorted_clades), type_color="hex")[0]
        cladeID_to_color["btw clades"] = "gray"

        # make the figure
        for clade in sorted(set(df_pairwise_differences.clade)):
            df_c = df_pairwise_differences[df_pairwise_differences.clade==clade]

            if clade=="btw clades": 
                marker = "o"
                color = "gray"
            #else: marker = "$%s$"%clade
            else: 
                marker = "o" 
                color = "red"

            # define the alphas
            species_to_alpha = {"Candida_glabrata":.01, "Candida_albicans":.01, "Candida_auris":.01, "Candida_tropicalis":.2, "Candida_parapsilosis":.5, "Candida_orthopsilosis":.5}
            plt.scatter(df_c.log_distance_km, df_c.log_vars_kb, c=color, marker=marker, alpha=species_to_alpha[species], s=4)
        

        # add legend
        if Ic==(len(sorted_species_byPhylogeny)-1):

            def get_legend_element(color, label):  return Line2D([0], [0], marker='o', linestyle="none", color=color, label=label, markerfacecolor=color, markersize=7)
            legend_elements = [get_legend_element(color, label) for color, label in [("white", "type compared isolates"), ("gray","btw clades"), ("red","same clade")]]
            ax.legend(handles=legend_elements, loc='lower left', bbox_to_anchor=(1.01, 0))


        """


        # add tile
        ax.set_title("C. %s\nr=%.4f, p=%.4f"%(species.split("_")[1], r_spearman, p_spearman))

        # add in the title
        ax.set_xlabel(xlabel)
        ax.set_ylim(ylim)
        ax.set_xlim(xlim)

        if Ic==0:
            ax.set_ylabel(ylabel)

        else:
            ax.set_yticklabels([])
            ax.set_ylabel("")

    # make fig
    plt.subplots_adjust(wspace=0.01, hspace=0.0)
    plt.show()
    filename = "%s/correlation_SNP_distance_vs_km_distance_all_species_log%s.pdf"%(PlotsDir, log)
    fig.savefig(filename, bbox_inches='tight')

    #######################



def plot_pairwise_diversity_different_typesVariants(DataDir, ProcessedDataDir, PlotsDir, species_to_ref_genome, species_to_tree, species_to_gff, metadata_df, threads=4, color_range="default", type_comparison="number_vars", plots={"pairwise_triangles", "boxplots"}, log_scale=False, only_strains_same_clade=False, boxplot_ylines=[]):


    """This function plots a subplot of pairwise distances between different types of vars (SNPs, INDELs, SVs) in the rows. Each column would be one species"""

    # keep
    metadata_df = cp.deepcopy(metadata_df)

    # define an outdir to store processed datasets
    if type_comparison=="number_vars": outdir = "%s/pairwise_diversity_datasets"%ProcessedDataDir
    elif type_comparison=="number_genes_protAltering": outdir = "%s/pairwise_diversity_datasets_ngenesAffected_protAltering"%ProcessedDataDir
    elif type_comparison=="number_genes_all": outdir = "%s/pairwise_diversity_datasets_ngenesAffected"%ProcessedDataDir

    make_folder(outdir)

    # define  parms
    #sorted_species = sorted(species_to_ref_genome)    
    sorted_species = ["Candida_glabrata", "Candida_auris", "Candida_albicans", "Candida_tropicalis", "Candida_metapsilosis", "Candida_orthopsilosis", "Candida_parapsilosis"]
    #all_typesVars =  ["homo SNP", "hetero SNP", "homo IN/DEL", "hetero IN/DEL", "SNP", "IN/DEL", "SV", "CNV"]
    all_typesVars =  ["SNP", "IN/DEL", "SV", "CNV"]

    ######### GET DF #######

    df_pairwiseDifferences_all_file = "%s/df_pairwiseDifferences_all_strainsSameClade_%s.py"%(outdir, only_strains_same_clade)

    if file_is_empty(df_pairwiseDifferences_all_file):
        print("generating df_pairwiseDifferences_all_file")

        df_pairwiseDifferences_all = pd.DataFrame()
        for type_var in all_typesVars:        
            for species in sorted_species:
                print(type_var, species)

                #if species!="Candida_parapsilosis": continue

                # get the long df with pairwise differences
                df_pairwiseDifferences_file = "%s/%s_%s_df_pairWiseDifferences.py"%(outdir, type_var.replace("/", "-").replace(" ", "_"), species)
                taxID_dir = "%s/%s_%i"%(DataDir, species, sciName_to_taxID[species])

                if type_comparison=="number_vars":

                    # define the df with the pairwise differences
                    df_pairwiseDifferences = get_df_pairwise_differences_oneSpecies_and_typeVar(taxID_dir, df_pairwiseDifferences_file, threads, type_var, taxID_to_ploidy[sciName_to_taxID[species]], set(species_to_tree[species].get_leaf_names()))
                    
                    # add fields
                    genome_len = sum(get_chr_to_len(species_to_ref_genome[species]).values())
                    df_pairwiseDifferences["vars/kb"] = (df_pairwiseDifferences.diffent_positions / genome_len)*1000

                elif type_comparison in {"number_genes_all", "number_genes_protAltering"}:

                    # define a df with the pariwise differences
                    df_pairwiseDifferences = get_df_pairwise_differences_oneSpecies_and_typeVar_nGenes(taxID_dir, df_pairwiseDifferences_file, threads, type_var, taxID_to_ploidy[sciName_to_taxID[species]], set(species_to_tree[species].get_leaf_names()), type_comparison)

                    # define all expected genes
                    gff_df = load_gff3_intoDF(species_to_gff[species])
                    all_genes = set(gff_df[gff_df.feature.isin({"gene", "pseudogene"})].upmost_parent)
                    all_protein_coding_genes = set(gff_df[gff_df.feature.isin({"CDS", "mRNA"})].upmost_parent)
                    expected_geneIDs = {"number_genes_all":all_genes, "number_genes_protAltering":all_protein_coding_genes}[type_comparison]
                    print("There are %i expected genes"%len(expected_geneIDs))

                    # add fields
                    df_pairwiseDifferences["pct genes altered"] = (df_pairwiseDifferences.different_genes / len(expected_geneIDs))*100

                    # check that there is no genes above 100%
                    if any(df_pairwiseDifferences["pct genes altered"]>100): raise ValueError("There are some genes that are not annotated")


                # set as strings    
                for f in ["origin_sample", "target_sample"]: df_pairwiseDifferences[f] = df_pairwiseDifferences[f].apply(str)

                # filter out comparisons that are not from the same clade
                if only_strains_same_clade is True:

                    # map each sample to a clade (if there are no clades, skip)
                    df_s = metadata_df[~(pd.isna(metadata_df.cladeID_previousPaper)) & (metadata_df.species_name==species)]
                    if len(df_s)==0: continue

                    sampleID_to_cladeID = dict(df_s.set_index("sampleID")["cladeID_previousPaper"])

                    # keep only comparisons where both samples have a clade
                    samples_with_clade = set(sampleID_to_cladeID)
                    df_pairwiseDifferences = df_pairwiseDifferences[(df_pairwiseDifferences.origin_sample.isin(samples_with_clade)) & (df_pairwiseDifferences.target_sample.isin(samples_with_clade))]

                    df_pairwiseDifferences["origin_sample_clade"] = df_pairwiseDifferences.origin_sample.map(sampleID_to_cladeID)
                    df_pairwiseDifferences["target_sample_clade"] = df_pairwiseDifferences.target_sample.map(sampleID_to_cladeID)

                    df_pairwiseDifferences = df_pairwiseDifferences[(df_pairwiseDifferences.origin_sample_clade==df_pairwiseDifferences.target_sample_clade)]

                # keep
                df_pairwiseDifferences["type_var"] = type_var
                df_pairwiseDifferences["species"] = species
                df_pairwiseDifferences_all = df_pairwiseDifferences_all.append(df_pairwiseDifferences)


        # add a string with the sorted origin and target
        def get_string_united_list(list_obj): return "_".join(list_obj)
        df_pairwiseDifferences_all["sorted_origin_target_str"] = df_pairwiseDifferences_all[["origin_sample", "target_sample"]].apply(sorted, axis=1).apply(get_string_united_list)
 
        print("saving")
        save_object(df_pairwiseDifferences_all, df_pairwiseDifferences_all_file)

    if run_in_cluster is True: return

    # load
    df_pairwiseDifferences_all = load_object(df_pairwiseDifferences_all_file)

    # redefine
    df_pairwiseDifferences_all["origin_sample"] = df_pairwiseDifferences_all.origin_sample.apply(str)
    df_pairwiseDifferences_all["target_sample"] = df_pairwiseDifferences_all.target_sample.apply(str)

    ########################

    # define the yfield depending on the type of data plot
    if type_comparison=="number_vars": 
        zfield = "vars/kb"
        label_text = "# vars/kb"

    elif type_comparison=="number_genes_all":
        zfield = "pct genes altered"
        label_text = "% genes changing"

    elif type_comparison=="number_genes_protAltering":  
        zfield = "pct genes altered"
        label_text = "% proteins changing"

    # define things related to the logscale
    if log_scale is True: 

        pseudocount = np.percentile(df_pairwiseDifferences_all[df_pairwiseDifferences_all[zfield]>0][zfield], 1)
        print("pseudocounting %.4f"%pseudocount)
        df_pairwiseDifferences_all[zfield] = np.log10(df_pairwiseDifferences_all[zfield] + pseudocount)
        label_text = "log10(%s)"%label_text

    # pairwise diversity patterns
    if "pairwise_triangles" in plots:


        # init fig
        nrows = len(all_typesVars)
        ncols = len(sorted_species)
        fig = plt.figure(figsize=(ncols*3, nrows*3)); Ip=1

        print("plotting")

        for Iv, type_var in enumerate(all_typesVars):
            for Is, species in enumerate(sorted_species):
                print(type_var, species)

                #if species!="Candida_parapsilosis": continue # debug

                # get df
                df_pairwiseDifferences = df_pairwiseDifferences_all[(df_pairwiseDifferences_all.species==species) & (df_pairwiseDifferences_all.type_var==type_var)]
                if len(df_pairwiseDifferences)==0: raise ValueError("there are no vars for %s, %s"%(type_var, species))

                # define the vmax and vmin
                if color_range=="default": 
                    vmax = None
                    vmin = None
                    cbar = True

                elif color_range=="same_all_plots": 
                    
                    vmax = max(df_pairwiseDifferences_all[zfield])
                    vmin = min(df_pairwiseDifferences_all[zfield])
                    cbar = (species==sorted_species[-1])

                # get as a square df
                sorted_samples = species_to_tree[species].get_leaf_names()
                df_plot = df_pairwiseDifferences[["origin_sample", "target_sample", zfield]].pivot(index="origin_sample", columns="target_sample", values=zfield).loc[sorted_samples, sorted_samples]

                # define the colorbar label
                if species==sorted_species[-1]: cbar_label = label_text
                else: cbar_label = ""

                # plot as a heatmap
                ax = plt.subplot(nrows, ncols, Ip); Ip+=1
                mask = np.zeros_like(df_plot)
                mask[np.triu_indices_from(mask)] = True
                ax = sns.heatmap(df_plot, cbar_kws={"label":cbar_label}, mask=mask, vmax=vmax, cbar=cbar, vmin=vmin)

                ax.set_xticklabels([])
                ax.set_yticklabels([])
                ax.set_xticks([])
                ax.set_yticks([])
                ax.set_xlabel("")
                ax.set_ylabel("")

                if Iv==0: ax.set_title("C. %s"%(species.split("_")[1]))
                if Is==0: ax.set_ylabel(type_var)
                #if species==sorted_species[-1] and 

                # define the 

        # lims of the plot
        #plt.subplots_adjust(wspace=0.025, hspace=0.025)

        # save fig
        print("saving figure")
        filename = "%s/pairwise_distances_%s_colorRange-%s_log%s.png"%(PlotsDir, type_comparison, color_range, log_scale)
        fig.savefig(filename, bbox_inches='tight')
        #plt.close(fig)


    # distribution of raw vals as kdeplots
    if "distplots" in plots:

        # map each type of variant to the limit
        type_var_to_limitVarsPerKb = {"SNP":1}


        # init fig
        ncols = len(all_typesVars)
        nrows = len(sorted_species)
        fig = plt.figure(figsize=(ncols*2, nrows*2)); Ip=1

        for Is, species in enumerate(sorted_species):
            for Iv, type_var in enumerate(all_typesVars):

                # init subplot
                ax = plt.subplot(nrows, ncols, Ip); Ip+=1

                # get df
                df_pairwiseDifferences = df_pairwiseDifferences_all[(df_pairwiseDifferences_all.species==species) & (df_pairwiseDifferences_all.type_var==type_var)]

                # drop duplicated pairwise comparisons
                df_pairwiseDifferences = df_pairwiseDifferences.drop_duplicates(subset=["sorted_origin_target_str"])

                # get the distplot
                sns.distplot(df_pairwiseDifferences[zfield], color=species_to_color[species], hist=False, rug=False, kde=True, kde_kws={"lw":3})

                if type_var in type_var_to_limitVarsPerKb: plt.axvline(type_var_to_limitVarsPerKb[type_var], color="k", linewidth=.9, linestyle="--")

                # add labels
                if species==sorted_species[-1]: ax.set_xlabel(label_text)
                else: ax.set_xlabel("")

                if Is==0: ax.set_title(type_var)

                if Iv==0: ax.set_ylabel("density")

                #ax.set_ylabel("%s\nsample density"%(type_var))
                ax.set_yticklabels([])
                ax.set_yticks([])

                # add the legend
                if type_var==all_typesVars[-1] and Is==0:
                    legend_elements = [Line2D([0], [0], color=species_to_color[species], lw=4, label=species, alpha=1.0) for species in sorted_species]
                    ax.legend(handles=legend_elements, loc='right', bbox_to_anchor=(2.6, 0.3))


        plt.subplots_adjust(wspace=0.005, hspace=0.2)



    # distribution of raw vals
    if "boxplots" in plots:
        print("plotting boxplots")

        fig = plt.figure(figsize=(10, 3)) # init fig

        # keep only all_typesVars
        df_pairwiseDifferences_all = df_pairwiseDifferences_all[df_pairwiseDifferences_all.type_var.isin(set(all_typesVars))]

        # sort by species
        species_to_I = dict(zip(sorted_species, range(len(sorted_species))))
        df_pairwiseDifferences_all["species_I"]  = df_pairwiseDifferences_all.species.map(species_to_I)

        typeVar_to_I = dict(zip(all_typesVars, range(len(all_typesVars))))
        df_pairwiseDifferences_all["type_var_I"]  = df_pairwiseDifferences_all.type_var.map(typeVar_to_I)

        df_pairwiseDifferences_all = df_pairwiseDifferences_all.sort_values(by=["type_var_I", "species_I"])

        """
        meta_data = df_pairwiseDifferences_all[(df_pairwiseDifferences_all.type_var=="SNP") & (df_pairwiseDifferences_all.species=="Candida_metapsilosis")][zfield]

        sns.distplot(meta_data)

        print(meta_data)

        sys.exit(0)
        """

        # get a boxplot
        #ax = sns.boxplot(data=df_pairwiseDifferences_all, x="type_var", y=zfield, hue="species", palette=species_to_color)

        palette = {"SNP":"red", "IN/DEL":"magenta", "SV":"navy", "CNV":"cyan"}
        ax = sns.boxplot(data=df_pairwiseDifferences_all, x="species", y=zfield, hue="type_var", palette=palette)

        ax.set_xticklabels([s.split("_")[1] for s in sorted_species_byPhylogeny], rotation=45)


        # add lines
        for y in boxplot_ylines: plt.axhline(y, color="k", linestyle="--", linewidth=".9")

        # edit boxplot
        for i,artist in enumerate(ax.artists): # each artist has 6 elements. This is from https://stackoverflow.com/questions/36874697/how-to-edit-properties-of-whiskers-fliers-caps-etc-in-seaborn-boxplot

            # get color
            col = artist.get_facecolor()
            r, g, b, a = col
            artist.set_edgecolor(col)
            artist.set_facecolor((r, g, b, 0.3))

            for j in range(i*6,i*6+6):
                line = ax.lines[j]
                line.set_color(col)
                line.set_mfc(col)
                line.set_mec(col)

        ax.set_ylabel(label_text)
        #ax.set_yscale({True:"log", False:"linear"}[log_scale])
        ax.legend(bbox_to_anchor=(1.01, 1), loc=2, borderaxespad=0.)


        print("saving figure")
        filename = "%s/pairwise_distances_boxplot_%s_log%s.png"%(PlotsDir, type_comparison, log_scale)
        fig.savefig(filename, bbox_inches='tight', dpi=1200)

def get_stacked_barplot_noLegend(df, xfield, yfield, hue, sorted_xvalues, sorted_huevalues, palette):

    """It plots a stacked barplot and returns the ax with it"""

    # redefine the df
    df = cp.deepcopy(df)
    df = df[[xfield, yfield, hue]]

    # reverse
    sorted_huevalues = list(reversed(sorted_huevalues))

    # reformat df to inclue all hue vals for all x vals
    all_x_and_hue = set(df.apply(lambda r: (r[xfield], r[hue]), axis=1))

    for xval in sorted_xvalues:
        for hueval in sorted_huevalues:

            # if not existing, add a 0
            if (xval, hueval) not in all_x_and_hue:
                df_dict = {(xval, hueval): {xfield : xval, hue: hueval, yfield: 0.0}}
                df = df.append(pd.DataFrame(df_dict).transpose())

    # sort by hue and the sorted xvalues
    xval_to_sortingVal = dict(zip(sorted_xvalues, range(len(sorted_xvalues))))
    df["xfield_sortingVal"] = df[xfield].apply(lambda xval: xval_to_sortingVal[xval])
    df = df.sort_values(by=[hue, "xfield_sortingVal"])

    # go through ordered hue values
    for I, hue_val in enumerate(sorted_huevalues):
        color = palette[hue_val]

        # get the df hue
        df_hue = df[df[hue]==hue_val]

        # get the array of bottom
        if I==0: bottom = [0]*len(df_hue)
        else: bottom = np.add(bottom, previous_Ys).tolist()

        # create the vars
        ax = plt.bar(df_hue[xfield], df_hue[yfield], color=color, bottom=bottom) # edgecolor='white', width=barWidth

        # keep for the next 
        previous_Ys = list(df_hue[yfield])

    return ax

def plot_stacked_bar_subplots_eachEventOneRow(df_input, xfield="svtype", rowfield="species", hue="repeat_family", yfield="n_events", palette=None, sorted_xvalues=None, sorted_rows=None, sorted_huevalues=None, title="", filename=None, multiplier_width=0.3, multiplier_height=1.3):

    """It takes a df where each of the rows is one event, related to xfield, rows and hue. It plots one suplot with several rows (as in rowfield) and and x according to xfield. type_y can be fraction or absolute"""

    ########## GET A DF WITH THE NUMBERS ##########

    # add a field that unites everything
    df_input["x_row_hue"] = df_input[xfield].apply(str) + "|||" + df_input[rowfield].apply(str) + "|||" + df_input[hue].apply(str)

    # get a df where each row is one combination
    df_plot = pd.DataFrame({"n_events" : df_input.groupby("x_row_hue").apply(len)}).reset_index()
    df_plot[xfield] = df_plot.x_row_hue.apply(lambda x: x.split("|||")[0])
    df_plot[rowfield] = df_plot.x_row_hue.apply(lambda x: x.split("|||")[1])
    df_plot[hue] = df_plot.x_row_hue.apply(lambda x: x.split("|||")[2])

    # add the fraction
    x_and_row_to_nEvents = df_plot.groupby([xfield, rowfield]).apply(lambda df: sum(df.n_events))
    df_plot["fraction_events"] = df_plot.n_events /  df_plot.apply(lambda r: x_and_row_to_nEvents[(r[xfield], r[rowfield])], axis=1)

    # define different graphical things
    if sorted_huevalues is None: sorted_huevalues = sorted(set(df_plot[hue]))
    if palette is None: palette = get_value_to_color(sorted_huevalues, palette="tab20", type_color="hex")[0]
    if sorted_xvalues is None: sorted_xvalues = sorted(set(df_plot[xfield]))
    if sorted_rows is None: sorted_rows = sorted(set(df_plot[rowfield]))

    # check that the values are meaningful
    missing_xvalues = set(df_plot[xfield]).difference(set(sorted_xvalues))
    if len(missing_xvalues)>0: raise ValueError("There are missing elements in sorted_xvalues: %s"%missing_xvalues)

    missing_huevalues = set(df_plot[hue]).difference(set(sorted_huevalues))
    if len(missing_huevalues)>0: raise ValueError("There are missing elements in sorted_huevalues: %s"%missing_huevalues)

    ###############################################

    ######### PLOT #########

    # init fig
    nrows=len(sorted_rows)

    fig = plt.figure(figsize=(len(sorted_xvalues)*multiplier_width, nrows*multiplier_height))

    # one barplot for each row
    for Ir, row in enumerate(sorted_rows):
        print(row)

        # init subplot
        ax = plt.subplot(nrows, 1, Ir+1)

        # get the df
        df_r = df_plot[df_plot[rowfield]==row]

        # generate the stacked barplot
        get_stacked_barplot_noLegend(df_r, xfield, yfield, hue, sorted_xvalues, sorted_huevalues, palette)

        # add NaNs
        for Ix, x in enumerate(sorted_xvalues):
            if sum(df_r[xfield]==x)==0: ax.text(Ix-0.4, 0, "NA", color="k", fontsize=8)

        # change the axes
        if row!=sorted_rows[-1]: ax.set_xticklabels([])
        else: ax.set_xticklabels(sorted_xvalues, rotation=90)

        if Ir==(int(len(sorted_rows)/2)): ax.set_ylabel("%s\n%s"%(yfield, row))
        else: ax.set_ylabel(row)

        if Ir==0: ax.set_title(title)

        #ax.set_ylim([0, max(df_plot[yfield])])

        # add legend once
        if Ir==0:
            legend_items = [plt.Rectangle((0,0),1,1, fc="white", edgecolor = 'none')] + [plt.Rectangle((0,0),1,1, fc=palette[hueval], edgecolor = 'none') for hueval in sorted_huevalues]
            plt.legend(legend_items, [hue]+sorted_huevalues, loc=2, ncol=1, prop={'size':12}, bbox_to_anchor=(1.01, 1), borderaxespad=0.)

    


    plt.subplots_adjust(wspace=0.0, hspace=0.15)

    # save fig if specified
    if filename is not None: fig.savefig(filename, bbox_inches="tight")

    ########################

def get_unique_SV_CNV_df_with_overlaps_simple_repeats(unique_SV_CNV_df, min_pct_overlap_CNV, simple_repeats):

    """Adds a overlaps_simple_repeats to the unique_SV_CNV_df"""

    # add whether the cnvs are overlaped by repeats or the breakpoints are overlapped by repeats
    def get_intersection_simple_repeats(x): return x.intersection(simple_repeats)
    
    unique_SV_CNV_df["SVregion_overlaped_by_repeats"] = (unique_SV_CNV_df[["repeat#%s#pctCoverage"%r for r in simple_repeats]]>=min_pct_overlap_CNV).apply(any, axis=1)
    unique_SV_CNV_df["breakpoints_overlaped_by_repeats"] = unique_SV_CNV_df.all_overlapping_repeats_aroundBreakpoints.apply(get_intersection_simple_repeats).apply(len)>0

    unique_SV_CNV_df["overlaps_simple_repeats"] = ((unique_SV_CNV_df.type_var=="SV") & (unique_SV_CNV_df.breakpoints_overlaped_by_repeats)) |  ((unique_SV_CNV_df.type_var=="coverageCNV") & (unique_SV_CNV_df.SVregion_overlaped_by_repeats))

    return unique_SV_CNV_df

def get_unique_small_vars_df_with_overlaps_simple_repeats(unique_small_vars_df, simple_repeats):

    """Same as get_unique_SV_CNV_df_with_overlaps_simple_repeats for small vars """

    # add whether the small vars are overlapped by repeats
    def get_intersection_simple_repeats(x): return x.intersection(simple_repeats)
    unique_small_vars_df["overlaps_simple_repeats"] = unique_small_vars_df.overlapping_repeats_set.apply(get_intersection_simple_repeats).apply(len)>0

    return unique_small_vars_df


def plot_vars_overlapping_simpleRepeats(unique_SV_CNV_df, unique_small_vars_df, PlotsDir, min_pct_overlap_CNV=25):

    """This function plots whether each of the vars overlaps simple repeats or not"""

    ########## PREPARE THE DFS ##########
    print("adding the overlaps repeats")

    # debug
    if any(pd.isna(unique_SV_CNV_df[unique_SV_CNV_df.type_var=="coverageCNV"]["repeat#Low_complexity#pctCoverage"])): raise ValueError("There can't be Nans in Low_complexity pct") 
    if any(pd.isna(unique_SV_CNV_df[unique_SV_CNV_df.type_var=="coverageCNV"]["repeat#Simple_repeat#pctCoverage"])): raise ValueError("There can't be Nans in Simple_repeat pct") 

    # get whether the variants overlap simple repeats
    simple_repeats = {"Simple_repeat", "Low_complexity"}
    unique_SV_CNV_df = get_unique_SV_CNV_df_with_overlaps_simple_repeats(unique_SV_CNV_df, min_pct_overlap_CNV, simple_repeats)
    unique_small_vars_df = get_unique_small_vars_df_with_overlaps_simple_repeats(unique_small_vars_df, simple_repeats)

    # add type overlap
    bool_to_typeOverlap = {True:"overlap repeats", False:"no overlap repeats"}
    unique_SV_CNV_df["type_overlap"] = unique_SV_CNV_df.overlaps_simple_repeats.map(bool_to_typeOverlap)
    unique_small_vars_df["type_overlap"] = unique_small_vars_df.overlaps_simple_repeats.map(bool_to_typeOverlap)

    # merge the vars into one single 
    print("merging")

    unique_small_vars_df["variant_type"] = unique_small_vars_df.ISSNP.map({True:"SNP", False:"IN/DEL"})
    unique_SV_CNV_df["variant_type"] = unique_SV_CNV_df.svtype

    var_fields = ["variant_type", "species", "type_overlap"]
    df_plot = pd.concat([d[var_fields] for d in [unique_small_vars_df, unique_SV_CNV_df]])

    # debug
    if any(pd.isna(df_plot.type_overlap)): raise ValueError("there has to be some type overlap")

    ######################################

    ######## MAKE PLOTS ########

    # define things
    print("plotting")

    df_plot["species"] = df_plot.species.apply(lambda x: x.split("_")[1])
    sorted_species = [x.split("_")[1] for  x in sorted_species_byPhylogeny]

    sorted_vartypes = ["SNP", "IN/DEL", "deletion", "tandem_duplication", "inversion", "copyPaste_insertion", "cutPaste_insertion", "inverted_copyPaste_insertion", "inverted_translocation", "inverted_cutPaste_insertion", "coverage_deletion", "coverage_duplication", "unclassified_breakpoint", "bal_translocation"]

    title = "variants overlapping %s\nCNVs >=%1.f%s overlap"%(",".join(sorted(simple_repeats)), min_pct_overlap_CNV, "%")

    palette = {"overlap repeats":"red", "no overlap repeats":"c"}

    # define the plots dir
    plots_dir = "%s/variantsBreakpointsOverlapping_simpleRepeats"%PlotsDir; make_folder(plots_dir)

    for type_vars in ["all_vars", "only_SVs"]:

        # get the df
        if type_vars=="all_vars": df_p = df_plot
        elif type_vars=="only_SVs": df_p = df_plot[~(df_plot.variant_type.isin({"SNP", "IN/DEL"}))]

        # define the interesting var types
        all_vartypes = set(df_p.variant_type)
        interesting_sorted_vartypes = [v for v in sorted_vartypes if v in all_vartypes]


        for yfield in ["fraction_events", "n_events"]: # also n_events 
            plot_stacked_bar_subplots_eachEventOneRow(df_p, xfield="variant_type", rowfield="species", hue="type_overlap", yfield=yfield, sorted_rows=sorted_species, sorted_xvalues=interesting_sorted_vartypes, sorted_huevalues=["overlap repeats", "no overlap repeats"], palette=palette, title=title, filename="%s/%s_%s.pdf"%(plots_dir, type_vars, yfield))

    ############################

def get_df_bed_for_SV_CNV_r_for_SV_CNV_density(r):

    """Takes a row of the df and returns a bed-like df"""

    # if there is no SV region, get a bed for each breakend
    if pd.isna(r.SVregion_chromosome):

        # debug
        if r.svtype not in {"unclassified_breakpoint", "bal_translocation"}: raise ValueError("%s is invalid"%r)

        chroms = [r.chrA, r.chrB]
        starts = [r.startA, r.startB]
        ends = [r.endA, r.endB]

    # for variants where ther is an SV region that spans only one chunk
    elif r.lenA==1:

        chroms = [r.SVregion_chromosome]
        starts = [r.SVregion_start]
        ends = [r.SVregion_end]

    # for variants where ther is an SV region, but also an insertion site
    elif r.lenA>1:

        # checks
        if r.svtype not in {"copyPaste_insertion", "cutPaste_insertion", "inverted_copyPaste_insertion", "inverted_cutPaste_insertion"}: raise ValueError("%s is invalid"%r)

        chroms = [r.SVregion_chromosome, r.chrB]
        starts = [r.SVregion_start, r.startB]
        ends = [r.SVregion_end, r.endB]

    # make the bed
    df_bed = pd.DataFrame({"chromosome":chroms, "start":starts, "end":ends})
    df_bed["ID"] = r.variantID_across_samples
    df_bed["type_var"] = r.type_var
    df_bed["overlaps_simple_repeats"] = r.overlaps_simple_repeats

    if any(df_bed.start<0) or any(df_bed.end<0): raise ValueError("<0 coordinates in r:\n%s"%r)

    return df_bed

def get_df_variant_density_several_species(unique_SV_CNV_df, unique_small_vars_df, ProcessedDataDir, species_to_ref_genome, threads, window_size, min_pct_overlap_CNV_simpleRepeats):

    """This function generates a df, saved under ProcessedDataDir, that contains different regions of the genome and the variant densities"""

    df_variant_density_file = "%s/df_variant_density_uniqueVariants_severalSpecies_windows%ibp_min%ipctCNVoverlapRepeats.py"%(ProcessedDataDir, window_size, min_pct_overlap_CNV_simpleRepeats)

    if file_is_empty(df_variant_density_file): 
        print("getting %s"%df_variant_density_file)

        # init df
        df_variant_density = pd.DataFrame()

        # go through each species
        for taxID, species in taxID_to_sciName.items():
            print(species)

            #if species!="Candida_glabrata": continue

            ########### GET BED DFS ##############

            # get the dataframes, with the repeat info, for this species
            print("getting dfs")
            simple_repeats = {"Low_complexity", "Simple_repeat"}
            SV_CNV_df = get_unique_SV_CNV_df_with_overlaps_simple_repeats(unique_SV_CNV_df[unique_SV_CNV_df.species==species], min_pct_overlap_CNV_simpleRepeats, simple_repeats)

            small_vars_df = get_unique_small_vars_df_with_overlaps_simple_repeats(unique_small_vars_df[unique_small_vars_df.species==species], simple_repeats)

            print("getting beds")
            # add the type of variant
            small_vars_df["type_var"] = small_vars_df.ISSNP.map({True:"SNP", False:"IN/DEL"})

            # define the bed for the small vars
            small_vars_bed = small_vars_df[["#CHROM", "POS", "#Uploaded_variation", "type_var", "overlaps_simple_repeats"]].rename(columns={"#CHROM":"chromosome", "POS":"end", "#Uploaded_variation":"ID"})
            small_vars_bed["start"] = small_vars_bed.end-1

            # define a bed with the SVs and CNVs
            print("running get_df_bed_for_SV_CNV_r_for_SV_CNV_density")
            SV_CNV_bed = pd.concat(SV_CNV_df.apply(get_df_bed_for_SV_CNV_r_for_SV_CNV_density, axis=1).values).reset_index(drop=True)
            if any(SV_CNV_bed.start>=SV_CNV_bed.end): raise ValueError("start should be < end")

            if any(SV_CNV_bed.start<0): raise ValueError("There are some <0 starts")
            if any(SV_CNV_bed.end<0): raise ValueError("There are some <0 ends")

            # merge and debug
            print("merging")
            vars_bed_df_all = small_vars_bed.append(SV_CNV_bed)
            for f in vars_bed_df_all.keys():
                if any(pd.isna(vars_bed_df_all[f])): raise ValueError("there can't be nans in %s"%f)

            vars_bed_df_all["start"] = vars_bed_df_all.start.apply(int)
            vars_bed_df_all["end"] = vars_bed_df_all.end.apply(int)

            # change the variant ID by a numeric varID
            sorted_vars = sorted(set(vars_bed_df_all["ID"]))
            varID_to_numericID = dict(zip(sorted_vars, range(len(sorted_vars))))
            vars_bed_df_all["ID"] = vars_bed_df_all["ID"].map(varID_to_numericID)
            if any(pd.isna(vars_bed_df_all.ID)): raise ValueError("the vars can't be NaN")

            ######################################

            ############# CALCULATE DENSITY FOR DIFFERENT VARIANT TYPES ##############

            # define a tmpdir to save files
            tmpdir = "%s/%s_calculating_unique_var_density"%(ProcessedDataDir, species); make_folder(tmpdir)

            # define a windows df (already sorted)
            windows_bed  = get_windows_for_reference_genome(species_to_ref_genome[species], window_size)
            df_windows = pd.read_csv(windows_bed, sep="\t", names=["chromosome", "start", "end"])

            for type_vars_SimpleRepeats in ["all_vars", "only_vars_noSimpleRepeats"]:
                for type_var in ["SNP", "IN/DEL", "SV", "coverageCNV"]: # IN/DEL
                #for type_var in ["SV"]: 

                    print(type_vars_SimpleRepeats, type_var)

                    # filter vars
                    vars_bed_df = vars_bed_df_all[vars_bed_df_all.type_var==type_var]
                    if type_vars_SimpleRepeats=="only_vars_noSimpleRepeats": vars_bed_df = vars_bed_df[vars_bed_df.overlaps_simple_repeats==False]

                    # get a bed with the variants
                    fileprefix =  "%s/%s_%s"%(tmpdir, type_vars_SimpleRepeats, type_var.replace("/", "-"))
                    variants_bed = "%s_vars.bed"%(fileprefix)
                    vars_bed_df[["chromosome", "start", "end", "ID"]].sort_values(by=["chromosome", "start", "end"]).to_csv(variants_bed, sep="\t", header=False, index=False)

                    # run bedmap
                    bedmap_outfile = "%s_bedmap.out"%fileprefix
                    run_cmd("bedmap --range 0 --echo-map-id --delim '\t' %s %s > %s"%(windows_bed, variants_bed, bedmap_outfile), env="perSVade_env")

                    # add to df windows
                    df_nVars = cp.deepcopy(df_windows)

                    def get_n_vars_from_l(l): return len(set(l.strip().split(";")).difference({""}))
                    df_nVars["n_vars"] = list(map(get_n_vars_from_l, open(bedmap_outfile, "r").readlines()))


                    # add extra fields
                    df_nVars["length"] = df_nVars.end-df_nVars.start
                    df_nVars["vars_per_bp"] = df_nVars.n_vars/df_nVars.length

                    median_vars_per_bp = np.median(df_nVars[df_nVars.vars_per_bp>0].vars_per_bp)
                    df_nVars["relative_vars_per_bp"] = df_nVars.vars_per_bp / median_vars_per_bp
    
                    # add to df
                    df_nVars["species"] = species
                    df_nVars["type_vars_SimpleRepeats"] = type_vars_SimpleRepeats
                    df_nVars["type_var"] = type_var

                    df_variant_density = df_variant_density.append(df_nVars)

            # clean
            delete_folder(tmpdir)

            ##########################################################################

        # save
        save_object(df_variant_density, df_variant_density_file)

    # load
    df_variant_density = load_object(df_variant_density_file)



    return df_variant_density

def plot_variant_density_acoss_the_genome(unique_SV_CNV_df, unique_small_vars_df, ProcessedDataDir, PlotsDir, species_to_ref_genome, min_pct_overlap_CNV_simpleRepeats=25, threads=4, window_size=10000):

    """This function plots the SNP, IN/DEL, CNV and SV density across windows of the genome for each species"""

    ####### PREPARE DFS ######

    unique_SV_CNV_df = cp.deepcopy(unique_SV_CNV_df)
    unique_small_vars_df = cp.deepcopy(unique_small_vars_df)

    # get a df with the variant densities across windows of the genome
    df_variant_density = get_df_variant_density_several_species(unique_SV_CNV_df, unique_small_vars_df, ProcessedDataDir, species_to_ref_genome, threads, window_size, min_pct_overlap_CNV_simpleRepeats)

    ##########################


    ###### PLOTS ######

    # one plot for each type_vars_SimpleRepeats
    for type_vars_SimpleRepeats in ["all_vars", "only_vars_noSimpleRepeats"]:
    #for type_vars_SimpleRepeats in ["all_vars"]:

        print(type_vars_SimpleRepeats)

        #for type_var in ["SNP", "IN/DEL", "SV", "coverageCNV"]: 

        # init figure
        nrows = len(sorted_species_byPhylogeny)
        fig = plt.figure(figsize=(20, nrows*3))

        # go through each species
        for Is, species in enumerate(sorted_species_byPhylogeny):
            print(species)

            #print("chrA SVs:\n", unique_SV_CNV_df[(unique_SV_CNV_df.type_var=="SV") & (unique_SV_CNV_df.species==species)].groupby("chrA").apply(len))
            #print("chrB SVs:\n", unique_SV_CNV_df[(unique_SV_CNV_df.type_var=="SV") & (unique_SV_CNV_df.species==species)].groupby("chrB").apply(len))

            # define the df
            df_s = df_variant_density[(df_variant_density.species==species) & (df_variant_density.type_vars_SimpleRepeats==type_vars_SimpleRepeats)]

            # add fields
            df_s["position"] = (df_s.start + (df_s.length/2)).apply(int)
            df_s = get_df_vars_with_Xoffset_position(df_s, species_to_ref_genome[species], chrom_f="chromosome", pos_f="position", xoffset_f="genome_pos", offset_val=50000)

            # sort df
            df_s = df_s.sort_values(by=["type_var", "genome_pos"])

            # set to max
            df_s["relative_vars_per_bp"] = df_s.relative_vars_per_bp.apply(lambda x: min([x, 3]))

            # init the subplot
            ax = plt.subplot(nrows, 1, Is+1)

            # go through each chromosome
            sorted_chromosomes = sorted(set(df_s.chromosome))
            #sorted_vartypes = ["SNP", "IN/DEL", "SV", "coverageCNV"]
            sorted_vartypes = ["SNP", "SV", "coverageCNV"]
            #sorted_vartypes = ["SV"]

            for Ic, chrom in enumerate(sorted_chromosomes):
                for Iv, type_var in enumerate(sorted_vartypes):

                    # define the df
                    df_c = df_s[(df_s.chromosome==chrom) & (df_s.type_var==type_var)]        

                    df_c["lowess_relative_vars_per_bp"] = statsmodels.nonparametric.smoothers_lowess.lowess(df_c.relative_vars_per_bp.values, df_c.genome_pos.values, frac=0.25, it=3, delta=0.0, is_sorted=False, missing='raise', return_sorted=False) 

                    """
                    # add a rolling median
                    def get_relative_vars_per_bp_smooth_or_not(r):
                        if pd.isna(r.relative_vars_per_bp_smooth): return r.relative_vars_per_bp
                        else: return r.relative_vars_per_bp_smooth

                    df_c["relative_vars_per_bp_smooth"] = df_c.relative_vars_per_bp.rolling(10).median()
                    df_c["relative_vars_per_bp"] = df_c.apply(get_relative_vars_per_bp_smooth_or_not, axis=1)

                    """
                    

                    # make a lineplot
                    palette = {"SNP":"red", "IN/DEL":"magenta", "SV":"navy", "coverageCNV":"cyan"}
                    ax = sns.lineplot(data=df_c, x="genome_pos", y="lowess_relative_vars_per_bp", hue="type_var", palette=palette, linewidth=2.5)
                    ax = sns.scatterplot(data=df_c, x="genome_pos", y="relative_vars_per_bp", hue="type_var", palette=palette, alpha=.3, size=3)

                    # add lines
                    for x in [min(df_c.genome_pos), max(df_c.genome_pos)]: plt.axvline(x, linestyle="--", color="gray", linewidth=.9)
                    for y in [0, 1, 2]: plt.axhline(y, linestyle="--", color="gray", linewidth=.9)

                    # get the small chromosome name        
                    xtext = min(df_c.genome_pos) + (max(df_c.genome_pos)-min(df_c.genome_pos))/2
                    plt.text(xtext, 3, "chr%s"%get_shortChrom_from_chrName(chrom, species), color="k", fontsize=12, rotation=90)

                
                    #if chrom!=sorted_chromosomes[-1]: 
                    ax.get_legend().remove()
                    if species!=sorted_species_byPhylogeny[-1]: ax.set_xlabel("")

                    # add the title
                    if Is==0: ax.set_title(type_vars_SimpleRepeats)

                    # add legend once
                    if Is==0 and chrom==sorted_chromosomes[-1]:

                        legend_elements = [Line2D([0], [0], color=palette[type_var], lw=4, label=type_var, alpha=1.0) for type_var in sorted_vartypes]
                        ax.legend(handles=legend_elements, loc='right', bbox_to_anchor=(1.1, 0.3))

                     
                    ax.set_ylabel("relative vars/bp\n%s"%(species.split("_")[1]))


        # add things to fig
        plt.show()
        filename = "%s/variantDensity_across_genome_%s.pdf"%(PlotsDir, type_vars_SimpleRepeats)
        fig.savefig(filename, bbox_inches="tight")


    ###################


def get_annot_df_filt_SV_CNV_or_small_vars(DataDir, ProcessedDataDir, type_vars="SV_CNV", species_to_gff=None, replace=False):

    """This function gets an annotation df for all species, only filtered vars. The treatment depends on the type_vars. Note that this takes only the ploidy related small variants."""

    # debug
    if type_vars not in {"smallVars", "SV_CNV"}: raise ValueError("%s is not a valid type_vars"%type_vars)

    all_annot_df_filt_file = "%s/all_annot_df_filt_%s.py"%(ProcessedDataDir, type_vars)
    if file_is_empty(all_annot_df_filt_file) or replace is True:
        print("generating %s"%all_annot_df_filt_file)

        # init df
        all_annot_df_filt = pd.DataFrame()

        # go through each species 
        for taxID, species in taxID_to_sciName.items():
            print(species)

            #if species!="Candida_parapsilosis": continue

            # load the dfs
            integrated_varcallsDir = "%s/%s_%i/integrated_varcalls"%(DataDir, species, taxID)
            vars_df = load_object("%s/%s_filt.py"%(integrated_varcallsDir, type_vars))
            vars_annot_df = get_tab_as_df_or_empty_df("%s/%s_annot.tab"%(integrated_varcallsDir, type_vars))

            # for small vars, only keep the variants of the necessary ploidy
            if type_vars=="smallVars": vars_df = vars_df[vars_df.calling_ploidy==taxID_to_ploidy[taxID]]

            # remove the bad samples
            vars_df = vars_df[~(vars_df.sampleID.apply(int).isin(sciName_to_badSamples[species]))]

            # keep only annotations from the filtered vars
            typeVar_to_IDfield = {"SV_CNV":"ID", "smallVars":"#Uploaded_variation"}
            vars_annot_df = vars_annot_df[vars_annot_df["#Uploaded_variation"].isin(set(vars_df[typeVar_to_IDfield[type_vars]]))]

            # for SVs, add a couple of fields
            if type_vars=="SV_CNV":

                # go through each desired field
                for f in ["INFO_variantID", "variantID_across_samples"]:

                    # map each ID to the fvalue
                    ID_to_fvalue = dict(vars_df[["ID", f]].drop_duplicates().set_index("ID")[f])
                    vars_annot_df[f] = vars_annot_df["#Uploaded_variation"].map(ID_to_fvalue)
                    
                    # checks
                    #set(vars_annot_df[f])
                    if any(pd.isna(vars_annot_df[f])): raise ValueError("There can't be NaNs for %s"%f)

            # for small vars, just add the variantID_across_samples as the #Uploaded_variation
            elif type_vars=="smallVars": 
                vars_annot_df["variantID_across_samples"] = vars_annot_df["#Uploaded_variation"]
                vars_annot_df["INFO_variantID"] = vars_annot_df["#Uploaded_variation"]

            # add things about the gff
            if species_to_gff is not None:

                # load df
                gff_df = load_gff3_intoDF(species_to_gff[species])

                # checks
                if any(gff_df.end<gff_df.start): raise ValueError("The end has to be after start")

                # add the protein coding genes
                all_protein_coding_genes = set(gff_df[gff_df.feature.isin({"CDS", "mRNA"})].upmost_parent).intersection(set(vars_annot_df.Gene))
                vars_annot_df["is_protein_coding_gene"] = vars_annot_df.Gene.isin(all_protein_coding_genes)

                # define the length of a fraction of the df_g that correponds to one gene
                def get_length_df_g(df_g):
                    start = int(min(df_g.start))
                    end = int(max(df_g.end))
                    return (end - start)

                # add the CDS length
                gene_to_CDS_length = dict(gff_df[gff_df.feature=="CDS"].groupby("upmost_parent").apply(get_length_df_g))
                for gene in set(gff_df.upmost_parent).difference(set(gene_to_CDS_length)): gene_to_CDS_length[gene] = 0
                gene_to_CDS_length["-"] = 0
                vars_annot_df["CDS_length"] = vars_annot_df.Gene.map(gene_to_CDS_length).apply(int)

                # check that all protein coding genes have a CDS > 0
                if any(vars_annot_df[vars_annot_df.is_protein_coding_gene].CDS_length<=0): raise ValueError("The CDS has to be >0 in all prot coding genes")

                # add the gene length
                gene_to_length = dict(gff_df.groupby("upmost_parent").apply(get_length_df_g))
                gene_to_length["-"] = 0
                vars_annot_df["gene_length"] = vars_annot_df.Gene.map(gene_to_length).apply(int)

                # checks
                for f in ["CDS_length", "gene_length"]:
                    if any(pd.isna(vars_annot_df[f])): raise ValueError("There are nans in %s"%f)

            # keep
            vars_annot_df["species"] = species
            all_annot_df_filt = all_annot_df_filt.append(vars_annot_df)

        # save
        print("saving")
        save_object(all_annot_df_filt, all_annot_df_filt_file)

    # load
    all_annot_df_filt = load_object(all_annot_df_filt_file)
    return all_annot_df_filt

def get_df_variationRatePerGene(gene_features_df, DataDir, unique_SV_CNV_df, unique_small_vars_df, ProcessedDataDir, min_pct_overlap_CNV_simpleRepeats=25, threads=4, replace=False):

    """This function calculates the number of variants (SNPs, IN/DELs, CNVs, SVs) per gene when considering overlapping or non overlapping repeats"""

    # define the file
    df_variationRatePerGene_file = "%s/variation_per_gene_rate_normByLength.py"%ProcessedDataDir

    if file_is_empty(df_variationRatePerGene_file) or replace is True:

        # get the variant annotations for filtered variants 
        SV_CNV_annot_filt = get_annot_df_filt_SV_CNV_or_small_vars(DataDir, ProcessedDataDir, type_vars="SV_CNV")
        small_vars_annot_filt = get_annot_df_filt_SV_CNV_or_small_vars(DataDir, ProcessedDataDir, type_vars="smallVars")

        # add whether each of the variants overlaps repeats
        simple_repeats = {"Low_complexity", "Simple_repeat"}
        unique_SV_CNV_df = get_unique_SV_CNV_df_with_overlaps_simple_repeats(unique_SV_CNV_df, min_pct_overlap_CNV_simpleRepeats, simple_repeats)
        unique_small_vars_df = get_unique_small_vars_df_with_overlaps_simple_repeats(unique_small_vars_df, simple_repeats)

        SV_CNV_annot_filt  = SV_CNV_annot_filt.merge(unique_SV_CNV_df[["species", "variantID_across_samples", "overlaps_simple_repeats"]].drop_duplicates(), on=["species", "variantID_across_samples"], how="left", validate="many_to_one")
        if any(pd.isna(SV_CNV_annot_filt.overlaps_simple_repeats)): raise ValueError("There should not be any NaNs in simple repeats")

        small_vars_annot_filt  = small_vars_annot_filt.merge(unique_small_vars_df[["species", "#Uploaded_variation", "overlaps_simple_repeats", "ISSNP"]].drop_duplicates(), on=["species", "#Uploaded_variation"], how="left", validate="many_to_one")
        if any(pd.isna(small_vars_annot_filt.overlaps_simple_repeats)): raise ValueError("There should not be any NaNs in simple repeats")

        # add fields and generate a df with the genes
        def get_type_var_from_INFO_variantID(x):
            if x.startswith("coverage"): return "coverageCNV"
            else: return "SV"

        SV_CNV_annot_filt["type_var"] = SV_CNV_annot_filt.INFO_variantID.apply(get_type_var_from_INFO_variantID)
        small_vars_annot_filt["type_var"] = small_vars_annot_filt.ISSNP.map({True:"SNP", False:"IN/DEL"})

        fields = ["variantID_across_samples", "Gene", "species", "type_var", "overlaps_simple_repeats"]
        all_annotations_df = SV_CNV_annot_filt[fields].append(small_vars_annot_filt[fields]).drop_duplicates(subset=fields)

        # init the mutation rate df
        df_variationRatePerGene = pd.DataFrame()

        # go through each type of overlaps_simple_repeats, type_var and species
        for species in sorted_species_byPhylogeny:
            for type_vars_SimpleRepeats in ["all_vars", "only_vars_noSimpleRepeats"]:
                for type_var in ["SNP", "IN/DEL", "SV", "coverageCNV"]: 

                    print(species, type_vars_SimpleRepeats, type_var)

                    # filter vars
                    annotations_df = all_annotations_df[(all_annotations_df.type_var==type_var) & (all_annotations_df.species==species) & (all_annotations_df.Gene!="-")]
                    if type_vars_SimpleRepeats=="only_vars_noSimpleRepeats": annotations_df = annotations_df[annotations_df.overlaps_simple_repeats==False]

                    # get the genes for this species
                    gene_features_df_s = gene_features_df[gene_features_df.species==species]

                    # map each gene to the number of variants (also add the missing)
                    gene_to_nvars = dict(annotations_df[["Gene", "variantID_across_samples"]].drop_duplicates().groupby("Gene").apply(len))
                    all_genes = set(gene_features_df_s.gff_upmost_parent)
                    missing_genes = all_genes.difference(set(gene_to_nvars))

                    if len(missing_genes)>0:
                        print("there are %i/%i genes with no variants"%(len(missing_genes), len(all_genes)))
                        for g in missing_genes: gene_to_nvars[g] = 0

                    # debug
                    genes_with_vars_not_in_gff = set(gene_to_nvars).difference(all_genes)
                    if len(genes_with_vars_not_in_gff)>0: raise ValueError("there are some genes not in the gff:\n %s"%genes_with_vars_not_in_gff)

                    # create a df with the number of mutations per gene
                    df_variationRatePerGene_s = gene_features_df_s[["gff_upmost_parent", "gene_length"]]
                    df_variationRatePerGene_s["n_mutations"] = df_variationRatePerGene_s.gff_upmost_parent.map(gene_to_nvars)
                    if any(pd.isna(df_variationRatePerGene_s["n_mutations"])): raise ValueError("there are weird genes with nans")

                    df_variationRatePerGene_s["mutations_per_bp"] = df_variationRatePerGene_s.n_mutations/df_variationRatePerGene_s.gene_length

                    # keep
                    df_variationRatePerGene_s["species"] = species
                    df_variationRatePerGene_s["type_vars_SimpleRepeats"] = type_vars_SimpleRepeats
                    df_variationRatePerGene_s["type_var"] = type_var

                    df_variationRatePerGene = df_variationRatePerGene.append(df_variationRatePerGene_s).reset_index(drop=True)

        # save
        print("saving")
        save_object(df_variationRatePerGene, df_variationRatePerGene_file)

    # load
    df_variationRatePerGene = load_object(df_variationRatePerGene_file)
    return df_variationRatePerGene


def get_namespace_to_gene_to_GOterms(gene_to_GOterms, obodag):

    """converts a gene_to_GOterms into a get_namespace_to_gene_to_GOterms"""

    # define the obsolete terms
    all_terms = set.union(*gene_to_GOterms.values())
    goterms_obsolete = {goID for goID, goInfo in obodag.items() if goInfo.is_obsolete}.intersection(all_terms)    

    # map each GO term to the new GO term, removing obsoletes
    oldID_to_newID = {}
    for oldID in all_terms:

        # for obsolete IDs, try to replace them
        if oldID in goterms_obsolete: 

            # define the alternative IDs
            non_obsolete_altIDs = {altID for altID in obodag[oldID].alt_ids if not obodag[altID].is_obsolete}
            replaced_by = obodag[oldID].replaced_by

            # if there is a replacement
            if len(replaced_by)>0: oldID_to_newID[oldID] = replaced_by

            # if there are some altIDs
            elif len(non_obsolete_altIDs)==1: oldID_to_newID[oldID] = next(iter(non_obsolete_altIDs))

            # debug
            elif len(non_obsolete_altIDs)>1: raise ValueError("There are more than 1 alt IDs: %s"%non_obsolete_altIDs)

        # if they are not obsolete, keep them the same
        else: oldID_to_newID[oldID] = oldID

    # print
    missing_obsolete_terms = all_terms.difference(set(oldID_to_newID))
    print("there are %i/%i obsolete terms. There are %i of these that can't be replaced and are eliminated"%(len(goterms_obsolete), len(all_terms), len(missing_obsolete_terms)))

    # get the dict
    long_to_short_ns = {"biological_process":"BP", "cellular_component":"CC", "molecular_function":"MF"}
    ns_to_gene_to_GOids = {ns : {gene : {oldID_to_newID[GOterm] for GOterm in GOterms if GOterm in oldID_to_newID and long_to_short_ns[obodag[GOterm].namespace]==ns} for gene, GOterms in gene_to_GOterms.items()} for ns in ["CC", "MF", "BP"]}

    return ns_to_gene_to_GOids

def calculate_LOR_GOEA(goea_results):
    
    """ This function takes a list of GOEnrichmentRecord objects and returns it with LOG-ODDS ratio calculated. 
    If something is +-INF it sets it to +-1000, which makes the further usage easier """
    
    np.seterr(divide='ignore') # avoid Numpy division by 0 error
    
    print("calculating LORs")
    
    # now add to the object
    for r in goea_results:
        
        LOR = np.log2((r.ratio_in_study[0]/r.ratio_in_study[1]) / (r.ratio_in_pop[0]/r.ratio_in_pop[1]))
        
        if LOR==np.inf:
            r.LOR = 1000 #max_LOR
            
        elif LOR==-np.inf:
            r.LOR = -1000 #min_LOR
        else:
            r.LOR = LOR 
            
    return goea_results

def get_df_GOEA_target_on_all_genes(target_genes, all_genes, gene_to_GOterms, obo_file, plot_fileprefix, propagate_counts=True, alpha=0.05, min_LOR=1):

    """This function returns, on different namespaces, the GOEA and returns a df with the GO terms that have significant raw pvalues. It will run a fisher exact test. propagate_counts propagates GO terms all up in the graph, yielding more broad terms. It can be compensated by removing very general terms (i.e.: those that have a lot of children)"""

    # define the final file in tsv
    goea_results_tab_file = "%s.sig_results.tsv"%plot_fileprefix
    if file_is_empty(goea_results_tab_file):
        print("generating %s"%goea_results_tab_file)

        # load the obo file
        obodag = GODag(obo_file,  optional_attrs={'consider', 'replaced_by'}, load_obsolete=True, prt=None)

        # define a ns_to_gene_to_GOids
        ns_to_gene_to_GOids = get_namespace_to_gene_to_GOterms(gene_to_GOterms, obodag)

        # define the goea object
        goeaobj = GOEnrichmentStudyNS(all_genes, ns_to_gene_to_GOids, obodag, propagate_counts=propagate_counts, alpha=alpha, methods=["fdr_bh"], prt=None)

        # run the GOEA analysis and keep the significant terms
        goea_results = goeaobj.run_study(target_genes, prt=None)
        goea_results = calculate_LOR_GOEA(goea_results) # add the log-odds ratio

        # filter to keep significant terms with a positive LOR (log2)
        goea_results_sig = [r for r in goea_results if r.p_fdr_bh<alpha and r.LOR>=min_LOR]

        # plot the results
        print("plotting results...")
        plot_results("%s_{NS}.png"%plot_fileprefix, goea_results_sig, prt=None)

        # save as a tab file
        print("saving as tsv")
        goeaobj.wr_tsv(goea_results_tab_file, goea_results_sig)

        # if it is empty, write an empty file
        if file_is_empty(goea_results_tab_file): save_df_as_tab(pd.DataFrame(columns=["# GO", "NS", "enrichment", "name", "ratio_in_study",  "ratio_in_pop", "p_uncorrected",   "depth",  "study_count", "p_fdr_bh", "study_items"]), goea_results_tab_file)

    # load
    goea_results_df = get_tab_as_df_or_empty_df(goea_results_tab_file).rename(columns={"# GO":"GO"})

    # add fields
    if len(goea_results_df)>0:

        # add the numbers of genes
        goea_results_df["ngenes_study_and_GO"] = goea_results_df.ratio_in_study.apply(lambda x: int(x.split("/")[0]))
        goea_results_df["ngenes_study"] = goea_results_df.ratio_in_study.apply(lambda x: int(x.split("/")[1]))
        goea_results_df["ngenes_pop_and_GO"] = goea_results_df.ratio_in_pop.apply(lambda x: int(x.split("/")[0]))
        goea_results_df["ngenes_pop"] = goea_results_df.ratio_in_pop.apply(lambda x: int(x.split("/")[1]))

        # add the LOR
        def get_LOR_goea_r(r):

            # calculate the LOR
            LOR = np.log2((r.ngenes_study_and_GO/r.ngenes_study) / (r.ngenes_pop_and_GO/r.ngenes_pop))

            # adjust
            if LOR==np.inf: return 1000
            elif LOR==-np.inf: return -1000
            else: return LOR

        np.seterr(divide='ignore') # avoid Numpy division by 0 error
        goea_results_df["LOR"] = goea_results_df.apply(get_LOR_goea_r, axis=1)

        # chekcs
        if any(goea_results_df.LOR<min_LOR): raise ValueError("There are some LORs below min_LOR")

    return goea_results_df

def get_gene_to_GOterms_with_propagated_terms(gene_to_GOterms_raw, obodag, expected_GOterms):

    """Calculates a mapping of gene to go terms only for genes that have terms in expected_GOterms"""

    # define the gene to GO terms in a way that already takes into account missing terms
    ns_to_gene_to_GOids = get_namespace_to_gene_to_GOterms(gene_to_GOterms_raw, obodag)

    gene_to_GOterms = {}
    for gene in gene_to_GOterms_raw.keys():

        # define raw GO terms
        GOterms = set.union(*[gene_to_GOids[gene] for gene_to_GOids in ns_to_gene_to_GOids.values()])

        # get the parents of these GO terms
        parent_GOterms = set.union(*[obodag[go].get_all_parents() for go in GOterms] + [set()])

        # define all go terms, and keep only the GO terms that are related to expected_GOterms
        all_GOterms = GOterms.union(parent_GOterms)
        gene_to_GOterms[gene] = expected_GOterms.intersection(all_GOterms)

    return gene_to_GOterms

def get_df_pairwise_Lin_semantic_similarity(df_GOEA, obo_file, species_to_gene_to_GOterms):

    """Calculates the pairwise semantic similarity matrix for all GO terms, returning a df where the rows and cols are the GO, and the value is the semantic similarity. It will be the median semantic similarity across different species"""

    print("running get_df_pairwise_Lin_semantic_similarity")

    # load the obodag
    obodag = GODag(obo_file,  optional_attrs={'consider', 'replaced_by'}, load_obsolete=True, prt=None)

    # define all the GO terms
    all_GO_terms = set(df_GOEA.GO)

    # define all pairwise comparisons
    sorted_tuple_pairwise_comparisons = sorted({(termA, termB) for termA in all_GO_terms for termB in all_GO_terms if termA!=termB})

    # init the df with the pairwise similarities
    df_pairwise_similarities = pd.DataFrame(index=sorted_tuple_pairwise_comparisons)

    # go through all the implicated species
    for species in sorted(set(df_GOEA.species)):

        # get the curated gene_to_GOterms
        gene_to_GOterms =  get_gene_to_GOterms_with_propagated_terms(species_to_gene_to_GOterms[species], obodag, all_GO_terms)

        # define the termcounts
        print("get the termcounts")
        termcounts = goatools_semantic.TermCounts(obodag, gene_to_GOterms)

        # add to the df
        df_pairwise_similarities[species] = pd.Series(dict(zip(sorted_tuple_pairwise_comparisons , map(lambda x: goatools_semantic.lin_sim(x[0], x[1], obodag, termcounts), sorted_tuple_pairwise_comparisons))))

    # get the median similarities
    def get_median_semantic_similarities(r):
        if any(r.values<0): raise ValueError("there should be no negatives")

        non0_similarities = r[r>0].values
        if len(non0_similarities)==0: return 0.0
        else: return np.median(non0_similarities)

    df_pairwise_similarities["semantic_similarity"] = df_pairwise_similarities.apply(get_median_semantic_similarities, axis=1)
    if any(pd.isna(df_pairwise_similarities.semantic_similarity)): raise ValueError("There should be no nans")

    # add the terms
    df_pairwise_similarities["termA"] = [x[0] for x in df_pairwise_similarities.index]
    df_pairwise_similarities["termB"] = [x[1] for x in df_pairwise_similarities.index]

    # get important fields
    df_pairwise_similarities = df_pairwise_similarities[["termA", "termB", "semantic_similarity"]].reset_index(drop=True)
    df_pairwise_similarities["tuple_terms"] = df_pairwise_similarities[["termA", "termB"]].apply(sorted, axis=1).apply(tuple)

    return df_pairwise_similarities


def get_list_clusters_from_dict(key_to_setVals, integrate_keyAndValues=True):

    """Takes a dictionary mapping strings to strings and returns a list of sets, each of them having uniquely clusters of connected data"""

    # initialize a list_clusters
    list_clusters = []

    # initialize a list of mapped regions
    if integrate_keyAndValues is True: list_partial_clusters = [cp.deepcopy({key}.union(setVals)) for key, setVals in key_to_setVals.items()]
    else: list_partial_clusters = list(map(cp.deepcopy, key_to_setVals.values()))

    # go through each element in dict
    for all_items in list_partial_clusters:

        # if there is any overlap with any of the clusters, add it there
        cluster_found = False
        for cluster in list_clusters:
            if len(cluster.intersection(all_items))>0: 
                cluster.update(all_items)
                cluster_found = True
                break

        # else initialize with all elements of this cluster
        if cluster_found is False: list_clusters.append(all_items)

    return list_clusters

def get_representative_GO_for_clusterGOs(cluster_GOs, df_sortedGOs, obodag):

    """Gets a set with the cluster of GOs and returns the representative GO terms, according to a logic that prioritizes specific terms and lower pvalues"""
    
    # get ta df with these GOs
    df_cluster = df_sortedGOs[df_sortedGOs.GO.isin(cluster_GOs)]

    # if there is only one term, define it as representative
    if len(df_cluster)==1: return df_cluster.iloc[0].GO

    # keep the GO terms that are specific. If there is only one term which is specific, keep it. If there are no specific terms, consider all of them for downstream filtering
    if any(df_cluster.fraction_genes_withGO<0.05): df_cluster = df_cluster[df_cluster.fraction_genes_withGO<0.05]
    if len(df_cluster)==1:  return df_cluster.iloc[0].GO
    if len(df_cluster)==0: raise ValueError("There can't be 0 items after removing fraction_genes_withGO")

    # keep terms that have a particularly low pvalue (<2x minimum pval). If there is only one such GO term, keep it as representative
    min_pval = min(df_cluster.pval)*2
    df_cluster = df_cluster[df_cluster.pval<=min_pval]
    if len(df_cluster)==1: return df_cluster.iloc[0].GO
    if len(df_cluster)==0: raise ValueError("There can't be 0 items after considering pvals")

    # keep GO that has children. If there are more than one GOs with children, take the ones that have more children
    remaining_GOs = set(df_cluster.GO)
    df_cluster["children_GOs"] = pd.Series({GO : (obodag[GO].get_all_children()).intersection(remaining_GOs) for GO in remaining_GOs})
    df_cluster["n_children"] = df_cluster.children_GOs.apply(len)

    df_cluster = df_cluster.sort_values(by=["n_children", "pval", "GO"], ascending=[False, True, True])

    # return
    return df_cluster.iloc[0].GO

def get_GO_to_keep_comparing_2terms(termA, termB, GO_to_median_FractionGenesGO, GO_to_median_pval, obodag):

    """This function takes two terms and returns the best term, according to the REVIGO pipeline."""

    if termA==termB: raise ValueError("terms should not be the same")

    # get whether one of the terms has a very general interpretation
    if GO_to_median_FractionGenesGO[termA]>=0.05 and GO_to_median_FractionGenesGO[termB]<0.05: return termB
    if GO_to_median_FractionGenesGO[termB]>=0.05 and GO_to_median_FractionGenesGO[termA]<0.05: return termA

    # if one term has a much lower pval than the other (pA<(pB*0.5)), discard the other
    if GO_to_median_pval[termB]<(GO_to_median_pval[termA]/2): return termB
    if GO_to_median_pval[termA]<(GO_to_median_pval[termB]/2): return termA

    # if one term is the parent of the other, keep the parent. If the parent is composed mostly by the child (75%) keep the child
    term_to_children = {t : obodag[t].get_all_children() for t in [termA, termB]}
    all_children = set.union(*term_to_children.values())
    if len(all_children)>0:

        fraction_shared_children = len(set.intersection(*term_to_children.values())) / len(all_children)
        child_parent_similar = fraction_shared_children>=0.75

    else: child_parent_similar = False

    # termB is parent and the terms are similar. Keep child
    if termA in term_to_children[termB] and child_parent_similar: return termA

    # termB is parent and the terms are not similar. Keep parent
    if termA in term_to_children[termB] and not child_parent_similar: return termB

    # termA is parent and the terms are similar. Keep child
    if termB in term_to_children[termA] and child_parent_similar: return termB

    # termA is parent and the terms are not similar. Keep parent
    if termB in term_to_children[termA] and not child_parent_similar: return termA

    # if none of the above is true, keep the first, according to sorting
    return sorted([termA, termB])[0]

def get_df_GOEA_with_clusterInfo_bySemantic_similarity(df_GOEA, obo_file, df_pairwise_semantic_similarity, semantic_similarity_tsh=0.5):

    """This function takes a df of GO terms and makes clusters by semantic similarity. We calculate Lin's semantic similarity and use the method by REVIGO, as explained in their paper"""

    print("running get_df_GOEA_with_clusterInfo_bySemantic_similarity")
    
    # keep
    df_pairwise_semantic_similarity = cp.deepcopy(df_pairwise_semantic_similarity)
    # get the obo file
    obodag = GODag(obo_file,  optional_attrs={'consider', 'replaced_by'}, load_obsolete=True, prt=None)

    # map each GO term to the median fractionGenesWith term across species
    df_GOEA["fraction_genes_withGO"] = df_GOEA.ngenes_pop_and_GO/df_GOEA.ngenes_pop
    if any(pd.isna(df_GOEA.fraction_genes_withGO)) or not all(df_GOEA.fraction_genes_withGO>0): raise ValueError("some terms are incorrect")

    GO_to_median_FractionGenesGO = dict(df_GOEA[["species", "GO", "fraction_genes_withGO"]].drop_duplicates().groupby("GO").apply(lambda df_go: np.median(df_go.fraction_genes_withGO)))

    # map each GO to the corrected pvalue
    GO_to_median_pval = dict(df_GOEA[["species", "GO", "p_fdr_bh"]].groupby("GO").apply(lambda df_go: np.median(df_go.p_fdr_bh)))

    # sort by semantic similarity, keeping only unique comparisons. sort so that most similar comparisons come first
    df_sim = df_pairwise_semantic_similarity[df_pairwise_semantic_similarity.semantic_similarity>=semantic_similarity_tsh].drop_duplicates(subset="tuple_terms").sort_values(by=["semantic_similarity", "termA", "termB"], ascending=[False, True, True])

    # map each GO term to the most similar GO (adding singletons)
    rejected_GOs = set()

    # iterate through all comparisons, so that you take the most relevant comparisons first
    for termA, termB in df_sim[["termA", "termB"]].values:

        # define the term to reject and the one to accept
        accepted_term = get_GO_to_keep_comparing_2terms(termA, termB, GO_to_median_FractionGenesGO, GO_to_median_pval, obodag)
        rejected_term = [t for t in [termA, termB] if t!=accepted_term][0]

        rejected_GOs.add(rejected_term)

    # define the representative terms
    all_GO_terms = set(df_GOEA.GO)
    representative_GOs = all_GO_terms.difference(rejected_GOs)

    # map each rejected GO to the closest representative GO
    df_sim_mapping = df_pairwise_semantic_similarity[(df_pairwise_semantic_similarity.termA.isin(rejected_GOs)) & (df_pairwise_semantic_similarity.termB.isin(representative_GOs))].sort_values(by=["termA", "semantic_similarity", "termB"], ascending=[True, False, True]).drop_duplicates(keep="first")

    rejectedGO_to_closestRepresentativeGO = dict(df_sim_mapping.groupby("termA").apply(lambda df_A: df_A.termB.iloc[0]))

    # define a GOterm_to_clusterGOterm
    GOterm_to_clusterGOterm = {}
    for GO in all_GO_terms:

        # define the cluster GO
        if GO in representative_GOs: GOterm_to_clusterGOterm[GO] = GO
        else: GOterm_to_clusterGOterm[GO] = rejectedGO_to_closestRepresentativeGO[GO]

    # get a list of clusters of similar GO terms
    print("there are %i clusters of %i GO terms"%(len(representative_GOs), len(all_GO_terms)))
    df_GOEA["cluster_representative_GO"] = df_GOEA.GO.apply(lambda x: GOterm_to_clusterGOterm[x])

    # checks
    if set(df_GOEA.cluster_representative_GO)!=representative_GOs: raise ValueError("there was an error with the repGOs")

    # get the name of the cluster
    GO_to_name = dict(df_GOEA.groupby("GO").apply(lambda df_go: df_go.iloc[0]["name"]))
    df_GOEA["cluster_representative_name"] = df_GOEA.cluster_representative_GO.apply(lambda x: GO_to_name[x])
  
    return df_GOEA

def get_df_GOEA_with_MDScoordinates_bySematic_similarity(df_GOEA, obo_file, df_pairwise_semantic_similarity):
    
    """This function runs MDS scaling based on semantic similarities for GO terms. It adds the axis1, axis2 to the df_GOEA"""

    # change the index
    df_pairwise_semantic_similarity = cp.deepcopy(df_pairwise_semantic_similarity).set_index(["termA", "termB"], drop=False)
    print("running get_df_GOEA_with_MDScoordinates_bySematic_similarity")

    # define the sorted GO terms
    sorted_GO_terms = sorted(set(df_GOEA.GO))

    # Create a sem_array containing semantic similarities between each GO term
    nterms = len(sorted_GO_terms)
    sem_array = np.zeros((nterms, nterms))

    for I1, go1 in enumerate(sorted_GO_terms):
        for I2, go2 in enumerate(sorted_GO_terms):
            if go1==go2: sem_array[I1, I2] = 1.0
            else: sem_array[I1, I2] = df_pairwise_semantic_similarity.loc[(go1, go2)].semantic_similarity

    from sklearn.manifold import MDS

    # run MDS and create a df
    embedding = MDS(n_components=2, dissimilarity='euclidean', random_state=1) 
    sem_array_transformed = embedding.fit_transform(sem_array)
    df_MDS = pd.DataFrame(data=sem_array_transformed, index=sorted_GO_terms, columns=['GO_MDS_axis1', 'GO_MDS_axis2'])
    df_MDS["GO"] = df_MDS.index
    
    # add to the df
    df_GOEA = df_GOEA.merge(df_MDS.reset_index(drop=True), on="GO", how="left", validate="many_to_one")
    for f in ["GO_MDS_axis1", "GO_MDS_axis2"]: 
        if any(pd.isna(df_GOEA[f])): raise ValueError("nans in %s"%f)

    return df_GOEA

def generate_REVIGOlike_plot_subplots(df_GOEA, filename_plot, filename_data, obo_file, species_to_gene_to_GOterms, rowfield="species", colfield="type_var", sorted_rows=None, sorted_cols=None, semantic_similarity_tsh=0.5, replace=False, title="", factor_size_minus_log10pval = 5, marker_alpha=.5, bbox_to_anchor=(5.5, 1), max_minus_log10pval=5, keep_oneTerm_per_group=False, figsize_multiplier=2, remove_ticks=False):

    """Generates a plot like revigo with subplots. The axis1 and axis2 are the same for all the clusters. The color and shape is related to the cluster of similar terms. The size will be proportional to the -log10(pvalue). The df_GOEA should be filtered."""


    ######### GET THE DF WITH INFO FOR THE PLOT ############

    if file_is_empty(filename_data) or replace is True:
        print("getting %s"%filename_data)
        df_GOEA = cp.deepcopy(df_GOEA)

        # get the pairwise semantic similarity df
        df_pairwise_semantic_similarity = get_df_pairwise_Lin_semantic_similarity(df_GOEA, obo_file, species_to_gene_to_GOterms) # it is bidirectional

        # get the df_GOEA with a cluster name which is related to the most representative term of the cluster, by semantic similarity
        df_GOEA = get_df_GOEA_with_clusterInfo_bySemantic_similarity(df_GOEA, obo_file, df_pairwise_semantic_similarity, semantic_similarity_tsh=semantic_similarity_tsh)

        # keep only one representative GO term for each group, the one that has the lowest pvalue for each sampleID
        if keep_oneTerm_per_group is True:

            # define the sample ID
            df_GOEA["unique_sampleID"] = df_GOEA[rowfield] + "_" + df_GOEA[colfield]

            # keep one  GO term of each cluster_representative_GO for each unique_sampleID
            df_GOEA = df_GOEA.sort_values(by=["unique_sampleID", "cluster_representative_GO", "p_fdr_bh", "GO"]).drop_duplicates(subset=["unique_sampleID", "cluster_representative_GO"], keep="first")

        # get the df_GOEA with info about the MDS coordinates, which should be the same for all species
        df_GOEA = get_df_GOEA_with_MDScoordinates_bySematic_similarity(df_GOEA, obo_file, df_pairwise_semantic_similarity)

        # save
        save_object(df_GOEA, filename_data)

    df_GOEA = load_object(filename_data)

    ########################################################

    ########### PLOT ############

    # add the graphics (based on the representative GO terms of each cluster)
    sorted_GO_terms = list(df_GOEA[["cluster_representative_GO", "GO_MDS_axis1"]].drop_duplicates(subset=["cluster_representative_GO"], keep="first").sort_values(by="GO_MDS_axis1").cluster_representative_GO)
    GO_to_name = dict(df_GOEA[["cluster_representative_GO", "cluster_representative_name"]].drop_duplicates().set_index("cluster_representative_GO").cluster_representative_name)

    GO_to_color  = get_value_to_color(sorted_GO_terms, palette="cubehelix", n=len(sorted_GO_terms), type_color="hex", center=None)[0] # cubehelix, icefire, gist_ncarSpectral, gist_ncar
    df_GOEA["color"] = df_GOEA.cluster_representative_GO.map(GO_to_color)

    GO_to_marker = dict(zip(sorted_GO_terms, (">ovHs^h<DP"*len(sorted_GO_terms))[0:len(sorted_GO_terms)] ))
    #GO_to_marker = {GO:"o" for GO in sorted_GO_terms}
    df_GOEA["marker"] = df_GOEA.cluster_representative_GO.map(GO_to_marker)

    df_GOEA["minus_log10pval"] = -np.log10(df_GOEA.p_fdr_bh)
    sorted_minus_log10pval = sorted(set(df_GOEA.minus_log10pval))
    df_GOEA["size"] = df_GOEA.minus_log10pval*factor_size_minus_log10pval
    
    max_size = max_minus_log10pval*factor_size_minus_log10pval
    df_GOEA["size"] = df_GOEA["size"].apply(lambda x: min([x, max_size]))


    add_to_xlim = (max(df_GOEA.GO_MDS_axis1)-min(df_GOEA.GO_MDS_axis1))*0.2
    xlim = [min(df_GOEA.GO_MDS_axis1)-add_to_xlim, max(df_GOEA.GO_MDS_axis1)+add_to_xlim]

    add_to_ylim = (max(df_GOEA.GO_MDS_axis2)-min(df_GOEA.GO_MDS_axis2))*0.2
    ylim = [min(df_GOEA.GO_MDS_axis2)-add_to_ylim, max(df_GOEA.GO_MDS_axis2)+add_to_ylim]

    # define the cols and rows
    if sorted_rows is None: sorted_rows = sorted(set(df_GOEA[rowfield]))
    if sorted_cols is None: sorted_cols = sorted(set(df_GOEA[colfield]))

    # init the fig
    nrows = len(sorted_rows)
    ncols = len(sorted_cols)
    fig = plt.figure(figsize=(ncols*figsize_multiplier, nrows*figsize_multiplier)); Ip=1

    # define general gr
    for Ir, row in enumerate(sorted_rows):
        for Ic, col in enumerate(sorted_cols):

            # init subplot
            ax = plt.subplot(nrows, ncols, Ip); Ip+=1

            # get the df and check that the GO terms are unique
            df_plot = df_GOEA[(df_GOEA[rowfield]==row) & (df_GOEA[colfield]==col)]
            if len(set(df_plot.GO))!=len(df_plot): raise ValueError("there are non unique GOs")

            # plot NA for empty terms
            if len(df_plot)==0: 
                center_x = xlim[0] + (xlim[1]-xlim[0])/2
                center_y = ylim[0] + (ylim[1]-ylim[0])/2
                plt.text(center_x, center_y, "NA", color="k", fontsize=12)

            # add one face for each term
            else:
                for I,r in df_plot.iterrows(): plt.plot([r.GO_MDS_axis1], [r.GO_MDS_axis2], markersize=r["size"], marker=r.marker, markeredgecolor='k', markerfacecolor=r.color, alpha=marker_alpha, markeredgewidth=1) # alpha=marker_alpha

            # set same axis
            ax.set_ylim(ylim)
            ax.set_xlim(xlim)

            # define axis
            if Ic!=0:
                ax.set_yticklabels([])
                ax.set_ylabel("")

            elif Ir!=int(len(sorted_rows)/2): ax.set_ylabel(row)
            else:  ax.set_ylabel("GO space MDS axis 2\n%s"%row)

            if row!=sorted_rows[-1]:
                ax.set_xticklabels([])
                ax.set_xlabel("")

            elif Ic==(int(len(sorted_cols)/2)-1): ax.set_xlabel("GO space MDS axis 1")


            # remove all the tivks
            if remove_ticks is True:
                ax.set_xticklabels([])
                ax.set_yticks([])
                ax.set_yticklabels([])
                ax.set_xticks([])


            # define titles
            if Ir==0 and Ic!=0: ax.set_title(col)
            elif Ir==0: ax.set_title("%s\n%s"%(title, col))

            # Add legend
            if col==(sorted_cols[-1]) and Ir==0:

                def get_empty_legend(label): return [Line2D([0], [0], marker="o", label=label, markersize=0, lw=0)]


                """
                def get_pval_str_from_lp(x):
                    pval = 10**(-x)
                    if pval>=0.01: return "%.2f"%pval
                    else: return "10e-%i"%x

                legend_elements = get_empty_legend("representative term") + [Line2D([0], [0], marker=GO_to_marker[GO], label=GO_to_name[GO], alpha=marker_alpha, markerfacecolor=GO_to_color[GO], markeredgecolor="k", markersize=15, lw=0) for GO in sorted_GO_terms] + get_empty_legend("") + get_empty_legend("p value") + [Line2D([0], [0], marker='o', label=get_pval_str_from_lp(lp), alpha=.7, markerfacecolor="white", markeredgecolor="k", markersize=(lp*factor_size_minus_log10pval), lw=0) for lp in [1, 1.35, 2, 3, 4, 5, 6, 7, 8] if lp>=min(df_GOEA["minus_log10pval"]) and lp<=max(df_GOEA["minus_log10pval"]) and lp<=max_minus_log10pval]
                """



                legend_elements = get_empty_legend("representative term") + [Line2D([0], [0], marker=GO_to_marker[GO], label=GO_to_name[GO], alpha=marker_alpha, markerfacecolor=GO_to_color[GO], markeredgecolor="k", markersize=15, lw=0) for GO in sorted_GO_terms] + get_empty_legend("") + get_empty_legend("FDR p value") + [Line2D([0], [0], marker='o', label=str(pval), alpha=.7, markerfacecolor="white", markeredgecolor="k", markersize=(-np.log10(pval)*factor_size_minus_log10pval), lw=0) for pval in [0.05, 0.01, 0.005, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7] if pval>=min(df_GOEA["p_fdr_bh"]) and -np.log10(pval)<=max_minus_log10pval]


                ax.legend(handles=legend_elements, loc=(1.5,-2))

    # adjust 
    plt.subplots_adjust(wspace=0.02, hspace=0.02)
    fig.savefig(filename_plot, bbox_inches='tight')

    #############################

def generate_clusteredGOterms_plot(df_GOEA, filename_plot, filename_data, obo_file, species_to_gene_to_GOterms, first_colfield="short_species", second_colfield="type_var", sorted_first_colfield=None, sorted_second_colfield=None, semantic_similarity_tsh=0.5, keep_oneTerm_per_group=False, replace=False, title="", zfield="minus_log10pval", first_colfield_colors={}, second_colfield_colors={}, colorbar_label="-log(p)"):

    """This function generates a clustermap for the df_GOEA, in a way that the first_colfield and second_colfield indicate the order of each nested structure of the samples in the cols. GO terms go in the rows and are clustered by semantic similarity"""

    # keep
    df_GOEA = cp.deepcopy(df_GOEA)

    ########### GET A DF PLOT WITH THE SIMILARITY DISTANCES AND CLUSTER INFORMATION ###########

    # get the semantic similarity
    df_pairwise_semantic_similarity_file = "%s.semantic_similarity.py"%filename_data
    if file_is_empty(df_pairwise_semantic_similarity_file) or replace is True: save_object(get_df_pairwise_Lin_semantic_similarity(df_GOEA, obo_file, species_to_gene_to_GOterms), df_pairwise_semantic_similarity_file)

    df_pairwise_semantic_similarity = load_object(df_pairwise_semantic_similarity_file)

    # get the df_GOEA with the cluster info    
    if file_is_empty(filename_data) or replace is True:

        # get the df_GOEA with a cluster name which is related to the most representative term of the cluster, by semantic similarity
        df_GOEA = get_df_GOEA_with_clusterInfo_bySemantic_similarity(df_GOEA, obo_file, df_pairwise_semantic_similarity, semantic_similarity_tsh=semantic_similarity_tsh)

        # add the sampleID
        df_GOEA["unique_sampleID"] = df_GOEA[[first_colfield, second_colfield]].apply(tuple, axis=1) 

        # keep only one representative GO term for each group, the one that has the lowest pvalue for each sampleID
        if keep_oneTerm_per_group is True:

            # keep one  GO term of each cluster_representative_GO for each unique_sampleID
            df_GOEA = df_GOEA.sort_values(by=["unique_sampleID", "cluster_representative_GO", "p_fdr_bh", "GO"]).drop_duplicates(subset=["unique_sampleID", "cluster_representative_GO"], keep="first")

            # change
            df_GOEA["GO"] = df_GOEA.cluster_representative_GO
            df_GOEA["name"] = df_GOEA.cluster_representative_name

        # save
        save_object(df_GOEA, filename_data)

    df_GOEA = load_object(filename_data)

    # get a squared distance df
    all_GOterms = set(df_GOEA.GO)
    sorted_all_GOterms = sorted(all_GOterms)
    df_pairwise_semantic_similarity_equalTerms = pd.DataFrame({"termA":sorted_all_GOterms, "termB":sorted_all_GOterms, "semantic_similarity":[1.0]*len(all_GOterms)})

    df_pairwise_semantic_similarity = df_pairwise_semantic_similarity.append(df_pairwise_semantic_similarity_equalTerms)

    df_GO_distance = 1 - df_pairwise_semantic_similarity[(df_pairwise_semantic_similarity.termA.isin(all_GOterms)) & (df_pairwise_semantic_similarity.termB.isin(all_GOterms))][["termA", "termB", "semantic_similarity"]].pivot(index="termA", columns="termB", values="semantic_similarity").loc[sorted_all_GOterms, sorted_all_GOterms]

    # check that there are no NaNs
    def raise_if_nan(x):
        if pd.isna(x): raise ValueError("there is a nan")
    df_GO_distance.applymap(raise_if_nan)

    ########################################################################################

    ############# MAKE PLOT #############

    # define the columns and rows
    if sorted_first_colfield is None: sorted_first_colfield = sorted(set(df_GOEA[first_colfield]))
    if sorted_second_colfield is None: sorted_second_colfield = sorted(set(df_GOEA[second_colfield]))

    # make a df with the zfield for each of the samples as cols
    df_GOEA["minus_log10pval"] = -np.log10(df_GOEA.p_fdr_bh)
    df_plot = pd.DataFrame(index=sorted_all_GOterms)

    sorted_cols = []
    for first_colfield_val in sorted_first_colfield:
        for second_colfield_val in sorted_second_colfield:

            # map each GO to the zvalue
            df_g = df_GOEA[(df_GOEA[first_colfield]==first_colfield_val) & (df_GOEA[second_colfield]==second_colfield_val)]
            if len(df_g)!=len(set(df_g.GO)): raise ValueError("there are repeated GOs")


            GO_to_zval = dict(df_g.set_index("GO")[zfield])
            for GO in all_GOterms.difference(set(df_g.GO)): GO_to_zval[GO] = np.nan

            # skip empty cols
            if len(df_g)==0: continue

            col = (first_colfield_val, second_colfield_val)
            df_plot[col] = [min(GO_to_zval[x], 7.0) for x in df_plot.index]
            sorted_cols.append(col)


    # check the similarity
    for termA in sorted_all_GOterms: 
        for termB in sorted_all_GOterms:

            distA_to_B = df_GO_distance.loc[termA, termB]
            distB_to_A = df_GO_distance.loc[termB, termA]

            if distA_to_B!=distB_to_A: 
                print(distA_to_B, distB_to_A, termA, termB)
                print(df_GO_distance)
                raise ValueError("not simmetric")

    # get the sorted GO terms by similarity
    if len(sorted_all_GOterms)>1: 
        GO_linkage = scipy.cluster.hierarchy.linkage(scipy.spatial.distance.squareform(df_GO_distance), method='average', metric="euclidean")

        cm_labels = sns.clustermap(df_plot, row_linkage=GO_linkage, col_cluster=False, yticklabels=True, xticklabels=True)
        sorted_GO_terms_bySimilarity = [y.get_text() for y in cm_labels.ax_heatmap.get_yticklabels()]
        plt.close()

    elif len(sorted_all_GOterms)==1: 
        GO_linkage = None
        sorted_GO_terms_bySimilarity = sorted_all_GOterms

    else: raise ValueError("there are no go terms")

    # define mappings
    GO_to_color  = get_value_to_color(sorted_GO_terms_bySimilarity, palette="tab10", n=len(sorted_GO_terms_bySimilarity), type_color="hex", center=None)[0] # cubehelix, icefire, gist_ncarSpectral, gist_ncar
    GO_to_name = dict(df_GOEA[["GO", "name"]].drop_duplicates().set_index("GO")["name"])


    # define the row and col colors dfs
    row_colors_df = pd.DataFrame({"term":{GO : GO_to_color[GO]  for GO in set(df_GOEA.GO)}})
    col_colors_df = pd.DataFrame({col : {colfield : color_dict[col[I]] for I, (colfield, color_dict) in enumerate([(first_colfield, first_colfield_colors), (second_colfield, second_colfield_colors)])} for col in sorted_cols}).transpose()

    # keep only the vars with some enrichment in some species
    #df_plot = df_plot[[c for c in df_plot.columns if any(~pd.isna(df_plot[c]))]]

    # get the linkage by distance in GO terms
    df_GOEA["unique_sampleID_str"] = df_GOEA[first_colfield]+"#"+df_GOEA[second_colfield]
    sampleStr_to_sampleID = dict(df_GOEA[["unique_sampleID_str", "unique_sampleID"]].drop_duplicates().set_index("unique_sampleID_str").unique_sampleID)

    sample_to_GOterms = {sampleStr_to_sampleID[s] : terms for s, terms in dict(df_GOEA.groupby("unique_sampleID_str").apply(lambda df_s: set(df_s.GO))).items()}
    for s in set(sorted_cols).difference(set(sample_to_GOterms)): sample_to_GOterms[s] = set()

    def get_sig_GOterms_distance_btw_samples(sampleA, sampleB):

        all_GOs = sample_to_GOterms[sampleA].union(sample_to_GOterms[sampleB])
        shared_GOs = sample_to_GOterms[sampleA].intersection(sample_to_GOterms[sampleB])

        if len(all_GOs)==0: return 0.0
        else: return 1-(len(shared_GOs)/len(all_GOs))


    df_sample_distance = pd.DataFrame({sampleA : {sampleB : get_sig_GOterms_distance_btw_samples(sampleA, sampleB) for sampleB in sorted_cols} for sampleA in sorted_cols}).loc[sorted_cols, sorted_cols]

    if len(df_plot.columns)>1: 
        samples_linkage = scipy.cluster.hierarchy.linkage(scipy.spatial.distance.squareform(df_sample_distance), method='average', metric="euclidean")
        col_cluster = True 

    else: 
        samples_linkage = None
        col_cluster = False

    if len(sorted_all_GOterms)>1: row_cluster = True
    else: row_cluster = False

    # define the clustermap
    mask = df_plot.isnull()
    cm = sns.clustermap(df_plot, row_linkage=GO_linkage, row_cluster=row_cluster, col_cluster=col_cluster, col_linkage=samples_linkage, yticklabels=True, xticklabels=False, col_colors=col_colors_df, linecolor="gray", linewidth=.1, cbar_kws={'label': colorbar_label}, mask=mask, cmap="rocket_r"); # row_colors = row_colors_df


    # define the ylabels
    cm.ax_heatmap.set_yticklabels([GO_to_name[GO.get_text()] for GO in cm.ax_heatmap.get_yticklabels()], rotation=0)

    # set the NaNs as white
    cm.ax_heatmap.set_facecolor("white")

    # define general things
    pixels_per_square = 0.025
    hm_height = len(df_plot)*pixels_per_square
    hm_width = len(df_plot.columns)*pixels_per_square
    #rc_width = len(row_colors_df.columns)*pixels_per_square
    rc_width = 0

    # adjust the heatmap positions
    hm_pos = cm.ax_heatmap.get_position()
    hm_y0 = (hm_pos.y0+hm_pos.height)-hm_height
    cm.ax_heatmap.set_position([hm_pos.x0, hm_y0, hm_width, hm_height]); hm_pos = cm.ax_heatmap.get_position()

    rd_pos = cm.ax_row_dendrogram.get_position()
    cm.ax_row_dendrogram.set_position([hm_pos.x0-rd_pos.width, hm_y0, rd_pos.width, hm_height])

    cc_pos = cm.ax_col_colors.get_position()
    cm.ax_col_colors.set_position([cc_pos.x0, cc_pos.y0+pixels_per_square/5, hm_width, len(col_colors_df.columns)*pixels_per_square])

    cd_pos = cm.ax_col_dendrogram.get_position()
    cm.ax_col_dendrogram.set_position([cc_pos.x0, cd_pos.y0, hm_width, cd_pos.height]); cd_pos = cm.ax_col_dendrogram.get_position()

    # add the title
    cm.ax_col_dendrogram.set_title(title)

    # add legend in the bottom of the heatmap
    def get_empty_legend(label): return [Line2D([0], [0], marker="o", label=label, markersize=0, lw=0)]

    legend_elements = get_empty_legend(first_colfield) + [mpatches.Patch(facecolor=first_colfield_colors[cf], edgecolor="gray", label=cf) for cf in sorted_first_colfield] + get_empty_legend("") + get_empty_legend(second_colfield) + [mpatches.Patch(facecolor=second_colfield_colors[cf], edgecolor="gray", label=cf) for cf in sorted_second_colfield]
    cc_pos = cm.ax_col_colors.get_position()
    cm.ax_col_dendrogram.legend(handles=legend_elements, loc=[4.1, 0])

    plt.show()

    # save
    #print("saving %s"%filename_plot)
    cm.savefig(filename_plot, bbox_inches="tight")
    



    #####################################




def plot_variation_rates_per_gene_and_enriched_functions(DataDir, gene_features_df, df_variationRatePerGene, ProcessedDataDir, PlotsDir, plots={"distribution_mutation_rate", "enriched_GO_terms"}, percentiles_thresholds=[5, 95], replace=False, semantic_similarity_tsh=0.5):

    """Plots the distribution of the variation per gen variation/gene. It will also analyze whether there are some enriched functions in the genes that have particularly high or low mutation rates (<5% percentile and >95% percentile)"""


    ######## PLOT THE DISTRIBUTION OF MUTATION RATES ##########
    if "distribution_mutation_rate" in plots:

        # go through each type_vars_SimpleRepeats
        for type_vars_SimpleRepeats in ["all_vars", "only_vars_noSimpleRepeats"]:

            # go through diferents sets of var types
            for type_vars_general, sorted_vartypes in [("all_vars",  ["SNP", "IN/DEL", "coverageCNV", "SV"]), ("onlySV", ["coverageCNV", "SV"])]:

                # define variants df
                df_vars = df_variationRatePerGene[(df_variationRatePerGene.type_vars_SimpleRepeats==type_vars_SimpleRepeats) & (df_variationRatePerGene.type_var.isin(sorted_vartypes))]

                # define lines to be drawn 
                species_to_vartype_to_xlines = {}

                for species in sorted_species_byPhylogeny:
                    for vartype in sorted_vartypes:
                        mutationRate_series = df_vars[(df_vars.species==species) & (df_vars.type_var==vartype) & (df_vars.mutations_per_bp>0)].mutations_per_bp
                        species_to_vartype_to_xlines.setdefault(species, {}).setdefault(vartype, [np.percentile(mutationRate_series, pct) for pct in percentiles_thresholds])

                # define fields for the histogram
                xfield = "mutations_per_bp"
                vartype_field = "type_var"
                plots_dir = "%s/plots_distribution_mutation_rate"%PlotsDir; make_folder(plots_dir)

                for logscale_x in [True]:

                    filename = "%s/%s_%s_log%s.pdf"%(plots_dir, type_vars_SimpleRepeats, type_vars_general, logscale_x)

                    # get the plot
                    plot_histograms_rows_in_species_vatypes_in_cols(df_vars, sorted_vartypes, xfield, filename, vartype_field, logscale_x=logscale_x, logscale_y=True, pseudocount=0.1, xlabel="mutation rate (# vars/bp)", input_xticks=None, input_yticks=None, extended_xrange=0.1, ylabel="number genes (%s)"%type_vars_SimpleRepeats, species_to_vartype_to_xlines=species_to_vartype_to_xlines)

    ###########################################################

    ############ PLOT ENRICHED GO TERMS ############
    if "enriched_GO_terms" in plots:

        # this figure makes a GO-Figure plot for each type_genes, type_vars_SimpleRepeats, type_variant. Only GO terms with an OR>1 Each row is one species and each column is one type of GO terms. The 

        # define the obo file
        obo_file = "%s/annotation_files/go-basic_30062021.obo"%(DataDir) # this was got from http://purl.obolibrary.org/obo/go/go-basic.obo

        # define the file
        df_GOEA_all_file = "%s/mutationRates_df_GOEA_file.py"%ProcessedDataDir
        if file_is_empty(df_GOEA_all_file) or replace is True:

            # prepare the data, get a df with the significant GO terms
            df_GOEA_all = pd.DataFrame()

            for type_genes in ["low_mutation_rate", "high_mutation_rate"]:
                for type_vars_SimpleRepeats in ["all_vars", "only_vars_noSimpleRepeats"]:
                    for type_var in ["SNP", "IN/DEL", "coverageCNV", "SV"]:
                        for species in sorted_species_byPhylogeny:
                            print("GOEA", type_genes, type_vars_SimpleRepeats, type_var, species)

                            # define a df related to the interesting vars
                            df_vars = df_variationRatePerGene[(df_variationRatePerGene.type_vars_SimpleRepeats==type_vars_SimpleRepeats) & (df_variationRatePerGene.type_var==type_var) & (df_variationRatePerGene.species==species)]

                            # define all genes
                            all_genes = set(gene_features_df[gene_features_df.species==species].gff_upmost_parent)

                            # define the genes that are paticularly interesting
                            mutationRate_series = df_vars[df_vars.mutations_per_bp>0].mutations_per_bp
                            low_percentile = np.percentile(mutationRate_series, percentiles_thresholds[0])
                            high_percentile = np.percentile(mutationRate_series, percentiles_thresholds[1])

                            if type_genes=="low_mutation_rate": target_genes = set(df_vars[df_vars.mutations_per_bp<=low_percentile].gff_upmost_parent)
                            elif type_genes=="high_mutation_rate": target_genes = set(df_vars[df_vars.mutations_per_bp>=high_percentile].gff_upmost_parent)
                            else: raise ValueError("erro in type_genes")

                            # skip if there are no genes
                            if len(target_genes)==0:
                                print("There are %i/%i genes with %s for %ss in %s. Considering %s"%(len(target_genes), len(all_genes), type_genes, type_var, species, type_vars_SimpleRepeats))
                                continue

                            # map each gene to the go terms 
                            gene_to_GOterms = dict(gene_features_df[gene_features_df.species==species].set_index("gff_upmost_parent").GOterms)

                            # get the df_GOEA and add to df
                            goatools_plots_dir = "%s/GOEA_goatools_enriched_mutationRate_perGene_plots"%PlotsDir; make_folder(goatools_plots_dir)
                            plot_fileprefix = "%s/%s_%s_%s_%s"%(goatools_plots_dir, species, type_genes, type_var.replace("/", "-"), type_vars_SimpleRepeats)
                            df_GOEA = get_df_GOEA_target_on_all_genes(target_genes, all_genes, gene_to_GOterms, obo_file, plot_fileprefix)

                            # add things and keep
                            df_GOEA["type_genes"] = type_genes
                            df_GOEA["type_vars_SimpleRepeats"] = type_vars_SimpleRepeats
                            df_GOEA["type_var"] = type_var
                            df_GOEA["species"] = species

                            df_GOEA_all = df_GOEA_all.append(df_GOEA).reset_index(drop=True)

            # save
            print("saving")
            save_object(df_GOEA_all, df_GOEA_all_file)

        # load
        df_GOEA_all = load_object(df_GOEA_all_file)

        # get one revigo plot for each namespace (NS), type_genes and type_vars_SimpleRepeats
        #for NS in ["CC", "MF", "BP"]:
        for NS in ["BP"]:

            for type_genes in ["low_mutation_rate", "high_mutation_rate"]:
            #for type_genes in ["high_mutation_rate"]:

                #for type_vars_SimpleRepeats in ["all_vars", "only_vars_noSimpleRepeats"]: 
                for type_vars_SimpleRepeats in ["only_vars_noSimpleRepeats"]: 

                    #for keep_oneTerm_per_group in [True, False]:
                    for keep_oneTerm_per_group in [True]:


                        print("ploting", NS, type_genes, type_vars_SimpleRepeats)

                        # define the df to plot
                        df_GOEA = df_GOEA_all[(df_GOEA_all.NS==NS) & (df_GOEA_all.type_genes==type_genes) & (df_GOEA_all.type_vars_SimpleRepeats==type_vars_SimpleRepeats)]

                        if len(df_GOEA)==0: raise ValueError("there should be some GOEA")

                        # create a gene_to_GOterms that is unique
                        species_to_gene_to_GOterms = dict(gene_features_df.groupby("species").apply(lambda df_s: dict(df_s.set_index("gff_upmost_parent").GOterms)))

                        # add the short species
                        df_GOEA["short_species"] = df_GOEA.species.apply(lambda x: x.split("_")[1])
                        sorted_species_byPhylogeny_short = [s.split("_")[1] for s in sorted_species_byPhylogeny if s in set(df_GOEA.species)]

                        ########### REVIGO PLOT ###########

                        """

                        # make the plot
                        plots_dir_revigo = "%s/plots_revigo_many_groups_perGeneMutationRates"%PlotsDir; make_folder(plots_dir_revigo)
                        filename_plot = "%s/%s_%s_%s_sim%.2f_onlyRepTerm%s.pdf"%(plots_dir_revigo, type_genes, type_vars_SimpleRepeats, NS, semantic_similarity_tsh, keep_oneTerm_per_group)
                        
                        processed_data_revigo = "%s/revigo_many_groups_perGeneMutationRates"%ProcessedDataDir; make_folder(processed_data_revigo)
                        filename_data = "%s/%s_%s_%s_%.2f_onlyRepTerm%s_df_GOEA_with_MDSdata.py"%(processed_data_revigo, type_genes, type_vars_SimpleRepeats, NS, semantic_similarity_tsh, keep_oneTerm_per_group)

                        generate_REVIGOlike_plot_subplots(df_GOEA, filename_plot, filename_data, obo_file, species_to_gene_to_GOterms, rowfield="short_species", colfield="type_var", sorted_rows=sorted_species_byPhylogeny_short, sorted_cols=["SNP", "IN/DEL", "coverageCNV", "SV"], semantic_similarity_tsh=semantic_similarity_tsh, keep_oneTerm_per_group=keep_oneTerm_per_group, replace=False, title="%s, %s, %s\nGOEA p_fdr<0.05. similarity_tshd=%.2f, onlyOneTermPerGroup=%s"%(type_genes, type_vars_SimpleRepeats, NS, semantic_similarity_tsh, keep_oneTerm_per_group))

                        """

                        ###################################

                        ########### CLUSTERING PLOT ############

                        # make the plot
                        plots_dir_cluster = "%s/plots_GOs_clustered_many_groups_perGeneMutationRates"%PlotsDir; make_folder(plots_dir_cluster)
                        filename_plot = "%s/%s_%s_%s_sim%.2f_onlyRepTerm%s.pdf"%(plots_dir_cluster, type_genes, type_vars_SimpleRepeats, NS, semantic_similarity_tsh, keep_oneTerm_per_group)
                        
                        processed_data_cluster = "%s/cluster_GOterms_many_groups_perGeneMutationRates"%ProcessedDataDir; make_folder(processed_data_cluster)
                        filename_data = "%s/%s_%s_%s_%.2f_onlyRepTerm%s_df_GOEA.py"%(processed_data_cluster, type_genes, type_vars_SimpleRepeats, NS, semantic_similarity_tsh, keep_oneTerm_per_group)

                        first_colfield_colors = {s.split("_")[1] : c for s,c in species_to_color.items()}

                        second_colfield_colors = {"SNP":"red", "IN/DEL":"magenta", "SV":"navy", "coverageCNV":"cyan"}

                        generate_clusteredGOterms_plot(df_GOEA, filename_plot, filename_data, obo_file, species_to_gene_to_GOterms, first_colfield="short_species", second_colfield="type_var", sorted_first_colfield=sorted_species_byPhylogeny_short, sorted_second_colfield=["SNP", "IN/DEL", "coverageCNV", "SV"], semantic_similarity_tsh=semantic_similarity_tsh, keep_oneTerm_per_group=keep_oneTerm_per_group, replace=False, title="%s, %s, %s\nGOEA p_fdr<0.05. similarity_tshd=%.2f\nonlyOneTermPerGroup=%s"%(type_genes, type_vars_SimpleRepeats, NS, semantic_similarity_tsh, keep_oneTerm_per_group), first_colfield_colors=first_colfield_colors, second_colfield_colors=second_colfield_colors)



                        ########################################

    ################################################



def plot_repeats_overlapping_SVregions(SV_CNV_df, PlotsDir):

    """This function takes the SV_CNV_df and plots the repeats that overlap each of the SV regions (it does not consider unclassified_breakpoint and bal_translocation) """

    ##### get the df to plot ######

    # keep vars where there is some SV region
    SV_CNV_df = SV_CNV_df[~pd.isna(SV_CNV_df.SVregion_chromosome)]

    # add the repeat family of the best overlappng repeat
    def get_repeat_family(r):
        if pd.isna(r.BestRepeatSVregion_repeat): return "no_repeat"

        elif r.BestRepeatSVregion_type!="Unknown" or r.BestRepeatSVregion_clusterID_repeatModeller=="no_cluster" or r.BestRepeatSVregion_clusterID_repeatModeller.startswith("singleton_"): return r.BestRepeatSVregion_type.split("/")[0]
        
        else: return "%s_cluster"%(r.BestRepeatSVregion_clusterID_repeatModeller.split("_")[0])

    SV_CNV_df["repeat_family"] = SV_CNV_df.apply(get_repeat_family, axis=1)

    # change the species
    SV_CNV_df["species"] = SV_CNV_df.species.apply(lambda x: x.split("_")[1])

    # add the fraction of SV covered by the repeat
    def get_fraction_SVregion_covered_byRepeat(r):
        if r.repeat_family=="no_repeat": return 1.0
        else: return (r.BestRepeatSVregion_end_repeat-r.BestRepeatSVregion_begin_repeat) / r.len_event

    SV_CNV_df["fraction_SVregion_covered_byRepeat"] = SV_CNV_df.apply(get_fraction_SVregion_covered_byRepeat, axis=1)

    ###############################

    # plot the stacked bar plot
    df_plot = SV_CNV_df[["species", "svtype", "repeat_family", "fraction_SVregion_covered_byRepeat"]]
    sorted_svtypes = ["deletion", "tandem_duplication", "inversion", "copyPaste_insertion", "cutPaste_insertion", "inverted_copyPaste_insertion", "inverted_translocation", "inverted_cutPaste_insertion", "coverage_deletion", "coverage_duplication"]

    for min_fraction_SVregion_covered_byRepeat in [0.0, 0.10, 0.25]:

        # redefine repeats based on min_fraction_SVregion_covered_byRepeat
        def get_repeat_family_based_on_min_fraction_SVregion_covered_byRepeat(r):
            if r.fraction_SVregion_covered_byRepeat>=min_fraction_SVregion_covered_byRepeat: return r.repeat_family
            else: return "no_repeat"

        df_p = cp.deepcopy(df_plot); df_p["repeat_family"] = df_p.apply(get_repeat_family_based_on_min_fraction_SVregion_covered_byRepeat, axis=1)
        title = "repeat matches >%.2f%s of SV"%(min_fraction_SVregion_covered_byRepeat*100, "%")

        for yfield in ["fraction_events", "n_events"]: # also n_events 
            plot_stacked_bar_subplots_eachEventOneRow(df_p, xfield="svtype", rowfield="species", hue="repeat_family", yfield=yfield, sorted_rows=[x.split("_")[1] for  x in sorted_species_byPhylogeny], sorted_xvalues=sorted_svtypes, sorted_huevalues=list(repeatFamily_to_color.keys()), palette=repeatFamily_to_color, title=title)


def plot_mechanisms_of_breakpointFormation_onePlotForAllMechanisms(SV_CNV_df, PlotsDir, repeats_df, min_pct_overlap_CNV_simpleRepeats=25, min_fraction_to_qualify_as_repeatSVregion=0.1):

    """This function assigns each variant to one mechanism of SV formation: TE, simple repeats, rRNA/tRNA (these three require that there is at least 10% of the SV region covered by the repeat), microhomology, inextact microhomology, others """

    SV_CNV_df = cp.deepcopy(SV_CNV_df)

    ##### GET THE DF PLOT ######

    # add whether the breakpoints overlaps simple repeats (adds "overlaps_simple_repeats")
    simple_repeats = {"Low_complexity", "Simple_repeat"}
    SV_CNV_df = get_unique_SV_CNV_df_with_overlaps_simple_repeats(SV_CNV_df, min_pct_overlap_CNV_simpleRepeats, simple_repeats)
    
    # add the fraction of SV region covered by the repeat
    def get_fraction_SVregion_covered_byRepeat(r):
        if pd.isna(r.BestRepeatSVregion_repeat): return 1.0 # no repeat overlapping
        else: return (r.BestRepeatSVregion_end_repeat-r.BestRepeatSVregion_begin_repeat) / r.len_event

    SV_CNV_df["fraction_SVregion_covered_byRepeat"] = SV_CNV_df.apply(get_fraction_SVregion_covered_byRepeat, axis=1)

    # add the repeat family of the best overlappng repeat. This will only be considered if the SV region is above the threshold min_fraction_to_qualify_as_repeatSVregion
    def get_repeat_family(r):
        if pd.isna(r.BestRepeatSVregion_repeat) or r.fraction_SVregion_covered_byRepeat<min_fraction_to_qualify_as_repeatSVregion: return "no_repeat"

        elif r.BestRepeatSVregion_type!="Unknown" or r.BestRepeatSVregion_clusterID_repeatModeller=="no_cluster" or r.BestRepeatSVregion_clusterID_repeatModeller.startswith("singleton_"): return r.BestRepeatSVregion_type.split("/")[0]
        
        else: return "%s_cluster"%(r.BestRepeatSVregion_clusterID_repeatModeller.split("_")[0])

    SV_CNV_df["repeat_family"] = SV_CNV_df.apply(get_repeat_family, axis=1)

    # redefine microhomologies
    SV_CNV_df["max_micro_homology_length"] = SV_CNV_df.micro_homology_length.apply(max)
    SV_CNV_df["max_inexact_homology_length"] = SV_CNV_df.inexact_homology_length.apply(max)

    # add the mechanism of SV formation
    repeatFamily_to_mechanismSVformation = {'LINE': 'transposable element', 'tRNA': 'tRNA/rRNA', 'DNA': 'transposable element', 'LTR': 'transposable element', 'Low_complexity': 'simple repeat', 'Simple_repeat': 'simple repeat', 'multiSpecies_cluster': 'transposable element', 'rRNA': 'tRNA/rRNA', 'singleSpecies_cluster': 'transposable element', 'Unknown': 'transposable element'}

    def get_mechanism_SV_formation(r):


        # if it has TEs, get them. This can also be rRNA/tRNA or simple repeat
        if r.repeat_family!="no_repeat": return repeatFamily_to_mechanismSVformation[r.repeat_family]

        # short microhomology (btw 2 and 10 bp)
        elif r.max_micro_homology_length>=2 and r.max_micro_homology_length<=10: return "short exact microhomology"

        # long homology
        elif r.max_micro_homology_length>10: return "exact homology"

        # short inexact microhomology (btw 2 and 10 bp)
        elif r.max_inexact_homology_length>=2 and r.max_inexact_homology_length<=10: return "short inexact microhomology"

        # long inexact homology
        elif r.max_inexact_homology_length>10: return "inexact homology"

        else: return "other"

    SV_CNV_df["mechanism SV formation"] = SV_CNV_df.apply(get_mechanism_SV_formation, axis=1)

    # change the species
    SV_CNV_df["species"] = SV_CNV_df.species.apply(lambda x: x.split("_")[1])


    # define the sorted_mechanisms_SVformation 
    sorted_mechanisms_SVformation = ['transposable element', 'tRNA/rRNA', 'simple repeat', 'short exact microhomology', 'short inexact microhomology', 'exact homology', 'inexact homology', 'other']
    if set(sorted_mechanisms_SVformation)!=set(SV_CNV_df["mechanism SV formation"]): raise ValueError("there are missing mechanisms of SV formation")

    SVmechanism_to_color = {"transposable element":"black", "tRNA/rRNA":"olive", "simple repeat":"cyan", "short exact microhomology":"red", "short inexact microhomology":"magenta", 'exact homology':'teal', 'inexact homology':'blue', "other":"gray"}

    ############################

    ######### MAKE PLOTS #############
    print("making plots")

    plots_dir = "%s/all_mechansims_SVformation_onePlot"%PlotsDir; make_folder(plots_dir)

    # one plot with / without SVs overlapping repeats
    for type_vars_SimpleRepeats in ["all_vars", "only_vars_noSimpleRepeats"]:

        # get the df to plot
        if type_vars_SimpleRepeats=="only_vars_noSimpleRepeats": df_plot = SV_CNV_df[SV_CNV_df.overlaps_simple_repeats==False]
        else: df_plot = SV_CNV_df

        # keep important fields
        df_plot = df_plot[["species", "svtype", "mechanism SV formation"]]
        sorted_svtypes = ["deletion", "tandem_duplication", "inversion", "copyPaste_insertion", "cutPaste_insertion", "inverted_copyPaste_insertion", "inverted_translocation", "inverted_cutPaste_insertion", "bal_translocation", "unclassified_breakpoint", "coverage_deletion", "coverage_duplication"]

        # define the title
        title = "mechanisms SV formation %s\nSV region matching >%.2fx of repeat"%(type_vars_SimpleRepeats, min_fraction_to_qualify_as_repeatSVregion)

        # one plot for absolute and relative vals
        for yfield in ["fraction_events", "n_events"]: # also n_events 

            # define filename
            filename = "%s/%s_%s.pdf"%(plots_dir, type_vars_SimpleRepeats, yfield)

            # redefine the sorted_mechanisms_SVformation
            sorted_mechanisms_SVformation_p = [x for x in sorted_mechanisms_SVformation if x in set(df_plot["mechanism SV formation"])]

            plot_stacked_bar_subplots_eachEventOneRow(df_plot, xfield="svtype", rowfield="species", hue="mechanism SV formation", yfield=yfield, sorted_rows=[x.split("_")[1] for  x in sorted_species_byPhylogeny], sorted_xvalues=sorted_svtypes, sorted_huevalues=sorted_mechanisms_SVformation_p, palette=SVmechanism_to_color, title=title, filename=filename)

    ##################################

def plot_investigate_mechanisms_of_breakpointFormation(SV_CNV_df, PlotsDir, repeats_df):

    """This function plots the putative mechanisms of breakpoint formation (including repeats of the same family overlapping the breakpoints and microhomology/inexact homology)"""

    ########## GET A DF WHERE EACH ROW IS ONE BREAKPOINT ###################

    # keep only SVs that can't be explained by repeats matching the SV region
    SV_CNV_df = SV_CNV_df[pd.isna(SV_CNV_df.BestRepeatSVregion_type)]

    # get a df where each row is one breakpoint and it contains the repeats and homology around each of them. Init with the rows that have one breakpoint
    SV_CNV_df["nbreakpoints"] = SV_CNV_df.RepeatsAroundBreakpoints.apply(len)
    if any(SV_CNV_df.nbreakpoints<1): raise ValueError("there are some missing bps")

    breakpoint_df_onePerSV = SV_CNV_df[SV_CNV_df.nbreakpoints==1][["svtype", "species", "RepeatsAroundBreakpoints", "micro_homology_length", "inexact_homology_length"]]
    for f in ["RepeatsAroundBreakpoints", "micro_homology_length", "inexact_homology_length"]: breakpoint_df_onePerSV[f]  = breakpoint_df_onePerSV[f].apply(lambda x: x[0])

    # add the stacked breakpoints for SVs where there is more than 1 breakpooint
    SV_CNV_df_mutlipleBps_perSV = SV_CNV_df[SV_CNV_df.nbreakpoints>1]

    def get_breakpoint_df_from_SV_CNV_df_r(r): return pd.DataFrame({"svtype":[r.svtype]*len(r.RepeatsAroundBreakpoints), "species":[r.species]*len(r.RepeatsAroundBreakpoints), "RepeatsAroundBreakpoints":r.RepeatsAroundBreakpoints, "micro_homology_length":r.micro_homology_length, "inexact_homology_length":r.inexact_homology_length})

    breakpoint_df_multiplePerSV = pd.concat(SV_CNV_df_mutlipleBps_perSV.apply(get_breakpoint_df_from_SV_CNV_df_r, axis=1).values)

    # merge
    breakpoint_df = breakpoint_df_onePerSV.append(breakpoint_df_multiplePerSV)

    # format a unique repeat df
    repeats_fields = ["species", "repeat", "type", "clusterID_repeatModeller"]
    repeats_df = repeats_df[repeats_fields].sort_values(by=repeats_fields).drop_duplicates(subset=repeats_fields)
    for species in sorted(set(breakpoint_df.species)): 
        repeats_df = repeats_df.append(pd.DataFrame({"species":[species], "repeat":[""], "type":[""], "clusterID_repeatModeller":[""]})).reset_index(drop=True)

    # manually remove a repeat that is duplicated for some reason
    repeats_df = repeats_df[~((repeats_df.species=="Candida_tropicalis") & (repeats_df.repeat=="Eutr2B") & (repeats_df.type=="DNA"))]
    #print(repeats_df[repeats_df.duplicated(subset=["species", "repeat"], keep=False)])

    # add repeats to breakpoint df
    breakpoint_df = breakpoint_df.merge(repeats_df, how="left", left_on=["species", "RepeatsAroundBreakpoints"], right_on=["species", "repeat"], validate="many_to_one")
    if any(pd.isna(breakpoint_df.clusterID_repeatModeller)): raise ValueError("there can't be nans")

    # get the family
    def get_repeat_family_from_breakpoint_df_r(r):
        if r.RepeatsAroundBreakpoints=="": return "no_repeat"

        elif r["type"]!="Unknown" or r.clusterID_repeatModeller=="no_cluster" or r.clusterID_repeatModeller.startswith("singleton_"): return r["type"].split("/")[0]
        
        else: return "%s_cluster"%(r.clusterID_repeatModeller.split("_")[0])

    breakpoint_df["repeat_family"] = breakpoint_df.apply(get_repeat_family_from_breakpoint_df_r, axis=1)

    # add the microhomology cathegory
    def get_microhomology_from_breakpoint_df_r(r):
        if not (r.micro_homology_length>=0 and r.inexact_homology_length>=0): raise ValueError("%s is invalid"%r)

        if r.micro_homology_length>=2: return "exact_micohomology"
        elif r.inexact_homology_length>=5: return "inexact_micohomology"
        else: return "no_microhomology"

    breakpoint_df["microhomology_type"] = breakpoint_df.apply(get_microhomology_from_breakpoint_df_r, axis=1)

    ########################################################################

    ########## PLOTS ########

    # define general things
    sorted_svtypes = ["deletion", "tandem_duplication", "inversion", "copyPaste_insertion", "cutPaste_insertion", "inverted_copyPaste_insertion", "inverted_translocation", "inverted_cutPaste_insertion", "coverage_deletion", "coverage_duplication", "unclassified_breakpoint", "bal_translocation"]

    sorted_species = [x.split("_")[1] for  x in sorted_species_byPhylogeny]
    breakpoint_df["species"] = breakpoint_df.species.apply(lambda x: x.split("_")[1])

    # plot the fraction of events that have some repeat mapping the breakpoints    
    plot_stacked_bar_subplots_eachEventOneRow(breakpoint_df, xfield="svtype", rowfield="species", hue="repeat_family", yfield="fraction_events", sorted_rows=sorted_species, sorted_xvalues=sorted_svtypes, sorted_huevalues=list(repeatFamily_to_color.keys()), palette=repeatFamily_to_color, title="repeats overlapping breakpoints for SVs outside repeats")

    # plot the microhomology type for breakpoints not overlapped by the same repeat type
    breakpoint_df_noRepeats = breakpoint_df[(breakpoint_df.repeat_family=="no_repeat") & ~(breakpoint_df.svtype.isin({"coverage_deletion", "coverage_duplication"}))]
    microhomology_to_color = {"exact_micohomology":"red", "inexact_micohomology":"magenta", "no_microhomology":"grey"}

    sorted_svtypes_MH = ["deletion", "tandem_duplication", "inversion", "copyPaste_insertion", "cutPaste_insertion", "inverted_copyPaste_insertion", "inverted_translocation", "inverted_cutPaste_insertion", "unclassified_breakpoint", "bal_translocation"]


    for yfield in ["fraction_events", "n_events"]: # also n_events 
 
        plot_stacked_bar_subplots_eachEventOneRow(breakpoint_df_noRepeats, xfield="svtype", rowfield="species", hue="microhomology_type", yfield=yfield, sorted_rows=sorted_species, sorted_xvalues=sorted_svtypes_MH, sorted_huevalues=list(microhomology_to_color.keys()), palette=microhomology_to_color, title="breakpoints with no repeats microhomology")


    #########################



def get_mosdepth_coverage_per_windows_output_likeBamStats(fileprefix, sorted_bam, windows_bed, replace=False, extra_threads=0):

    """This function uses mosdepth to get the coverage for some regions in bed for a sorted_bam """

    print("running function for %s"%fileprefix)

    # get outfiles
    fileprefix_tmp = "%s.tmp"%fileprefix
    regions_file = "%s.regions.bed.gz"%fileprefix 
    regions_file_tmp = "%s.regions.bed.gz"%fileprefix_tmp 
    thresholds_file = "%s.thresholds.bed.gz"%fileprefix 
    thresholds_file_tmp = "%s.thresholds.bed.gz"%fileprefix_tmp

    if file_is_empty(regions_file) or file_is_empty(thresholds_file) or replace is True:
        print("running mosdepth for %s"%fileprefix)
        
        # change the end, setting it to +1, and also sorting
        windows_1_based = "%s.windows_bed_1_based.bed"%fileprefix
        windows_1_based_stderr = "%s.generating.stderr"%windows_1_based
        run_cmd(""" awk '{print $1 "\t" ($2+1) "\t" ($3)}' %s | sort -k1,1 -k2,2n > %s 2>%s"""%(windows_bed, windows_1_based, windows_1_based_stderr))
        remove_file(windows_1_based_stderr)

        # get the cmd
        mosdepth_std = "%s.generating.std"%fileprefix_tmp
        run_cmd("mosdepth --threads %i --by %s --no-per-base --fast-mode --thresholds 1 --use-median %s %s > %s 2>&1"%(extra_threads, windows_1_based, fileprefix_tmp, sorted_bam, mosdepth_std)) # mosdepth does not look at internal cigar operations or correct mate overlaps (recommended for most use-cases). It is also faster

        # clean
        remove_file(mosdepth_std)
        remove_file(windows_1_based)
 
        # remove innecessary files
        for sufix in ["mosdepth.global.dist.txt", "mosdepth.region.dist.txt", "mosdepth.summary.txt", "thresholds.bed.gz.csi", "regions.bed.gz.csi"]: remove_file("%s.%s"%(fileprefix_tmp, sufix))

        # keep
        os.rename(regions_file_tmp, regions_file)
        os.rename(thresholds_file_tmp, thresholds_file)

    # get as dfs
    df_regions = pd.read_csv(regions_file, sep="\t", header=None, names=["#chrom",  "start", "end", "mediancov_1"]).drop_duplicates(subset=["#chrom",  "start", "end"])
    df_thresholds = pd.read_csv(thresholds_file, sep="\t").drop_duplicates(subset=["#chrom",  "start", "end"])

    # add the number of basepairs in each region that are covered by at least one
    try: df = df_regions.merge(df_thresholds, on=["#chrom",  "start", "end"], validate="one_to_one").rename(columns={"1X":"nbp_more_than_1x"})
    except:
        print_if_verbose("!!!!!!!!regions:%s, \n thresholds:%s, \n prefix:%s"%(regions_file, thresholds_file, fileprefix))
        raise ValueError("There was an error with joining the mosdepth outputs")

    def convert_to_float(x):
        
        """Takes a number and converts to float, and 0 if NaN"""

        if pd.isna(x): return 0.0
        else: return float(x)


    # add some trivial info
    df["length"] = df.end - df.start
    df["nocoveragebp_1"] = df.length - df.nbp_more_than_1x
    df["percentcovered_1"] = 100 - ((df.nocoveragebp_1/df.length) * 100).apply(convert_to_float)

    # get as 0-based
    df["start"] = df.start - 1

    # delete unnecessary files
    #for f in [regions_file, thresholds_file]: remove_file(f)

    return df


def get_coverage_per_windows_several_samples(df_windows, coverage_df_file, pctCovered_df_file, outdir, df_inputs, threads):

    """Generates a df written in coverage_df_file with the coverage of all the windows of in df_windows. df_inputs should have sorted bams for each sample"""

    if file_is_empty(coverage_df_file) or file_is_empty(pctCovered_df_file):
        print("getting coverage per windows")

        make_folder(outdir)

        # run mosdepth to get the coverage for each sample
        all_samples = sorted(set(df_inputs.sampleID))

        # define the mosdepth_outdir
        coverage_files_dir = "%s/coverage_files"%outdir; make_folder(coverage_files_dir)

        # rename the input
        df_inputs = df_inputs.set_index("sampleID")

        # define the windows bed
        windows_bed = "%s/windows_bed.bed"%outdir
        df_windows[["chromosome", "start", "end"]].to_csv(windows_bed, sep="\t", header=False, index=False)

        # run mosdepth in parallel
        inputs_fn = []
        for sampleID in all_samples:

            # define the sorted bam
            sorted_bam = df_inputs.loc[sampleID, "sorted_bam"]
            if file_is_empty(sorted_bam): raise ValueError("%s is not found"%sorted_bam)

            # define the coverage file
            coverage_fileprefix = "%s/%s.coverage.%s"%(coverage_files_dir, get_file(coverage_df_file), sampleID)

            # get the inputs
            inputs_fn.append((coverage_fileprefix, sorted_bam, windows_bed, False, 0))

        # run the function in parallel
        print("running get_mosdepth_coverage_per_windows_output_likeBamStats in parallel")
        with multiproc.Pool(threads) as pool:

            list_coverage_dfs = pool.starmap(get_mosdepth_coverage_per_windows_output_likeBamStats, inputs_fn)
            pool.close()
            pool.terminate()


        # init two dfs 
        df_all_coverage = cp.deepcopy(df_windows)
        df_all_pctCovered = cp.deepcopy(df_windows)

        # add to positions_df
        print("integrating")
        for Is, coverage_df_raw in enumerate(list_coverage_dfs):
            print("appending sample %i/%i"%(Is+1, len(all_samples)))

            # define the sample
            sampleID = all_samples[Is]

            # keep the coverage
            coverage_df = coverage_df_raw.rename(columns={"mediancov_1":sampleID, "#chrom":"chromosome"})[["chromosome", "start", "end", sampleID]]
            df_all_coverage = df_all_coverage.merge(coverage_df, on=["chromosome", "start", "end"], validate="one_to_one")

            # keep the percent covered
            pct_covered_df = coverage_df_raw.rename(columns={"percentcovered_1":sampleID, "#chrom":"chromosome"})[["chromosome", "start", "end", sampleID]]
            df_all_pctCovered = df_all_pctCovered.merge(pct_covered_df, on=["chromosome", "start", "end"], validate="one_to_one")

        # clean
        delete_folder(coverage_files_dir)

        # save
        print("saving")
        save_object(df_all_coverage, coverage_df_file)
        save_object(df_all_pctCovered, pctCovered_df_file)

    # load
    df_all_coverage = load_object(coverage_df_file)
    df_all_pctCovered = load_object(pctCovered_df_file)

    return df_all_coverage, df_all_pctCovered

def get_pairwiseSNPdistances_per_window_jobs(small_vars_filt_file, outdir, mitochondrial_chromosome, df_sorted_bams, reference_genome, ploidy,threads_pairwiseSNPdistance=4, replace=False, min_coverage_pos=12, threads=4, window_size=10000, min_pct_covered=95, df_windows=None):

    """This function generates a bed file with windows (only windows with a median coverage>12 for all samples), a .tab file with variants (only for positions with coverage >12 for all sample). It generates one job for each sample, which will take these files and calculate the number of different SNPs across each window in all samples . It returns a list with all the jobs. """

    ######## GET GENERAL FILES ########

    # define all the samples (these are numeric)
    all_samples = sorted(set(df_sorted_bams.sampleID))

    # debug
    if window_size is None and df_windows is None: raise ValueError("df_windows or windows_size should be specified")
    if window_size is not None and df_windows is not None: raise ValueError("df_windows or windows_size shold be None")

    ###################################

    ######### GET DF VARS WITH THE REQUIRED COVERAGE IN ALL POSITIONS #########

    df_variants_file = "%s/SNPs_table_coverage_atLeast%ix_AllSamples_ploidy%i.py"%(outdir, min_coverage_pos, ploidy)

    if file_is_empty(df_variants_file) or replace is True:
        print("getting the SNPs file")

        # load the variants df (already filtered)
        print("loading variants")
        small_vars = load_object(small_vars_filt_file)

        # add some fields
        small_vars["common_GT"] = small_vars.common_GT.apply(str)

        def get_upper(x): return x.upper()
        small_vars["REF"] = small_vars.REF.apply(get_upper)
        small_vars["ALT"] = small_vars.ALT.apply(get_upper)
        
        # filters
        small_vars = small_vars[small_vars.calling_ploidy==ploidy]
        small_vars["sampleID"] = small_vars.sampleID.apply(int)
        small_vars = small_vars[small_vars.sampleID.isin(all_samples)]

        acgt = {"A", "C", "G", "T"}
        small_vars = small_vars[(small_vars.common_GT.isin({"0/1", "1/1", "1"})) & (small_vars.ISSNP) & (small_vars.REF.isin(acgt)) & (small_vars.ALT.isin(acgt))]

        # get the small vars df with the required positions
        small_vars = get_small_vars_with_atLeastSomeCoverageInAllPositions(small_vars, outdir, df_sorted_bams, min_coverage_pos, threads=threads)

        # add numeric variantID
        sorted_vars = sorted(set(small_vars["#Uploaded_variation"]))
        varID_to_numericID = dict(zip(sorted_vars, range(len(sorted_vars))))
        small_vars["numeric_variantID"] = small_vars["#Uploaded_variation"].map(varID_to_numericID)
        if any(pd.isna(small_vars.numeric_variantID)): raise ValueError("the vars can't be NaN")

        # debug the fact that nothing was left
        if len(small_vars)==0: raise ValueError("there are no SNPs")

        # keep the important fields and save
        small_vars = small_vars[["sampleID", "#CHROM", "POS", "REF", "ALT", "numeric_variantID"]].sort_values(by=["sampleID", "#CHROM", "POS", "REF", "ALT"]).reset_index(drop=True)
        save_object(small_vars, df_variants_file)
        
    ###########################################################################

    ####### GENERATE A BED WITH WINDOWS OF window_size AND COVERAGE IN ALL SAMPLES #######

    if window_size is not None: 

        filtered_windows_bed = "%s/windows_%ibp_atLeast%ix_%ipctCovered_AllSamples.bed"%(outdir, window_size, min_coverage_pos, min_pct_covered)

        if file_is_empty(filtered_windows_bed) or replace is True:
            print("generating windows bed")

            # generate all the windows bed
            df_windows = pd.read_csv(get_windows_for_reference_genome(reference_genome, window_size), sep="\t", names=["chromosome", "start", "end"])

            # get the coverage and pct coverage based per windows across several samples
            coverage_df_file = "%s.coverage_per_windowsAllSamples.py"%filtered_windows_bed
            pctCovered_df_file = "%s.coverage_per_windowsAllSamples_pctCovered.py"%filtered_windows_bed
            outdir_generate_coverage_df_file = "%s.generating"%coverage_df_file
            #delete_folder(outdir_generate_coverage_df_file)
            df_all_coverage, df_all_pctCovered = get_coverage_per_windows_several_samples(df_windows, coverage_df_file, pctCovered_df_file, outdir_generate_coverage_df_file, df_sorted_bams, threads)

            # define all samples as string
            all_samples_str = [str(x) for x in all_samples]

            # keep the positions that have at least min_coverage_pos reads covering in all samples, and also those that have enough pct covered
            df_all_coverage["correct_coverage"] = (df_all_coverage[all_samples]>=min_coverage_pos).apply(all, axis=1)
            df_all_pctCovered["correct_pctCovered"] = (df_all_pctCovered[all_samples]>=min_pct_covered).apply(all, axis=1)

            fields_windows = ["chromosome", "start", "end"]
            df_windows_withAdds = df_all_coverage[fields_windows + ["correct_coverage"]].merge(df_all_pctCovered[fields_windows + ["correct_pctCovered"]], on=fields_windows, validate="one_to_one", how="left")

            if len(df_windows_withAdds)!=len(df_windows): raise ValueError("There are some missing windows")
            if any(pd.isna(df_windows_withAdds.correct_coverage)) or any(pd.isna(df_windows_withAdds.correct_pctCovered)): raise ValueError("no Nans expected")

            # get the filtered windows
            df_windows_filt = df_windows_withAdds[(df_windows_withAdds.correct_coverage) & (df_windows_withAdds.correct_pctCovered)][fields_windows].sort_values(by=fields_windows)
            print("There are %i/%i windows with coverage >%ix and pct covered >%i in all samples"%(len(df_windows_filt), len(df_windows), min_coverage_pos, min_pct_covered))

            # clean
            delete_folder(outdir_generate_coverage_df_file)

            # save
            df_windows_filt.to_csv(filtered_windows_bed, sep="\t", header=False, index=False)

    else:

        # get all the windows
        positions_fields = ["chromosome", "start", "end"]
        df_windows = df_windows[positions_fields].sort_values(positions_fields)

        filtered_windows_bed = "%s/windows_provided.bed"%(outdir)
        df_windows.to_csv(filtered_windows_bed, sep="\t", header=False, index=False)

    ######################################################################################

    ####### GENERATE AND RETURN THE JOBS ###########

    all_cmds = []
    run_calculate_pairwise_varDifferences_one_sample_py =  "%s/CandidaMine_data_generation/v1/run_calculate_pairwise_varDifferences_one_sample.py"%ParentDir

    # define the samples that have no vars
    samples_withNoVars = ",".join(sorted([str(x) for x in set(all_samples).difference(set(load_object(df_variants_file).sampleID))]))
    if samples_withNoVars=="": samples_withNoVars = "no_samples"

    for sampleID in all_samples:

        # define the outdir
        outdir_sample = "%s/calculating_for_sample_%s"%(outdir, sampleID)
        outfile = "%s/pairwise_SNP_distances_sample%i.tab"%(outdir, sampleID)

        if file_is_empty(outfile): all_cmds.append("%s --outdir %s --threads %i --df_variants_file %s --query_sampleID %s --filtered_windows_bed %s --samples_withNoVars %s --outfile %s"%(run_calculate_pairwise_varDifferences_one_sample_py, outdir_sample, threads_pairwiseSNPdistance, df_variants_file, sampleID, filtered_windows_bed, samples_withNoVars, outfile))

    return all_cmds

    ################################################


def get_df_windows_fastGEAR_any_recombination(taxID_dir, outdir_pairwiseSNP_distances_fastGEAR_regions, ploidy, min_coverage_pos, reference_genome, replace=False, threads=4):

    """Takes an outdir of one taxID and generates a df with the windows of fastGEAR in both genomic coordinates and fastGEAR coordinates. It assumes that the diploids were run on the random resampling and the haploids on the raw data."""


    df_windows_fastGEAR_file = "%s/df_windows_fastGEAR.py"%outdir_pairwiseSNP_distances_fastGEAR_regions
    if file_is_empty(df_windows_fastGEAR_file) or replace is True:

        # define the df with the positions with SNPs
        integrated_varcallsDir = "%s/integrated_varcalls"%taxID_dir 

        if ploidy==2: df_positions = load_object("%s/homo_and_hetero_SNPs_positions_noINDELS_cov_atLeast_%ix.py"%(integrated_varcallsDir, min_coverage_pos))

        elif ploidy==1: df_positions = load_object("%s/homoSNPs_positions_noINDELS_noHetSNPs_cov>%ix.py"%(integrated_varcallsDir, min_coverage_pos))

        # keep important fields and sorted df
        positions_fields = ["#CHROM", "POS"]
        df_positions = df_positions[positions_fields].sort_values(by=positions_fields).drop_duplicates().reset_index(drop=True)

        # get the df with all the windows, and also a multifasta for the length comparison
        if ploidy==2: 

            # get the windows_df dfs in parallel
            nresamples = 100
            inputs_fn = [(reference_genome, taxID_dir, I) for I in range(1, nresamples+1)]

            with multiproc.Pool(threads) as pool:

                df_windows_fastGEAR_coords = pd.concat(pool.starmap(get_recombination_fastGEAR_df_resamplingHeteroSNPs_oneResample, inputs_fn))[["chromosome", "start", "end"]].drop_duplicates() # this is 1-based, as the POSITIONS df
                
                pool.close()
                pool.terminate()

            # define multifasta
            multifasta_fastGEAR = "%s/generate_tree_from_SNPs_resamplingHetSNPs/resamplings/resample_1/multifasta_several_samples_randomlyChosenHetSNPs.fasta"%taxID_dir 

        elif ploidy==1: 

            # get the outdir and multifasta
            outdir_fastGEAR = "%s/run_fastGEAR_predefinedClades_homoSNPs/"%taxID_dir
            multifasta_fastGEAR = "%s/homoSNPs_positions_noINDELS_noHetSNPs_cov_atLeast_%ix_sequence.fasta"%(integrated_varcallsDir, min_coverage_pos) # called haploid_snps_multifasta

            # get the windows 
            df_windows_fastGEAR_coords = get_recombination_fastGEAR_df_severalChroms_one_outdir(outdir_fastGEAR, reference_genome, multifasta_fastGEAR)[["chromosome", "start", "end"]].drop_duplicates() # 1-based

        # change the names
        df_windows_fastGEAR_coords = df_windows_fastGEAR_coords.rename(columns={"start":"start_fastGEAR", "end":"end_fastGEAR"})

        # generate the final df_windows_fastGEAR_file for each chrom
        df_windows_fastGEAR = pd.DataFrame()
        for chrom in sorted(set(df_positions["#CHROM"])):
            print("working on chrom %s"%chrom)

            # get the df of the positions
            df_positions_chrom = df_positions[df_positions["#CHROM"]==chrom]

            # check that the length of the positions is the same as in fastGEAR
            len_chrom_fastGEAR = len(next(iter(SeqIO.parse("%s.%s.fasta"%(multifasta_fastGEAR, chrom), "fasta"))).seq)
            if len(df_positions_chrom)!=len_chrom_fastGEAR: raise ValueError("the length of the fastGEAR multifasta is not the same as the SNPs used")

            # init the df_windows for this chrom
            df_windows_fastGEAR_chrom = df_windows_fastGEAR_coords[df_windows_fastGEAR_coords.chromosome==chrom]

            # add the real location
            df_positions_chrom["POS_fastGEAR"] = list(range(1, len_chrom_fastGEAR+1))
            fastgearPOS_to_realPOS = dict(df_positions_chrom.set_index("POS_fastGEAR").POS)

            for field in ["start", "end"]:

                df_windows_fastGEAR_chrom[field] = df_windows_fastGEAR_chrom["%s_fastGEAR"%field].map(fastgearPOS_to_realPOS)
                if any(pd.isna(df_windows_fastGEAR_chrom[field])): raise ValueError("there can't be NaNs in the %s"%field)

            # sort
            df_windows_fastGEAR_chrom = df_windows_fastGEAR_chrom.sort_values(by=["chromosome", "start", "end"])

            # keep
            df_windows_fastGEAR = df_windows_fastGEAR.append(df_windows_fastGEAR_chrom)


        # save
        save_object(df_windows_fastGEAR, df_windows_fastGEAR_file)

    # load
    df_windows_fastGEAR = load_object(df_windows_fastGEAR_file)

    return df_windows_fastGEAR

def get_sampleID_to_set_different_vars_compared_to_query_sampleID(df_vars, query_sampleID, threads, all_expected_samples):

    """Takes a df with vars and returns a dict mapping each sample to a set with the vars that are different as compared to query sampleID"""
        
    # get the sample_to_vars (adding the missing)
    print("mapping sample to variants ")
    def get_vars_from_df(df_s): return set(df_s.numeric_variantID)
    sample_to_vars = dict(df_vars[["sampleID", "numeric_variantID"]].groupby("sampleID").apply(get_vars_from_df))

    print("adding missing samples")
    missing_samples = all_expected_samples.difference(set(sample_to_vars.keys()))
    if len(missing_samples)>0: 
        print("WARNING!!!! There are some samples with no vars: %s"%missing_samples)
        for s in missing_samples: sample_to_vars[s] = set()

    # define the query vars
    query_vars = sample_to_vars[query_sampleID]

    # define a function that returns a set with the non-intersecting variants between two samples
    print("getting non intersecting vars")
    def get_non_intesecting_vars_from_target_sample(target_sample):

        target_vars = sample_to_vars[target_sample]

        unique_vars_query = query_vars.difference(target_vars)
        unique_vars_target = target_vars.difference(query_vars)

        return unique_vars_query.union(unique_vars_target)

    # map each sample to the differentvars
    sorted_samples = sorted(all_expected_samples)
    sampleID_to_different_vars = dict(zip(sorted_samples , map(get_non_intesecting_vars_from_target_sample , sorted_samples))) 

    return sampleID_to_different_vars


def generate_generate_pairwise_variantDistances_outfile_one_target_sample(Is, df_vars_target_sample, filtered_windows_bed, outdir, query_sampleID, target_sampleID):

    """This function takes a variants df for one target sample and writes an outfile with no header"""

    # define the outfile
    outfile_target_sample = "%s/running_%s_on_%s.tab"%(outdir, query_sampleID, target_sampleID)

    if file_is_empty(outfile_target_sample):
        print("getting %s"%outfile_target_sample)

        # write the vars into a bed with the positions
        vars_bed = "%s.variants_bed.tab"%outfile_target_sample
        df_vars_target_sample.to_csv(vars_bed, sep="\t", index=False, header=False)

        # map with bedmap
        bedmap_outfile = "%s.bedmap.out"%outfile_target_sample
        run_cmd("bedmap --range 0 --count --delim '\t' %s %s > %s"%(filtered_windows_bed, vars_bed, bedmap_outfile), env="perSVade_env")

        # create a windwos df with all the elements
        df_windows = pd.read_csv(filtered_windows_bed, sep="\t", header=None, names=["chromosome", "start", "end"])

        def get_int_from_l(l): return int(l.strip())
        df_windows["n_vars"] = list(map(get_int_from_l, open(bedmap_outfile, "r").readlines()))

        df_windows["query_sampleID"] = query_sampleID
        df_windows["target_sampleID"] = target_sampleID

        # write
        outfile_target_sample_tmp = "%s.tmp"%outfile_target_sample
        df_windows[["chromosome", "start", "end", "query_sampleID", "target_sampleID", "n_vars"]].to_csv(outfile_target_sample_tmp, sep="\t", header=False, index=False)
        os.rename(outfile_target_sample_tmp, outfile_target_sample)

    return outfile_target_sample

def concatenate_list_headerLess_files_into_headerLess_tab_file(headerLess_fileList, file):

    """This function takes a list of files and concatenates them into file """

    if file_is_empty(file):
        print("concatenating %i files into %s"%(len(headerLess_fileList), file))

        # init with the first file
        file_tmp = "%s.tmp"%file
        run_cmd("cat %s > %s"%(headerLess_fileList[0], file_tmp))

        for If, f in enumerate(headerLess_fileList[1:]):

            print("appending file %i/%i"%(If, len(headerLess_fileList)))
            run_cmd("cat %s >> %s"%(f, file_tmp))

        os.rename(file_tmp, file)

def generate_pairwise_variantDistances_outfile(df_vars, filtered_windows_bed, query_sampleID, sampleID_to_differentVars, threads, outfile, outdir, all_expected_samples):

    """This function generates outfile, a file that has chromosome, start, end, query_sampleID, target_sampleID, nvars with no header of the comparison to each of the sampleIDs"""

    if file_is_empty(outfile):
        print("running generate_pairwise_variantDistances_outfile")

        # sort the df to run bedmap
        df_vars = df_vars.sort_values(by=["#CHROM", "POS"]).rename(columns={"POS":"end"})

        # add start
        df_vars["start"] = df_vars.end-1

        # define the inputs for generate_generate_pairwise_variantDistances_outfile_one_target_sample
        sorted_target_samples = sorted(all_expected_samples)
        
        inputs_fn = []
        for Is, target_sampleID in enumerate(sorted_target_samples):

            # define the df_vars for this sample
            df_vars_target_sample = df_vars[df_vars.numeric_variantID.isin(sampleID_to_differentVars[target_sampleID])][["#CHROM", "start", "end"]]

            # add inputs
            inputs_fn.append((Is, df_vars_target_sample, filtered_windows_bed, outdir, query_sampleID, target_sampleID))

        # run in parallel to obtain a lot of headerless files
        with multiproc.Pool(threads) as pool:

            list_single_sample_outputs = pool.starmap(generate_generate_pairwise_variantDistances_outfile_one_target_sample, inputs_fn)
            pool.close()
            pool.terminate()

        # concatenate into a headerless outfile
        concatenate_list_headerLess_files_into_headerLess_tab_file(list_single_sample_outputs, outfile)

        # clean at the end
        delete_folder(outdir)

def get_mode_multimodal_continuous_data(data, make_plots=False):

    """Takes a sorted iterable of data and returns the mode"""

    # sort it
    data = np.array(sorted(data))

    # if there is only one different data point,  change it
    if len(set(data))==1: return data[0]

    # infer the KDE (frequency)
    #inferred_data = KDEMultivariate(data, 'c', bw = 'cv_ml').pdf(data) # CV, which is not very fast
    frequency_KDE = KDEMultivariate(data, 'c', bw = 'normal_reference').pdf(data) # this uses the default way to infer the bandwith

    # get the points in data that have the maximum frequency
    data_with_maxFreq = set(data[frequency_KDE==max(frequency_KDE)])
    if len(data_with_maxFreq)>1: raise ValueError("there are many points with the max freq: %s"%data_with_maxFreq)
    mode = next(iter(data_with_maxFreq))

    if make_plots is True:

        plt.hist(data, color="gray", label="data")
        plt.plot(data, frequency_KDE, label="KDE", color="crimson")
        plt.axvline(mode, label="mode", color="red")
        plt.axvline(np.mean(data), label="mean", color="blue")
        plt.axvline(np.median(data), label="median", color="olive")

        plt.legend(loc="best")
        plt.show()

    return mode

def generate_generate_pairwiseSNPdistances_per_window_globalStats_file_oneSample(Is, nsamples, pairwise_SNPdistances_file, replace, make_plots):

    """Gets one file with the pairwise SNP distances and writes another one with one file for each sample"""

    # define input
    print("working on sample %i/%i"%(Is+1, nsamples))
    summary_stats_file = "%s.global_stats.py"%pairwise_SNPdistances_file

    if file_is_empty(summary_stats_file) or replace is True:

        # load df
        df_comparisons = pd.read_csv(pairwise_SNPdistances_file, sep="\t", header=None, names=["chromosome", "start", "end", "query_sample", "target_sample", "n_vars"])

        # add general fields
        df_comparisons["length_window"] = df_comparisons.end-df_comparisons.start
        df_comparisons["vars_per_bp"] = df_comparisons.n_vars/df_comparisons.length_window

        # define a function that takes a df for one comparison and returns a series with the global stats
        def get_series_global_stats_from_one_target_sample(df):

            # init dict with central statistics
            nvars = sum(df.n_vars)
            median_vars_per_bp = np.median(df.vars_per_bp)
            mean_vars_per_bp = np.mean(df.vars_per_bp)
            genome_size = sum(df.length_window)
            expected_vars_per_bp = nvars/genome_size
            mode_vars_per_bp = get_mode_multimodal_continuous_data(df.vars_per_bp, make_plots=make_plots)

            stats_dict = {"nvars":nvars, "median_vars_per_bp":median_vars_per_bp, "mean_vars_per_bp":mean_vars_per_bp, "expected_vars_per_bp":expected_vars_per_bp, "mode_vars_per_bp":mode_vars_per_bp}

            # add the fraction of the genome that has less than tsh_vars_per_bp vars_per_bp
            for tsh_vars_per_bp in [0.01, 0.001, 0.0001]: # 10SNP/kb, 1SNP/kb, 0.1SNP/kb

                # get the df
                df_lowDivergence = df[df.vars_per_bp<tsh_vars_per_bp]
                stats_dict["fraction_genome_pairwiseSNPdensity_below_%s"%tsh_vars_per_bp] = sum(df_lowDivergence.length_window)/genome_size

            return pd.Series(stats_dict)

        summary_stats_df = df_comparisons.groupby("target_sample").apply(get_series_global_stats_from_one_target_sample)

        # add fields
        summary_stats_df["target_sampleID"] = summary_stats_df.index
        summary_stats_df["query_sampleID"] = df_comparisons.query_sample.iloc[0]

        # write
        save_object(summary_stats_df, summary_stats_file)

    summary_stats_df = load_object(summary_stats_file).reset_index(drop=True)
    return summary_stats_df
      
def get_df_pairwise_SNP_distances_perWindow_summaryStats(DataDir, ProcessedDataDir, species_to_tree, replace=False, threads=4, make_plots=False):

    """Thi function integrates all the pairwise SNP distances per window into a single file that has stats about the comparison between samples of all samples"""

    # define file
    df_pairwise_SNP_distances_perWindow_summaryStats_file = "%s/df_pairwise_SNP_distances_perWindow_summaryStats.py"%ProcessedDataDir

    if file_is_empty(df_pairwise_SNP_distances_perWindow_summaryStats_file) or replace is True:
        print("generating %s"%df_pairwise_SNP_distances_perWindow_summaryStats_file)

        # init dict
        df_pairwise_SNP_distances_perWindow_summaryStats = pd.DataFrame()

        # go through each species 
        for taxID, species in taxID_to_sciName.items():
            print(species)

            # get the oudtir with the pairwise SNP distances
            outdir_pairwiseSNP_distances = "%s/%s_%i/calculating_pairwiseSNP_distances"%(DataDir, species, taxID)

            # define the samples
            all_sorted_samples = sorted([int(s) for s in species_to_tree[species].get_leaf_names()])

            # run the calculations for one each query sample in parallel
            nsamples = len(all_sorted_samples)
            inputs_fn = [(Is, nsamples, "%s/pairwise_SNP_distances_sample%i.tab"%(outdir_pairwiseSNP_distances, s), replace, make_plots) for Is, s in enumerate(all_sorted_samples)]

            if threads>1:

                with multiproc.Pool(threads) as pool:

                    df_pairwise_SNPdistances_windows = pd.concat(pool.starmap(generate_generate_pairwiseSNPdistances_per_window_globalStats_file_oneSample, inputs_fn))
                    pool.close()
                    pool.terminate()

            else:

                df_pairwise_SNPdistances_windows = pd.concat(map(lambda x: generate_generate_pairwiseSNPdistances_per_window_globalStats_file_oneSample(x[0], x[1], x[2], x[3], x[4]), inputs_fn))


            # keep
            df_pairwise_SNPdistances_windows["species"] = species
            df_pairwise_SNP_distances_perWindow_summaryStats = df_pairwise_SNP_distances_perWindow_summaryStats.append(df_pairwise_SNPdistances_windows)

        # save
        save_object(df_pairwise_SNP_distances_perWindow_summaryStats, df_pairwise_SNP_distances_perWindow_summaryStats_file)

    df_pairwise_SNP_distances_perWindow_summaryStats = load_object(df_pairwise_SNP_distances_perWindow_summaryStats_file)

    #if run_in_cluster is True: sys.exit("stopping after generating data")

    return df_pairwise_SNP_distances_perWindow_summaryStats
          
def load_InterProAnnotation_Cmetapsilosis(file):

    """This function retuns a dataframe out of the interpro annotation for candida metapsilosis (done by Veronica)"""

    # rewrite file so that it is prepared for loading into interpro
    good_lines = []
    for l in open(file, "r").readlines():

        # keep good lines
        if not l.startswith("##"): good_lines.append(l)

        # when the fasta begins, break
        if l.startswith("##FASTA"): break

    file_goodLines = "%s.IPlines.tab"%file
    open(file_goodLines, "w").write("".join(good_lines))

    col_names = ["proteinID", "type_analysis", "type_match", "start_location", "end_location", "score", "strand", "unknown", "misc"]

    return pd.read_csv(file_goodLines, sep="\t", header=None, names=col_names)

def load_InterProAnnotation(path_to_InterPro_annotation):

    """This function retuns a dataframe out of the interpro annotation"""

    col_names = ["proteinID", "MD5digest", "seq_length", "type_analysis", "signature_accession", "signature_description", "start_location", "end_location", "score", "status", "date", "InterPro_accession", "InterPro_annotation_description", "GO_InterPro", "Pathway_Interpro"]

    return pd.read_csv(path_to_InterPro_annotation, names=col_names, sep="\t", header=None)

def get_df_GOterms_all_species(DataDir, species_to_gff, ProcessedDataDir):

    """

    This function generates a table with the go terms of each gene from the gff (upmost_gene) from different sources. I take most of the GO terms from http://www.candidagenome.org/download/go/. This is some info about where I took the GO terms for each species:

    - C. auris: I have the reference genome gDNA from GCA_002759435.2 and the mtDNA from ASM716870v1, from august 2020. I take the GI annotations from http://www.candidagenome.org/download/go/archive/gene_association.20200901.cgd.gz. I don't have GO terms for mtDNA genes here. There are >1000 genes with no annotations.

    - C. glabrata: reference genome from CGD: the latest version by 12/03/2019, which is v_s02-m07-r35. I take the GO annotations from http://www.candidagenome.org/download/go/archive/gene_association.20190313.cgd.gz, which is the exact same genome version. There are only 14 gDNA genes with no annotation.

    - C. albicans: I took the haplotypeA reference genome version from CGD 'A22-s07-m01-r110'. I take the matching annotation from the 06/06/2020 from http://www.candidagenome.org/download/go/archive/gene_association.20200607.cgd.gz, for strain SC5314. There are 160 genes with no annotation in the gDNA.

    - C. tropicalis: The reference genome is from NCBI for strain MYA-3404. It was taken in July 2020. I will use one from a similar time: http://www.candidagenome.org/download/go/archive/gene_association.20200901.cgd.gz. There are ~2000 gDNA genes with no annotations.

    - C. parapsilosis: I took the reference genome from NCBI in July 2020. I will use one from a similar time: http://www.candidagenome.org/download/go/archive/gene_association.20200901.cgd.gz. There are ~100 gDNA genes with no terms. There are ~100 genes with no annotation.

    - C. orthopsilosis: I took the reference genome from NCBI in July 2020, mtDNA sepparate from gDNA. I will use one from a similar time: http://www.candidagenome.org/download/go/archive/gene_association.20200901.cgd.gz. There are ~1800 missing gDNA genes and all the mtDNA ones.

    In addition, I add the GO terms transfered from interproscan run for all coding genes

    """

    df_GOterms_all_file = "%s/df_GOterms_all.py"%ProcessedDataDir
    if file_is_empty(df_GOterms_all_file):
        print("generating df_GOterms_all")

        # init a df with all the terms
        df_GOterms_all = pd.DataFrame() # it will have CC, BP, MF and a set with the terms for all the genes in the gff. Some of the info from the CGD files will also be transfered

        # go through each species
        for taxID, species in taxID_to_sciName.items():
            if species not in species_to_gff: continue
            print(taxID, species)

            #if species!="Candida_parapsilosis": continue

            # load the gff (and keep only genes)
            gff_df = load_gff3_intoDF(species_to_gff[species])
            all_genes = set(gff_df[gff_df.feature.isin({"gene", "pseudogene"})].upmost_parent)
            gff_df = gff_df[gff_df.upmost_parent.isin(all_genes)]

            ######## LOAD THE MANUALLY CURATED ANNOTATIONS #########

            # CGD-based species
            if species in {"Candida_auris", "Candida_glabrata", "Candida_albicans", "Candida_tropicalis", "Candida_parapsilosis", "Candida_orthopsilosis"}:

                # load the CGD file
                species_to_CGDfile = {"Candida_auris":"gene_association.20200901.cgd", "Candida_glabrata":"gene_association.20190313.cgd", "Candida_albicans":"gene_association.20200607.cgd", "Candida_tropicalis":"gene_association.20200901.cgd", "Candida_parapsilosis":"gene_association.20200901.cgd", "Candida_orthopsilosis":"gene_association.20200901.cgd"}

                cgd_file = "%s/annotation_files/%s"%(DataDir, species_to_CGDfile[species])
                columns = ["DB", "DB_Object_ID", "DB_Object_Symbol", "Qualifier", "GO_id", "DB_reference", "Evidence", "With_from", "Aspect", "DB_Object_Name", "DB_Object_Synonym", "DB_Object_Type", "taxon", "Date", "Assigned_by", "blank1", "blank2"]
                df_CGD = pd.read_csv(cgd_file, sep="\t", skiprows=sum(map(lambda l: l.startswith("!"), open(cgd_file, "r").readlines())), names=columns)

                # keep only the taxID and the required aspect
                species_to_GOtaxID = {"Candida_auris":498019, "Candida_glabrata":5478, "Candida_albicans":237561, "Candida_tropicalis":294747, "Candida_parapsilosis":578454, "Candida_orthopsilosis":1136231}
                df_CGD = df_CGD[(df_CGD.taxon==("taxon:%i"%species_to_GOtaxID[species]))]

                # add some extra fields
                df_CGD["gene_and_DB_Object_Synonym"] = "gene-" + df_CGD.DB_Object_Synonym
                df_CGD["first_gene_and_DB_Object_Synonym"] = df_CGD.gene_and_DB_Object_Synonym.apply(lambda x: str(x).split("|")[0])
                df_CGD["gene_and_DB_Object_ID"] = "gene-" + df_CGD.DB_Object_ID
                df_CGD["first_DB_Object_Synonym"] = df_CGD.DB_Object_Synonym.apply(lambda x: str(x).split("|")[0])

                # check things
                if species=="Candida_auris" and any(df_CGD.DB_Object_Synonym.apply(lambda s: "|" in str(s))): raise ValueError("There are '|' in some C. auris DB_Object_Synonym")

                # keep the df_CGD with the important fields
                mapping_f_dict = {"Candida_auris":"gene_and_DB_Object_Synonym", "Candida_glabrata":"first_DB_Object_Synonym", "Candida_albicans":"first_DB_Object_Synonym", "Candida_tropicalis":"gene_and_DB_Object_ID", "Candida_parapsilosis":"first_gene_and_DB_Object_Synonym", "Candida_orthopsilosis":"gene_and_DB_Object_ID"}
                mapping_f = mapping_f_dict[species]
                df_CGD = df_CGD[df_CGD[mapping_f].isin(all_genes)][[mapping_f, "GO_id"]].rename(columns={mapping_f:"gff_upmost_parent"})

                # define the GO df
                long_df_GOterms = df_CGD

            # species with no manually curated annotations
            elif species=="Candida_metapsilosis": long_df_GOterms = pd.DataFrame(columns=["GO_id", "gff_upmost_parent"])

            else: raise ValueError("%s has not been considered"%species)

            # define the genes
            mtDNA_genes = set(gff_df[gff_df.chromosome==taxID_to_mtChromosome[taxID]].upmost_parent)
            gDNA_genes = set(gff_df[gff_df.chromosome!=taxID_to_mtChromosome[taxID]].upmost_parent)

            # print the genes that have no annotation
            gDNA_genes_with_no_GO_annotation = gDNA_genes.difference(set(long_df_GOterms[~pd.isna(long_df_GOterms.gff_upmost_parent)].gff_upmost_parent))
            mtDNA_genes_with_no_GO_annotation = mtDNA_genes.difference(set(long_df_GOterms[~pd.isna(long_df_GOterms.gff_upmost_parent)].gff_upmost_parent))

            #print("There are %i/%i gDNA genes with no annotations in the manually curated set"%(len(gDNA_genes_with_no_GO_annotation), len(gDNA_genes)))
            #print("There are %i/%i mtDNA genes with no annotations in the manually curated set"%(len(mtDNA_genes_with_no_GO_annotation), len(mtDNA_genes)))

            # create the df for all genes
            gene_to_GOterms = dict(long_df_GOterms.groupby("gff_upmost_parent").apply(lambda df_g: set(df_g.GO_id)))

            # keep the gene to GO terms
            gene_to_GOtrems_curated = cp.deepcopy(gene_to_GOterms)
            
            ########################################################

            ##### ADD INTERPROSCAN ANNOTATIONS ############

            # these are annotations generated in this project

            # load df and keep only mappings that have go terms
            df_interpro = load_InterProAnnotation("%s/%s_%i/InterproScan_annotation/interproscan_annotation.out"%(DataDir, species, taxID))
            df_interpro = df_interpro[~pd.isna(df_interpro.GO_InterPro) & (df_interpro.GO_InterPro!="-")][["proteinID", "GO_InterPro"]]

            # map  each gene to the interpro go ternms
            def get_set_GO_ids(x): return set(x.split("|")).difference({""})
            df_interpro["set_GO_ids"] = df_interpro.GO_InterPro.apply(str).apply(get_set_GO_ids)

            def get_ipGOterms(df_p): return set.union(*df_p.set_GO_ids)
            gene_to_ipGOterms = dict(df_interpro.groupby("proteinID").apply(get_ipGOterms))

            # add to gene_to_GOterms
            for gene, ipGOterms in gene_to_ipGOterms.items():
                if gene in gene_to_GOterms: gene_to_GOterms[gene].update(ipGOterms)
                else: gene_to_GOterms[gene] = ipGOterms

            ###############################################

            ######### FINAL WRAP UP #######

            # add missing genes and keep
            for g in all_genes.difference(set(gene_to_GOterms)): gene_to_GOterms[g] = set()
            df_GOterms = pd.DataFrame({"GOterms":gene_to_GOterms})
            df_GOterms["gff_upmost_parent"] = df_GOterms.index

            # add the curated GO terms
            for g in all_genes.difference(set(gene_to_GOtrems_curated)): gene_to_GOtrems_curated[g] = set()
            df_GOterms["GOterms_cgdCurated"] = df_GOterms.gff_upmost_parent.map(gene_to_GOtrems_curated)
            if any(pd.isna(df_GOterms.GOterms_cgdCurated)): raise ValueError("There can't be nans in GOterms_cgdCurated")

            nGenes_with_new_IPgoterms = sum(df_GOterms.GOterms!=df_GOterms.GOterms_cgdCurated)
            if nGenes_with_new_IPgoterms==0: raise ValueError("Not all the GO terms can be as in curated")
            print("There are %i genes with new IP GOs"%nGenes_with_new_IPgoterms)

            # print the genes that have no annotation
            gDNA_genes_with_no_GO_annotation = {g for g in gDNA_genes if len(gene_to_GOterms[g])==0}
            mtDNA_genes_with_no_GO_annotation = {g for g in mtDNA_genes if len(gene_to_GOterms[g])==0}

            print("There are %i/%i gDNA genes with no annotations in the final set of GO terms"%(len(gDNA_genes_with_no_GO_annotation), len(gDNA_genes)))
            print("There are %i/%i mtDNA genes with no annotations in the final set of GO terms"%(len(mtDNA_genes_with_no_GO_annotation), len(mtDNA_genes)))

            # keep 
            df_GOterms["species"] = species
            df_GOterms_all = df_GOterms_all.append(df_GOterms)

            ###############################

        # save
        print("saving")
        save_object(df_GOterms_all, df_GOterms_all_file)

    df_GOterms_all = load_object(df_GOterms_all_file)
    return df_GOterms_all


def get_species_to_gene_to_orthofinder_orthologCluster(DataDir, ProcessedDataDir, threads=4):

    """Runs orthofinder for all proteins of all candidas and returns a mapping between each proteinID and the ortholog cluster"""

    ######## RUN ORTHOFINDER ########

    # make 
    orthofinder_outdir = "%s/orthofinder_all_species"%ProcessedDataDir
    final_file = "%s/orthofinder_ran.txt"%orthofinder_outdir

    if file_is_empty(final_file):
        print("running orthofinder")
        delete_folder(orthofinder_outdir); make_folder(orthofinder_outdir)

        # prepare the data for orthofinder running
        input_dir = "%s/proteomes"%orthofinder_outdir
        make_folder(input_dir)
        for taxID, species in taxID_to_sciName.items():
            print(species)

            # define the proteomes
            origin_proteome = "%s/%s_%i/InterproScan_annotation/proteins.fasta"%(DataDir, species, taxID)

            # write the species
            dest_proteome = "%s/%s.fasta"%(input_dir, species)
            seq_records = [SeqRecord(seq.seq, id="%s#%s"%(seq.id, species), name="", description="") for seq in SeqIO.parse(origin_proteome, "fasta")]
            SeqIO.write(seq_records, dest_proteome, "fasta")

        # run orthofinder with a tree MSA based approach
        output_dir = "%s/output_orthofinder"%orthofinder_outdir
        print("running orthofinder on %i threads"%threads)
        run_cmd("orthofinder -f %s  -o %s -t %i -M dendroblast -S diamond"%(input_dir, output_dir, threads))

        print("orthofinder was ran into %s"%output_dir)

        # write the final file
        open(final_file, "w").write("orthofinder ran")

        # exitting
        if run_in_cluster is True: sys.exit("exitting after orthofinder run")

    #################################

    ####### LOAD RESULTS TO GET THE GENE_TO_ORTHOLOG_CLUSTER ############

    # define outdir with orthogroups
    output_dir = "%s/output_orthofinder"%orthofinder_outdir
    orthogroups_dir = "%s/%s/Orthogroups"%(output_dir, [f for f in os.listdir(output_dir) if f.startswith("Results_")][0])

    # get the dfs with the genes assigned (also the unassigned ones)
    df_orthogroups = get_tab_as_df_or_empty_df("%s/Orthogroups.tsv"%orthogroups_dir)
    df_orthogroups = df_orthogroups.append(get_tab_as_df_or_empty_df("%s/Orthogroups_UnassignedGenes.tsv"%orthogroups_dir)).reset_index(drop=True)
    if len(df_orthogroups)!=len(set(df_orthogroups.Orthogroup)): raise ValueError("orthogroups are not unique")

    # get the dict
    species_to_gene_to_orthogroup = {}
    for species in sorted_species_byPhylogeny:
        print(species)

        # map each group to the genes
        def get_genes_from_genes_str(x): 
            if pd.isna(x): return []
            else: return [g.split("#")[0] for g in x.split(", ")]

        group_to_genes = dict(df_orthogroups.set_index("Orthogroup")[species].apply(get_genes_from_genes_str).apply(set))

        # map each gene to the group and keep
        gene_to_group = {}
        for group, genes in group_to_genes.items(): 
            for g in genes: gene_to_group[g] = group

        species_to_gene_to_orthogroup[species] = gene_to_group


    return species_to_gene_to_orthogroup

    #####################################################################

def get_gene_features_df_all_species(DataDir, species_to_gff, ProcessedDataDir, replace=False):

    """This function generates a df with the gene features of all genes from the gff. It includes GO terms and the .tab files from CGD. This is the source of the .tab files:
    
    - C. auris: I have the reference genome gDNA from GCA_002759435.2 and the mtDNA from ASM716870v1, from august 2020. I take the .tab file from a similar time: http://www.candidagenome.org/download/chromosomal_feature_files/C_auris_B8441/archive/C_auris_B8441_version_s01-m01-r11_chromosomal_feature.tab.gz. The mtDNA genes have no annotation.

    - C. glabrata: reference genome from CGD: the latest version by 12/03/2019, which is v_s02-m07-r35. I take the gene feats from http://www.candidagenome.org/download/chromosomal_feature_files/C_glabrata_CBS138/archive/C_glabrata_CBS138_version_s02-m07-r35_chromosomal_feature.tab.gz (same version). This table has the centromeres, which will not be considered.

    - C. albicans: I took the haplotypeA reference genome version from CGD 'A22-s07-m01-r110'. I take the corresponding annotations from http://www.candidagenome.org/download/chromosomal_feature_files/C_albicans_SC5314/archive/C_albicans_SC5314_version_A22-s07-m01-r110_chromosomal_feature.tab.gz


    - C. tropicalis: The reference genome is from NCBI for strain MYA-3404. It was taken in July 2020. I take the only annotation that there is at 28/06/2021: http://www.candidagenome.org/download/chromosomal_feature_files/C_tropicalis_MYA-3404/C_tropicalis_MYA-3404_chromosomal_feature.tab. There are a few missing genes because the assembly is much newer.

    - C. parapsilosis: I took the reference genome from NCBI in July 2020. I will use a .tab file from a similar time: http://www.candidagenome.org/download/chromosomal_feature_files/C_parapsilosis_CDC317/archive/C_parapsilosis_CDC317_version_s01-m03-r45_chromosomal_feature.tab.gz. There are a few missing genes, mostly because the mtDNAs are not the same.

    - C. orthopsilosis: I took the reference genome from NCBI in July 2020, mtDNA sepparate from gDNA. I take the only annotation: http://www.candidagenome.org/download/chromosomal_feature_files/C_orthopsilosis_Co_90-125/archive/C_orthopsilosis_Co_90-125_2013_12_11_chromosomal_feature.tab, which is very old.

    - C. tropicalis: There are no tab file
    """

    # define the file
    gene_features_df_file = "%s/all_species_gene_features_df.py"%ProcessedDataDir

    if file_is_empty(gene_features_df_file) or replace is True: 
        print("running get_gene_features_df_all_species")

        ########## ADD THE FEATURES .TAB FOR EACH SPECIES ###########

        # define the fields
        fields_geneFeatures_table = ["geneID", "gene_name", "aliases", "feature_type", "chromosome", "start", "end", "strand", "primary_CGDID", "secondary_CGDID", "description", "date", "sequence_coordinate_version_date", "blank1", "blank2", "date_geneName_reservation", "is_it_standardName", "Scerevisiae_orthologs"]

        # init df
        gene_features_df = pd.DataFrame()

        # go through each species
        for taxID, species in taxID_to_sciName.items():
            print(taxID, species)

            # load the gff (and keep only genes)
            gff_df = load_gff3_intoDF(species_to_gff[species])
            all_genes = set(gff_df[gff_df.feature.isin({"gene", "pseudogene"})].upmost_parent)
            gff_df = gff_df[gff_df.upmost_parent.isin(all_genes)]

            # species with .tab files
            if species in {"Candida_auris", "Candida_glabrata", "Candida_albicans", "Candida_tropicalis", "Candida_parapsilosis", "Candida_orthopsilosis"}:

                # load the chromosomal features df
                species_to_chromosomal_features_file = {"Candida_auris":"C_auris_B8441_version_s01-m01-r11_chromosomal_feature.tab", "Candida_glabrata":"C_glabrata_CBS138_version_s02-m07-r35_chromosomal_feature.tab", "Candida_albicans":"C_albicans_SC5314_version_A22-s07-m01-r110_chromosomal_feature.tab", "Candida_tropicalis":"C_tropicalis_MYA-3404_chromosomal_feature.tab", "Candida_parapsilosis":"C_parapsilosis_CDC317_version_s01-m03-r45_chromosomal_feature.tab", "Candida_orthopsilosis":"C_orthopsilosis_Co_90-125_2013_12_11_chromosomal_feature.tab"}

                TABANNOdir = "%s/annotation_files/%s"%(DataDir, species_to_chromosomal_features_file[species])
                gene_features_df_species = pd.read_csv(TABANNOdir, sep="\t", names=fields_geneFeatures_table, skiprows=list(range(len([line for line in open(TABANNOdir, "r") if line.startswith("!")]))))  

                # add fields
                gene_features_df_species["gene_and_geneID"] = "gene-" + gene_features_df_species.geneID

                # add the upmost parent as the index
                species_to_upmostParent_f = {"Candida_auris":"gene_and_geneID", "Candida_glabrata":"geneID", "Candida_albicans":"geneID", "Candida_tropicalis":"gene_and_geneID", "Candida_parapsilosis":"gene_and_geneID", "Candida_orthopsilosis":"gene_and_geneID"}
                gene_features_df_species.index = gene_features_df_species[species_to_upmostParent_f[species]]


            # species with no .tab files or debug
            elif species in {"Candida_metapsilosis"}: gene_features_df_species = pd.DataFrame(columns=fields_geneFeatures_table)
            else: raise ValueError("%s is not valid"%species)

            # print the numbers of missing genes
            all_genes_in_tabFile = set(gene_features_df_species.index)
            missing_genes = all_genes.difference(all_genes_in_tabFile)
            genes_in_tabFile_notInGff = all_genes_in_tabFile.difference(all_genes)

            #print("There are %i/%i genes from the gff not in the .tab file:\n\n%s"%(len(missing_genes), len(all_genes), missing_genes))
            #print("There are %i/%i genes from the .tab file not in the gff:\n\n%s"%(len(genes_in_tabFile_notInGff), len(all_genes_in_tabFile), genes_in_tabFile_notInGff))

            #print("There are %i/%i genes from the gff not in the .tab file"%(len(missing_genes), len(all_genes)))
            #print("There are %i/%i genes from the .tab file not in the gff"%(len(genes_in_tabFile_notInGff), len(all_genes_in_tabFile)))

            # keep genes that are in the gff
            gene_features_df_species = gene_features_df_species.loc[all_genes.intersection(all_genes_in_tabFile)]

            # add missing genes
            if len(missing_genes)>0:
                df_missing_genes = pd.DataFrame({f : {gene : np.nan for gene in sorted(missing_genes)} for f in fields_geneFeatures_table})
                gene_features_df_species = gene_features_df_species[fields_geneFeatures_table].append(df_missing_genes)

            # debugs
            if len(gene_features_df_species)!=len(set(gene_features_df_species.index)): raise ValueError("non unique index")
            if set(gene_features_df_species.index)!=all_genes: raise ValueError("not all genes considered")

            # print the finally missing genes and others
            print("There are %i/%i missing genes"%(sum(pd.isna(gene_features_df_species.geneID)), len(gene_features_df_species)))
            print("There are %i/%i genes with S. cerevisiae orthologs"%(sum(~pd.isna(gene_features_df_species.Scerevisiae_orthologs)), len(gene_features_df_species)))
            print("There are %i/%i genes with description"%(sum(~pd.isna(gene_features_df_species.description)), len(gene_features_df_species)))

            # add the gene length
            gene_features_df_species["gff_upmost_parent"] = gene_features_df_species.index

            upmost_parent_to_start = dict(gff_df.groupby("upmost_parent").apply(lambda df_s: df_s.start.values).apply(min))
            upmost_parent_to_end = dict(gff_df.groupby("upmost_parent").apply(lambda df_s: df_s.end.values).apply(max))

            gene_features_df_species["start"] = gene_features_df_species.gff_upmost_parent.map(upmost_parent_to_start)
            gene_features_df_species["end"] = gene_features_df_species.gff_upmost_parent.map(upmost_parent_to_end)

            if any(pd.isna(gene_features_df_species.start)): raise ValueError("there are nans in start")
            if any(pd.isna(gene_features_df_species.end)): raise ValueError("there are nans in end")

            gene_features_df_species["gene_length"] = gene_features_df_species.end - gene_features_df_species.start
            if any(gene_features_df_species.gene_length<=0): raise ValueError("there can't be negatives")

            # keep
            gene_features_df_species = gene_features_df_species.sort_values(by="gff_upmost_parent")
            gene_features_df_species["species"] = species
            gene_features_df = gene_features_df.append(gene_features_df_species).reset_index(drop=True)

        # keep some fields
        gene_features_df = gene_features_df[["species", "gff_upmost_parent", "gene_name", "aliases", "feature_type", "description", "Scerevisiae_orthologs", "start", "end", "gene_length", "chromosome"]]
        
        #############################################################

        ######### ADD EXTRA INFORMATION ###########

        # add the ortholog cluster for the proteins
        species_to_gene_to_orthofinder_orthologCluster = get_species_to_gene_to_orthofinder_orthologCluster(DataDir, ProcessedDataDir, threads=threads)

        def get_orthologCluster_from_dict(r):
            if r.gff_upmost_parent in species_to_gene_to_orthofinder_orthologCluster[r.species]: return species_to_gene_to_orthofinder_orthologCluster[r.species][r.gff_upmost_parent]
            else: return np.nan

        gene_features_df["orthofinder_orthocluster"] = gene_features_df[["species", "gff_upmost_parent"]].apply(get_orthologCluster_from_dict, axis=1)

        # print the number of genes with no orthoclusters
        for species in sorted_species_byPhylogeny:
            gene_features_df_s = gene_features_df[gene_features_df.species==species]
            genes_no_orthocluster = sum(pd.isna(gene_features_df_s.orthofinder_orthocluster))
            print("There are %i/%i genes not considered in orthofinder for %s"%(genes_no_orthocluster, len(gene_features_df_s), species))

        # add the GO terms
        df_GOterms = get_df_GOterms_all_species(DataDir, species_to_gff, ProcessedDataDir).reset_index(drop=True)
        gene_features_df = gene_features_df.merge(df_GOterms, on=["species", "gff_upmost_parent"], how="left", validate="one_to_one")
        if any(pd.isna(gene_features_df.GOterms)): raise ValueError("There are some nan go terms")

        ############################################
        
        # write
        print("writing")
        save_object(gene_features_df, gene_features_df_file)

    # load
    gene_features_df = load_object(gene_features_df_file)

    return gene_features_df




def run_augustus(genome, outfile_gff, replace, train_species="saccharomyces_cerevisiae_S288C"):

    """This function runs augustus for a .fasta genome"""

    # run augustus
    if file_is_empty(outfile_gff) or replace is True:
        print("generating %s"%outfile_gff)

        #### RUN AUGUSTUS ########

        # run augustus
        outfile_augustus = "%s.augustus_raw.gff3"%outfile_gff
        run_cmd("augustus --strand=both --genemodel=partial --singlestrand=true --outfile=%s --protein=on --progress=true --introns=on --start=on --stop=on --cds=on --codingseq=on --species=%s --gff3=on %s"%(outfile_augustus, train_species, genome), env="augustus_env")

        outfile_augustus_lines = "%s.gff_lines.gff3"%outfile_augustus
        run_cmd("egrep -v '^#' %s > %s"%(outfile_augustus, outfile_augustus_lines))

        ##########################

        ############ CURRECT GFF ########
        print("cleaning gff")

        # load into a df
        gff_fields = ["chromosome", "source", "type_feature", "start", "end", "score", "strand", "phase", "attributes"]
        df_augustus = pd.read_csv(outfile_augustus_lines, header=None, sep="\t", names=gff_fields)

        # rename the features and change them 
        type_feat_to_new_type_feat = {"transcript":"mRNA"}
        def change_type_feature(tf):

            if tf in type_feat_to_new_type_feat: return type_feat_to_new_type_feat[tf]
            else: return tf

        df_augustus["type_feature"] = df_augustus.type_feature.apply(change_type_feature)

        # delete the introns and stop codons
        df_augustus = df_augustus[~df_augustus.type_feature.isin({"intron"})]

        # write the gtf 
        raw_gff = "%s.raw_annotations.gff"%outfile_gff
        header_gff = "##gff-version 3"
        gff_lines = df_augustus[gff_fields].to_csv(sep="\t", header=False, index=False)
        open(raw_gff, "w").write(header_gff + "\n" + gff_lines)

        # write the final gff
        outfile_gff_tmp = "%s.tmp.gff"%outfile_gff
        remove_file(outfile_gff_tmp)
        run_cmd("gt dupfeat -dest exon -source CDS  %s | gt mergefeat | gt gff3 -retainids -sort -tidy -checkids -o %s"%(raw_gff, outfile_gff_tmp), env="genometools_env")

        #################################

        # clean and keep
        for f in [outfile_augustus, outfile_augustus_lines, raw_gff]: remove_file(f)
        os.rename(outfile_gff_tmp, outfile_gff)

def get_gff_has_mtDNA_annotations(gff_file, taxID):

    """returns whether the gff has some mtDNA annotations"""
    
    # load the mtDNA genes
    df_gff = load_gff3_intoDF(gff_file)
    all_genes = set(df_gff[df_gff.feature.isin({"gene", "pseudogene"})].upmost_parent)
    mtDNA_genes = df_gff[(df_gff.upmost_parent.isin(all_genes)) & (df_gff.chromosome==taxID_to_mtChromosome[taxID])]

    return len(mtDNA_genes)>0

def get_gff_with_mtDNA_annotations_throughAugustus(gff, taxID, genome, replace=False, threads=4):

    """This function rewrites the gff so that it has mtDNA annotated through augustus"""

    # check that the input DNA has no annotations
    if get_gff_has_mtDNA_annotations(gff, taxID) is True: raise ValueError("%s has mtDNA annotations"%gff)

    # generate a gff for the mtDNA
    output_gff = "%s.added_mtDNA_annotations.gff"%gff

    if file_is_empty(output_gff) or replace is True:
        print("running get_gff_with_mtDNA_annotations_throughAugustus")

        # get the mtDNA
        mtDNA_genome = "%s.mtDNA.%s.fasta"%(genome, taxID_to_mtChromosome[taxID])
        mtDNA_chroms = [seq for seq in SeqIO.parse(genome, "fasta") if seq.id==taxID_to_mtChromosome[taxID]]
        SeqIO.write(mtDNA_chroms, mtDNA_genome, "fasta")

        # run augustus on the mtDNA genome
        output_gff_mtDNA = "%s.augustus_mtDNA.gff"%output_gff
        run_augustus(mtDNA_genome, output_gff_mtDNA, replace, train_species="candida_albicans")

        # init output_gff tmp
        output_gff_tmp = "%s.tmp"%output_gff
        run_cmd("rsync %s %s"%(gff, output_gff_tmp))

        # add to annotations
        run_cmd("egrep -v '^#' %s >> %s"%(output_gff_mtDNA, output_gff_tmp))

        # clean and save
        remove_file(output_gff_mtDNA)
        os.rename(output_gff_tmp, output_gff)

    # check that there are annotations
    return output_gff

def generate_interproscan_annotation_from_gff(gff, genome, outdir, taxID, threads=4, replace=False):

    """This function generates Interproscan annotations for the gff provide"""

    ############ GENERATE A .fasta WITH THE PROTEIN-CODING TRANSCRIPTS #########    

    print("running generate_interproscan_annotation_from_gff")
    make_folder(outdir)

    # load gff
    gff_df = load_gff3_intoDF(gff)

    # keep only CDSs
    all_real_genes = set(gff_df[gff_df.feature=="gene"].upmost_parent)
    gff_df = gff_df[(gff_df.feature=="CDS") & (gff_df.upmost_parent.isin(all_real_genes))]
    all_protein_coding_genes = set(gff_df.upmost_parent)

    # define the parent as the mRNA id
    gff_df["mRNA_id"] = gff_df.ANNOTATION_Parent

    # check that each gene (upmost_parent) has only one mRNA id
    upmost_parent_to_n_mRNAs = gff_df.groupby("upmost_parent").apply(lambda df_p: set(df_p.mRNA_id)).apply(len)
    if any(upmost_parent_to_n_mRNAs>1): raise ValueError("There are some upmost parents that have more than 1 mRNA:\n %s"%upmost_parent_to_n_mRNAs[upmost_parent_to_n_mRNAs>1]) 

    # check that each mRNA corresponds to one same upmost_parent
    mRNA_to_nUpmostParents = gff_df.groupby("mRNA_id").apply(lambda df_r: set(df_r.upmost_parent)).apply(len)
    if any(mRNA_to_nUpmostParents>1): raise ValueError("There are some mRNAs that have more than 1 upmost_parent:\n %s"%mRNA_to_nUpmostParents[mRNA_to_nUpmostParents>1]) 

    # add fields to the gff
    gff_df["numeric_row_idx"] = list(range(len(gff_df)))
    gff_df["annotation"] = "ID=" + gff_df.upmost_parent + "_cds" + gff_df.numeric_row_idx.apply(str) + ";Parent=" + gff_df.upmost_parent
    gff_fields = ["chromosome", "source", "feature", "start", "end", "blank1", "strand", "blank2", "annotation"]

    # write a gff with the CDSs
    cds_gff = "%s/cds_annotations.gff"%outdir
    header_gff = "##gff-version 3"
    gff_lines = gff_df[gff_fields].to_csv(sep="\t", header=False, index=False)
    open(cds_gff, "w").write(header_gff + "\n" + gff_lines)

    # get the transcripts
    spliced_transcripts = "%s.transcripts.fasta"%cds_gff
    run_cmd("gffread %s -g %s -x %s"%(cds_gff, genome, spliced_transcripts)) # -x has to be after -g

    # check that there is one transcript for each gene
    n_transcripts = int("".join([x for x in str(subprocess.check_output("egrep '^>' %s | wc -l"%spliced_transcripts, shell=True)) if x.isdigit()]))
    n_proteins = len(all_protein_coding_genes)
    if n_proteins!=n_transcripts: raise ValueError("there are %i prots and %i mRNAs"%(n_proteins, n_transcripts))

    # translate according to each code
    print("writing proteins")
    geneID_to_seq = {seq.id:seq for seq in SeqIO.parse(spliced_transcripts, "fasta")}

    mtDNA_chroms = {taxID_to_mtChromosome[taxID]}
    gDNA_chroms = set(gff_df.chromosome).difference(mtDNA_chroms)

    all_proteins = []
    for type_genome, chroms, translation_code in [("mtDNA", mtDNA_chroms, taxID_to_mDNA_code[taxID]), ("gDNA", gDNA_chroms, taxID_to_gDNA_code[taxID])]:

        # get the genes for this type of genome
        geneIDs = set(gff_df[gff_df.chromosome.isin(chroms)].upmost_parent)
        if len(geneIDs)==0: raise ValueError("There are no chroms in geneIDs")

        # init the number of strange prots
        n_strange_prots = 0

        # get the proteins
        for geneID in geneIDs:

            # define the protein seq (removing the last asterisk)
            protein_seq = str(geneID_to_seq[geneID].translate(table=translation_code).seq).rstrip("*")

            # check the sequence
            if ("*" in protein_seq) or ("X" in protein_seq): n_strange_prots+=1

            # replace the asterisks with 'X'
            protein_seq = protein_seq.replace("*", "X")

            # keep
            all_proteins.append(SeqRecord(Seq(protein_seq), id=geneID, name="", description=""))

        print("There are %i/%i proteins with bad characters in %s"%(n_strange_prots, len(geneIDs), type_genome))

    # write 
    proteins_fasta = "%s/proteins.fasta"%outdir
    SeqIO.write(all_proteins, proteins_fasta, "fasta")

    ############################################################################

    ######### RUN INTERPROSCAN TO ANNOTATE THE PROTEINS #######

    # define the outfile
    outfile_interpro = "%s/interproscan_annotation.out"%outdir
    if file_is_empty(outfile_interpro) or replace is True:
        print("running interproscan")

        # run interproscan
        outfile_interpro_tmp = "%s.tmp"%outfile_interpro
        tempdir = "%s/tmp_interproscan"%outdir; delete_folder(tempdir)
        interproscan_sh = "%s/software/interproscan_v5.52-86.0/interproscan-5.52-86.0/interproscan.sh"%ParentDir
        interproscan_libdir = "/gpfs/projects/bsc40/mschikora/anaconda3/envs/InterProScan_env/lib" # this needs to be added

        run_cmd("export LD_LIBRARY_PATH=%s:$LD_LIBRARY_PATH && %s -i %s -f tsv -goterms -appl Pfam,ProSitePatterns,ProSiteProfiles,PANTHER,TIGRFAM,SFLD,SUPERFAMILY,Gene3D,Hamap,Coils,SMART,CDD,PRINTS,PIRSR,MobiDBLite,PIRSF --cpu %i -o %s --pathways --seqtype p --tempdir %s"%(interproscan_libdir, interproscan_sh, proteins_fasta, threads, outfile_interpro_tmp, tempdir), env="InterProScan_env")

        delete_folder(tempdir)
        os.rename(outfile_interpro_tmp, outfile_interpro)

    ###########################################################


def plot_SNPdistance_vs_fractionWindowsWithLowDivergence(df_pairwise_SNP_distances_perWindow_summaryStats, PlotsDir, metadata_df, species_to_ref_genome, ProcessedDataDir, replace=False):

    """Plots the pairwise SNP distances vs fraction of windows with low diversity. There will be colors for same clade. cladeID_Tree_and_BranchLen""" 

    ######### DEFINE DATA TO PLOT #######

    df_plot_file = "%s/df_pairwise_SNP_distances_perWindow_summaryStats_file_df_plot.py"%ProcessedDataDir
    if file_is_empty(df_plot_file) or replace is True:

        # keep only df with different samples compared
        df_pairwise_SNP_distances_perWindow_summaryStats = df_pairwise_SNP_distances_perWindow_summaryStats[df_pairwise_SNP_distances_perWindow_summaryStats.query_sampleID!=df_pairwise_SNP_distances_perWindow_summaryStats.target_sampleID]

        # change the data
        metadata_df  = cp.deepcopy(metadata_df)
        metadata_df["sampleID"] = metadata_df.sampleID.apply(int)

        # define the df plot
        df_plot = pd.DataFrame()

        for species in sorted_species_byPhylogeny:
            print(species)

            # get the df of this species
            df_pairwise_SNP_distances = df_pairwise_SNP_distances_perWindow_summaryStats[df_pairwise_SNP_distances_perWindow_summaryStats.species==species]

            # keep only unique comparisons
            df_pairwise_SNP_distances["sorted_samples_tuple"] = df_pairwise_SNP_distances[["query_sampleID", "target_sampleID"]].apply(sorted, axis=1).apply(tuple)

            df_pairwise_SNP_distances = df_pairwise_SNP_distances.drop_duplicates(subset="sorted_samples_tuple")

            # add the vars per kb
            len_genome = sum(get_chr_to_len(species_to_ref_genome[species]).values())
            df_pairwise_SNP_distances["vars / kb"] = (df_pairwise_SNP_distances.nvars/len_genome)*1000

            # add the cladeID (only if both target and query are from the same clade)
            print("adding cladeID")
            sampleID_to_cladeID = dict(metadata_df[metadata_df.species_name==species].set_index("sampleID").cladeID_Tree_and_BranchLen)
            sorted_clades = sorted([c for c in set(sampleID_to_cladeID.values()) if not pd.isna(c)])
            df_pairwise_SNP_distances["query_sample_cladeID"] = df_pairwise_SNP_distances.query_sampleID.map(sampleID_to_cladeID)
            df_pairwise_SNP_distances["target_sample_cladeID"] = df_pairwise_SNP_distances.target_sampleID.map(sampleID_to_cladeID)

            def get_cladeID_both_Samples(r):
                if r.query_sample_cladeID==r.target_sample_cladeID: return r.query_sample_cladeID
                else: return np.nan

            df_pairwise_SNP_distances["cladeID_both"] = df_pairwise_SNP_distances[["query_sample_cladeID", "target_sample_cladeID"]].apply(get_cladeID_both_Samples, axis=1)


            df_plot = df_plot.append(df_pairwise_SNP_distances)

        # save
        save_object(df_plot, df_plot_file)

    df_plot = load_object(df_plot_file)

    #####################################

    ######### FIGURE ###########
    plots_dir = "%s/SNP_distance_vs_fractionGenome_lowSNPdistance"%PlotsDir; make_folder(plots_dir)

    for Is, species in enumerate(sorted_species_byPhylogeny):
        print(species)

        # init fig
        fig = plt.figure(figsize=(5, 5))
        #if species!="Candida_metapsilosis": continue

        # get thd df
        df_s = df_plot[df_plot.species==species]

        # get the graphics 
        sorted_clades = [str(c) for c in sorted(set(df_s[~pd.isna(df_s.cladeID_both)].cladeID_both.apply(int)))]
        if len(sorted_clades)<=10: palette="tab10"
        else: palette="tab20"

        cladeID_to_color = get_value_to_color(sorted_clades, palette=palette, n=len(sorted_clades), type_color="hex")[0]
        markers = "^ovs<P>HD"*1000
        cladeID_to_marker = dict(zip(sorted_clades, markers[:len(sorted_clades)]))

        # redefine the cladeID_both
        df_s = df_s.sort_values(by="cladeID_both")

        def get_nans_to_btwClades(x):
            if pd.isna(x): return "btw clades"
            else: return int(x)

        df_s["cladeID_both"] = df_s.cladeID_both.apply(get_nans_to_btwClades).apply(str)
        cladeID_to_color["btw clades"] = "gray"
        cladeID_to_marker["btw clades"] = "o"

        # make plot
        df_s["fraction genome <1SNP/kb"] = df_s["fraction_genome_pairwiseSNPdensity_below_0.001"]
        df_s["median vars/kb"] = df_s.median_vars_per_bp*1000
        df_s["mean vars/kb"] = df_s.mean_vars_per_bp*1000
        df_s["mode vars/kb"] = df_s.mode_vars_per_bp*1000

        xfield = "mode vars/kb"
        yfield = "fraction genome <1SNP/kb"

        ax = sns.scatterplot(data=df_s, x=xfield, y=yfield, hue="cladeID_both", palette=cladeID_to_color, style="cladeID_both", alpha=0.7, markers=cladeID_to_marker)

        plt.axvline(1, linestyle="--", color="k")
        plt.axhline(0.5, linestyle="--", color="k")

        #max_val = max([max(df_s[xfield]), max(df_s[yfield])])
        #plt.plot([0, max_val], [0, max_val], linestyle="--", color="k", lw=.7)

        # title
        ax.set_title("C. %s"%(species.split("_")[1]))
        ax.legend(loc=[1.03, 0])

        plt.show()

        # ticks
        #if species!=(sorted_species_byPhylogeny[-1]): ax.set_xlabel("")

        # save
        filename = "%s/%s.pdf"%(plots_dir, species)
        fig.savefig(filename, bbox_inches="tight")


    ############################

def get_unique_positions_df_from_small_vars_df(small_vars):

    """Takes a small_vars df and returns a df with the positions (in bed style, 0-based and including all the affected position) of the affected vars"""

    # keep non-redundant vars
    small_vars = small_vars[["#CHROM", "POS", "REF", "ALT"]].drop_duplicates()

    # print the strange characters
    acgt = {"A", "C", "G", "T"}
    strange_chars_REF = set.union(*small_vars.REF.apply(set)).difference(acgt)
    strange_chars_ALT = set.union(*small_vars.ALT.apply(set)).difference(acgt)

    if len(strange_chars_REF)>0: print("These are the strange REF characters:%s"%strange_chars_REF)
    if len(strange_chars_ALT)>0: print("These are the strange ALT characters:%s"%strange_chars_ALT)

    # the length of the var define the length of the vars
    small_vars["len_var"] = small_vars.REF.apply(len)

    # define bed-like vars
    small_vars["start"] = small_vars.POS-1
    small_vars["end"] = small_vars.start + small_vars.len_var

    # keep positions
    bed_fields = ["#CHROM", "start", "end"]
    pos_df = small_vars[bed_fields].sort_values(by=bed_fields).drop_duplicates()

    return pos_df

def get_snps_df_outside_targetPositions(snps_df, wrong_positions_df, tmpdir):

    """This function keeps only the small vars that are not in wrong_positions_df """

    # make files
    make_folder(tmpdir)

    positions_bed = "%s/wrong_positions.bed"%tmpdir
    wrong_positions_df[["#CHROM", "start", "end"]].sort_values(by=["#CHROM", "start", "end"]).to_csv(positions_bed, sep="\t", header=False, index=False)

    unique_vars_bed = "%s/unique_vars.bed"%tmpdir
    snps_df["start"] = snps_df.POS-1
    bed_fields = ["#CHROM", "start", "POS", "#Uploaded_variation"]
    snps_df[bed_fields].sort_values(by=bed_fields).drop_duplicates().to_csv(unique_vars_bed, sep="\t", header=False, index=False)

    # run bedmap to find variants in  positions_bed overlapped by unique_vars_bed
    bedmap_outfile = "%s/vars_overlapping_wrong_positions.bed"%tmpdir
    run_cmd("bedmap --range 0 --echo-map-id --delim '\t' %s %s > %s"%(positions_bed, unique_vars_bed, bedmap_outfile), env="perSVade_env")

    wrong_vars = set.union(*pd.read_csv(bedmap_outfile, sep="\t", header=None, names=["wrong_var"]).wrong_var.apply(lambda x: x.split(";")).apply(set))

    snps_df = snps_df[~snps_df["#Uploaded_variation"].isin(wrong_vars)]

    # clean
    delete_folder(tmpdir)

    return snps_df

def generate_haploid_snps_df_uniFormPositions_file(small_vars_filt_file, df_sorted_bams, haploid_snps_df_uniFormPositions_file, min_coverage_pos, outdir, ploidy, threads=4):

    """This function generates a df with snps from the filtered small vars, only keeping positions that have at least >min_coverage_pos coverage, have no indels nor heterozygous SNPs. """

    if file_is_empty(haploid_snps_df_uniFormPositions_file):
        make_folder(outdir)
        print("generating %s on %i threads"%(haploid_snps_df_uniFormPositions_file, threads))

        # define all the samples
        all_samples = set(df_sorted_bams.sampleID)

        # load the variants df (already filtered)
        print("loading variants")
        small_vars = load_object(small_vars_filt_file)

        # add some fields
        small_vars["common_GT"] = small_vars.common_GT.apply(str)

        def get_upper(x): return x.upper()
        small_vars["REF"] = small_vars.REF.apply(get_upper)
        small_vars["ALT"] = small_vars.ALT.apply(get_upper)
        small_vars["sampleID"] = small_vars.sampleID.apply(int)

        # remove variants that are not in all samples
        small_vars = small_vars[small_vars.sampleID.isin(all_samples)]

        # initialize the snps_df that are homozygous and of the interesting ploiyd, and ACGT
        acgt = {"A", "C", "G", "T"}
        haploid_snps = small_vars[(small_vars.common_GT.isin({"1/1", "1"})) & (small_vars.ISSNP) & (small_vars.REF.isin(acgt)) & (small_vars.ALT.isin(acgt)) & (small_vars.calling_ploidy==ploidy)]

        ntotal_snps = len(set(haploid_snps["#Uploaded_variation"]))
        print("There are %i total haploid or homozygous SNPs"%ntotal_snps)

        # filter out SNPs that are in positions with some INDEL in some sample
        positions_INDELs = get_unique_positions_df_from_small_vars_df(small_vars[(small_vars.calling_ploidy==ploidy) & ~(small_vars.ISSNP)])
        haploid_snps = get_snps_df_outside_targetPositions(haploid_snps, positions_INDELs, "%s/removing_positionsWithINDELS"%outdir)

        print("There are %i/%i remaining after filtering out positions with INDELs"%(len(set(haploid_snps["#Uploaded_variation"])), ntotal_snps))

        # filter out SNPs that are in positions with some heterozygous snp in some sample
        positions_df_heteroSNPs = get_unique_positions_df_from_small_vars_df(small_vars[~(small_vars.common_GT.isin({"1/1", "1"})) & (small_vars.calling_ploidy==2) & (small_vars.ISSNP)])
        haploid_snps = get_snps_df_outside_targetPositions(haploid_snps, positions_df_heteroSNPs, "%s/removing_positionsWithHetSNPs"%outdir)

        print("There are %i/%i remaining after filtering out positions with het SNPs"%(len(set(haploid_snps["#Uploaded_variation"])), ntotal_snps))

        # filter out SNPs that are in positions not covered in all samples
        haploid_snps = get_small_vars_with_atLeastSomeCoverageInAllPositions(haploid_snps, outdir, df_sorted_bams, min_coverage_pos, threads=threads)

        print("There are %i/%i remaining after filtering out positions with low coverage in some samples"%(len(set(haploid_snps["#Uploaded_variation"])), ntotal_snps))

        # save
        save_object(haploid_snps, haploid_snps_df_uniFormPositions_file)

        del haploid_snps
        del small_vars


def generate_homo_and_hetero_snps_df_correctPositions(small_vars_filt_file, df_sorted_bams, homo_and_hetero_snps_df_correctPositions_file, min_coverage_pos, outdir, threads=4):

    """This function generates a df with diploid snps from the filtered small vars, only keeping positions that have at least >min_coverage_pos coverage, have no indels. """

    if file_is_empty(homo_and_hetero_snps_df_correctPositions_file):
        make_folder(outdir)
        print("generating %s on %i threads"%(homo_and_hetero_snps_df_correctPositions_file, threads))

        # define all the samples
        all_samples = set(df_sorted_bams.sampleID)

        # load the variants df (already filtered)
        print("loading variants")
        small_vars = load_object(small_vars_filt_file)

        # add some fields
        small_vars["common_GT"] = small_vars.common_GT.apply(str)

        def get_upper(x): return x.upper()
        small_vars["REF"] = small_vars.REF.apply(get_upper)
        small_vars["ALT"] = small_vars.ALT.apply(get_upper)
        small_vars["sampleID"] = small_vars.sampleID.apply(int)

        # remove variants that are not in all samples
        small_vars = small_vars[small_vars.sampleID.isin(all_samples)]

        # initialize the snps_df that are diploid 2 and of the interesting ploiyd, and ACGT
        acgt = {"A", "C", "G", "T"}
        diploid_snps = small_vars[(small_vars.common_GT.isin({"1/1", "0/1"})) & (small_vars.ISSNP) & (small_vars.REF.isin(acgt)) & (small_vars.ALT.isin(acgt)) & (small_vars.calling_ploidy==2)]

        ntotal_snps = len(set(diploid_snps["#Uploaded_variation"]))
        print("There are %i total diploid SNPs"%ntotal_snps)

        # filter out SNPs that are in positions with some INDEL in some sample
        positions_INDELs = get_unique_positions_df_from_small_vars_df(small_vars[(small_vars.calling_ploidy==2) & ~(small_vars.ISSNP)])
        diploid_snps = get_snps_df_outside_targetPositions(diploid_snps, positions_INDELs, "%s/removing_positionsWithINDELS"%outdir)

        print("There are %i/%i remaining after filtering out positions with INDELs"%(len(set(diploid_snps["#Uploaded_variation"])), ntotal_snps))

        # filter out SNPs that are in positions not covered in all samples
        diploid_snps = get_small_vars_with_atLeastSomeCoverageInAllPositions(diploid_snps, outdir, df_sorted_bams, min_coverage_pos, threads=threads)

        print("There are %i/%i remaining after filtering out positions with low coverage in some samples"%(len(set(diploid_snps["#Uploaded_variation"])), ntotal_snps))

        # save
        save_object(diploid_snps, homo_and_hetero_snps_df_correctPositions_file)

        del diploid_snps
        del small_vars


def generate_bestMLtree_withBootstrap_from_resampledTrees(trees_list, outdir):

    """Takes the best tree from the tree_list (the one with the lowest BIC score) as a reference, it uses all the trees to calculate bootstrap supports. Minimizing BIC is the default iqtree behavior, and also what is done in phylome DB to pick the best tree."""

    # define files
    all_trees_file = "%s/merged_trees_file.txt"%outdir
    supertree_file = "%s/tree_lowestBIC_withBootstraps.nw"%outdir
    bestTree_file = "%s/tree_lowestBIC.nw"%outdir

    if file_is_empty(supertree_file):

        # clean previous files
        for f in [bestTree_file, all_trees_file]: remove_file(f)

        # define the reference tree as the one with the lowest BIC
        def get_BIC_for_t(t): 
            iqtree_file = ".".join(t.split(".")[:-1]) + ".iqtree"
            BIC_measurements = [float(l.strip().split(": ")[1]) for l in open(iqtree_file, "r").readlines() if l.startswith("Bayesian information criterion (BIC) score:")]
            if len(BIC_measurements)!=1: raise ValueError("there can't be more than one BIC")
            return BIC_measurements[0]

        tree_to_BIC = pd.Series({t : get_BIC_for_t(t) for t in trees_list}).sort_values()
        origin_bestTree_file = tree_to_BIC.index[0]

        # get to the outdir
        rsync_file(origin_bestTree_file, bestTree_file)

        # get the trees into a file concatenated 
        print("getting individual trees in a file")
        run_cmd("cat '%s' > %s"%(trees_list[0], all_trees_file))
        for Itree, t in enumerate(trees_list[1:]): 
            print("adding tree %i/%i"%(Itree+2, len(trees_list)))
            run_cmd("cat '%s' >> %s"%(t, all_trees_file))

        # add the bootstrap to the lowest tree
        print("adding bootstrap to the best tree")
        run_cmd("iqtree -sup %s %s"%(bestTree_file, all_trees_file))

        # clean
        for f in [bestTree_file, all_trees_file, "%s.log"%all_trees_file]: remove_file(f)

        # keep the supertree 
        os.rename("%s.suptree"%all_trees_file, supertree_file)




def generate_consensus_withBootstrap_from_resampledTrees(trees_list, outdir, replace=False):

    """Takes the consensus tree from the tree_list it uses all the trees to calculate bootstrap supports and branch lengths"""

    ######## GET ALL THE TREES ROOTED AND ADDED TO A SINGLE FILE #########

    all_trees_file = "%s/all_merged_trees_file.txt"%outdir

    if file_is_empty(all_trees_file) or replace is True:
        print("getting individual trees in a file")

        # init tmp
        all_trees_file_tmp = "%s.tmp"%all_trees_file
        remove_file(all_trees_file_tmp)
        run_cmd("touch %s"%all_trees_file_tmp)

        for Itree, treefile in enumerate(trees_list): 
            print("adding tree %i"%(Itree+1))

            # get the rooted tree
            tree = Tree(treefile)
            tree.set_outgroup(tree.get_midpoint_outgroup())

            # check that all nodes have 2 children
            for n in tree.traverse():
                if not n.is_leaf() and len(n.get_children())!=2: raise ValueError("There are some collapsed branches in the tree")

            # add to the file with all trees
            open(all_trees_file_tmp, "a").write(tree.write(format=2)+"\n")

        os.rename(all_trees_file_tmp, all_trees_file)

    ######################################################################


    ######### GET CONSENSUS TREE WITH SUPPORT ############

    # define files
    consensus_treefile_withBL = "%s/tree_consensus_withBootstraps_and_branchLengths.nw"%outdir
    consensus_treefile = "%s.contree"%all_trees_file

    if file_is_empty(consensus_treefile_withBL) or replace is True:
        print("generating consensus tree")

        # define tmps
        consensus_treefile_withBL_tmp = "%s.tmp.nw"%consensus_treefile_withBL

        # get consensus tree (by default the --sup-min is 0, meaning that it is an extended consensus)
        run_cmd("iqtree -con -t %s --sup-min 0.0"%all_trees_file)

        # add the branch lengths
        get_consensus_tree_with_branchLengths_R = "%s/CandidaMine_data_generation/v1/get_consensus_tree_with_branchLengths.R"%(ParentDir)
        run_cmd("%s --input_consensus_treefile %s --output_consensus_treefile %s --all_trees_file %s"%(get_consensus_tree_with_branchLengths_R, consensus_treefile, consensus_treefile_withBL_tmp, all_trees_file), env="phylotools_R_env")

        # check that the trees are equal
        #consensus_tree_withBL = Tree(consensus_treefile_withBL_tmp)
        #ts = TreeStyle(); ts.show_branch_support = True
        #consensus_tree_withBL.show(tree_style=ts)

        #rf_distance = consensus_tree.robinson_foulds(consensus_tree_withBL)[0]
        #if rf_distance!=0: raise ValueError("The get_consensus_tree_with_branchLengths_R changed the topology of the tree")

        # keep
        remove_file(consensus_treefile)
        os.rename(consensus_treefile_withBL_tmp, consensus_treefile_withBL)

def get_multifasta_onlyVariableSites_snpSites(multifasta):

    """This function gets positions withput SNPs from MSA"""

    # define final file
    multifasta_onlyVariableSites = "%s.onlyVariableSites.fasta"%multifasta

    if file_is_empty(multifasta_onlyVariableSites):

        # get the multifasta
        multifasta_onlyVariableSites_tmp = "%s.tmp"%multifasta_onlyVariableSites
        run_cmd("snp-sites -m -o %s %s"%(multifasta_onlyVariableSites_tmp, multifasta))

        os.rename(multifasta_onlyVariableSites_tmp, multifasta_onlyVariableSites)

    return multifasta_onlyVariableSites

def get_representative_leafNames_from_tree(tree_node, nrepresentative_samples):

    """Takes a tree node and gets nrepresentative_samples"""

    # get a matrix with pairwise distances
    sorted_samples = cp.deepcopy(sorted(tree_node.get_leaf_names()))

    # if there are few samples return them all 
    if len(sorted_samples)<=nrepresentative_samples: return sorted_samples

    # calculate the distances df
    def get_distances_nodes(tree_node, sA, sB):
        if sA==sB: return 0.0
        else: return float(tree_node.get_distance(sA, sB))

    distances_df = pd.DataFrame(dict(zip(sorted_samples, map(lambda sA: dict(zip(sorted_samples, map(lambda sB: get_distances_nodes(tree_node, sA, sB), sorted_samples))), sorted_samples)))).loc[sorted_samples, sorted_samples]

    max_distance = max(get_uniqueVals_df(distances_df))
    distances_df = distances_df/max_distance

    # run MDS and create a df
    embedding = MDS_sklearn(n_components=2, dissimilarity='precomputed', random_state=1) 
    distance_array_transformed = embedding.fit_transform(distances_df.values)
    df_MDS = pd.DataFrame(data=distance_array_transformed, index=sorted_samples, columns=['distance_MDS_axis1', 'distance_MDS_axis2'])
    df_MDS["sampleID"] = df_MDS.index
    df_MDS = df_MDS.sort_values(by=["distance_MDS_axis1", "sampleID"])

    # select the samples that are lineraly distribured across the first component
    ideal_MDS_values = np.linspace(min(df_MDS.distance_MDS_axis1), max(df_MDS.distance_MDS_axis1), nrepresentative_samples)

    representative_samples = set()
    for ideal_MDS_value in ideal_MDS_values:

        # get a df that doesn't have the representative samples and has the closest value
        df_MDS_noReps = df_MDS[~df_MDS.sampleID.isin(representative_samples)]
        df_nearest = df_MDS_noReps[df_MDS_noReps.distance_MDS_axis1==find_nearest(df_MDS_noReps.distance_MDS_axis1, ideal_MDS_value)]

        representative_samples.add(df_nearest.iloc[0].sampleID)

    if len(representative_samples)!=nrepresentative_samples: raise ValueError("There are too few rep samples")
    
    return sorted(representative_samples)

def get_clade_to_representative_samples_from_tree(tree, input_sampleID_to_cladeID, threads, nrepresentative_samples=5):

    """Takes a tree object and  a sample to clade ID, returning a dict mapping each clade to the representative samples"""

    print("getting representative elements of clade")

    # make the samples strings
    sampleID_to_cladeID = {str(s):c for s,c in input_sampleID_to_cladeID.items()}

    # add the cladeID
    tree = cp.deepcopy(tree)
    for l in tree.get_leaves(): 
        cladeID = sampleID_to_cladeID[l.name]
        if pd.isna(cladeID): raise ValueError("There can't be nans in clade")
        l.cladeID = cladeID

    # checks
    all_clades = set(sampleID_to_cladeID.values())
    all_clades_inTree = {l.cladeID for l in tree.get_leaves()}
    if all_clades!=all_clades_inTree: raise ValueError("There are some unassigned clades: %s vs %s"%(sorted(all_clades), sorted(all_clades_inTree)))

    # define the inputs of a function that will take a node with a cladeID and return the representative clades
    sorted_clades = sorted(all_clades)

    def get_just_one_element_of_list(list_obj):
        if len(list_obj)!=1: 

            for x in list_obj: print(x)
            raise ValueError("there is not only element")
        return list_obj[0]

    inputs_fn = [(get_just_one_element_of_list(list(tree.get_monophyletic(values=[cladeID], target_attr="cladeID"))), nrepresentative_samples) for cladeID in sorted_clades]

    # get the representative samples in parallel
    with  multiproc.Pool(threads) as pool:

        list_lists_repsamples = pool.starmap(get_representative_leafNames_from_tree, inputs_fn)

        pool.close()
        pool.terminate()

    cladeID_to_repSamples = dict(zip(sorted_clades, list_lists_repsamples))

    # change to ints if input samples are ints
    if type(next(iter(input_sampleID_to_cladeID.keys())))==int: cladeID_to_repSamples = {c : sorted(map(int, samples)) for c, samples in cladeID_to_repSamples.items()}

    return cladeID_to_repSamples


def get_cmds_fastGEAR_predefined_clades(outdir, chrom_to_multifasta, metadata_df, sciName, reference_genome, ploidy, threads, taxID_dir, running_resamples=False):

    """Generates a list of cmds for running fastgear into some outdir with predefined clades according to metadata_df. According to fastGEAR's manual, When defining lineages manually, the largest lineage should have the largest cluster label. For example, if there are three lineages of sizes 15,100,30, then the cluster labels should be 1,3,2, respectively. I will create a file that maps the sampleID (numeric), cladeID_Tree_and_BranchLen, and fastGEAR_cladeID. One cmd will be created for each chromosome. """


    ######### PREPARE CLADES FILE ############

    #print("preparing fastGEAR cmds")

    # make a file that maps each sample to the fastGEAR clade ID

    # make the outdir
    make_folder(outdir)

    cladeIDs_mapping_file = "%s/cladeIDs_mapping.tab"%outdir
    if file_is_empty(cladeIDs_mapping_file):

        # check
        if len(metadata_df)!=len(set(metadata_df.sampleID)): raise ValueError("sampleID is not unique")

        # redefine the sampleID
        metadata_df["sampleID"] = metadata_df.sampleID.apply(int)

        # define a metadata_df that has all the samples 
        metadata_df = metadata_df[["cladeID_Tree_and_BranchLen", "sampleID"]].reset_index(drop=True)

        """
        #(also the 'bad' ones)
        for bad_sampleID in sciName_to_badSamples[sciName]: 
            metadata_df = metadata_df.append(pd.DataFrame({len(metadata_df): {"cladeID_Tree_and_BranchLen":np.nan, "sampleID":bad_sampleID}}).transpose())
        """

        # redefine the cladeID_Tree_and_BranchLen so that each 'NaN' clade ID gets a new extra clade
        nclades = int(max(metadata_df[~pd.isna(metadata_df.cladeID_Tree_and_BranchLen)].cladeID_Tree_and_BranchLen))
        extra_clade = 0
        list_cladeID_Tree_and_BranchLen_all = []
        for cID in metadata_df.cladeID_Tree_and_BranchLen:
            if not pd.isna(cID): new_cID = int(cID)                
            else: 
                extra_clade += 1
                new_cID = nclades + extra_clade

            list_cladeID_Tree_and_BranchLen_all.append(new_cID)

        metadata_df["cladeID_Tree_and_BranchLen_all"] = list_cladeID_Tree_and_BranchLen_all

        # add the number of samples with each clade ID
        cladeID_to_nsamples = dict(metadata_df.groupby("cladeID_Tree_and_BranchLen_all").apply(len))
        metadata_df["nsamples_with_cladeID"] = metadata_df.cladeID_Tree_and_BranchLen_all.apply(lambda x: cladeID_to_nsamples[x])

        # add the fastGEAR cladeID according to the nsamples
        metadata_df = metadata_df.sort_values(by=["nsamples_with_cladeID", "cladeID_Tree_and_BranchLen_all", "sampleID"])
        sorted_cladeIDs = list(metadata_df.cladeID_Tree_and_BranchLen_all.unique())

        cladeID_to_fastGEAR_cladeID = dict(zip(sorted_cladeIDs, range(1, len(sorted_cladeIDs)+1)))
        metadata_df["fastGEAR_cladeID"] = metadata_df.cladeID_Tree_and_BranchLen_all.apply(lambda x: cladeID_to_fastGEAR_cladeID[x])

        # write a file with the ID mapping
        metadata_df[["sampleID", "cladeID_Tree_and_BranchLen", "nsamples_with_cladeID", "fastGEAR_cladeID"]].to_csv(cladeIDs_mapping_file, sep="\t", index=False, header=True)

    # load df and map each sampleID to the fastGEAR cladeID
    df_IDmapping = get_tab_as_df_or_empty_df(cladeIDs_mapping_file)
    df_IDmapping["sampleID"] = df_IDmapping.sampleID.apply(int)
    df_IDmapping["fastGEAR_cladeID"] = df_IDmapping.fastGEAR_cladeID.apply(int)

    # define mapings
    sampleID_to_fastGEAR_cladeID = dict(df_IDmapping.set_index("sampleID")["fastGEAR_cladeID"])
    FGcladeID_to_samples = dict(df_IDmapping.groupby("fastGEAR_cladeID").apply(lambda df_c: sorted(set(df_c.sampleID))))
    all_FGclades = set(FGcladeID_to_samples.keys())

    ##########################################

    ######### PREPARE FASTGEAR CMDS #####

    # define the file that maps rep clades (it should be in the 'resample_X' structure)
    FGcladeID_to_representativeSamples_file = "%s/FGcladeID_to_representativeSamples.py"%outdir
    resample1_FGcladeID_to_representativeSamples_file = "%s/../resample_1/FGcladeID_to_representativeSamples.py"%outdir

    # redefine the FGcladeID_to_representativeSamples_file
    if running_resamples is True and not file_is_empty(resample1_FGcladeID_to_representativeSamples_file): FGcladeID_to_representativeSamples_file = resample1_FGcladeID_to_representativeSamples_file

    # get representative samples and save them
    if file_is_empty(FGcladeID_to_representativeSamples_file):

        # load tree
        if ploidy==1: tree = get_correct_tree_midpointRooted("%s/generate_tree_from_SNPs/iqtree_unroted.treefile"%(taxID_dir))
        elif ploidy==2: tree = Tree("%s/generate_tree_from_SNPs_resamplingHetSNPs/tree_consensus_withBootstraps_and_branchLengths.nw"%(taxID_dir))

        # remove samples that are bad
        print("removing bad samples: %s"%sciName_to_badSamples[sciName])
        samples_to_keep = {sampleID for sampleID in tree.get_leaf_names() if not int(sampleID) in sciName_to_badSamples[sciName]}
        tree.prune(samples_to_keep, preserve_branch_length=True)

        # get the representatives
        FGcladeID_to_representativeSamples = get_clade_to_representative_samples_from_tree(tree, sampleID_to_fastGEAR_cladeID, threads)
        save_object(FGcladeID_to_representativeSamples, FGcladeID_to_representativeSamples_file)

    FGcladeID_to_representativeSamples = load_object(FGcladeID_to_representativeSamples_file)

    # define all the representative samples
    all_representative_samples = set(make_flat_listOflists(FGcladeID_to_representativeSamples.values()))

    # init all the cmds
    all_cmds = []

    # go through each chromosome
    for chrom, multifasta_chrom in chrom_to_multifasta.items():
        #print(chrom)

        # define the outdir of this chromosome
        outdir_chrom = "%s/%s"%(outdir, chrom); make_folder(outdir_chrom)

        # map each sample to the seq of the chrom
        sampleID_to_seq = {int(seq.id) : seq for seq in SeqIO.parse(multifasta_chrom, "fasta")}

        # map set of sorted samples to the clades
        sortedSamples_to_sortedFGclades = {}

        # go through each fastGEAR clade, taken as receptor
        for fgClade in sorted(all_FGclades):

            # define all the samples that should be in this analysis (all of the donor and all representatives of the other clade)
            sorted_samples_fgClade = sorted(set(FGcladeID_to_samples[fgClade]).union(all_representative_samples))
            tuple_sorted_samples_fgClade = tuple(sorted_samples_fgClade)

            # maybe this sample configuration has been done before
            if tuple_sorted_samples_fgClade in sortedSamples_to_sortedFGclades:
                sortedSamples_to_sortedFGclades[tuple_sorted_samples_fgClade].append(fgClade)
                continue

            else: sortedSamples_to_sortedFGclades[tuple_sorted_samples_fgClade] = [fgClade]

            # define outdir
            outdir_fgClade = "%s/receptor_FGclade_%i"%(outdir_chrom, fgClade)

            # add the cmd if necessary
            final_files_fastGEAR = ["%s/output/%s"%(outdir_fgClade, f) for f in ["lineage_information.txt", "recombinations_ancestral.txt", "recombinations_recent.txt"]] # these files are generated once fastGEAR is ran

            if any([file_is_empty(f) for f in final_files_fastGEAR]):

                # make folders
                delete_folder(outdir_fgClade); make_folder(outdir_fgClade)

                # make the multifasta with only the desired samples
                multifasta_fgClade = "%s/sequences.fasta"%outdir_fgClade
                SeqIO.write([sampleID_to_seq[s] for s in sorted_samples_fgClade], multifasta_fgClade, "fasta")

                # make the partitions file (each line is the clade assigned to each of the samples as specified in the multifasta. The smallest clades should be in order so that the smallest clades are the first ones)
                partitions_file = "%s/preSpecified_clades.txt"%outdir_fgClade
                open(partitions_file, "w").write("\n".join([str(sampleID_to_fastGEAR_cladeID[sampleID]) for sampleID in sorted_samples_fgClade]))

                # clean
                for f in ["output", "output_dir.mat", "output_snpData.mat", "results.mat"]: delete_file_or_folder("%s/%s"%(outdir_fgClade, f))

                # prepare the SPECFILE for fastGEAR
                specFile = "%s/technical_specs.txt"%outdir_fgClade
                specFile_lines = ["15 # Number of iterations",
                                  "1000 # Upper bound for the number of clusters (possibly multiple values)",
                                  "0 # Run clustering for all upper bounds (0=no / 1=yes)",
                                  "%s # File containing a partition for strains"%partitions_file,
                                  "1 # 1=produce reduced output, 0=produce complete output"]

                open(specFile, "w").write("\n".join(specFile_lines)+"\n")

                # generate the fastGEAR command
                fastGEAR_cmd = "export LD_LIBRARY_PATH=/gpfs/projects/bsc40/mschikora/anaconda3/envs/fastGEAR_env/lib && %s/software/fastGEAR/fastGEARpackageLinux64bit/run_fastGEAR.sh '%s/software/fastGEAR/MCR_dir/v901' '%s' '%s/results.mat' '%s'"%(ParentDir, ParentDir, multifasta_fgClade, outdir_fgClade, specFile)

                all_cmds.append(fastGEAR_cmd)

            # clean
            else:

                #print("cleaning")
                for f in ["preSpecified_clades.txt", "results.mat", "results_snpData.mat", "sequences.fasta", "technical_specs.txt", "output/lineage_comparison.mat"]: remove_file("%s/%s"%(outdir_fgClade, f))

        # write the samples that have been grouped together
        save_object(sortedSamples_to_sortedFGclades, "%s/sortedSamples_to_sortedFGclades.py"%outdir_chrom)

    ####################################


    return all_cmds




def get_cmds_run_fastGEAR_predefinedClades_RandomHetSNPs_one_resample(outdir_fastGEAR_predefinedClades_randomHetSNPs, I, taxID_dir, metadata_df, sciName, genome, ploidy, all_possible_chroms):

    """Gets a set of commands for one resample of the run_fastGEAR_predefinedClades_RandomHetSNPs step in ./get_data.py"""

    print("%s resample %i"%(sciName, I))

    # define the outdir
    outdir_I_fastGEAR = "%s/resample_%i"%(outdir_fastGEAR_predefinedClades_randomHetSNPs, I) 

    # define the prefix of the multifasta
    multifasta_I_fastGEAR = "%s/generate_tree_from_SNPs_resamplingHetSNPs/resamplings/resample_%i/multifasta_several_samples_randomlyChosenHetSNPs.fasta"%(taxID_dir, I) # this was generated with get_SNPs_trees_resamplingHeteroSNPs

    # map each chromosome to the multifasta I
    def def_get_resampled_hetSNPs_multifasta_file(c): return "%s.%s.fasta"%(multifasta_I_fastGEAR, c)
    chrom_to_multifasta_I = {c : def_get_resampled_hetSNPs_multifasta_file(c)  for c in all_possible_chroms if not file_is_empty(def_get_resampled_hetSNPs_multifasta_file(c))}

    # get the cmds
    return get_cmds_fastGEAR_predefined_clades(outdir_I_fastGEAR, chrom_to_multifasta_I, metadata_df, sciName, genome, ploidy, 1, taxID_dir, running_resamples=True)

def get_recombination_fastGEAR_df_severalChroms_one_outdir(outdir_fastGEAR, reference_genome, multifasta_fastGEAR):

    """Takes a folder with  the results of fastGEAR for many chromosomes and returns the df with the recombination events """

    # define file
    df_recombination_allChroms_file = "%s/df_recombination_allChroms.py"%outdir_fastGEAR
    if file_is_empty(df_recombination_allChroms_file):

        # load the clade ID mapping, with the samples with no clade with the fastGEAR name
        df_clades = get_tab_as_df_or_empty_df("%s/cladeIDs_mapping.tab"%outdir_fastGEAR) # sampleID here is the sample as defined in metadata_Df

        def get_redefined_cladeID_Tree_and_BranchLen(r):
            if pd.isna(r.cladeID_Tree_and_BranchLen): return "unassignedSample_%i"%r.sampleID
            else: return int(r.cladeID_Tree_and_BranchLen)

        df_clades["cladeID_Tree_and_BranchLen"] = df_clades.apply(get_redefined_cladeID_Tree_and_BranchLen, axis=1)

        # map each clade to the samples
        cladeID_treeBranchLen_to_sampleIDs = dict(df_clades.groupby("cladeID_Tree_and_BranchLen").apply(lambda df_c: set(df_c.sampleID)))
        fastGEAR_cladeID_to_sampleIDs = dict(df_clades.groupby("fastGEAR_cladeID").apply(lambda df_c: set(df_c.sampleID)))

        # map clades
        fastGEAR_cladeID_to_BranchLen_cladeID = dict(df_clades[["fastGEAR_cladeID", "cladeID_Tree_and_BranchLen"]].drop_duplicates().set_index("fastGEAR_cladeID")["cladeID_Tree_and_BranchLen"])


        # define all cladeIDs that are valid
        external_cladeID = max(set(df_clades.fastGEAR_cladeID))+1

        # init
        df_recombination_allChroms = pd.DataFrame()

        # go through each chromosome
        for chrom in set(get_chr_to_len(reference_genome)): 

            # define the outdir of the chrom
            outdir_chrom = "%s/%s"%(outdir_fastGEAR, chrom)
            fasta_chrom = "%s.%s.fasta"%(multifasta_fastGEAR, chrom)
            if file_is_empty(fasta_chrom): 
                print("WARNING: chromosome %s was not considered"%chrom)
                continue

            # get the length of the chromosome
            len_chrom = [len(seq) for seq in SeqIO.parse(fasta_chrom, "fasta")][0]

            # create a dict that maps each FG clade to the folder that ran it
            lists_FGcladesRanTogether = list(load_object("%s/sortedSamples_to_sortedFGclades.py"%outdir_chrom).values())
            fastGEAR_cladeID_to_run_cladeID = {fgCladeID : [x[0] for x in lists_FGcladesRanTogether if fgCladeID in x][0] for fgCladeID in fastGEAR_cladeID_to_BranchLen_cladeID}

            # We ran fastGEAR on different groups of samples, each of them having all samples of one receptor clade and a subset of each of the other donor clades. This means that we only want to keep, for each group, the events of the corresponding receptor clade
            for receptor_cladeID in sorted(fastGEAR_cladeID_to_run_cladeID):
                print(chrom, receptor_cladeID)

                # define the outdir according to fastGEAR_cladeID_to_run_cladeID
                outdir_receptorClade = "%s/receptor_FGclade_%i"%(outdir_chrom, fastGEAR_cladeID_to_run_cladeID[receptor_cladeID])

                # load the recent recombinations and add things
                df_recombination_recent = pd.read_csv("%s/output/recombinations_recent.txt"%outdir_receptorClade, sep="\s+", skiprows=1).rename(columns={"Start":"start", "End":"end", "StrainName":"receptor_sampleID", "DonorLineage":"donor_clade_fastGEAR", "log(BF)":"logBF"})

                df_recombination_recent["type_recombination"] = "recent"

                # check that receptor_sampleID is the same as in df_clades.sampleID
                strange_samples = set(df_recombination_recent.receptor_sampleID).difference(set(df_clades.sampleID))
                if len(strange_samples)>0: raise ValueError("There are strange samples: %s"%strange_samples)

                # load the ancestral recombinations
                df_recombination_ancestral_initial = pd.read_csv("%s/output/recombinations_ancestral.txt"%outdir_receptorClade, sep="\s+", skiprows=1).rename(columns={"Start":"start", "End":"end", "Lineage1":"donor_clade_fastGEAR", "Lineage2":"receptor_clade_fastGEAR", "log(BF)":"logBF"})

                # add things to the ancestral recombinations
                if len(df_recombination_ancestral_initial)>0:

                    df_recombination_ancestral_initial["type_recombination"] = "ancestral"

                    # add the sampleIDs from ancestral recombinations
                    def get_df_recombination_ancestral_one_event(r): 
                        sorted_samplesClade = sorted(fastGEAR_cladeID_to_sampleIDs[r.receptor_clade_fastGEAR])
                        df_rec = pd.DataFrame({"receptor_sampleID":sorted_samplesClade})
                        for k in r.keys(): df_rec[k] = r[k]
                        return df_rec

                    df_recombination_ancestral = pd.concat(df_recombination_ancestral_initial.apply(get_df_recombination_ancestral_one_event, axis=1).values)

                    # check that all the samples from the same cladeID have the same df
                    receptorSample_to_nancestralEvents = df_recombination_ancestral.groupby("receptor_sampleID").apply(len)
                    all_receptors = set(receptorSample_to_nancestralEvents.index)

                    for clade, sampleIDs in cladeID_treeBranchLen_to_sampleIDs.items():

                        sampleIDs_in_all_receptors = sampleIDs.intersection(all_receptors)
                        if len(sampleIDs_in_all_receptors)>0:

                            # check that all the clades are there
                            if sampleIDs_in_all_receptors!=sampleIDs: raise ValueError("not all the samples of clade %s are in the df"%clade)

                            # check that all the samples have the same length
                            sampleIDs_lens = set(receptorSample_to_nancestralEvents.loc[sampleIDs_in_all_receptors])
                            if len(sampleIDs_lens)!=1: raise ValueError("Not all samples have the same")
                
                else: df_recombination_ancestral = pd.DataFrame()

                # merge
                df_recombination = df_recombination_recent.append(df_recombination_ancestral)

                # keep only those events where the receptor clade is the corresponding one
                df_recombination["receptor_sampleID"] = df_recombination.receptor_sampleID.apply(int)
                if any(pd.isna(df_recombination.receptor_sampleID)): raise ValueError("There can't be nans in receptor_sampleID")

                df_recombination = df_recombination[df_recombination.receptor_sampleID.isin(fastGEAR_cladeID_to_sampleIDs[receptor_cladeID])]
                #if len(df_recombination)==0: print("WARNING: There are 0 recombination events in the FG clade %i (chrom %s), which has these samples: %s"%(receptor_cladeID, chrom, fastGEAR_cladeID_to_sampleIDs[receptor_cladeID]))

                df_ancestral_filtered = df_recombination[df_recombination.type_recombination=="ancestral"]
                if len(df_ancestral_filtered)>0 and sum(df_ancestral_filtered.receptor_clade_fastGEAR!=receptor_cladeID)>0: raise ValueError("There are unexpected receptor cladeIDs in fastGEAR")


                if len(df_recombination)>0:

                    # add fields
                    df_recombination["chromosome"] = chrom
                    df_recombination["chromosome_len"] = len_chrom

                    def get_receptor_entity(r):
                        if r.type_recombination=="recent": return "sample%i"%(r.receptor_sampleID)
                        elif r.type_recombination=="ancestral": return "fastGEAR%i"%(r.receptor_clade_fastGEAR)

                    df_recombination["receptor_entity"] = df_recombination.apply(get_receptor_entity, axis=1)

                    def get_recombination_ID(r): return "%s_%i_%i_fastGEAR%i_to_%s"%(r.chromosome, r.start, r.end, r.donor_clade_fastGEAR, r.receptor_entity)          
                    df_recombination["recombination_ID"] = df_recombination.apply(get_recombination_ID, axis=1)


                    def get_donor_clade_Tree_and_BranchLen_fromFastGEAR(fastGEAR_cladeID):
                        if fastGEAR_cladeID in fastGEAR_cladeID_to_BranchLen_cladeID: return fastGEAR_cladeID_to_BranchLen_cladeID[fastGEAR_cladeID]
                        elif fastGEAR_cladeID==external_cladeID: return "external"
                        else: raise ValueError("%s is not valid"%fastGEAR_cladeID)

                    df_recombination["donor_cladeID_TreeBranchLen"] = df_recombination.donor_clade_fastGEAR.apply(get_donor_clade_Tree_and_BranchLen_fromFastGEAR)

                    # keep only some fields
                    df_recombination_allChroms = df_recombination_allChroms.append(df_recombination[["chromosome", "start", "end", "chromosome_len", "donor_cladeID_TreeBranchLen", "receptor_sampleID", "recombination_ID", "type_recombination", "receptor_entity", "logBF"]])

        # change some things
        df_recombination_allChroms["receptor_sampleID"] = df_recombination_allChroms.receptor_sampleID.apply(int)
        df_recombination_allChroms = df_recombination_allChroms.sort_values(by=["chromosome", "start", "end", "donor_cladeID_TreeBranchLen", "receptor_sampleID", "type_recombination"]).reset_index(drop=True)

        # save
        print("saving")
        save_object(df_recombination_allChroms, df_recombination_allChroms_file)

    return load_object(df_recombination_allChroms_file)


def get_list_clusters_one_recombination_chromosome(df_bedmap_chr):

    """Defines a list of clusters for one recombination chromosome"""
    
    # define
    ID_to_overlappingIDs = dict(df_bedmap_chr.set_index("recombination_ID")["overlapping_IDs_set"])

    # get the list of clusters
    return get_list_clusters_from_dict(ID_to_overlappingIDs)


def get_list_clusters_overlapping_df_recombination_resamples(df_recombination_allResamples, outdir, pct_overlap, threads):

    """Takes a recombination df from get_recombination_fastGEAR_df_resamplingHeteroSNPs and gets a list of clusters of recombination events"""

    print("getting list of clusters recombinantin events")
    delete_folder(outdir); make_folder(outdir)

    # define a df with the necessary data
    df_bed = df_recombination_allResamples[["recombination_ID", "chromosome", "start", "end", "receptor_entity", "donor_cladeID_TreeBranchLen", "type_recombination"]].drop_duplicates()

    df_bed["chromosome_recombination"] = "chrom" +  df_bed.donor_cladeID_TreeBranchLen.apply(str) + "_to_" + df_bed.receptor_entity + "_" + df_bed.type_recombination + "_" + df_bed.chromosome

    df_bed = df_bed.sort_values(by=["chromosome_recombination", "start", "end", "recombination_ID"])

    # check that the uniqueIDs are truly unique
    if len(set(df_bed.recombination_ID))!=len(df_bed): raise ValueError("recombination_ID is not unique")

    # add the numeric ID
    recID_to_numericID = dict(zip(df_bed.recombination_ID.values, range(len(df_bed))))
    df_bed["numericID"] = df_bed.recombination_ID.map(recID_to_numericID)
    numericID_to_recID = dict(df_bed.set_index("numericID").recombination_ID)

    # addd 1 to the end
    df_bed["end"] += 1

    # write a bed with the intersecting IDs
    bed_recombination = "%s/df_recombination.bed"%outdir
    df_bed[["chromosome_recombination", "start", "end", "numericID"]].to_csv(bed_recombination, sep="\t", header=False, index=False)

    # run bedmap to get a file where each line corresponds to the regions to which each recombination event
    bedmap_outfile = "%s/overlapping_IDs.txt"%outdir
    bedmap_stderr = "%s.stderr"%bedmap_outfile

    print("running bedmap. The stderr is in %s"%bedmap_stderr)
    run_cmd("bedmap --fraction-both %.2f --echo-map-id  --delim '\t' %s > %s 2>%s"%(pct_overlap, bed_recombination, bedmap_outfile, bedmap_stderr), env="perSVade_env")

    # load as df, which already has the same order as df_bed
    df_bedmap = pd.read_csv(bedmap_outfile, sep="\t", header=None, names=["overlapping_IDs"])
    df_bedmap["overlapping_IDs_set"] = df_bedmap.overlapping_IDs.apply(str).apply(lambda x: {int(y) for y in x.split(";")})
    df_bedmap["numericID"] =  df_bed.numericID.values
    df_bedmap["chromosome_recombination"] =  df_bed.chromosome_recombination.values

    # check that all the values fit each other
    df_bedmap_strange = df_bedmap[~df_bedmap.apply(lambda r: r.numericID in r.overlapping_IDs_set, axis=1)]
    if len(df_bedmap_strange)>0:
        print(df_bedmap_strange)
        raise ValueError("not all IDs are self matched")

    # check that all the beds have something
    if any(df_bedmap.overlapping_IDs_set.apply(len)==0): raise ValueError("there can't be bedmaps without a match")


    # redefine the df_bedmap to include the overlapping_IDs_set
    print("adding the non-numeric IDs")
    def get_overlapping_IDs_set_from_numeric(over_IDs): return set(map(lambda x: numericID_to_recID[x], over_IDs))
    df_bedmap["overlapping_IDs_set"] = df_bedmap.overlapping_IDs_set.apply(get_overlapping_IDs_set_from_numeric)

    # add the recombination ID
    df_bedmap["recombination_ID"] = df_bed.recombination_ID.values

    # define the list of clusters in parallel
    print("define inputs")
    df_bedmap = df_bedmap.set_index("chromosome_recombination")[["recombination_ID", "overlapping_IDs_set"]]
    sorted_chromosome_recombinations = sorted(set(df_bedmap.index))
    inputs_fn = list(map(lambda x: (df_bedmap.loc[{x}],), sorted_chromosome_recombinations))

    print("running in parallel the list of inputs definition in %i threads"%threads)
    with multiproc.Pool(threads) as pool:
        list_clusters = make_flat_listOflists(pool.starmap(get_list_clusters_one_recombination_chromosome, inputs_fn, chunksize=1))

        # close the pool
        pool.close()
        pool.terminate()

    # check
    print("checking")
    all_IDs = set(df_recombination_allResamples.recombination_ID)
    all_IDs_in_cluster = set.union(*list_clusters)
    if all_IDs!=all_IDs_in_cluster: raise ValueError("all IDs should be in clusters")

    delete_folder(outdir)

    return list_clusters

def get_recombination_fastGEAR_df_resamplingHeteroSNPs_oneResample(reference_genome, taxID_dir, I):

    """Generates a df_recombination_I for one replicate from get_recombination_fastGEAR_df_resamplingHeteroSNPs"""

    print("resample %i"%I)

    # get the outdir
    outdir_fastGEAR = "%s/run_fastGEAR_predefinedClades_RandomHetSNPs/resample_%i"%(taxID_dir, I)

    # define the multifasta
    multifasta_I_fastGEAR = "%s/generate_tree_from_SNPs_resamplingHetSNPs/resamplings/resample_%i/multifasta_several_samples_randomlyChosenHetSNPs.fasta"%(taxID_dir, I) 

    # get the recombination dataframe for this I
    df_recombination_I = get_recombination_fastGEAR_df_severalChroms_one_outdir(outdir_fastGEAR, reference_genome, multifasta_I_fastGEAR)

    # add things to the df
    df_recombination_I["resampleID"] = I

    return df_recombination_I


def get_recombination_fastGEAR_df_resamplingHeteroSNPs(reference_genome, taxID_dir, threads, pct_overlap):

    """This function returns the df for one species in get_recombination_fastGEAR_df and resampling heterozygous SNPs """

    ########## GET A DF WITH ALL CONCATENATED RESAMPLED DFS ###########

    # get the recombination dfs in parallel
    nresamples = 100
    inputs_fn = [(reference_genome, taxID_dir, I) for I in range(1, nresamples+1)]

    with multiproc.Pool(threads) as pool:

        df_recombination_allResamples = pd.concat(pool.starmap(get_recombination_fastGEAR_df_resamplingHeteroSNPs_oneResample, inputs_fn))
        
        pool.close()
        pool.terminate()

    ###################################################################

    #### ADD THE SUPPORT (FRACTION OF RESAMPLES WITH EACH RECOMBINATION DETECTED) ########

    # recombination events of the same type (recent or ancestral), same donor and acceptor clade, same chromosome and overlapping start and end (by >50%) are considered to be overlapping
    print("adding clusterIDs")

    # define the list of clusters of recombinant events
    outdir_listClusters = "%s/calculating_listClusters_recIDs"%taxID_dir
    list_clusters_recombinationIDs =  get_list_clusters_overlapping_df_recombination_resamples(df_recombination_allResamples, outdir_listClusters, pct_overlap, threads)

    # add the cluster ID to the df_recombination_allResamples
    print("adding metadata")
    clusterID_to_recombinationIDs = dict(zip(range(len(list_clusters_recombinationIDs)), list_clusters_recombinationIDs))
    recombID_to_clusterID = {}
    for clusterID, recombIDs in clusterID_to_recombinationIDs.items():
        for rID in recombIDs: recombID_to_clusterID[rID] = clusterID

    df_recombination_allResamples["recombination_clusterID"] = df_recombination_allResamples.recombination_ID.apply(lambda x: recombID_to_clusterID[x])

    # get the unique recombination IDs, so that the highest BF is taken first
    print("adding other fields")
    df_recombination_allResamples["end"] += 1 # add 1
    df_recombination_allResamples["length_event"] = df_recombination_allResamples.end - df_recombination_allResamples.start
    df_recombination_unique = df_recombination_allResamples.sort_values(by=["recombination_clusterID", "logBF", "length_event", "donor_cladeID_TreeBranchLen", "receptor_sampleID"], ascending=[True, False, False, True, True]).drop_duplicates(subset=["recombination_clusterID", "donor_cladeID_TreeBranchLen", "receptor_sampleID"], keep="first")

    #if len(df_recombination_unique)!=len(set(df_recombination_unique.recombination_clusterID)): raise ValueError("The recombination_clusterID is not unique")

    # add the fraction of resamples with this cluster
    clusterID_to_nresamples = dict(df_recombination_allResamples[["recombination_clusterID", "resampleID"]].drop_duplicates().groupby("recombination_clusterID").apply(len))
    df_recombination_unique["n_bootstraps_withRecombination"] = df_recombination_unique.recombination_clusterID.apply(lambda x: clusterID_to_nresamples[x])
    df_recombination_unique["fraction_bootstraps_withRecombination"] = df_recombination_unique.n_bootstraps_withRecombination/nresamples

    ######################################################################################

    return df_recombination_unique[['chromosome', 'start', 'end', 'chromosome_len', 'donor_cladeID_TreeBranchLen', 'receptor_sampleID', 'recombination_ID', 'type_recombination', 'receptor_entity', 'logBF', 'length_event', 'n_bootstraps_withRecombination', 'fraction_bootstraps_withRecombination']]

def get_recombination_fastGEAR_df(metadata_df, DataDir, ProcessedDataDir, species_to_ref_genome, replace=False, threads=4, pct_overlap=0.50):


    """Generates a df with species, chromosome, start, end, chromosome_len (the length of the alignment with SNPs), donor_cladeID_TreeBranchLen, receptor_sampleID, recombination_ID, fraction_bootstraps_withRecombination, type_recombination. It also returs the length of each chromosome used in the alignment by fasgear"""

    print("getting recombination df")

    # define file
    df_recombination_all_file = "%s/df_recombination_all_fastGEAR.py"%ProcessedDataDir
    species_to_chrom_to_fastGEARlen_file = "%s/species_to_chrom_to_fastGEARlen.py"%ProcessedDataDir

    if file_is_empty(df_recombination_all_file) or file_is_empty(species_to_chrom_to_fastGEARlen_file) or replace is True:

        # init the df
        df_recombination_all = pd.DataFrame()

        # inint the dict
        species_to_chrom_to_fastGEARlen = {}

        # go through each species
        for taxID, species in taxID_to_sciName.items():
            print(species)

            # debug
            #if species!="Candida_glabrata": continue

            # get genome
            reference_genome = species_to_ref_genome[species]

            # define the taxIDs
            taxID_dir = "%s/%s_%i"%(DataDir, species, taxID)

            # define the expected fields of the recombination df
            df_recombination_fields = ['chromosome', 'start', 'end', 'chromosome_len', 'donor_cladeID_TreeBranchLen', 'receptor_sampleID', 'recombination_ID', 'type_recombination', 'receptor_entity', 'logBF', 'length_event', 'n_bootstraps_withRecombination', 'fraction_bootstraps_withRecombination']


            # define the df with all recombination events and the multifasta from which fastGEAR was ran
            if taxID_to_ploidy[taxID]==1: 

                # define the multifasta with which fastGEAR was ran
                multifasta_fastGEAR = "%s/integrated_varcalls/homoSNPs_positions_noINDELS_noHetSNPs_cov_atLeast_12x_sequence.fasta"%(taxID_dir)

                # define the recombination df
                df_recombination = get_recombination_fastGEAR_df_severalChroms_one_outdir("%s/run_fastGEAR_predefinedClades_homoSNPs"%taxID_dir, reference_genome, multifasta_fastGEAR)

                # add fields to make it compatible with the diploid species
                df_recombination["n_bootstraps_withRecombination"] = 1
                df_recombination["fraction_bootstraps_withRecombination"] = 1.0
                df_recombination["length_event"] = df_recombination.end - df_recombination.start
                df_recombination = df_recombination[df_recombination_fields]
                
            # for diploids you need to integrated resampling
            else:  

                # get the recombination df
                df_recombination = get_recombination_fastGEAR_df_resamplingHeteroSNPs(reference_genome, taxID_dir, threads, pct_overlap)[df_recombination_fields]

                # define the multifasta with the genomes for the first genome
                multifasta_fastGEAR = "%s/generate_tree_from_SNPs_resamplingHetSNPs/resamplings/resample_1/multifasta_several_samples_randomlyChosenHetSNPs.fasta"%taxID_dir 

            # save the length of the fastgear aln
            species_to_chrom_to_fastGEARlen[species] = {chrom : len(next(iter(SeqIO.parse("%s.%s.fasta"%(multifasta_fastGEAR, chrom), "fasta"))).seq) for chrom in set(get_chr_to_len(reference_genome)) if not file_is_empty("%s.%s.fasta"%(multifasta_fastGEAR, chrom))}

            # plot the distribution
            ax = sns.distplot(df_recombination.fraction_bootstraps_withRecombination)
            ax.set_title(species)

            # add things
            df_recombination["species"] = species

            # keep
            df_recombination_all = df_recombination_all.append(df_recombination)

        save_object(df_recombination_all, df_recombination_all_file)
        save_object(species_to_chrom_to_fastGEARlen, species_to_chrom_to_fastGEARlen_file)

    df_recombination_all = load_object(df_recombination_all_file)
    species_to_chrom_to_fastGEARlen = load_object(species_to_chrom_to_fastGEARlen_file)

    # map each species to the chr to len
    species_to_chrom_to_len = {spp : get_chr_to_len(ref)  for spp, ref in species_to_ref_genome.items()}

    # check that there are no intersecting chroms
    for sppA, chrToLenA in species_to_chrom_to_len.items():
        for sppB, chrToLenB in species_to_chrom_to_len.items():
            if len(set(chrToLenA).intersection((chrToLenB)))>0 and sppA!=sppB: raise ValueError("There are intersecting chromosomes btw %s and %s"%(sppA, sppB))
    
    # add fields of the length of the events
    print("adding fields")
    all_chr_to_len = {}
    for spp, chr_to_len in species_to_chrom_to_len.items(): all_chr_to_len = {**all_chr_to_len, **chr_to_len}
    df_recombination_all["real_chromosome_len"] = df_recombination_all.chromosome.map(all_chr_to_len)
    df_recombination_all["transformed_length_event"] = df_recombination_all.length_event * (df_recombination_all.real_chromosome_len/df_recombination_all.chromosome_len) # the transformed length taking into account that the chromosomes have different length

    if any(pd.isna(df_recombination_all["transformed_length_event"])): raise ValueError("There can't be nans in transformed_length_event")

    return df_recombination_all, species_to_chrom_to_fastGEARlen


def generate_plots_quality_control_fastGEAR_resampling_heteroSNPs(df_recombination, PlotsDir, xfield, yfield):

    """Plots the xfield vs yfield in df_recombination for different species and types of recombination"""

    # species
    all_species_rec = set(df_recombination.species)#.difference({"Candida_auris", "Candida_glabrata"})
    sorted_species = [species for species in sorted_species_byPhylogeny if species in all_species_rec]

    # init fig
    nrows =  len(sorted_species)
    ncols = 2
    fig = plt.figure(figsize=(ncols*4, nrows*4)); I=1

    # define lims
    df_recombination_species = df_recombination[df_recombination.species.isin(all_species_rec)]

    xlim = [min(df_recombination_species[xfield]), max(df_recombination_species[xfield])]
    ylim = [min(df_recombination_species[yfield]), max(df_recombination_species[yfield])]

    x_range = xlim[1]-xlim[0]
    xlim = [xlim[0]-x_range*0.05, xlim[1]+x_range*0.05]

    y_range = ylim[1]-ylim[0]
    ylim = [ylim[0]-y_range*0.05, ylim[1]+y_range*0.05]

    # iterate through species
    for Ir, species in enumerate(sorted_species):
        print(species)

        # only diploids
        #if taxID_to_ploidy[sciName_to_taxID[species]]==1: continue  

        # get df of the species, getting each recombination ID unique
        df_spp = df_recombination[df_recombination.species==species].drop_duplicates(subset="recombination_ID")

        # add fields
        df_spp["chrom"] = "chr" + df_spp.chromosome.apply(get_shortChrom_from_chrName, species=species).apply(str)
        df_spp = df_spp.sort_values(by="chrom")
        df_spp["type_genome"] = (df_spp.chrom=="chrMT").map({True:"mtDNA", False:"gDNA"})

        # iterate through types of recombination
        for Ic, type_recombination in enumerate(["recent", "ancestral"]):

            # get the df plot
            df_plot = df_spp[df_spp.type_recombination==type_recombination]


            # plot
            if len(df_plot)==0: continue
            ax = plt.subplot(nrows, ncols, I); I+=1

            ax = sns.scatterplot(x=xfield, y=yfield, data=df_plot, hue="type_genome", style="type_genome", alpha=0.05)


            # add the spearman r
            r_spearman, p_spearman = stats.spearmanr(df_plot[xfield], df_plot[yfield], nan_policy="raise")


            ax.add_artist(matplotlib.offsetbox.AnchoredText("spearman\nr=%.2f\np=%.6f"%(r_spearman, p_spearman), "right"))

            #plt.text(0, 1, "hi", loc="best")

            # logscale
            #ax.set_xscale("log")

            # legend
            if Ic==1: ax.legend(loc=(1.03, 0))
            else: ax.get_legend().remove()

            # get ylabel
            ax.set_ylim([-0.05, 1.05])
            if Ic==1: 
                ax.set_ylabel("")
                ax.set_yticklabels([])

            elif Ic==0: ax.set_ylabel("%s\n%s"%(species, yfield))

            # set limits
            ax.set_xlim(xlim)
            ax.set_ylim(ylim)


            # xlabel
            if species!=(sorted_species[-1]): ax.set_xlabel("")

            # title
            if Ir==0: ax.set_title("%s recombination"%type_recombination)


    # adjust
    plt.subplots_adjust(wspace=0.02, hspace=0.02)
    
    dirplot = "%s/qualityControl_fastGEAR_recombination"%PlotsDir; make_folder(dirplot)
    filename_plot = "%s/%s_vs_%s.pdf"%(dirplot, xfield, yfield)
    print("saving %s"%filename_plot)
    fig.savefig(filename_plot, bbox_inches='tight')
    print("saved")


def get_series_receptorSample_to_fractionGenomeRecombined_fastgear(df_rec_clade, possible_donor_clade, all_sorted_samples, tmpdir, genome_size):

    """Takes a df recombination of one donor clade and all the fields and returns a series with the fraction of recombined genome in all samples"""

    # define a field withe name
    donor_clade_field = "donorClade_%s"%possible_donor_clade

    # if there are no recombining samples, skip
    if len(df_rec_clade)==0: receptorSample_to_fractionGenomeRecombined = pd.Series([0.0]*len(all_sorted_samples), index=all_sorted_samples)
    
    # else calculate, for each sample, the fraction of the genome recombined to each sampleID
    else:

        # map each receptor sample to fraction of the genome recombined with this donor
        df_rec_clade["chrom_and_receptorSample"] = df_rec_clade.chromosome + "|||" + df_rec_clade.receptor_sampleID.apply(str)

        # make a bed file with the recombination regions, note that only chroms of the same receptor will be merged
        bed_fields = ["chrom_and_receptorSample", "start", "end"]
        bedfile = "%s/all_events_%s.bed"%(tmpdir, possible_donor_clade)
        df_rec_clade[bed_fields].sort_values(by=bed_fields).to_csv(bedfile, sep="\t", header=False, index=False)

        # merge
        merged_bed = "%s/merged_events_%s.bed"%(tmpdir, possible_donor_clade)
        run_cmd("bedtools merge -i %s > %s"%(bedfile, merged_bed), env="Candida_mine_env")
        df_rec_clade_merged = pd.read_csv(merged_bed, sep="\t", header=None, names=bed_fields)
        df_rec_clade_merged["len_event"] = df_rec_clade_merged.end-df_rec_clade_merged.start

        # add fields
        def get_receptor_sampleID(x): return x.split("|||")[1]
        df_rec_clade_merged["receptor_sampleID"] = df_rec_clade_merged.chrom_and_receptorSample.apply(get_receptor_sampleID).apply(int)

        # map each receptor sampleID to the recombined fraction of the genome
        def get_sum_len_events(df_s): return sum(df_s.len_event)
        receptorSample_to_fractionGenomeRecombined = df_rec_clade_merged[["receptor_sampleID", "len_event"]].groupby("receptor_sampleID").apply(get_sum_len_events)/genome_size

        # add the missing
        zero_receptor_samples = sorted(set(all_sorted_samples).difference(set(receptorSample_to_fractionGenomeRecombined.index)))
        receptorSample_to_fractionGenomeRecombined =  receptorSample_to_fractionGenomeRecombined.append(pd.Series([0.0]*len(zero_receptor_samples), index=zero_receptor_samples))

    # change the name
    receptorSample_to_fractionGenomeRecombined.name = donor_clade_field

    return receptorSample_to_fractionGenomeRecombined

def get_df_fraction_recombined_genome_from_fastGEAR_events_df(df_recombination, metadata_df, threshold_fractionBootstraps, threshold_logBF, genome_size, tmpdir, threads):

    """Takes a df recombination of one species and a given type of data and generates a df were each line is one sample of metadata df and the columns are the fraction of recombination attributable to the different clades of the metadata df. genome_size should be the one of the alignment with which fastGEAR was calculated"""

    # filter
    df_recombination = df_recombination[(df_recombination.fraction_bootstraps_withRecombination>=threshold_fractionBootstraps) & (df_recombination.logBF>=threshold_logBF)]

    # init the df with the fraction of initialized samples
    df_fraction_recombined_genome = metadata_df[["sampleID", "cladeID_Tree_and_BranchLen"]]
    df_fraction_recombined_genome["sampleID"] = df_fraction_recombined_genome.sampleID.apply(int)
    df_fraction_recombined_genome = df_fraction_recombined_genome.sort_values(by="sampleID").set_index("sampleID", drop=False)

    # checks
    all_samples = set(df_fraction_recombined_genome.sampleID)
    strange_samples = set(df_recombination.receptor_sampleID).difference(all_samples)
    if len(strange_samples)>0: raise ValueError("there are strange samples in df_recombination: %s"%strange_samples)

    all_clades = sorted(set(df_fraction_recombined_genome[~pd.isna(df_fraction_recombined_genome.cladeID_Tree_and_BranchLen)].cladeID_Tree_and_BranchLen.apply(int)))
    strange_clades = set(df_recombination[df_recombination.donor_cladeID_TreeBranchLen.apply(type)==int].donor_cladeID_TreeBranchLen).difference(all_clades)
    if len(strange_clades)>0: raise ValueError("There are strange cladeIDs in df_recombination: %s"%strange_clades)

    # go through each donor clade
    unassigned_samples_donors = list("unassignedSample_" + df_fraction_recombined_genome[pd.isna(df_fraction_recombined_genome.cladeID_Tree_and_BranchLen)].sampleID.apply(str))
    all_possible_donor_clades = all_clades + ["external"] + unassigned_samples_donors
    all_sorted_samples = sorted(all_samples)

    # get a list of series with the fraction of the genome related to each donnor clade
    delete_folder(tmpdir); make_folder(tmpdir)
    print("generate inputs...")
    inputs_fn = list(map(lambda x: (cp.deepcopy(df_recombination[df_recombination.donor_cladeID_TreeBranchLen==x]), x, all_sorted_samples, tmpdir, genome_size), all_possible_donor_clades))

    # add the anyClade, which includes any donnor. This will be used for the total_fraction_genome_recombined
    inputs_fn.append((cp.deepcopy(df_recombination), "anyClade", all_sorted_samples, tmpdir, genome_size))

    print("run multiproc...")
    with multiproc.Pool(threads) as pool:

        list_series_fractionRecombined = pool.starmap(get_series_receptorSample_to_fractionGenomeRecombined_fastgear, inputs_fn)
        
        pool.close()
        pool.terminate()

    # add to the df
    for series_fractionRecombined in list_series_fractionRecombined: 
        df_fraction_recombined_genome[series_fractionRecombined.name] = series_fractionRecombined.loc[df_fraction_recombined_genome.index]
        if any(pd.isna(df_fraction_recombined_genome[series_fractionRecombined.name])): raise ValueError("there can't be nans in fraction rec. genome")

    # add the sum
    df_fraction_recombined_genome["total_fraction_genome_recombined"] = df_fraction_recombined_genome.donorClade_anyClade

    # check that the sum is below 1
    if any(df_fraction_recombined_genome.total_fraction_genome_recombined>=1): 
        print(df_fraction_recombined_genome[df_fraction_recombined_genome.total_fraction_genome_recombined>=1])
        raise ValueError("there are some samples with >1 fraction of the genome recombined")

    # check that the sum is above all the others
    for f in [x for x in df_fraction_recombined_genome.keys() if x.startswith("donorClade_") and x!="donorClade_anyClade"]:
        if any(df_fraction_recombined_genome[f]>df_fraction_recombined_genome.donorClade_anyClade): raise ValueError("There are some samples that have more donor region from clade %s than the donorClade_anyClade"%f)


    # cleam
    delete_folder(tmpdir)

    return df_fraction_recombined_genome


def get_df_fraction_recombined_genome_several_thresholds(recombination_df, metadata_df, species_to_chrom_to_fastGEARlen, ProcessedDataDir, threads=4, replace=False):

    """Generates the fraction of recombinant DNA for different thresholds"""

    # define thresgolds
    thresholds_fractionBootstraps = [0, 0.1, 0.3, 0.5, 0.8, 0.9, 0.95]
    thresholds_logBF = [0, 25, 50, 75]

    # define all species
    all_species_rec = set(recombination_df.species)
    sorted_species = [species for species in sorted_species_byPhylogeny if species in all_species_rec]

    # map the type recombination to a set
    type_recombination_to_types_recombination_set = {"recent":{"recent"}, "ancestral":{"ancestral"}, "all":{"recent", "ancestral"}}

    ####### GENERATE DATA ######

    # define file
    df_fraction_recombined_genome_all_file = "%s/df_fraction_recombined_genome_all_severalThresholds_fastGEAR.py"%ProcessedDataDir

    if file_is_empty(df_fraction_recombined_genome_all_file) or replace is True:

        # define an outdir to write intermediate files
        tmpdir = "%s/writing_fractionRecombination_per_sample"%ProcessedDataDir; make_folder(tmpdir)
        tmpdir_dfs = "%s/writing_fractionRecombination_per_sample_dfs"%ProcessedDataDir; make_folder(tmpdir_dfs)

        # init df 
        df_fraction_recombined_genome_all = pd.DataFrame()

        for species in sorted_species:

            # debug
            #if species!="Candida_albicans": continue

            for type_recombination, types_recombination_set in type_recombination_to_types_recombination_set.items():

                # get a df for this species and type of recombination
                df_rec  = recombination_df[(recombination_df.species==species) & (recombination_df.type_recombination.isin(types_recombination_set))]

                # get the metadata df of this species
                metadata_df_s = metadata_df[metadata_df.species_name==species]
                if len(metadata_df_s)==0: raise ValueError("there can't be 0 in the metadata")

                # calculate the genome size
                genome_size = sum(species_to_chrom_to_fastGEARlen[species].values())

                # go through different filters
                for threshold_fractionBootstraps in thresholds_fractionBootstraps:
                    for threshold_logBF in thresholds_logBF:
                        print(species, type_recombination, threshold_fractionBootstraps, threshold_logBF)

                        # define a file df_fraction_recombined_genome
                        df_fraction_recombined_genome_file = "%s/df_fraction_recombined_genome_%s_%s_%s_%s.py"%(tmpdir_dfs, species, type_recombination, threshold_fractionBootstraps, threshold_logBF)

                        if file_is_empty(df_fraction_recombined_genome_file) or replace is True:
                       
                            # get a df where each line is one sample of metadata df and it has the fraction of the genome attributable to the different donnor clades
                            df_fraction_recombined_genome = get_df_fraction_recombined_genome_from_fastGEAR_events_df(df_rec, metadata_df_s, threshold_fractionBootstraps, threshold_logBF, genome_size, tmpdir, threads)

                            df_fraction_recombined_genome["species"] = species
                            df_fraction_recombined_genome["type_recombination"] = type_recombination
                            df_fraction_recombined_genome["threshold_fractionBootstraps"] = threshold_fractionBootstraps
                            df_fraction_recombined_genome["threshold_logBF"] = threshold_logBF

                            save_object(df_fraction_recombined_genome, df_fraction_recombined_genome_file)

                        # load
                        df_fraction_recombined_genome = load_object(df_fraction_recombined_genome_file)

                        # keep
                        df_fraction_recombined_genome_all = df_fraction_recombined_genome_all.append(df_fraction_recombined_genome)

        # save
        delete_folder(tmpdir)
        delete_folder(tmpdir_dfs)
        save_object(df_fraction_recombined_genome_all, df_fraction_recombined_genome_all_file)

    # load
    df_fraction_recombined_genome_all = load_object(df_fraction_recombined_genome_all_file)

    return df_fraction_recombined_genome_all

def plot_fraction_recombinant_dna_fastGEAR_resampling_different_thresholds_boxplot(df_fraction_recombined_genome_all, PlotsDir):

    """Plots the fraction of recombinant DNA for different thresholds of logBF and fraction_bootstraps_withRecombination"""

    # define thresgolds
    thresholds_fractionBootstraps = sorted(set(df_fraction_recombined_genome_all.threshold_fractionBootstraps))
    thresholds_logBF = sorted(set(df_fraction_recombined_genome_all.threshold_logBF))

    # define all species
    all_species_rec = set(df_fraction_recombined_genome_all.species)#.difference({"Candida_auris", "Candida_glabrata"})
    sorted_species = [species for species in sorted_species_byPhylogeny if species in all_species_rec]


    # init fig
    nrows = len(sorted_species)
    ncols = len(thresholds_logBF)
    fig = plt.figure(figsize=(ncols*5, nrows*4)); Ip=1

    # define ylims
    ylim = [-0.05, max(df_fraction_recombined_genome_all.total_fraction_genome_recombined)+0.05]

    for Ir, species in enumerate(sorted_species):
        for Ic, threshold_logBF in enumerate(thresholds_logBF):

            # get the df of this suplot
            df_plot = df_fraction_recombined_genome_all[(df_fraction_recombined_genome_all.species==species) & (df_fraction_recombined_genome_all.threshold_logBF==threshold_logBF)].sort_values(by=["threshold_fractionBootstraps", "type_recombination"])

            # init subplot
            ax = plt.subplot(nrows, ncols, Ip); Ip+=1

            # add hist
            typeRec_to_color = {"recent":"blue", "ancestral":"black", "all":"red"}
            ax = sns.boxplot(x="threshold_fractionBootstraps", y="total_fraction_genome_recombined", data=df_plot, hue="type_recombination", palette=typeRec_to_color)

            ax.set_ylim(ylim)

            # edit boxplot
            for i,artist in enumerate(ax.artists): # each artist has 6 elements. This is from https://stackoverflow.com/questions/36874697/how-to-edit-properties-of-whiskers-fliers-caps-etc-in-seaborn-boxplot

                # get color
                col = artist.get_facecolor()
                r, g, b, a = col
                artist.set_edgecolor(col)
                artist.set_facecolor((r, g, b, 0.3))

                for j in range(i*6,i*6+6):
                    line = ax.lines[j]
                    line.set_color(col)
                    line.set_mfc(col)
                    line.set_mec(col)

            # add title
            if Ir==0: ax.set_title("log(BF) >= %.2f"%threshold_logBF)

            # legend
            if Ir==0 and threshold_logBF==(thresholds_logBF[-1]): ax.legend(loc=(1.03, 0))
            else: ax.get_legend().remove()


            # ylabel
            if Ic==0: ax.set_ylabel("%s\nfraction genome recombined"%(species))
            else:
                ax.set_ylabel("")
                ax.set_yticklabels([])

    plt.subplots_adjust(wspace=0.02, hspace=0.02)
    
    dirplot = "%s/qualityControl_fastGEAR_recombination"%PlotsDir; make_folder(dirplot)
    filename_plot = "%s/boxplot_fractionRecombinedGenome_severalThresholds.pdf"%(dirplot)
    print("saving %s"%filename_plot)
    fig.savefig(filename_plot, bbox_inches='tight')
    print("saved")



def plot_fraction_recombinant_dna_fastGEAR_resampling_one_threshold_boxplot(df_fraction_recombined_genome_all, PlotsDir):

    """Plots the fraction of recombinant DNA for one thresholds of logBF and fraction_bootstraps_withRecombination. All species in one plot"""

    # define the df with 95% of bootraps support and no BF filtering
    df_plot = df_fraction_recombined_genome_all[(df_fraction_recombined_genome_all.threshold_fractionBootstraps==0.95) & (df_fraction_recombined_genome_all.threshold_logBF==0)]

    # define all species
    all_species_rec = set(df_fraction_recombined_genome_all.species)#.difference({"Candida_auris", "Candida_glabrata"})
    sorted_species = [species for species in sorted_species_byPhylogeny if species in all_species_rec]

    # sort plot
    species_to_I = dict(zip(sorted_species, range(len(sorted_species))))
    df_plot["speciesI"] = df_plot.species.map(species_to_I)
    df_plot = df_plot.sort_values(by=["type_recombination", "speciesI"])

    # initi fig
    fig = plt.figure(figsize=(6, 3)); Ip=1

    # get boxplot
    typeRec_to_color = {"recent":"blue", "ancestral":"black", "all":"red"}
    ax = sns.boxplot(x="species", y="total_fraction_genome_recombined", data=df_plot, hue="type_recombination", palette=typeRec_to_color)

    # change the xticklabels
    ax.set_xticklabels([s.split("_")[1] for s in sorted_species], rotation=45)

    # edit boxplot
    for i,artist in enumerate(ax.artists): # each artist has 6 elements. This is from https://stackoverflow.com/questions/36874697/how-to-edit-properties-of-whiskers-fliers-caps-etc-in-seaborn-boxplot

        # get color
        col = artist.get_facecolor()
        r, g, b, a = col
        artist.set_edgecolor(col)
        artist.set_facecolor((r, g, b, 0.3))

        for j in range(i*6,i*6+6):
            line = ax.lines[j]
            line.set_color(col)
            line.set_mfc(col)
            line.set_mec(col)

    # change axis and legend
    ax.set_ylim([-0.05, 1])
    ax.legend(loc=(1.03, 0))

    # save
    filename_plot = "%s/boxplot_fractionRecombinedGenome_one_threshold.pdf"%(PlotsDir)
    fig.savefig(filename_plot, bbox_inches='tight')

def plot_fraction_recombinant_dna_fastGEAR_resampling_different_thresholds_lines(df_fraction_recombined_genome_all, metadata_df, PlotsDir, threshold_logBF=0.0):

    """Plots the fraction of recombinant DNA for different fraction_bootstraps_withRecombination."""

    # keep only the df without logBF
    df_fraction_recombined_genome_all_filt = df_fraction_recombined_genome_all[df_fraction_recombined_genome_all.threshold_logBF==threshold_logBF]

    # define thresgolds
    thresholds_fractionBootstraps = sorted(set(df_fraction_recombined_genome_all.threshold_fractionBootstraps))

    # define all species
    all_species_rec = set(df_fraction_recombined_genome_all_filt.species).difference({"Candida_auris", "Candida_glabrata"})
    sorted_species = [species for species in sorted_species_byPhylogeny if species in all_species_rec]

    # define the type of recombination
    types_recombination = ["ancestral", "recent", "all"]

    # init fig
    nrows = len(sorted_species)
    ncols = len(types_recombination)
    fig = plt.figure(figsize=(ncols*3, nrows*3)); Ip=1

    # define ylims
    #ylim = [-0.05, max(df_fraction_recombined_genome_all_filt.total_fraction_genome_recombined)+0.05]
    ylim = [-0.05, 1.05]

    for Ir, species in enumerate(sorted_species):


        # define clades and clade-graphics
        metadata_df["sampleID"] = metadata_df.sampleID.apply(int)
        def get_correct_cladeID_Tree_and_BranchLen(r):
            if pd.isna(r.cladeID_Tree_and_BranchLen): return "unassigned_%i"%(r.sampleID)
            else: return int(r.cladeID_Tree_and_BranchLen)

        sample_to_clade = metadata_df[metadata_df.species_name==species].set_index("sampleID", drop=False).apply(get_correct_cladeID_Tree_and_BranchLen, axis=1)


        # set clade to color
        sorted_numeric_clades = sorted(set(sample_to_clade[sample_to_clade.apply(type)==int]))
        if len(sorted_numeric_clades)<10: palette="tab10"
        else: palette="tab20"
        clade_to_color = get_value_to_color(sorted_numeric_clades, palette=palette, n=len(sorted_numeric_clades), type_color="hex")[0]

        for cID in set(sample_to_clade[sample_to_clade.apply(type)==str]): clade_to_color[cID] = "gray"

        # convert to str
        clade_to_color  = {str(cID):c for cID,c in clade_to_color.items()}


        for Ic, type_recombination in enumerate(types_recombination):

            print(species, type_recombination)

            # get the df of this suplot
            df_plot = df_fraction_recombined_genome_all_filt[(df_fraction_recombined_genome_all_filt.species==species) & (df_fraction_recombined_genome_all_filt.type_recombination==type_recombination)].reset_index(drop=True).sort_values(by=["sampleID", "threshold_fractionBootstraps"])

            # add the clade ID
            df_plot["clade"] = df_plot.sampleID.map(dict(sample_to_clade)).apply(str)
            if any(pd.isna(df_plot.clade)): raise ValueError("there can't be nans in clade")

            # init subplot
            ax = plt.subplot(nrows, ncols, Ip); Ip+=1

            # add lineplot
            ax = sns.lineplot(x="threshold_fractionBootstraps", y="total_fraction_genome_recombined", data=df_plot, units="sampleID", estimator=None, lw=1, alpha=0.5, hue="clade", palette=clade_to_color, markers=True)

            ax.set_ylim(ylim)

            # add title
            if Ir==0: ax.set_title("%s recombination"%type_recombination)

            # legend
            if type_recombination==(types_recombination[-1]): ax.legend(loc=(1.03, 0))
            else: ax.get_legend().remove()


            # ylabel
            if Ic==0: ax.set_ylabel("%s\nfraction genome recombined"%(species))
            else:
                ax.set_ylabel("")
                ax.set_yticklabels([])

    plt.subplots_adjust(wspace=0.02, hspace=0.02)
    
    dirplot = "%s/qualityControl_fastGEAR_recombination"%PlotsDir; make_folder(dirplot)
    filename_plot = "%s/boxplot_fractionRecombinedGenome_severalThresholds.pdf"%(dirplot)
    fig.savefig(filename_plot, bbox_inches='tight')


def get_tree_withcladeID_as_str(tree, leaf_to_clade):

    """Gets a tree and adds the clade"""

    tree = cp.deepcopy(tree)

    for n in tree.traverse():

        # leafs are easy
        if n.is_leaf(): n.cladeID = leaf_to_clade[n.name]

        # get the clades of the leaf names
        else:
            leafs_clades = {leaf_to_clade[l] for l in n.get_leaf_names()}
            if len(leafs_clades)==1: n.cladeID = next(iter(leafs_clades))
            else: n.cladeID = np.nan

    return tree


def get_relation_clades_with_previousPaper(metadata_df, species):

    """Takes the metadata df and relates returns the mapping between each sample or clade to the clade previously defined"""

    # define metadata df for this species
    metadata_df = metadata_df[metadata_df.species_name==species]

    # map each sample to the previous paper
    sample_to_cladePreviousPaper = cp.deepcopy(dict(metadata_df.set_index("sampleID").cladeID_previousPaper.apply(str)))
    sample_to_cladePreviousPaper = {str(s): c for s,c in sample_to_cladePreviousPaper.items()}

    # map each clade to the samples
    cladePreviousPaper_to_samples = dict(metadata_df[~pd.isna(metadata_df.cladeID_previousPaper)].groupby("cladeID_previousPaper").apply(lambda df_c: sorted(set(df_c.sampleID))))

    # map each sample to a clade
    metadata_df["sampleID"] = metadata_df.sampleID.apply(int)
    def get_correct_cladeID_Tree_and_BranchLen(r):
        if pd.isna(r.cladeID_Tree_and_BranchLen): return "unassignedSample_%i"%(r.sampleID)
        else: return int(r.cladeID_Tree_and_BranchLen)

    sample_to_clade = metadata_df.set_index("sampleID", drop=False).apply(get_correct_cladeID_Tree_and_BranchLen, axis=1)

    # map each clade to the clades of each sample
    cladePreviousPaper_to_cladesOfSamples = {cprev : sorted([sample_to_clade[int(s)] for s in samples if not str(sample_to_clade[int(s)]).startswith("unassignedSample_")]) for cprev, samples in cladePreviousPaper_to_samples.items()}

    # map each clade to the clade of the previous sample according to the most frequent element
    def get_mostfrequent_element_in_list(list_obj): return sorted([(x, list_obj.count(x)) for x in list_obj if not(pd.isna(x))], key=(lambda y: y[1]))[-1][0]
    cladePreviousPaper_to_clade = {cprev : str(get_mostfrequent_element_in_list(clades)) for cprev, clades in cladePreviousPaper_to_cladesOfSamples.items()}
    clade_to_cladePreviousPaper = {c: cprev  for cprev, c in cladePreviousPaper_to_clade.items()}

    # add the unassigned clades as in sample_to_cladePreviousPaper
    for clade in set(sample_to_clade):
        if str(clade) not in set(clade_to_cladePreviousPaper): clade_to_cladePreviousPaper[str(clade)] = "nan"

    return sample_to_clade, sample_to_cladePreviousPaper, clade_to_cladePreviousPaper

def plot_tree_with_fraction_recombinant_genome_eachClade_fastGEAR(df_fraction_recombined_genome_all, PlotsDir, metadata_df, species_to_tree, ProcessedDataDir, min_fraction_donor=0.05):

    """Makes a plot with the fraction of recombinant genome attributed to each clade from fastGEAR"""

    # keep only the df with logBF filter of 0 and with bootrsaps above 0.9
    threshold_logBF = 0.0
    threshold_fractionBootstraps = 0.9
    df_fraction_recombined_genome_all_filt = df_fraction_recombined_genome_all[(df_fraction_recombined_genome_all.threshold_logBF==threshold_logBF) & (df_fraction_recombined_genome_all.threshold_fractionBootstraps==threshold_fractionBootstraps)]

    # define all species
    all_species_rec = set(df_fraction_recombined_genome_all_filt.species)#.difference({"Candida_auris", "Candida_glabrata"})
    sorted_species = [species for species in sorted_species_byPhylogeny if species in all_species_rec]

    # define the type of recombination
    #types_recombination = ["ancestral", "recent", "all"]
    types_recombination = ["ancestral", "recent"]

    # define all the fraction values
    print("getting all fraction values")
    all_fraction_values = set()
    df_interesting_spp = df_fraction_recombined_genome_all_filt[df_fraction_recombined_genome_all_filt.species.isin(all_species_rec)]

    for c in df_interesting_spp.columns:
        if not c.startswith("donorClade_"): continue

        all_fraction_values.update(set(df_interesting_spp[~pd.isna(df_interesting_spp[c])][c]))

    all_fraction_values = [0.0] + sorted(all_fraction_values)

    # define the rectangel width 
    rect_width = 25

    # go through each species and types recombination
    for species in sorted_species:
        print(species)

        if species!="Candida_tropicalis": continue

        # define clades and clade-graphics
        sample_to_clade, sample_to_cladePreviousPaper, clade_to_cladePreviousPaper = get_relation_clades_with_previousPaper(metadata_df, species)
        clade_to_cladePreviousPaper["external"] = "nan"

        # set clade to color
        sorted_numeric_clades = sorted(set(sample_to_clade[sample_to_clade.apply(type)==int]))
        if len(sorted_numeric_clades)<10: palette="tab10"
        else: palette="tab20"
        clade_to_color = get_value_to_color(sorted_numeric_clades, palette=palette, n=len(sorted_numeric_clades), type_color="hex")[0]

        for cID in set(sample_to_clade[sample_to_clade.apply(type)==str]): clade_to_color[cID] = "gray"

        # convert to str
        clade_to_color  = {str(cID):c for cID,c in clade_to_color.items()}
        sample_to_clade = {str(s):str(c) for s,c in sample_to_clade.items()}

        # add the external one
        clade_to_color["external"] = "black"

        for type_recombination in types_recombination:
            print(species, type_recombination)

            # get the df of this suplot
            df_plot = cp.deepcopy(df_fraction_recombined_genome_all_filt[(df_fraction_recombined_genome_all_filt.species==species) & (df_fraction_recombined_genome_all_filt.type_recombination==type_recombination)].reset_index(drop=True))

            # check that the unassigned samples never work as donnors in >5% of the genome
            for clade in set(sample_to_clade.values()):
                if clade.startswith("unassignedSample_"):
                    df_unassigned_sampleDonor = df_plot[df_plot["donorClade_%s"%clade]>min_fraction_donor]

                    if len(df_unassigned_sampleDonor)>0: 
                        print(df_unassigned_sampleDonor["donorClade_%s"%clade])
                        raise ValueError("There are donors that are single samples in clade %s of %s"%(clade, species))

            # add the clade ID (a str)
            df_plot["clade"] = df_plot.sampleID.map(dict(sample_to_clade)).apply(str)
            if any(pd.isna(df_plot.clade)): raise ValueError("there can't be nans in clade")

            # remove bad samples
            tree = cp.deepcopy(species_to_tree[species])
            correct_samples = set(tree.get_leaf_names()).difference({str(x) for x in sciName_to_badSamples[species]})
            tree.prune(correct_samples)

            # remove low support branches
            tree = get_correct_tree(tree, min_support=95)

            # add the cladeID as a string
            tree = get_tree_withcladeID_as_str(tree, sample_to_clade)

            # make a palette for the colorscale
            float_to_color, color_palette = get_value_to_color(all_fraction_values, palette="coolwarm", n=10, type_color="hex")

            # define the sorted donor clades according to the tree, only tohse that have a minimum donor
            sorted_donor_clades = list(pd.Series([l.cladeID for l in tree.get_leaves()]).unique()) + ["external"]
            sorted_donor_clades = [c for c in sorted_donor_clades if any(df_plot["donorClade_%s"%(c)]>min_fraction_donor)]
            sorted_donor_clades_fields = ["donorClade_%s"%(c) for c in sorted_donor_clades]

            # if the type of recombination is ancestral, keep only ancestral nodes
            if type_recombination=="ancestral":

                # init a map between each node and the number of leafs
                clade_to_nleafs = {}

                # prune the tree to keep only ancestral nodes
                for n in tree.traverse():
                    if n.is_root(): continue

                    # if you found a clade
                    if pd.isna((n.get_ancestors()[0]).cladeID) and not pd.isna(n.cladeID): 

                        clade_to_nleafs[n.cladeID] = len(n.get_leaves())
                        for child in n.get_children(): child.detach()
                        n.name = n.cladeID

                # redefine the df_plot to include only one representative of each clade
                sorted_donor_clades_fields = ["donorClade_%s"%x for x in sorted_donor_clades]
                df_plot = df_plot[["sampleID"] + sorted_donor_clades_fields]
                df_plot["sampleID"] = df_plot.sampleID.apply(str).map(sample_to_clade)
                df_plot = df_plot.drop_duplicates()
                if any(pd.isna(df_plot.sampleID)): raise ValueError("There can't be nans in the sampleID")

            # get the indexed df_plot
            df_plot["sampleID"] = df_plot.sampleID.apply(str)
            df_plot = df_plot.set_index("sampleID", drop=False)

            # get th
            for n in tree.traverse():

                nst = NodeStyle()
                nst["hz_line_width"] = 8
                nst["vt_line_width"] = 8

                # define the size of the node
                if type_recombination=="ancestral" and n.is_leaf():  nst["size"] = clade_to_nleafs[n.name]
                else: nst["size"] = 0

                if pd.isna(n.cladeID): branch_color = "gray"
                else: branch_color = clade_to_color[n.cladeID]

                nst["fgcolor"] = branch_color
                nst["hz_line_color"] = branch_color
                nst["vt_line_color"] = branch_color
                n.set_style(nst)

                if n.is_leaf(): 

                    # define a multiplier based on whether there are any rec events in this node
                    #if any(df_plot.loc[n.name, sorted_donor_clades_fields]>min_fraction_donor): multiplier_width = 1.0
                    #else: multiplier_width = 0.1
                    multiplier_width = 1.0

                    # add a box with the clade ID
                    color_leaf =  clade_to_color[n.cladeID]

                    if n.cladeID.startswith("unassigned"): label = "?"
                    else: label = str(n.cladeID)

                    n.add_face(RectFace(rect_width*multiplier_width, rect_width*multiplier_width, fgcolor="black", bgcolor=color_leaf, label={"text":label, "color":get_annotationColor_on_bgcolor(color_leaf)}), position="aligned", column=0)

                    # add a box with the previously defined clade ID
                    if type_recombination=="ancestral": cladePreviousPaper = clade_to_cladePreviousPaper[n.name]
                    else: cladePreviousPaper = sample_to_cladePreviousPaper[n.name]

                    if cladePreviousPaper=="nan": 
                        color_prevPaper = "white"
                        label_prevPaper = ""

                    else:
                        color_prevPaper = color_leaf
                        label_prevPaper = cladePreviousPaper

                    n.add_face(RectFace(rect_width*multiplier_width, rect_width*multiplier_width, fgcolor="black", bgcolor=color_prevPaper, label={"text":label_prevPaper, "color":get_annotationColor_on_bgcolor(color_prevPaper)}), position="aligned", column=1)


                    # add a white box
                    n.add_face(RectFace(rect_width*multiplier_width, rect_width*multiplier_width, fgcolor="white", bgcolor="white", label={"text":"", "color":"black"}), position="aligned", column=2)

                    # for each donor clade plot a square with the sampleID
                    for Id, donor_clade in enumerate(sorted_donor_clades):

                        # add the node
                        fraction_recom = df_plot.loc[n.name, "donorClade_%s"%donor_clade]
                        color_fraction = float_to_color[fraction_recom]
                        n.add_face(RectFace(rect_width*multiplier_width, rect_width*multiplier_width, fgcolor="black", bgcolor=color_fraction, label={"text":"", "color":"black"}), position="aligned", column=Id+3)

            # remove leaf names
            if type_recombination=="ancestral": 
                for l in tree.get_leaves():
                    if l.name.startswith("unassignedSample_"): l.name = l.name.split("_")[1] + " "
                    else: l.name = ""

            # init treestyle
            ts = TreeStyle()
            ts.show_branch_length = False
            ts.show_branch_support = False
            ts.show_leaf_name = True

            

            # add the label of the aligned 

            # add the aligned footer and header the clades
            for Id, donor_clade in enumerate(sorted_donor_clades):


                # define the color and label of the previous paper
                cladePreviousPaper = clade_to_cladePreviousPaper[donor_clade]
                if cladePreviousPaper=="nan": 
                    color_prevPaper = "white"
                    label_prevPaper = ""

                else:
                    color_prevPaper = clade_to_color[donor_clade]
                    label_prevPaper = cladePreviousPaper


                # define the color and label of the clade defined here
                color_clade = clade_to_color[donor_clade]
                if donor_clade.startswith("unassigned"): label = "?"
                elif donor_clade=="external": label = "e"
                else: label = str(donor_clade)



                # add the footer
                ts.aligned_foot.add_face(RectFace(rect_width, rect_width, fgcolor="black", bgcolor=color_clade, label={"text":label, "color":get_annotationColor_on_bgcolor(color_clade)}), column=Id+3)

                ts.aligned_foot.add_face(RectFace(rect_width, rect_width, fgcolor="black", bgcolor=color_prevPaper, label={"text":label_prevPaper, "color":get_annotationColor_on_bgcolor(color_prevPaper)}), column=Id+3)


                # add the header
                ts.aligned_header.add_face(RectFace(rect_width, rect_width, fgcolor="black", bgcolor=color_prevPaper, label={"text":label_prevPaper, "color":get_annotationColor_on_bgcolor(color_prevPaper)}), column=Id+3)

                ts.aligned_header.add_face(RectFace(rect_width, rect_width, fgcolor="black", bgcolor=color_clade, label={"text":label, "color":get_annotationColor_on_bgcolor(color_clade)}), column=Id+3)


            # add legend

            # plot the label 
            ts.legend.add_face(TextFace("\nfraction\nrecombinant genome ", bold=True, fsize=12), column=0)
            ts.legend.add_face(TextFace(" ", bold=True, fsize=20), column=0)

            # go through each box of the colorbar
            for i, (val, color) in enumerate(color_palette.items()):

                # add the rectangle with the color
                ts.legend.add_face(RectFace(35, 35, fgcolor="black", bgcolor=get_correct_color(color)), column=i+1)

                # add the annotation
                nameF = TextFace("%.2f"%val, bold=True, fsize=12); nameF.rotation = -90
                ts.legend.add_face(nameF, column=i+1)

    
            ts.legend_position=3

            # add the title
            ts.title.add_face(TextFace("%s\n%s recombination\n"%(species, type_recombination), bold=True, fsize=12), column=0)
    
            tree.show(tree_style=ts)    

            plotsdir = "%s/trees_fastgear_fractionRecombination"%PlotsDir; make_folder(plotsdir)
            tree.render(file_name="%s/%s_%s_recombination.pdf"%(plotsdir, species, type_recombination), tree_style=ts)#, dpi=500,  units='mm', h=nleafs*2) #w=20


def get_species_to_drug_to_samplesForGWAS_clinicalIsolates(metadata_df):

    """Takes the metadata df and returns, for different species and drugs, the useful samples (only clinical isolates and in-mouse evolved lines)."""

    # define drugs
    azoles = ["FLC", "POS", "VRC", "IVZ", "ITR", "KET", "CLZ", "MIZ"]
    echinocandins = ["MIF", "ANI", "CAS"]
    other_drugs = ["AMB", "UCA", "BVN", "5FC", "TRB"]
    all_drugs = set(azoles + echinocandins + other_drugs)

    # init df
    species_to_drug_to_samplesForGWAS = {}

    for species in ["Candida_glabrata", "Candida_auris", "Candida_albicans"]:
        
        # get df with clinical isolates
        df = metadata_df[(metadata_df.species_name==species) & (metadata_df["type"].isin({"clinical"}))] # there are no samples evolved in mouse for which we have AF data for the 


        # go through each drug
        for drug in all_drugs:

            # get dfs
            df_R = df[df["%s_resistance"%drug].apply(str)=="R"]
            df_S = df[df["%s_resistance"%drug].apply(str)=="S"]

            if len(df_R)>=5 and len(df_S)>=5: 

                interesting_samples = set(df_R.sampleID).union(df_S.sampleID)
                species_to_drug_to_samplesForGWAS.setdefault(species, {}).setdefault(drug, interesting_samples)

                #print("in %s for drug %s there are %i/%i susceptible/resistant samples."%(species, drug, len(df_S), len(df_R)))


    return species_to_drug_to_samplesForGWAS


def plot_tree_AF_resistance_data_GWAS(species_to_drug_to_samplesForGWAS, species_to_tree, metadata_df, PlotsDir):

    """Plots trees for each species with AF resistance data for drugs where you can do GWAS"""

    # define drugs
    azoles = ["FLC", "POS", "VRC", "IVZ", "ITR", "KET", "CLZ", "MIZ"]
    echinocandins = ["MIF", "ANI", "CAS"]
    all_drugs = azoles + ["blank"] + echinocandins + ["blank", "5FC", "AMB"] 

    for species, drug_to_samples in species_to_drug_to_samplesForGWAS.items():

        # define clades and clade-graphics
        metadata_df["sampleID"] = metadata_df.sampleID.apply(int)
        def get_correct_cladeID_Tree_and_BranchLen(r):
            if pd.isna(r.cladeID_Tree_and_BranchLen): return "unassignedSample_%i"%(r.sampleID)
            else: return int(r.cladeID_Tree_and_BranchLen)

        sample_to_clade = metadata_df[metadata_df.species_name==species].set_index("sampleID", drop=False).apply(get_correct_cladeID_Tree_and_BranchLen, axis=1)

        # define a metadata df for this species
        metadata_df_s = metadata_df[metadata_df.species_name==species].set_index("sampleID", drop=False)

        # set clade to color
        sorted_numeric_clades = sorted(set(sample_to_clade[sample_to_clade.apply(type)==int]))
        if len(sorted_numeric_clades)<10: palette="tab10"
        else: palette="tab20"
        clade_to_color = get_value_to_color(sorted_numeric_clades, palette=palette, n=len(sorted_numeric_clades), type_color="hex")[0]

        for cID in set(sample_to_clade[sample_to_clade.apply(type)==str]): clade_to_color[cID] = "gray"

        # convert to str
        clade_to_color  = {str(cID):c for cID,c in clade_to_color.items()}
        sample_to_clade = {str(s):str(c) for s,c in sample_to_clade.items()}

        # define the tree
        samples_to_keep = set(species_to_tree[species].get_leaf_names()).difference({str(s) for s in sciName_to_badSamples[species]})
        tree = cp.deepcopy(species_to_tree[species]); tree.prune(samples_to_keep, preserve_branch_length=True)
        tree = get_tree_withcladeID_as_str(tree, sample_to_clade)

        # keep only some samples
        all_samples = {str(x) for x in set.union(*drug_to_samples.values())}
        tree.prune(all_samples, preserve_branch_length=True)

        # define sorted drugs
        sorted_drugs = [drug for drug in all_drugs if drug in drug_to_samples or drug=="blank"]

        # define graphical parms
        square_width = 30
        resistance_type_to_color = {"R":"red", "I":"plum", "S":"blue", "nan":"white"}
        typeSample_to_str = {"clinical":"C", "inmouse_evol_clone":"M"}

        # go through each node
        for n in tree.traverse():

            nst = NodeStyle()
            nst["hz_line_width"] = 8
            nst["vt_line_width"] = 8
            nst["size"] = 0


            if pd.isna(n.cladeID): branch_color = "gray"
            else: branch_color = clade_to_color[n.cladeID]

            nst["hz_line_color"] = branch_color
            nst["vt_line_color"] = branch_color
            n.set_style(nst)

            if n.is_leaf(): 

                # add a box with the clade ID
                color_leaf =  clade_to_color[n.cladeID]

                if n.cladeID.startswith("unassigned"): label = "?"
                else: label = str(n.cladeID)

                n.add_face(RectFace(square_width, square_width, fgcolor="black", bgcolor=color_leaf, label={"text":label, "color":get_annotationColor_on_bgcolor(color_leaf)}), position="aligned", column=0)

                n.add_face(RectFace(square_width*0.5, square_width, fgcolor="white", bgcolor="white", label={"text":"", "color":"black"}), position="aligned", column=1)

                # add a box for each drug
                for Id, drug in enumerate(sorted_drugs):

                    if drug!="blank":

                        # define the resistance
                        def get_correct_resistance_type(x):
                            if pd.isna(x): return "nan"
                            else: return x
                        resistance_type = get_correct_resistance_type(metadata_df_s.loc[int(n.name), "%s_resistance"%drug])
                        
                        bgcolor = resistance_type_to_color[resistance_type]
                        fgcolor = "black"
                        d_square_width = square_width

                    else: 
                        bgcolor = fgcolor = "white"
                        d_square_width = square_width*0.5
                        

                    n.add_face(RectFace(d_square_width, square_width, fgcolor=fgcolor, bgcolor=bgcolor, label={"text":"", "color":"black"}), position="aligned", column=Id+2)

                # add the type type of sample
                sample_type = metadata_df_s.loc[int(n.name), "type"]
                n.add_face(TextFace(typeSample_to_str[sample_type], bold=True, fsize=12), position="aligned", column=Id+3)

        # init treestyle
        ts = TreeStyle()
        ts.show_branch_length = False
        ts.show_branch_support = False
        ts.show_leaf_name = True

        # add the aligned footer and header the clades
        for Id, drug in enumerate(sorted_drugs):

            if drug in azoles: color = "cyan"
            elif drug in echinocandins: color = "magenta"
            elif drug=="5FC": color = "black"
            elif drug=="AMB": color = "dimgray"
            elif drug=="blank": color = "white"
            else: raise ValueError("invalid drug")

            if drug!="blank": 
                fgcolor = "blank"
                label = drug
                d_square_width = square_width

            else:
                fgcolor = "white"
                label = ""
                d_square_width = square_width*0.5


            rectFace = RectFace(square_width*2, d_square_width, fgcolor=fgcolor, bgcolor=color, label={"text":label, "color":get_annotationColor_on_bgcolor(color, threshold_gray=0.6)})
            rectFace.rotation = 90

            ts.aligned_foot.add_face(rectFace, column=Id+2)
            ts.aligned_header.add_face(cp.deepcopy(rectFace), column=Id+2)

            #ts.aligned_header.add_face(RectFace(20, 20, fgcolor="black", bgcolor=color, label={"text":drug, "color":get_annotationColor_on_bgcolor(color)}), column=Id+2)


        # plot the legend 
        ts.legend.add_face(TextFace("\nresistance type ", bold=True, fsize=12), column=0)
        ts.legend.add_face(TextFace(" ", bold=True, fsize=20), column=0)

        # go through each box of the colorbar
        resistance_type_to_color_legend = {t:c for t,c in resistance_type_to_color.items() if t!="nan"}
        for i, (val, color) in enumerate(resistance_type_to_color_legend.items()):

            # add the rectangle with the color
            ts.legend.add_face(RectFace(35, 35, fgcolor="black", bgcolor=get_correct_color(color)), column=i+1)

            # add the annotation
            nameF = TextFace(val, bold=True, fsize=12); nameF.rotation = -90
            ts.legend.add_face(nameF, column=i+1)

        ts.legend_position=3

        # add the title
        ts.title.add_face(TextFace("%s resistance clinical isolates\n"%(species ), bold=True, fsize=12), column=0)

        tree.show(tree_style=ts)


        plotsdir = "%s/trees_resistanceData_GWAS"%PlotsDir; make_folder(plotsdir)
        tree.render(file_name="%s/%s.pdf"%(plotsdir, species), tree_style=ts)#, dpi=500,  units='mm', h=nleafs*2) #w=20



def plot_tree_AF_resistance_data_GWAS_oneTreePerDrug(species_to_drug_to_samplesForGWAS, species_to_tree, metadata_df, PlotsDir):

    """Plots trees for each species with AF resistance data for drugs where you can do GWAS"""

    # define drugs
    azoles = ["FLC", "POS", "VRC", "IVZ", "ITR", "KET", "CLZ", "MIZ"]
    echinocandins = ["MIF", "ANI", "CAS"]
    all_drugs = azoles + echinocandins + ["5FC", "AMB"] 

    for species, drug_to_samples in species_to_drug_to_samplesForGWAS.items():

        # define clades and clade-graphics
        metadata_df["sampleID"] = metadata_df.sampleID.apply(int)
        def get_correct_cladeID_Tree_and_BranchLen(r):
            if pd.isna(r.cladeID_Tree_and_BranchLen): return "unassignedSample_%i"%(r.sampleID)
            else: return int(r.cladeID_Tree_and_BranchLen)

        sample_to_clade = metadata_df[metadata_df.species_name==species].set_index("sampleID", drop=False).apply(get_correct_cladeID_Tree_and_BranchLen, axis=1)

        # define a metadata df for this species
        metadata_df_s = metadata_df[metadata_df.species_name==species].set_index("sampleID", drop=False)

        # set clade to color
        sorted_numeric_clades = sorted(set(sample_to_clade[sample_to_clade.apply(type)==int]))
        if len(sorted_numeric_clades)<10: palette="tab10"
        else: palette="tab20"
        clade_to_color = get_value_to_color(sorted_numeric_clades, palette=palette, n=len(sorted_numeric_clades), type_color="hex")[0]

        for cID in set(sample_to_clade[sample_to_clade.apply(type)==str]): clade_to_color[cID] = "gray"

        # convert to str
        clade_to_color  = {str(cID):c for cID,c in clade_to_color.items()}
        sample_to_clade = {str(s):str(c) for s,c in sample_to_clade.items()}


        # define sorted drugs
        sorted_drugs = [drug for drug in all_drugs if drug in drug_to_samples or drug=="blank"]

        # define graphical parms
        square_width = 30
        #resistance_type_to_color = {"R":"red", "I":"plum", "S":"blue", "nan":"white"}
        resistance_type_to_color = {"R":"red", "S":"blue", "nan":"white"}
        typeSample_to_str = {"clinical":"C", "inmouse_evol_clone":"M"}

        # one tree for each drug
        for drug in sorted_drugs:
            print(species, drug)

            # define the tree
            samples_to_keep = set(species_to_tree[species].get_leaf_names()).difference({str(s) for s in sciName_to_badSamples[species]})
            tree = cp.deepcopy(species_to_tree[species]); tree.prune(samples_to_keep, preserve_branch_length=True)
            tree = get_tree_withcladeID_as_str(tree, sample_to_clade)

            # keep only some samples
            all_samples = {str(x) for x in drug_to_samples[drug]}
            tree.prune(all_samples, preserve_branch_length=True)

            # go through each node
            for n in tree.traverse():

                nst = NodeStyle()
                nst["hz_line_width"] = 8
                nst["vt_line_width"] = 8
                nst["size"] = 0

                if pd.isna(n.cladeID): branch_color = "gray"
                else: branch_color = clade_to_color[n.cladeID]

                nst["hz_line_color"] = branch_color
                nst["vt_line_color"] = branch_color
                n.set_style(nst)

                if n.is_leaf(): 

                    # add a box with the clade ID
                    color_leaf =  clade_to_color[n.cladeID]

                    if n.cladeID.startswith("unassigned"): label = "?"
                    else: label = str(n.cladeID)

                    n.add_face(RectFace(square_width, square_width, fgcolor="black", bgcolor=color_leaf, label={"text":label, "color":get_annotationColor_on_bgcolor(color_leaf)}), position="aligned", column=0)

                    #n.add_face(RectFace(square_width*0.5, square_width, fgcolor="white", bgcolor="white", label={"text":"", "color":"black"}), position="aligned", column=1)

                    # define the resistance
                    def get_correct_resistance_type(x):
                        if pd.isna(x): return "nan"
                        else: return x
                    resistance_type = get_correct_resistance_type(metadata_df_s.loc[int(n.name), "%s_resistance"%drug])
                    
                    bgcolor = resistance_type_to_color[resistance_type]
                    fgcolor = "black"
                    d_square_width = square_width
  
                    # add a box with the resistance type
                    n.add_face(RectFace(d_square_width, square_width, fgcolor=fgcolor, bgcolor=bgcolor, label={"text":"", "color":"black"}), position="aligned", column=1)

                    # add the type type of sample
                    #sample_type = metadata_df_s.loc[int(n.name), "type"]
                    #n.add_face(TextFace(typeSample_to_str[sample_type], bold=True, fsize=12), position="aligned", column=3)

            # init treestyle
            ts = TreeStyle()
            ts.show_branch_length = False
            ts.show_branch_support = False
            ts.show_leaf_name = True
            ts.mode = "c"
            #ts.root_opening_factor = 100 # the higher the morecompact
            #ts.optimal_scale_level = "full"
            #ts.arc_start = 180 # 0 degrees = 3 o'clock
            #ts.arc_span = 359
            #ts.force_topology = False

            if drug in azoles: color = "cyan"
            elif drug in echinocandins: color = "magenta"
            elif drug=="5FC": color = "black"
            elif drug=="AMB": color = "dimgray"
            elif drug=="blank": color = "white"
            else: raise ValueError("invalid drug")

            if drug!="blank": 
                fgcolor = "blank"
                label = drug
                d_square_width = square_width

            else:
                fgcolor = "white"
                label = ""
                d_square_width = square_width*0.5


            rectFace = RectFace(square_width*2, d_square_width, fgcolor=fgcolor, bgcolor=color, label={"text":label, "color":get_annotationColor_on_bgcolor(color, threshold_gray=0.6)})
            rectFace.rotation = 90

            ts.aligned_foot.add_face(rectFace, column=2)
            ts.aligned_header.add_face(cp.deepcopy(rectFace), column=2)

            # plot the legend 
            ts.legend.add_face(TextFace("\nresistance type ", bold=True, fsize=12), column=0)
            ts.legend.add_face(TextFace(" ", bold=True, fsize=20), column=0)

            # go through each box of the colorbar
            resistance_type_to_color_legend = {t:c for t,c in resistance_type_to_color.items() if t!="nan"}
            for i, (val, color) in enumerate(resistance_type_to_color_legend.items()):

                # add the rectangle with the color
                ts.legend.add_face(RectFace(35, 35, fgcolor="black", bgcolor=get_correct_color(color)), column=i+1)

                # add the annotation
                nameF = TextFace(val, bold=True, fsize=12); nameF.rotation = -90
                ts.legend.add_face(nameF, column=i+1)

            ts.legend_position=3

            # add the title
            ts.title.add_face(TextFace("%s %s resistance clinical isolates\n"%(species, drug ), bold=True, fsize=12), column=0)

            # show
            tree.show(tree_style=ts)

            # save
            plotsdir = "%s/trees_resistanceData_GWAS_eachDrug"%PlotsDir; make_folder(plotsdir)
            filename = "%s/%s_%s.pdf"%(plotsdir, species, drug)
            print("saving %s"%filename)
            tree.render(file_name=filename, tree_style=ts)#, dpi=500,  units='mm', h=nleafs*2) #w=20



def get_var_impacting_all_protein(r):

    """Takes a row of the annot_df and returns whether the variant is altering all the protein"""

    if r.is_protein_altering is False: return False
    elif len(r.consequences_set.intersection({"transcript_amplification", "transcript_ablation"}))>0: return True
    elif r.is_truncating is True and r.CDS_position=="-": return True 
    else: return False 


def get_protein_positions_ints(r):

    """Takes a row of the annot_df and returns a df with the protein positions"""

    if r.is_protein_altering is False or r.var_impacting_all_protein is True: return []
    else: return sorted({int(x) for x in r.Protein_position.split("-") if x!="?"})

def get_protein_alteration_start_annot_df_r(r):

    """Gets the protein alteration start for annot df r"""

   # for non protein altering vars, just retrieve nans
    if r.is_protein_altering is False: return np.nan

    # cases were the whole gene should be considered
    elif r.var_impacting_all_protein is True: return 1

    # other cases
    else:

        # all the protein alterations should have a position
        if r.Protein_position=="-": raise ValueError("The protein position can't be - for %s. consequences:%s"%(r, r.consequences_set))

        # for all the non-synonynmous variants, the start is the start of the protein
        return r.protein_postions_ints[0]

def get_protein_alteration_end_annot_df_r(r):

    """
    Gets the end of the protein alteration.
    """

    # for non protein altering vars, just retrieve nans
    if r.is_protein_altering is False: return np.nan

    # cases were the whole gene should be considered
    elif r.var_impacting_all_protein is True: return int(r.protein_length)

    # other cases
    else:

        # all the protein alterations should have a position
        if r.Protein_position=="-": raise ValueError("The protein position can't be - for %s. consequences:%s"%(r, r.consequences_set))

        # for truncations or regions of uncertain end, the alteration goes to the end of the protein
        if r.is_truncating is True or r.Protein_position.endswith("?"): return int(r.protein_length)

        # if there are two positions
        elif len(r.protein_postions_ints)==2: return r.protein_postions_ints[1]

        # if there is only one positions
        elif "-" not in r.Protein_position: return r.protein_postions_ints[0]+1

        else: raise ValueError("%s was not correctly parsed"%r.Protein_position)


def get_vars_df_only_subset_samples_for_GWAS(taxID_dir, sampleIDs, outdir, ploidy, replace, gff, tree):

    """Writes a df with the variants per sample in only a subset of samples. The variants df are written in a square manner with mut_1, mut_2 ... format and sorted as in the tree. """

    # define files
    variants_df_file = "%s/variants_df.py"%outdir
    annot_df_file = "%s/annot_df.py"%outdir

    ############## GET VARIANTS ##################
    if file_is_empty(variants_df_file) or replace is True:
        print("getting variants for GWAS")

        # load SV_CNVs and keep important fields
        print("SVs")
        SV_CNV_df = load_object("%s/integrated_varcalls/SV_CNV_filt.py"%taxID_dir)
        SV_CNV_df = SV_CNV_df[(SV_CNV_df.sampleID.isin(sampleIDs))][["#CHROM", "POS", "REF", "ALT", "ID", "INFO_merged_relative_CN", "sampleID", "variantID_across_samples", "type_var", "is_transcript_disrupting_inSomeGenes"]]
        SV_CNV_df["#Uploaded_variation"] = SV_CNV_df.ID
        
        # load small vars
        print("small vars")
        small_vars_df = load_object("%s/integrated_varcalls/smallVars_filt.py"%taxID_dir)
        small_vars_df = small_vars_df[(small_vars_df.sampleID.isin(sampleIDs))]

        # filter small vars depending on the ploidy
        if ploidy==1: 
            
            small_vars_df["is_duplicated_and_hetero"] = (small_vars_df.calling_ploidy==2) & (small_vars_df.common_GT=="0/1") & (small_vars_df.relative_CN>=2.0)
            small_vars_df = small_vars_df[(small_vars_df.calling_ploidy==1) | (small_vars_df.is_duplicated_and_hetero)]

        elif ploidy==2: small_vars_df = small_vars_df[small_vars_df.calling_ploidy==2]

        # keep some relevant fields
        small_vars_df = small_vars_df[['#Uploaded_variation', '#CHROM', 'POS', 'ISSNP', 'common_GT', 'relative_CN', 'sampleID','is_protein_altering_inSomeGenes']]
        small_vars_df["variantID_across_samples"] = small_vars_df["#Uploaded_variation"]
        small_vars_df["type_var"] = small_vars_df.ISSNP.map({True:"SNP", False:"INDEL"})

        # quality control
        if any(pd.isna(small_vars_df.type_var)): raise ValueError("There can't be nans in type_var")

        # merge
        print("merging")
        variants_df = small_vars_df.append(SV_CNV_df)

        # change some formats
        variants_df["sampleID"] = variants_df["sampleID"].apply(int).apply(str)

        # remove variants that are in all samples
        var_to_fractionSamplesWithVar = variants_df[["sampleID", "variantID_across_samples"]].drop_duplicates().groupby("variantID_across_samples").apply(len)/len(sampleIDs)
        variable_variants = set(var_to_fractionSamplesWithVar[var_to_fractionSamplesWithVar<1].index)
        print("There are %i/%i vars that are not in all samples"%(len(variable_variants), len(set(variants_df.variantID_across_samples))))
        variants_df = variants_df[variants_df.variantID_across_samples.isin(variable_variants)]

        # add a numeric sampleID
        all_variants = set(variants_df.variantID_across_samples)
        sorted_variants = sorted(all_variants)
        mutation_to_numericMutation = dict(zip(sorted_variants, range(1, len(sorted_variants)+1)))
        variants_df["numeric_mutationID"] = variants_df.variantID_across_samples.map(mutation_to_numericMutation)
        variants_df["mutation"] = "mut_" + variants_df.numeric_mutationID.apply(str)

        # keep important fields
        variants_df = variants_df[["variantID_across_samples", "numeric_mutationID", "sampleID", "#Uploaded_variation", "type_var", "mutation"]]

        # check
        for f in ["#Uploaded_variation", "type_var", "variantID_across_samples", "sampleID", "mutation"]: 
            if any(pd.isna(variants_df[f])): raise ValueError("There can't be nans in %s"%f)

        # check that all the samples have variants
        samples_with_no_vars = sampleIDs.difference(set(variants_df.sampleID))
        if len(samples_with_no_vars)>0: raise ValueError("There are some samples with no vars: %s"%samples_with_no_vars)

        # save vars
        print("saving")
        save_object(variants_df, variants_df_file)

    variants_df = load_object(variants_df_file)

    ##############################################

    ########## GET ANNOTATIONS ###################
    if file_is_empty(annot_df_file) or replace is True:
        print("getting subseted variant annotations")

        # get the small variants annotations
        small_vars_annot = get_tab_as_df_or_empty_df("%s/integrated_varcalls/smallVars_annot.tab"%taxID_dir)
        SV_CNV_annot = get_tab_as_df_or_empty_df("%s/integrated_varcalls/SV_CNV_annot.tab"%taxID_dir)
        annot_df = small_vars_annot.append(SV_CNV_annot)

        interesting_vars = set(variants_df["#Uploaded_variation"])
        annot_df = annot_df[annot_df["#Uploaded_variation"].isin(interesting_vars)].set_index("#Uploaded_variation", drop=False)

        # load the gff_df
        gff_df = load_gff3_intoDF(gff)

        # trim the df
        if any(pd.isna(annot_df.Gene)): raise ValueError("There can't be NaNs in Gene")

        # add the types of mutations
        print("adding types of mutations")
        all_protein_coding_genes = set(gff_df[gff_df.feature.isin({"CDS", "mRNA"})].upmost_parent).intersection(set(annot_df.Gene))
        annot_df["is_protein_coding_gene"] = annot_df.Gene.isin(all_protein_coding_genes)
        annot_df["consequences_set"] = annot_df.Consequence.apply(lambda x: x.split(",")).apply(set)
        annot_df["is_truncating"] = annot_df.consequences_set.apply(get_is_truncating_from_consequences)
        annot_df["is_synonymous"] = annot_df.consequences_set.apply(get_is_synonymous_from_consequences)

        # add the altered protein regions by each variant
        print("adding the altered protein by each regions")
        gene_to_proteinLen = dict(gff_df[(gff_df.upmost_parent.isin(all_protein_coding_genes)) & (gff_df.feature=="CDS")].groupby("upmost_parent").apply(get_length_protein_from_df_cds))
        annot_df["varI"] = list(range(len(annot_df)))
        annot_df["is_protein_altering"] = (annot_df.is_protein_coding_gene) & ~(annot_df.is_synonymous)
        annot_df["var_impacting_all_protein"] = annot_df.apply(get_var_impacting_all_protein, axis=1)
        annot_df["protein_postions_ints"] = annot_df.apply(get_protein_positions_ints, axis=1)
        annot_df["protein_length"] = annot_df.Gene.map(gene_to_proteinLen)
        annot_df["protein_alteration_start"] = annot_df.apply(get_protein_alteration_start_annot_df_r, axis=1)
        annot_df["protein_alteration_end"] = annot_df.apply(get_protein_alteration_end_annot_df_r, axis=1)

        # add things from variants_df
        for target_f in ["type_var", "variantID_across_samples", "mutation"]:
            print(target_f)

            uploadeded_var_to_f = dict(variants_df[["#Uploaded_variation", target_f]].drop_duplicates().set_index("#Uploaded_variation")[target_f])

            annot_df[target_f] = annot_df["#Uploaded_variation"].map(uploadeded_var_to_f)
            if any(pd.isna(annot_df[target_f])): raise ValueError("There can't be nans in %s"%target_f)

        # keep important fields
        annot_df = annot_df[["type_var", "variantID_across_samples", "mutation", "#Uploaded_variation", "Gene", "Consequence", "is_protein_coding_gene", "is_truncating", "is_synonymous", "protein_alteration_start", "protein_alteration_end"]].drop_duplicates()

        print("saving")
        save_object(annot_df, annot_df_file)

    # load and check
    print("load annotations")
    annot_df = load_object(annot_df_file)

    # checks
    for f in ["Gene", "variantID_across_samples"]: 
        if any(pd.isna(annot_df[f])): raise ValueError("There can't be nans in %s"%f)

    ##############################################

    return variants_df, annot_df


def get_SV_CNV_df_with_aneuploidies(SV_CNV_df, ploidy, tmpdir, threads, species, reference_genome, window_size=5000):

    """Gets a SV_CNV_df and appends aneuploidies called (chromosome_dup) or (chromosome_loss) as based on the get_CNVdensity_df function."""

    print("adding aneuploidies")
    
    # make folders
    #delete_folder(tmpdir)
    make_folder(tmpdir)

    # get the df with CNVs called
    CNV_density_df = get_CNVdensity_df(SV_CNV_df, tmpdir, threads, reference_genome, window_size, species, expected_samples=None)
    CNV_density_df.index = list(map(str, CNV_density_df.index))
    if set(SV_CNV_df.sampleID.apply(str))!=set(CNV_density_df.index): raise ValueError("The index is not the sampleID")

    # redefine the columns
    CNV_density_df.columns = list(map(lambda x: x.split("|||")[0], CNV_density_df.columns))

    # get, for each sample (each row), the fraction of windows duplicated (cov=>1.8) or deleted (cov<=0.2)
    aneuploidies_dict = {}

    # go through each SV type
    for svtype in ["duplication", "deletion"]:

        # go through each chrom
        chrom_to_len = get_chr_to_len(reference_genome)
        for chrom in sorted(set(chrom_to_len)):

            # get the df of this chrom
            CNV_density_df_c  = CNV_density_df[chrom]

            # get the fraction of aneuploid windows
            if svtype=="duplication": 
                sample_to_fractionAneuploidWindows = ((CNV_density_df_c>=1.8).apply(sum, axis=1))/len(CNV_density_df_c.columns)
                INFO_merged_relative_CN = 2.0
                INFO_SVTYPE = "DUP"

            elif svtype=="deletion": 
                sample_to_fractionAneuploidWindows = ((CNV_density_df_c<=0.2).apply(sum, axis=1))/len(CNV_density_df_c.columns)
                INFO_merged_relative_CN = 0.0
                INFO_SVTYPE = "DEL"

            else: raise ValueError("invalid svtype")

            # get the samples that have >50% of aneuploid windows
            aneuploid_samples = sorted(set(sample_to_fractionAneuploidWindows[sample_to_fractionAneuploidWindows>=0.5].index))

            # create a df and add to SV_CNV_df
            if len(aneuploid_samples)>0:
                print("There are %i samples with %s in %s"%(len(aneuploid_samples), svtype, chrom))

                # get df
                df_aneuploidy = pd.DataFrame({"sampleID":aneuploid_samples})
                df_aneuploidy["#CHROM"] = chrom
                df_aneuploidy["POS"] = 1.0
                df_aneuploidy["REF"] = "."
                df_aneuploidy["ALT"] = "chromosome_%s"%svtype
                df_aneuploidy["ID"] = "%s_%s"%(chrom, svtype)
                df_aneuploidy["INFO_merged_relative_CN"] = INFO_merged_relative_CN
                df_aneuploidy["variantID_across_samples"] = df_aneuploidy.ID
                df_aneuploidy["type_var"] = "chromosome_%s"%svtype
                df_aneuploidy["is_transcript_disrupting_inSomeGenes"] = True
                df_aneuploidy["INFO_SVTYPE"] = INFO_SVTYPE
                df_aneuploidy["#Uploaded_variation"] = df_aneuploidy.ID
                df_aneuploidy["INFO_END"] = chrom_to_len[chrom]

                # add 
                SV_CNV_df = SV_CNV_df.append(df_aneuploidy[list(SV_CNV_df.keys())])

    # clean
    delete_folder(tmpdir)

    return SV_CNV_df

def get_vars_df_only_subset_samples_for_GWAS_simplified(taxID_dir, sampleIDs, outdir, ploidy, replace, gff, threads, species, reference_genome):

    """Writes a df with the variants per sample in only a subset of samples. """

    # define files
    variants_df_file = "%s/variants_df.py"%outdir
    annot_df_file = "%s/annot_df.py"%outdir

    ############## GET VARIANTS ##################
    if file_is_empty(variants_df_file) or replace is True:
        print("getting variants for GWAS")

        # load SV_CNVs and keep important fields
        print("SVs")
        SV_CNV_df = load_object("%s/integrated_varcalls/SV_CNV_filt.py"%taxID_dir)
        SV_CNV_df = SV_CNV_df[(SV_CNV_df.sampleID.isin(sampleIDs))][["#CHROM", "POS", "REF", "ALT", "ID", "INFO_merged_relative_CN", "sampleID", "variantID_across_samples", "type_var", "is_transcript_disrupting_inSomeGenes", "INFO_SVTYPE", "INFO_END"]]
        SV_CNV_df["#Uploaded_variation"] = SV_CNV_df.ID

        # get the SV_CNV_df with info about aneuploidies
        SV_CNV_df = get_SV_CNV_df_with_aneuploidies(SV_CNV_df, ploidy, "%s/generating_aneuploidies"%outdir, threads, species, reference_genome)
        
        # load small vars
        print("small vars")
        small_vars_df = load_object("%s/integrated_varcalls/smallVars_filt.py"%taxID_dir)
        small_vars_df = small_vars_df[(small_vars_df.sampleID.isin(sampleIDs))]

        # filter small vars depending on the ploidy
        if ploidy==1: 
            
            small_vars_df["is_duplicated_and_hetero"] = (small_vars_df.calling_ploidy==2) & (small_vars_df.common_GT=="0/1") & (small_vars_df.relative_CN>=2.0)
            small_vars_df = small_vars_df[(small_vars_df.calling_ploidy==1) | (small_vars_df.is_duplicated_and_hetero)]

        elif ploidy==2: small_vars_df = small_vars_df[small_vars_df.calling_ploidy==2]

        # keep some relevant fields
        small_vars_df = small_vars_df[['#Uploaded_variation', '#CHROM', 'POS', 'ISSNP', 'common_GT', 'relative_CN', 'sampleID','is_protein_altering_inSomeGenes']]
        small_vars_df["variantID_across_samples"] = small_vars_df["#Uploaded_variation"]
        small_vars_df["type_var"] = small_vars_df.ISSNP.map({True:"SNP", False:"INDEL"})

        # quality control
        if any(pd.isna(small_vars_df.type_var)): raise ValueError("There can't be nans in type_var")

        # merge
        print("merging")
        variants_df = small_vars_df.append(SV_CNV_df)

        # change some formats
        variants_df["sampleID"] = variants_df["sampleID"].apply(int).apply(str)

        # keep important fields
        variants_df = variants_df[["variantID_across_samples", "sampleID", "#Uploaded_variation", "type_var"]]

        # check
        for f in ["#Uploaded_variation", "type_var", "variantID_across_samples", "sampleID"]: 
            if any(pd.isna(variants_df[f])): raise ValueError("There can't be nans in %s"%f)

        # check that all the samples have variants
        samples_with_no_vars = sampleIDs.difference(set(variants_df.sampleID))
        if len(samples_with_no_vars)>0: raise ValueError("There are some samples with no vars: %s"%samples_with_no_vars)

        # save vars
        print("saving")
        save_object(variants_df, variants_df_file)

    variants_df = load_object(variants_df_file)

    ##############################################

    ########## GET ANNOTATIONS ###################
    if file_is_empty(annot_df_file) or replace is True:
        print("getting subseted variant annotations")

        # get the small variants annotations
        small_vars_annot = get_tab_as_df_or_empty_df("%s/integrated_varcalls/smallVars_annot.tab"%taxID_dir)
        SV_CNV_annot = get_tab_as_df_or_empty_df("%s/integrated_varcalls/SV_CNV_annot.tab"%taxID_dir)
        annot_df = small_vars_annot.append(SV_CNV_annot)

        interesting_vars = set(variants_df["#Uploaded_variation"])
        annot_df = annot_df[annot_df["#Uploaded_variation"].isin(interesting_vars)].set_index("#Uploaded_variation", drop=False)

        # load the gff_df
        gff_df = load_gff3_intoDF(gff)

        # trim the df
        if any(pd.isna(annot_df.Gene)): raise ValueError("There can't be NaNs in Gene")

        # add the types of mutations
        print("adding types of mutations")
        all_protein_coding_genes = set(gff_df[gff_df.feature.isin({"CDS", "mRNA"})].upmost_parent).intersection(set(annot_df.Gene))
        annot_df["is_protein_coding_gene"] = annot_df.Gene.isin(all_protein_coding_genes)
        annot_df["consequences_set"] = annot_df.Consequence.apply(lambda x: x.split(",")).apply(set)
        annot_df["is_truncating"] = annot_df.consequences_set.apply(get_is_truncating_from_consequences)
        annot_df["is_synonymous"] = annot_df.consequences_set.apply(get_is_synonymous_from_consequences)

        # add the altered protein regions by each variant
        print("adding the altered protein by each regions")
        gene_to_proteinLen = dict(gff_df[(gff_df.upmost_parent.isin(all_protein_coding_genes)) & (gff_df.feature=="CDS")].groupby("upmost_parent").apply(get_length_protein_from_df_cds))
        annot_df["varI"] = list(range(len(annot_df)))
        annot_df["is_protein_altering"] = (annot_df.is_protein_coding_gene) & ~(annot_df.is_synonymous)
        annot_df["var_impacting_all_protein"] = annot_df.apply(get_var_impacting_all_protein, axis=1)
        annot_df["protein_postions_ints"] = annot_df.apply(get_protein_positions_ints, axis=1)
        annot_df["protein_length"] = annot_df.Gene.map(gene_to_proteinLen)
        annot_df["protein_alteration_start"] = annot_df.apply(get_protein_alteration_start_annot_df_r, axis=1)
        annot_df["protein_alteration_end"] = annot_df.apply(get_protein_alteration_end_annot_df_r, axis=1)

        # add things from variants_df
        for target_f in ["type_var", "variantID_across_samples"]:
            print(target_f)

            uploadeded_var_to_f = dict(variants_df[["#Uploaded_variation", target_f]].drop_duplicates().set_index("#Uploaded_variation")[target_f])

            annot_df[target_f] = annot_df["#Uploaded_variation"].map(uploadeded_var_to_f)
            if any(pd.isna(annot_df[target_f])): raise ValueError("There can't be nans in %s"%target_f)

        # keep important fields
        annot_df = annot_df[["type_var", "variantID_across_samples", "#Uploaded_variation", "Gene", "Consequence", "is_protein_coding_gene", "is_truncating", "is_synonymous", "protein_alteration_start", "protein_alteration_end"]].drop_duplicates()

        print("saving")
        save_object(annot_df, annot_df_file)

    # load and check
    print("load annotations")
    annot_df = load_object(annot_df_file)

    # checks
    for f in ["Gene", "variantID_across_samples"]: 
        if any(pd.isna(annot_df[f])): raise ValueError("There can't be nans in %s"%f)

    ##############################################

    return variants_df, annot_df, variants_df_file



def get_strains_tree_object_ploidy1or2(taxID_dir, ploidy, sciName):

    """Gets the starins tree"""

    # load tree
    if ploidy==1: tree = get_correct_tree_midpointRooted("%s/generate_tree_from_SNPs/iqtree_unroted.treefile"%(taxID_dir))
    elif ploidy==2: tree = Tree("%s/generate_tree_from_SNPs_resamplingHetSNPs/tree_consensus_withBootstraps_and_branchLengths.nw"%(taxID_dir))

    # remove samples that are bad
    #print("removing bad samples: %s"%sciName_to_badSamples[sciName])
    samples_to_keep = {sampleID for sampleID in tree.get_leaf_names() if not int(sampleID) in sciName_to_badSamples[sciName]}
    tree.prune(samples_to_keep, preserve_branch_length=True)

    return tree

################ DEFINE CONSEQUENCES OF MUTATIONS ########################

TRUNCATING_MUTATIONS = {'stop_gained', 'protein_altering_variant', 'frameshift_variant', 'start_lost', 'coding_sequence_variant_BND', 'intron_variant_BND', 'non_coding_transcript_exon_variant_BND', 'transcript_ablation', 'non_coding_transcript_variant_BND', 'coding_sequence_variant'}

NON_TRUNCATING_MUTATIONS = {'intron_variant', 'upstream_gene_variant', '5_prime_UTR_variant', 'inframe_insertion', 'synonymous_variant', 'non_coding_transcript_exon_variant', 'intergenic_variant', 'downstream_gene_variant', '3_prime_UTR_variant', 'missense_variant', 'splice_region_variant', 'splice_acceptor_variant', 'inframe_deletion', 'stop_lost', 'non_coding_transcript_variant', 'start_retained_variant', 'stop_retained_variant', 'incomplete_terminal_codon_variant', 'splice_donor_variant', '-', 'upstream_gene_variant_BND', 'feature_elongation', 'transcript_amplification', '5_prime_UTR_variant_BND', 'intergenic_variant_BND', '3_prime_UTR_variant_BND', 'downstream_gene_variant_BND', 'feature_truncation'}

NON_SYNONYMOUS_MUTATIONS =  {'stop_gained', 'protein_altering_variant', 'frameshift_variant', 'start_lost', 'coding_sequence_variant_BND', 'intron_variant_BND', 'non_coding_transcript_exon_variant_BND', 'transcript_ablation', 'non_coding_transcript_variant_BND', 'inframe_insertion', 'coding_sequence_variant', 'missense_variant', 'inframe_deletion', 'stop_lost', 'transcript_amplification'}

SYNONYMOUS_MUTATIONS = {'intron_variant', 'upstream_gene_variant', '5_prime_UTR_variant', 'synonymous_variant', 'non_coding_transcript_exon_variant', 'intergenic_variant', 'downstream_gene_variant', '3_prime_UTR_variant', 'non_coding_transcript_variant', 'start_retained_variant', 'stop_retained_variant', 'incomplete_terminal_codon_variant', '-', 'upstream_gene_variant_BND', '5_prime_UTR_variant_BND', 'intergenic_variant_BND', '3_prime_UTR_variant_BND', 'downstream_gene_variant_BND', 'feature_elongation', 'splice_region_variant', 'splice_acceptor_variant', 'splice_donor_variant', 'feature_truncation'}

def get_is_truncating_from_consequences(consequences_set):

    """This function takes a set of consequences and returns whether they truncate the gene. For protein coding genes, the consequences that eliminate or truncate the protein are considered (i.e. SVs in the introns are such). For non-protein coding genes all the consequences that break the gene are considered truncating (not small indels) """

    if len(consequences_set.intersection(TRUNCATING_MUTATIONS))>0: return True
    elif len(consequences_set.difference(NON_TRUNCATING_MUTATIONS))==0: return False
    else: raise ValueError("%s contains non-described vars"%consequences_set)

def get_is_synonymous_from_consequences(consequences_set):

    """This function takes a set of consequences and returns whether they are synonymous for the gene. For protein coding genes, the consequences that change the protein are considered as such. For non-protein coding genes all the consequences that break the gene are considered non-synonymous (not small indels). """

    if len(consequences_set.intersection(NON_SYNONYMOUS_MUTATIONS))>0: return False
    elif len(consequences_set.difference(SYNONYMOUS_MUTATIONS))==0: return True
    else: raise ValueError("%s contains non-described vars"%consequences_set)

##########################################################################

def get_r_annot_df_with_range_altered_protein(r, gene_to_proteinLen, nvars):

    """Takes a row of the annotations df and returns the start and the end of the aminoacids positions that are altering the protein. If there is a protein truncation it will define as altered region everything that is after the truncation """

    pct_progress = "pct progress: %.2f"%((r.varI/nvars)*100)
    if pct_progress.endswith("0"): print(pct_progress)

    # for non protein altering vars, just retrieve nans
    if r.is_protein_coding_gene is False or r.is_synonymous is True: 
        r["protein_alteration_start"] = np.nan
        r["protein_alteration_end"] = np.nan

    # cases were the whole gene should be considered
    elif len(r.consequences_set.intersection({"transcript_amplification", "transcript_ablation"}))>0: 

        r["protein_alteration_start"] = 1
        r["protein_alteration_end"] = gene_to_proteinLen[r.Gene]

    # cases where you have truncation and no info about CDS -> truncatuion of the whole gene
    elif r.is_truncating is True and r.CDS_position=="-": 

        r["protein_alteration_start"] = 1
        r["protein_alteration_end"] = gene_to_proteinLen[r.Gene]

    # other cases
    else:

        # all the protein alterations should have a position
        if r.Protein_position=="-": raise ValueError("The protein position can't be - for %s. consequences:%s"%(r, r.consequences_set))
        if pd.isna(r.Protein_position): raise ValueError("The protein position can't be nan for %s"%r)

        # for all the non-synonynmous variants, the start is the start of the protein
        protein_postions = sorted({int(x) for x in r.Protein_position.split("-") if x!="?"})
        r["protein_alteration_start"] = protein_postions[0]

        # for truncations, the alteration goes to the end of the protein
        if r.is_truncating is True: r["protein_alteration_end"] = gene_to_proteinLen[r.Gene]

        # if there are two positions
        elif len(protein_postions)==2: r["protein_alteration_end"] = protein_postions[1]

        # if there is only one positions
        elif "-" not in r.Protein_position: r["protein_alteration_end"] = protein_postions[0]+1

        # if there are two positions of uncertain end, get to the end
        elif r.Protein_position.endswith("?"): r["protein_alteration_end"] = gene_to_proteinLen[r.Gene]

        else: raise ValueError("%s was not correctly parsed"%r.Protein_position)

    return r

def get_length_protein_from_df_cds(df_cds):

    """Takes a gff df with cds's and returns the expected length of the protein"""

    len_cds = sum((df_cds.end-df_cds.start)+1)
    if (len_cds%3)!=0: print("WARNING: the cds length is not multiple of 3 for gene %s"%df_cds.name)
    return int(len_cds/3)


def get_domains_annotations_df(annot_df, outdir, species, df_interpro, gff, gene_features_df):

    """Takes an annotations df and maps each mutation and gene to the IPscan affected domains. It requires the running of ./analysis/descriptive_analysis.py to get these data."""

    domains_annot_df_file = "%s/domains_annot_df.py"%outdir
    if file_is_empty(domains_annot_df_file):
        print("Getting domain data")

        # trim the df
        annot_df = annot_df[annot_df.Gene!="-"]

        # map each uploaded variation to the variantID_across_samples
        uploadedVar_to_varID = dict(annot_df[["#Uploaded_variation", "variantID_across_samples"]].drop_duplicates().set_index("#Uploaded_variation").variantID_across_samples)

        # define a tmpdir to write files
        tmpdir = "%s/adding_to_annot_df"%outdir; make_folder(tmpdir)

        # make a bed file with the proteins altered by each variant
        protein_fields = ["Gene", "protein_alteration_start", "protein_alteration_end", "#Uploaded_variation"]
        annot_df_proteins = annot_df[~pd.isna(annot_df.protein_alteration_start)].reset_index(drop=True).sort_values(by=protein_fields).drop_duplicates(subset=protein_fields).reset_index(drop=True)

        for f in ["protein_alteration_start", "protein_alteration_end"]: annot_df_proteins[f] = annot_df_proteins[f].apply(int)

        annot_df_proteins["unique_ID"] = annot_df_proteins["#Uploaded_variation"] + "_" + annot_df_proteins.Gene

        # map each protein to the start and end, where mutations exist
        gene_to_range_protein_muts = annot_df_proteins.groupby("Gene").apply(lambda df_g: [min(df_g.protein_alteration_start), max(df_g.protein_alteration_end)] )

        """
        if len(annot_df_proteins)!=len(set(annot_df_proteins.unique_ID)): 

            protID_to_nsamples = annot_df_proteins.groupby("unique_ID").apply(len)
            non_uniqueIDs = set(protID_to_nsamples[protID_to_nsamples!=1].index)
            df_nonUiqueIDs = annot_df_proteins[annot_df_proteins.unique_ID.isin(non_uniqueIDs)]

            print_df_keys(df_nonUiqueIDs)
            print(df_nonUiqueIDs)
            for x in df_nonUiqueIDs.unique_ID: print(x)
            print(df_nonUiqueIDs.keys())
            
            raise ValueError("The ID's are not unique")

        """

        bed_variants = "%s/variants.bed"%tmpdir
        annot_df_proteins[["Gene", "protein_alteration_start", "protein_alteration_end", "unique_ID"]].to_csv(bed_variants, sep="\t", index=False, header=False)

        # keep some fields of df_interpro
        print("adding domains of chunks")
        all_ip_fields = ["signature_accession", "proteinID", "start_location", "end_location"]
        df_interpro = df_interpro[all_ip_fields].drop_duplicates()

        # add windows of the protein as domains
        def get_interpro_window_one_gene(gene, wsize):

            # deinfe the positions with mutations
            range_protein_muts = gene_to_range_protein_muts[gene]
            list_positions = list(range(range_protein_muts[0], range_protein_muts[1]+1))

            # create a df with one line for each chunk
            df_w = pd.DataFrame(list(map(lambda c: {"start_location":c[0], "end_location":c[-1]}, chunks(list_positions, wsize))))
            df_w["proteinID"] = gene
            df_w["signature_accession"] = "pChunk_" + df_w.start_location.apply(str) + "_" + df_w.end_location.apply(str) 

            return df_w

        for window_size in [10, 25, 50, 100]:
            print("chunks of size %i"%window_size)
            df_interpro_windows = pd.concat(list(map(lambda g: get_interpro_window_one_gene(g, window_size) , gene_to_range_protein_muts.index)))[all_ip_fields]
            df_interpro = df_interpro.append(df_interpro_windows)

        df_interpro = df_interpro.drop_duplicates()

        # make a bed with the domains
        bed_domains = "%s/ip_domains.bed"%tmpdir
        df_interpro["unique_ID"] = df_interpro.signature_accession + "#" + df_interpro.proteinID + "#" + df_interpro.start_location.apply(str) + "_" + df_interpro.end_location.apply(str)

        def semicolon_in_x(x): return (";" in x)
        if any(df_interpro.unique_ID.apply(semicolon_in_x)): raise ValueError("There can't be semicolons in the ID")

        ip_fields = ["proteinID", "start_location", "end_location", "unique_ID"]
        df_interpro[ip_fields].drop_duplicates().sort_values(by=ip_fields).to_csv(bed_domains, sep="\t", index=False, header=False)

        # run bedmap to map the domains on the mutations bed
        print("running bedmap to unify domains")
        bedmap_outfile = "%s/bedmap_outfile.txt"%tmpdir
        run_cmd("bedmap --bp-ovr 1 --echo-map-id --delim '\t' %s %s > %s"%(bed_variants, bed_domains, bedmap_outfile), env="perSVade_env")

        # add to the annotations_df
        annot_df_proteins["overlapping_IPdomains"] = open(bedmap_outfile, "r").readlines()
        def get_set_from_IDs(x): return set(x.strip().split(";")).difference({""})
        annot_df_proteins["overlapping_IPdomains"] = annot_df_proteins.overlapping_IPdomains.apply(get_set_from_IDs)

        # get relevant fields and generate a df were each row is one combination of gene, mutation and altered domain
        print("concatenating domains")
        annot_df_proteins = annot_df_proteins[["unique_ID", "#Uploaded_variation", "Gene", "overlapping_IPdomains"]]

        def get_df_domains(r):
            if len(r.overlapping_IPdomains)==0: return pd.DataFrame()
            else:
                df_domains = pd.DataFrame({"domain_ID":sorted(r.overlapping_IPdomains)})
                df_domains["Gene"] = r.Gene
                df_domains["#Uploaded_variation"] = r["#Uploaded_variation"]

                return df_domains

        domains_annot_df = pd.concat(annot_df_proteins.apply(get_df_domains, axis=1).values).reset_index(drop=True)

        # add the variant ID across samples
        domains_annot_df["variantID_across_samples"] = domains_annot_df["#Uploaded_variation"].map(uploadedVar_to_varID)
        if any(pd.isna(domains_annot_df.variantID_across_samples)): raise ValueError("There can't be nans in variantID_across_samples")
        
        # checks
        strange_domains = set(domains_annot_df.domain_ID).difference(set(df_interpro.unique_ID))
        if len(strange_domains)>0: raise ValueError("There are some unexpected domains: %s"%strange_domains)

        # clean
        delete_folder(tmpdir)
    
        # save
        save_object(domains_annot_df, domains_annot_df_file)

    return load_object(domains_annot_df_file)


def get_df_pathways_from_df_interpro(df_interpro, df_pathways_file):

    """Takes a df_interpro and generates a df with all the pathway data (from Reactome and Metacyc)"""

    if file_is_empty(df_pathways_file):

        print("getting pathway df from interproscan")

        # define a function that takes a row of the interproscan df and returns a df with the pathway IDs
        def get_Pathway_Interpro_df(r):

            # if there are no pathway data, return empty df
            if pd.isna(r.Pathway_Interpro) or r.Pathway_Interpro=="-": return pd.DataFrame()

            # get all the pathway annotations
            split_x = r.Pathway_Interpro.split("|")
            pathway_type_list = list(map(lambda y: y.split(":")[0], split_x))
            pathway_ID_list = list(map(lambda y: y.split()[1], split_x))

            df_path = pd.DataFrame({"pathway_type":pathway_type_list, "pathway_ID":pathway_ID_list})
            df_path["proteinID"] = r.proteinID

            return df_path.drop_duplicates()

        df_pathways = pd.concat(df_interpro.apply(get_Pathway_Interpro_df, axis=1).values).drop_duplicates()

        # checks
        if {'MetaCyc', 'Reactome'}!=set(df_pathways.pathway_type): raise ValueError("There are some strange pathways in df_interpro")

        save_object(df_pathways, df_pathways_file)

    print("loading pathways")
    return load_object(df_pathways_file).reset_index(drop=True)

def get_df_GOterms_transfering_across_theGraph(df_GOterms_per_gene, outdir, obo_file, max_fraction_genes_pathway_toBeConsidered):

    """Generates a df with all GO terms, as provided in the gene_features_df, and also transferring those of upper levels of the graph to each gene."""

    df_GOterms_file = "%s/df_GOterms_per_gene.py"%outdir
    if file_is_empty(df_GOterms_file):
        print("getting GO terms")   

        ####### GET ALL GO TERMS ######

        # init with the gene to GO terms raw
        gene_to_GOterms_raw = dict(df_GOterms_per_gene[df_GOterms_per_gene.GOterms.apply(len)>0].set_index("gff_upmost_parent").GOterms)

        # define the obodag
        obodag = GODag(obo_file,  optional_attrs={'consider', 'replaced_by'}, load_obsolete=True, prt=None)

        # define the gene to GO terms in a way that already takes into account missing terms
        ns_to_gene_to_GOids = get_namespace_to_gene_to_GOterms(gene_to_GOterms_raw, obodag)

        # generate the GO terms with propagated counts
        gene_to_GOterms_all = {}
        for gene in gene_to_GOterms_raw.keys():

            # define all GO terms
            GOterms = set.union(*[gene_to_GOids[gene] for gene_to_GOids in ns_to_gene_to_GOids.values()])

            # get the parents of these GO terms
            parent_GOterms = set.union(*[obodag[go].get_all_parents() for go in GOterms] + [set()])

            # define all go terms, and keep only the GO terms that are related to expected_GOterms
            gene_to_GOterms_all[gene] = GOterms.union(parent_GOterms)

        ###############################

        ########## CREATE DF ############
        print("getting df")

        def get_GOterms_df_per_gene_and_GO_terms(x):
            gene, GOterms = x
            df_go = pd.DataFrame({"GO" : sorted(GOterms)})
            df_go["Gene"] = gene
            return df_go

        df_GOterms = pd.concat(map(get_GOterms_df_per_gene_and_GO_terms, gene_to_GOterms_all.items())).sort_values(by=["Gene", "GO"]).reset_index(drop=True).drop_duplicates()

        # filter out GO terms that are very general (i.e. they are in >=10% of genes)
        GOterm_to_fractionGenes = df_GOterms.groupby("GO").apply(len)/len(set(df_GOterms.Gene))
        non_general_GOs = set(GOterm_to_fractionGenes[GOterm_to_fractionGenes<max_fraction_genes_pathway_toBeConsidered].index)
        
        print("There are %i/%i non general GOs"%(len(non_general_GOs), len(GOterm_to_fractionGenes)))
        df_GOterms = df_GOterms[df_GOterms.GO.isin(non_general_GOs)].reset_index(drop=True)


        # filter out GO terms that are not among the curated GO terms or their parents
        all_GOs_cgdCurated = set.union(*[set.union(*gene_to_terms.values()) for gene_to_terms in get_namespace_to_gene_to_GOterms(dict(df_GOterms_per_gene[df_GOterms_per_gene.GOterms_cgdCurated.apply(len)>0].set_index("gff_upmost_parent").GOterms_cgdCurated), obodag).values()])
        all_GOs_cgdCurated_parents = set.union(*map(lambda go: obodag[go].get_all_parents(), all_GOs_cgdCurated)) 
        all_GOs_cgdCurated = all_GOs_cgdCurated.union(all_GOs_cgdCurated_parents)

        all_GOs = set(df_GOterms.GO)
        print("There are %i/%i manually curated GO terms"%(len(set(all_GOs_cgdCurated.intersection(all_GOs))), len(all_GOs)))
        df_GOterms = df_GOterms[df_GOterms.GO.isin(all_GOs_cgdCurated)].reset_index(drop=True)

        print("savig")
        save_object(df_GOterms, df_GOterms_file)

        #################################

    return load_object(df_GOterms_file)

def get_all_parents_pathwayID(ID, childID_to_parentIDs):

    """Takes one ID and finds all the parents """

    # there are no parents
    if ID not in childID_to_parentIDs: return set()

    else: 
        parentIDs = childID_to_parentIDs[ID]
        parents_of_parentsIDs = set.union(*map(lambda pID: get_all_parents_pathwayID(pID, childID_to_parentIDs), parentIDs))

        return parentIDs.union(parents_of_parentsIDs)

def get_df_reactome_with_Parents_transferred(df_reactome, reactome_pathways_file, reactome_pathwayRelations_file, max_fraction_genes_pathway_toBeConsidered, outdir):

    """Adds the pathway descriptions to reactome df"""

    # define a df that will contain the reactome pathways
    df_reactome_file = "%s/df_reactome_per_gene_with_all_parents.py"%outdir
    if file_is_empty(df_reactome_file):

        # map each pathway to all the parents
        childID_to_parentIDs = dict(pd.read_csv(reactome_pathwayRelations_file, sep="\t", header=None, names=["parent_pathway_ID", "child_pathway_ID"]).groupby("child_pathway_ID").apply(lambda df_c: set(df_c.parent_pathway_ID)))
        all_pathways = sorted(set(df_reactome.pathway_ID))

        print("getting pathway parents for %i paths"%len(all_pathways))
        pathway_to_allParentPathways = pd.Series(dict(zip(all_pathways, map(lambda ID: get_all_parents_pathwayID(ID, childID_to_parentIDs), all_pathways))))

        print("There are %i/%i pathways with no parent"%(sum(pathway_to_allParentPathways.apply(len)==0), len(pathway_to_allParentPathways)))

        # define pathways that are within meaningful groups (their top parent makes sense in Candida)
        df_descriptions_all = pd.read_csv(reactome_pathways_file, sep="\t", header=None, names=["pathway_ID", "description", "organism"])
        df_descriptions  = df_descriptions_all[pd.isna(df_descriptions_all.pathway_ID.map(childID_to_parentIDs))]

        interesting_parent_pathways = {'Metabolism of proteins', 'Autophagy', 'Transport of small molecules', 'Gene expression (Transcription)', 'Cellular responses to stimuli', 'Reproduction', 'Digestion and absorption', 'Signal Transduction', 'Extracellular matrix organization', 'DNA Repair', 'Chromatin organization', 'Cell Cycle', 'Metabolism', 'Organelle biogenesis and maintenance', 'DNA Replication', 'Programmed Cell Death', 'Vesicle-mediated transport', 'Metabolism of RNA', 'Cell-Cell communication', 'Protein localization', 'DNA replication and repair'} # this was calculated from the previous one
        interesting_parent_pathwayIDs = set(df_descriptions[df_descriptions.description.isin(interesting_parent_pathways)].pathway_ID)

        # if you wanted to dicard fungi you should do: interesting_parent_pathwayIDs = set(df_descriptions[(df_descriptions.description.isin(interesting_parent_pathways)) & (df_descriptions.organism.isin({"Saccharomyces cerevisiae", "Schizosaccharomyces pombe"}))].pathway_ID)

        # keep the pathway_to_allParentPathways that have interesting_parent_pathwayIDs in the parent IDs
        pathways_df = pd.DataFrame({"parent_pathways":pathway_to_allParentPathways})
        pathways_df["pathway_ID"] = pathways_df.index
        def get_parents_intersection_interestingParents(parent_IDs): return parent_IDs.intersection(interesting_parent_pathwayIDs)
        pathways_df_filt = pathways_df[(pathways_df.parent_pathways.apply(get_parents_intersection_interestingParents).apply(len)>0) | (pathways_df.pathway_ID.isin(interesting_parent_pathwayIDs))]
        
        pathway_to_allParentPathways = pathways_df_filt.parent_pathways
        print("There are %i/%i pathways that belong to meaningful groups"%(len(pathways_df_filt), len(pathways_df)))

        # filter reactome to include only these pathways
        df_reactome = df_reactome[df_reactome.pathway_ID.isin(set(pathway_to_allParentPathways.index))]

        # map each gene to the pathways (including the parents)
        print("mapping each gene to the pathways")
        gene_to_pathways_raw = dict(df_reactome.groupby("proteinID").apply(lambda df_p: set(df_p.pathway_ID)))
        sorted_genes = sorted(gene_to_pathways_raw)

        def get_all_pathways_one_gene(g):
            pathways_raw = gene_to_pathways_raw[g]
            parent_pathways = set.union(*map(lambda p: pathway_to_allParentPathways[p], pathways_raw))
            return pathways_raw.union(parent_pathways)

        gene_to_pathways  = dict(zip(sorted_genes, map(get_all_pathways_one_gene, sorted_genes)))

        # get a df that has the new gene to pathway mapping
        print("getting as a df")

        def get_reactome_df_per_gene_and_pathways(x):
            gene, pathways = x
            df_p = pd.DataFrame({"pathway_ID" : sorted(pathways)})
            df_p["Gene"] = gene
            return df_p

        df_reactome = pd.concat(map(get_reactome_df_per_gene_and_pathways, gene_to_pathways.items())).sort_values(by=["Gene", "pathway_ID"]).reset_index(drop=True).drop_duplicates()

        # filter out pathways that are very general (i.e. they are in >=5% of genes)
        pathway_to_fractionGenes = df_reactome.groupby("pathway_ID").apply(len)/len(set(df_reactome.Gene))
        non_general_pathways = set(pathway_to_fractionGenes[pathway_to_fractionGenes<max_fraction_genes_pathway_toBeConsidered].index)
            
        print("There are %i/%i non general reactome pathways"%(len(non_general_pathways), len(pathway_to_fractionGenes)))
        df_reactome = df_reactome[df_reactome.pathway_ID.isin(non_general_pathways)].reset_index(drop=True)

        # checks
        df_descriptions_all.set_index("pathway_ID", drop=False).loc[set(df_reactome.pathway_ID)]

        #for I,r in df_descriptions_all.set_index("pathway_ID", drop=False).loc[set(df_reactome.pathway_ID)][["pathway_ID", "description"]].drop_duplicates().iterrows(): print(r.pathway_ID, r.description)

        # keep only reactome pathways that are described in s. cerevisiae or s. pombe (fungal)
        fungal_pathways_reactome = set(df_descriptions_all[df_descriptions_all.organism.isin({"Saccharomyces cerevisiae", "Schizosaccharomyces pombe"})].pathway_ID)
        if len(set(df_reactome.pathway_ID).difference(set(df_descriptions_all.pathway_ID)))>0: raise ValueError("There are some pathways in the gwas df that are not in the defined reactome pathways")

        df_reactome = df_reactome[df_reactome.pathway_ID.isin(fungal_pathways_reactome)]

        print("saving")
        save_object(df_reactome, df_reactome_file)

    return load_object(df_reactome_file)
    


def get_metacyc_pathwayTools_pathways_child_pathway_to_parentPathway_df(outdir):

    """This function loads the metacyc database for pathway tools and  writes a dict that maps each pathway to it's parent pathway. It requires the simultaneous running of the pathway tools software (in the BSC desktop it can be run with '/home/mschikora/software/pathway-tools/pathway-tools -lisp -python'."""

    pathway_relations_df_file = "%s/metacyc_child_to_parent_relations_df.py"%(outdir)
    if file_is_empty(pathway_relations_df_file):
        print("getting child-parent relations")

        print("You should run the pathway-tools software while running this. In the BSC desktop we do this with:\n\n/home/mschikora/software/pathway-tools/pathway-tools -lisp -python\n\n")

        # get the meta object, which has all biocyc
        import pythoncyc
        meta = pythoncyc.select_organism('meta')

        # init a list of tuples
        child_parent_relations = []

        # record all instances measured by a superclass
        all_pathways_under_superclass = set()

        # add the relation between each pathway and the children
        print("adding classes")
        for classID in meta.get_class_all_subs("|Pathways|"):

            # get the data
            data_dict = meta[classID].get_frame_data().__dict__

            # check that it is a class
            if data_dict["_isclass"] is False: raise ValueError("There are some non-classes")

            # add the instances
            if "instances" in data_dict:

                for instance in data_dict["instances"]:
                    if instance._isclass is True: raise ValueError("The instance can't be a class")
                    child_parent_relations.append((instance.frameid, classID))
                    all_pathways_under_superclass.add(instance.frameid)

            # add the non instances
            else: raise ValueError("All the classes should have instances")

        # check that all_pathways_under_superclass is as expected
        all_pathways_in_meta = set(map(lambda x: x.frameid, meta.pathways))
        if all_pathways_under_superclass!=all_pathways_in_meta: raise ValueError("Not all pathways have classes")

        # add the superpathway-pathway relations
        print("adding super pathway rels")
        for I,p in enumerate(meta.pathways):

            # get the data dict
            data_dict = p.get_frame_data().__dict__
            pID = data_dict["frameid"] # this has |pID|

            # add the parent relations
            if "in_pathway" in data_dict: child_parent_relations += [(pID, parentID) for parentID in data_dict["in_pathway"]]
            if "super_pathways" in data_dict: child_parent_relations += [(pID, parentID) for parentID in data_dict["super_pathways"]]
            if "sub_pathways" in data_dict: child_parent_relations += [(childID, pID) for childID in data_dict["sub_pathways"]]

        # get as a df
        pathway_relations_df = pd.DataFrame(sorted(set(child_parent_relations)), columns=["childID", "parentID"])
        for f in ["childID", "parentID"]: pathway_relations_df[f] = pathway_relations_df[f].apply(lambda x: x.split("|")[1])

        save_object(pathway_relations_df, pathway_relations_df_file)

    return load_object(pathway_relations_df_file)




def get_df_metacyc_with_pathways_from_parents(df_metacyc, outdir, max_fraction_genes_pathway_toBeConsidered):

    """Takes a metacyc pathway df and returns the mapping with also parent pathways with pathway tools (can only be ran in the local computer """

    df_metacyc_file = "%s/df_metacyc_per_gene_with_all_parents.py"%(outdir)

    if file_is_empty(df_metacyc_file):

        print("getting metacyc df")

        # map each pathway to all the parents, only keepin the target clade pathways
        df_pathway_relations = get_metacyc_pathwayTools_pathways_child_pathway_to_parentPathway_df(outdir)
        childID_to_parentIDs = dict(df_pathway_relations.groupby("childID").apply(lambda df_c: set(df_c.parentID)))
        all_pathways = sorted(set(df_metacyc.pathway_ID))

        print("getting pathway parents for %i paths"%len(all_pathways))
        pathway_to_allParentPathways = pd.Series(dict(zip(all_pathways, map(lambda ID: get_all_parents_pathwayID(ID, childID_to_parentIDs), all_pathways))))

        # check that all pathways have parents
        pathws_no_parent = set(pathway_to_allParentPathways[pathway_to_allParentPathways.apply(len)==0].index)
        if len(pathws_no_parent)!=0: print("WARNING: There are %i pathways with no parent: %s"%(len(pathws_no_parent), pathws_no_parent))

        # map each gene to the pathways (including the parents)
        print("mapping each gene to the pathways")
        gene_to_pathways_raw = dict(df_metacyc.groupby("proteinID").apply(lambda df_p: set(df_p.pathway_ID)))
        sorted_genes = sorted(gene_to_pathways_raw)

        def get_all_pathways_one_gene(g):
            pathways_raw = gene_to_pathways_raw[g]
            parent_pathways = set.union(*map(lambda p: pathway_to_allParentPathways[p], pathways_raw))
            return pathways_raw.union(parent_pathways)

        gene_to_pathways  = dict(zip(sorted_genes, map(get_all_pathways_one_gene, sorted_genes)))

        # get a df that has the new gene to pathway mapping
        print("getting as a df")

        def get_metacyc_df_per_gene_and_pathways(x):
            gene, pathways = x
            df_p = pd.DataFrame({"pathway_ID" : sorted(pathways)})
            df_p["Gene"] = gene
            return df_p

        df_metacyc = pd.concat(map(get_metacyc_df_per_gene_and_pathways, gene_to_pathways.items())).sort_values(by=["Gene", "pathway_ID"]).reset_index(drop=True).drop_duplicates()

        # filter out pathways that are very general (i.e. they are in >=5% of genes)
        pathway_to_fractionGenes = df_metacyc.groupby("pathway_ID").apply(len)/len(set(df_metacyc.Gene))
        non_general_pathways = set(pathway_to_fractionGenes[pathway_to_fractionGenes<max_fraction_genes_pathway_toBeConsidered].index)
            
        print("There are %i/%i non general MetaCyc pathways"%(len(non_general_pathways), len(pathway_to_fractionGenes)))
        df_metacyc = df_metacyc[df_metacyc.pathway_ID.isin(non_general_pathways)].reset_index(drop=True)

        # filter out non-Ascomycota pathways
        print("filtering out non-ascomycota pathways")
        valid_metacyc_pathways_file = "%s/GWAS_valid_metacyc_pathways_Ascomycota.py"%outdir
        valid_metacyc_pathways = get_metacyc_pathways_in_clade(set(df_metacyc.pathway_ID), "Ascomycota", valid_metacyc_pathways_file)
        df_metacyc = df_metacyc[df_metacyc.pathway_ID.isin(valid_metacyc_pathways)]
        
        print("savig")
        save_object(df_metacyc, df_metacyc_file)

    return load_object(df_metacyc_file)


def get_chunks_groups_grouping_df(grouping_df, nmutations_per_chunk):

    """Takes a grouping df and returns chunks of meaningful nmutations_per_chunk"""

    print("getting groups' chunks")

    # map eah group to the mutations related to it
    group_to_mutations = grouping_df.groupby("group").apply(lambda df_m: set(df_m.mutation)).sort_index()

    # keep adding chunks as long as you have around nmutations_per_chunk
    chunks_groups = []
    current_groups = set() 
    mutations_in_current_chunk = set()

    for group, mutations in group_to_mutations.iteritems():

        # add the current group
        current_groups.add(group)
        mutations_in_current_chunk.update(mutations)
        
        # if you have enough groups, update
        if len(mutations_in_current_chunk)>=nmutations_per_chunk: 

            chunks_groups.append(cp.deepcopy(current_groups))
            current_groups = set() 
            mutations_in_current_chunk = set()

    # add the last group
    if len(current_groups)>0: chunks_groups.append(cp.deepcopy(current_groups))

    # check
    strange_groups = set.union(*chunks_groups).difference(set(group_to_mutations.index))
    if len(strange_groups)>0: raise ValueError("There are some strange groups: %s"%strange_groups)

    missing_groups = set(group_to_mutations.index).difference(set.union(*chunks_groups))
    if len(missing_groups)>0: raise ValueError("There are %i missing groups: %s"%(len(missing_groups), missing_groups))
    
    # check that all mutations are somewere
    print("These are the sizes of the chunks: %s"%[len(x) for x in chunks_groups])

    return chunks_groups

def get_int_from_str_numeric_mutation(x): return int(x.split("_")[1])

def get_grouping_file_and_genotypes_file_oneChunk_hogwash_noGrouping(Ichunk, nchunks, sorted_mutations_chunk, genotypes_df_file, outdir_gwas_chunk):

    """Returns a tuple with the genotypes_file and grouping_file for one chunk of get_cmds_hogwash_GWAS_drugResistance_specificMutations_and_grouping """

    #print("getting data for chunk %i/%i"%(Ichunk, nchunks))

    # define the files
    make_folder(outdir_gwas_chunk)
    genotypes_file = "%s/genotypes_file.tab"%outdir_gwas_chunk
    if file_is_empty(genotypes_file):

        # load the df with the df
        genotypes_df_chunk = load_object(genotypes_df_file)[sorted_mutations_chunk]

        # print
        #print("chunk %i has %i mutations"%(Ichunk+1, len(sorted_mutations_chunk)))

        # get the genotypes of this chunk
        genotypes_df_chunk["sampleID"] = genotypes_df_chunk.index

        # write files for howgwash run
        save_df_as_tab(genotypes_df_chunk, genotypes_file)

        del genotypes_df_chunk

    return None, genotypes_file


def get_grouping_file_and_genotypes_file_oneChunk_hogwash_withGrouping(Ichunk, nchunks, grouping_df_file, genotypes_df_file, chunks_groups_I, outdir_gwas_chunk):

    """The same as get_grouping_file_and_genotypes_file_oneChunk_hogwash_noGrouping, but with grouping"""


    #print("getting data for chunk %i/%i"%(Ichunk, nchunks))

    # define the files
    make_folder(outdir_gwas_chunk)
    genotypes_file = "%s/genotypes_file.tab"%outdir_gwas_chunk
    grouping_file = "%s/grouping_file.tab"%outdir_gwas_chunk

    if file_is_empty(genotypes_file) or file_is_empty(grouping_file):

        # load the df for this chunk
        grouping_df_chunk = load_object(grouping_df_file).set_index("group", drop=False).loc[chunks_groups_I]

        # get the sorted mutations of this chunk
        sorted_mutations_chunk = sorted(set(grouping_df_chunk.mutation), key=get_int_from_str_numeric_mutation)

        # load the df with the df
        genotypes_df_chunk = load_object(genotypes_df_file)[sorted_mutations_chunk]

        # print
        #print("chunk %i has %i mutations"%(Ichunk+1, len(sorted_mutations_chunk)))

        # get the genotypes of this chunk
        genotypes_df_chunk["sampleID"] = genotypes_df_chunk.index

        # write files for howgwash run
        save_df_as_tab(genotypes_df_chunk, genotypes_file)
        save_df_as_tab(grouping_df_chunk, grouping_file)

        del genotypes_df_chunk
        del grouping_df_chunk

    return grouping_file, genotypes_file

def get_file_with_currentParentDir(file):

    """Takes a dir and changes the content"""

    # return the nones
    if file is None: return file

    # adapt the paths 
    samba_ParentDir = "/home/mschikora/samba"
    mn_ParentDir = "/gpfs/projects/bsc40/mschikora"

    if file.startswith(samba_ParentDir): return file.replace(samba_ParentDir, ParentDir)
    elif file.startswith(mn_ParentDir): return file.replace(mn_ParentDir, ParentDir)
    else: raise ValueError("The file was generated in a non BSC file")


def get_cmds_hogwash_GWAS_drugResistance_specificMutations_and_grouping(genotypes_df_file, grouping_df_file, outdir_gwas, resistance_file, treefile_rooted, ARfiles_dir):

    """Gets the cmds for different chunks for some genotypes and a grouping df"""

    print("getting inputs_fn")

    # go through different chunks of mutations and also different methods to do the gwas
    #nmutations_per_chunk = 1000 # check
    nmutations_per_chunk = 20000

    # define a dir that includes the list_groupingFile_and_genotypesFile
    list_groupingFile_and_genotypesFile_file = "%s/list_groupingFile_and_genotypesFile_perChunks_%imuts.py"%(outdir_gwas, nmutations_per_chunk)
    chunkI_to_outdir_gwas_file = "%s/chunkI_to_outdir_gwas_dict_%imuts.py"%(outdir_gwas, nmutations_per_chunk)

    # get the files
    if file_is_empty(list_groupingFile_and_genotypesFile_file) or file_is_empty(chunkI_to_outdir_gwas_file):

        # get a list with the grouping_file and genotype_file for each chunk in parallel (defining inputs_fn and get_groupingFile_and_genotypesFile_fn)
        if grouping_df_file is None: 

            # get the chunks of mutations
            sorted_mutations = list(load_object(genotypes_df_file).columns)
            chunks_sorted_mutations = list(chunks(sorted_mutations, nmutations_per_chunk))
            nchunks = len(chunks_sorted_mutations)

            # get the inputs_fn
            inputs_fn = [[Ichunk, nchunks, chunks_sorted_mutations[Ichunk], genotypes_df_file] for Ichunk in range(nchunks)]
            get_groupingFile_and_genotypesFile_fn = get_grouping_file_and_genotypes_file_oneChunk_hogwash_noGrouping

        else:

            # get the chunks of groups
            chunks_groups = get_chunks_groups_grouping_df(load_object(grouping_df_file), nmutations_per_chunk)
            nchunks = len(chunks_groups)

            # get the inputs_fn
            inputs_fn = [[Ichunk, nchunks, grouping_df_file, genotypes_df_file, chunks_groups[Ichunk]] for Ichunk in range(nchunks)]
            get_groupingFile_and_genotypesFile_fn = get_grouping_file_and_genotypes_file_oneChunk_hogwash_withGrouping

        # map each chunk to an outdir
        chunkI_to_outdir_gwas = {Ichunk : "%s/chunks_%imuts_%i"%(outdir_gwas, nmutations_per_chunk, Ichunk) for Ichunk in range(nchunks)}

        # add the outdir
        for x in inputs_fn: x.append(chunkI_to_outdir_gwas[x[0]])
        inputs_fn = list(map(tuple, inputs_fn))

        # get the grouping and genotypes files
        if run_in_cluster is True: threads = int(multiproc.cpu_count()/2) # run in mn
        else: threads = multiproc.cpu_count()
        print("running on %i threads"%threads)
        with multiproc.Pool(threads) as pool:

            list_groupingFile_and_genotypesFile = pool.starmap(get_groupingFile_and_genotypesFile_fn, inputs_fn)
            
            pool.close()
            pool.terminate()

        # save
        save_object(list_groupingFile_and_genotypesFile, list_groupingFile_and_genotypesFile_file)
        save_object(chunkI_to_outdir_gwas, chunkI_to_outdir_gwas_file)

    # load
    list_groupingFile_and_genotypesFile = list(map(lambda x: tuple(map(get_file_with_currentParentDir, x)), load_object(list_groupingFile_and_genotypesFile_file)))
    chunkI_to_outdir_gwas = {chunkI : get_file_with_currentParentDir(outdir_gwas) for chunkI, outdir_gwas in load_object(chunkI_to_outdir_gwas_file).items()}

    # define the cmds
    all_cmds = []

    print("getting cmds")
    for Ichunk, (grouping_file, genotypes_file) in enumerate(list_groupingFile_and_genotypesFile):

        # deifne files
        outdir_gwas_chunk = chunkI_to_outdir_gwas[Ichunk]
        outdir_hogwash_run = "%s/outdir_hogwash"%outdir_gwas_chunk
        final_files = ["%s/%s_results.tab"%(outdir_hogwash_run, m) for m in ["synchronous", "phyc"]]

        #  check that the files exist
        for f in [genotypes_file, resistance_file, treefile_rooted]:
            if file_is_empty(f): raise ValueError("The file %s should exist"%f)

        if any([file_is_empty(x) for x in final_files]):

            # define the dir with the outdir
            delete_folder(outdir_hogwash_run)
            make_folder(outdir_hogwash_run)

            # init cmd
            hogwash_libraries_dir = "%s/scripts/hogwash-1.2.5-edited/R"%ParentDir
            run_hogwash_R = "%s/CandidaMine_data_generation/v1/run_hogwash_binary_phenotype.R"%ParentDir
            hogwash_cmd = "%s --genotypes_table %s --phenotype_table %s --treefile %s --outdir %s --hogwash_libraries_dir %s --ARfiles_dir %s"%(run_hogwash_R, genotypes_file, resistance_file, treefile_rooted, outdir_hogwash_run, hogwash_libraries_dir, ARfiles_dir)

            # add the table
            if grouping_file is not None:  
                if file_is_empty(grouping_file): raise ValueError("The file %s should exist"%grouping_file)
                hogwash_cmd += " --grouping_table %s"%grouping_file

            # keep 
            all_cmds.append(hogwash_cmd)

    # once you got here, specify a cmd

    print("There are %i cmds"%len(all_cmds))

    return all_cmds

def get_genotypes_df_square_df(outdir_drug, variants_df_all, annot_df_all, sorted_samples):

    """Creates a square df were the cols are mutations and the rows are samples """

    print("getting square mutations df")
    genotypes_df_file = "%s/all_genotypes_df.py"%outdir_drug

    if file_is_empty(genotypes_df_file):

        # get the df with the variants filtered
        all_mutations = set(annot_df_all.mutation)
        variants_df = variants_df_all[["sampleID", "mutation"]].drop_duplicates()
        variants_df["sampleID"] = "sample_" + variants_df.sampleID.apply(int).apply(str)
        variants_df["mutation_presence"] = 1

        # checks
        if any(pd.isna(variants_df.mutation)): raise ValueError("There can't be nans in numeric_mutationID")
        if all_mutations!=set(variants_df.mutation): raise ValueError("There are some strange mutations in variants_df")

        # get the square df
        genotypes_df = variants_df.pivot(index="sampleID", columns="mutation", values="mutation_presence").fillna(0).applymap(int)
        if get_uniqueVals_df(genotypes_df)!={0, 1}: raise ValueError("The genotypes df should only be 1 and 0")

        # add missing samples as all 0s
        missing_samples = sorted(set(sorted_samples).difference(set(genotypes_df.index)))
        if len(missing_samples)>0:
            print("WARNING: there are %i samples with no mutations"%(len(missing_samples)))
            df_all0s = pd.DataFrame(np.zeros([len(missing_samples), len(genotypes_df.columns)]), index=missing_samples, columns=genotypes_df.columns).applymap(int)

            genotypes_df = genotypes_df.append(df_all0s)

        # get the correct order
        genotypes_df = genotypes_df.loc[sorted_samples, sorted(set(annot_df_all.mutation))]
        
        # save
        print("saving")
        save_object(genotypes_df, genotypes_df_file)

    return load_object(genotypes_df_file)


def get_grouping_df_all_vars_for_type_collapsing(annot_df, outdir_drug, type_collapsing, variants_with_domain_alteration, genes_with_goAnnotation, genes_with_reactomeAnnotation, genes_with_metacycAnnotation, domains_annot_df_all, df_GOterms_all, df_reactome_all, df_metacyc_all, mutation_field="mutation", return_type="df"):

    """Gets a collapsing df for the type_collapsing """

    grouping_df_file = "%s/grouping_df_%s.py"%(outdir_drug, type_collapsing)
    if file_is_empty(grouping_df_file):
        print("getting grouping df for %s"%type_collapsing)


        # keep some mutations
        if type_collapsing=="genes": annot_df = annot_df[annot_df.Gene!="-"]
        elif  type_collapsing=="domains": annot_df = annot_df[annot_df.variantID_across_samples.isin(variants_with_domain_alteration)]
        elif type_collapsing=="GO": annot_df = annot_df[annot_df.Gene.isin(genes_with_goAnnotation)]
        elif type_collapsing=="Reactome": annot_df = annot_df[annot_df.Gene.isin(genes_with_reactomeAnnotation)]
        elif type_collapsing=="MetaCyc": annot_df = annot_df[annot_df.Gene.isin(genes_with_metacycAnnotation)]
        else: raise ValueError("%s is not valid"%type_collapsing)

        # get the grouping df
        if type_collapsing=="genes": grouping_df = annot_df[[mutation_field, "Gene"]].drop_duplicates().rename(columns={"Gene":"group"})

        elif  type_collapsing=="domains":  

            if mutation_field=="variantID_across_samples": annot_df_fields = ["variantID_across_samples"]
            else: annot_df_fields = ["variantID_across_samples", mutation_field]

            grouping_df = annot_df[annot_df_fields].drop_duplicates().merge(domains_annot_df_all[["domain_ID", "variantID_across_samples"]].drop_duplicates(), on="variantID_across_samples", how="left", validate="one_to_many").rename(columns={"domain_ID":"group"})[[mutation_field, "group"]].drop_duplicates()

        elif type_collapsing=="GO": grouping_df = annot_df[[mutation_field, "Gene"]].drop_duplicates().merge(df_GOterms_all, on="Gene", how="left").rename(columns={"GO":"group"})[[mutation_field, "group"]].drop_duplicates()

        elif type_collapsing=="Reactome": grouping_df = annot_df[[mutation_field, "Gene"]].drop_duplicates().merge(df_reactome_all, on="Gene", how="left").rename(columns={"pathway_ID":"group"})[[mutation_field, "group"]].drop_duplicates()

        elif type_collapsing=="MetaCyc": grouping_df = annot_df[[mutation_field, "Gene"]].drop_duplicates().merge(df_metacyc_all, on="Gene", how="left").rename(columns={"pathway_ID":"group"})[[mutation_field, "group"]].drop_duplicates()

        else: raise ValueError("%s is not valid"%type_collapsing)

        # checks
        for f in [mutation_field, "group"]: 
            if any(pd.isna(grouping_df[f])): raise ValueError("There can't be nans in %s"%f)

        # save
        save_object(grouping_df, grouping_df_file)

    if return_type=="df": return load_object(grouping_df_file)
    elif return_type=="file": return grouping_df_file
    else: raise ValueError("%s is not valid"%return_type)

def get_cmds_hogwash_GWAS_drugResistance_specificMutations_and_grouping_runningFuction(outdir_gwas, annot_df_all, variants_with_domain_alteration, genes_with_goAnnotation, genes_with_reactomeAnnotation, genes_with_metacycAnnotation, drug, type_vars, type_vars_set, type_genes, type_mutations, type_collapsing, genotypes_df_all, type_collapsing_to_grouping_df_all, resistance_file, treefile_rooted, ARfiles_dir):

    """This function runs get_cmds_hogwash_GWAS_drugResistance_specificMutations_and_grouping preparing the inputs"""

    # defnie an outdir
    final_file_mutsAndCollapse = "%s/all_jobs_generated.txt"%outdir_gwas

    # if you already generated all files, continue
    if not file_is_empty(final_file_mutsAndCollapse): return []

    # make the folder
    make_folder(outdir_gwas)

    # define the files necessary for getting cmds
    if type_collapsing=="none": grouping_df_file = None
    else: grouping_df_file = "%s/grouping_df_all.py"%outdir_gwas

    genotypes_df_file = "%s/genotypes_df.py"%outdir_gwas

    if file_is_empty(genotypes_df_file) or (type_collapsing!="none" and file_is_empty(grouping_df_file)):
        print("getting grouping_df_file and genotypes_df_file")

        # get the annotations of the important variants
        print("filtering annot_df")
        annot_df = annot_df_all[(annot_df_all.type_var.isin(type_vars_set))]

        # filter only variants that affect type_genes
        if type_genes=="only_protein_coding": annot_df = annot_df[annot_df.is_protein_coding_gene]
        elif type_genes=="only_non_coding": annot_df = annot_df[~(annot_df.is_protein_coding_gene)]

        # filter the type of mutations
        if type_mutations=="syn_muts": annot_df = annot_df[annot_df.is_synonymous]
        elif type_mutations=="non_syn_muts": annot_df = annot_df[~(annot_df.is_synonymous)]
        elif type_mutations=="non_syn_non_truncating_muts": annot_df = annot_df[~(annot_df.is_synonymous) & ~(annot_df.is_truncating)]
        elif type_mutations=="truncating_muts": annot_df = annot_df[annot_df.is_truncating]

        # keep ìmportant fields
        annot_df = annot_df[["variantID_across_samples", "Gene", "mutation"]]

        # depending on the collapsing you'd like to filter the annot_df accordingly
        if type_collapsing=="none": pass
        elif type_collapsing=="genes": annot_df = annot_df[annot_df.Gene!="-"]
        elif  type_collapsing=="domains": annot_df = annot_df[annot_df.variantID_across_samples.isin(variants_with_domain_alteration)]
        elif type_collapsing=="GO": annot_df = annot_df[annot_df.Gene.isin(genes_with_goAnnotation)]
        elif type_collapsing=="Reactome": annot_df = annot_df[annot_df.Gene.isin(genes_with_reactomeAnnotation)]
        elif type_collapsing=="MetaCyc": annot_df = annot_df[annot_df.Gene.isin(genes_with_metacycAnnotation)]
        else: raise ValueError("%s is not valid"%type_collapsing)

        # if there are no mutations, just continue
        if len(annot_df)==0: 
            print("WARNING: There are no mutations in annot_df for %s"%outdir_gwas)
            open(final_file_mutsAndCollapse, "w").write("No mutations")
            return []

        # define the sorted mutations
        print("getting square grouping df")
        if file_is_empty(genotypes_df_file): 
            genotypes_df = genotypes_df_all[sorted(set(annot_df.mutation))]
            save_object(genotypes_df, genotypes_df_file)

        # define the grouping df
        print("getting grouping df")
        if type_collapsing!="none" and file_is_empty(grouping_df_file):

            # get the df with the mutations
            grouping_df = type_collapsing_to_grouping_df_all[type_collapsing]
            grouping_df = grouping_df[grouping_df.mutation.isin(set(annot_df.mutation))]

            # check and save
            if set(grouping_df.mutation)!=set(load_object(genotypes_df_file).columns): raise ValueError("All mutations should have a group")
            save_object(grouping_df, grouping_df_file)

    # get the cmds 
    all_cmds_specificMuts_and_group = get_cmds_hogwash_GWAS_drugResistance_specificMutations_and_grouping(genotypes_df_file, grouping_df_file, outdir_gwas, resistance_file, treefile_rooted, ARfiles_dir)

    # write the final file
    if len(all_cmds_specificMuts_and_group)==0: open(final_file_mutsAndCollapse, "w").write("All jobs generated")

    return all_cmds_specificMuts_and_group

def get_cmd_ancestral_GWAS_drugResistance_specificMutations_and_grouping_runningFunction(outdir_gwas, type_collapsing, resistance_file, threads, outdir_ASR_mutations, ASR_methods_mutations, ASR_methods_phenotypes, min_support, outdir_GWAS_jobs, grouping_df_file, resampled_phenotypes_pastml_out, Ijob, njobs):

    """This function first gets the cmds for GWAS reconstruction for different types of groupings."""

    #print("getting job %i/%i in outdir %s"%(Ijob, njobs, outdir_gwas.split(ParentDir)[1]))

    # if you already ran the cmd, which generates integrated_GWAS_stats.tab, skip
    final_file = "%s/integrated_GWAS_stats.tab"%outdir_GWAS_jobs
    if not file_is_empty(final_file): return None

    # make the folder
    make_folder(outdir_gwas)

    # init the cmd with things that are unrelaed to the GWAS
    get_GWAS_jobs_py = "%s/scripts/ancestral_GWAS/scripts/get_GWAS_jobs.py"%ParentDir
    cmd = "%s --outdir_ASR_mutations %s --phenotypes %s --outdir %s --phenotypes_pastml_prediction_method ALL --varID_field variantID_across_samples --threads %i --classification_method_phenotypes all_ignoreNaN --classification_method_mutations all_ignoreNaN --ASR_methods_mutations %s --ASR_methods_phenotypes %s --resampled_phenotypes_pastml_out %s --skip_tree_rendering --min_support %i --all_pval_methods phenotypes --interesting_gwas_methods synchronous"%(get_GWAS_jobs_py, outdir_ASR_mutations, resistance_file, outdir_GWAS_jobs, threads, ASR_methods_mutations, ASR_methods_phenotypes, resampled_phenotypes_pastml_out, min_support)

    # add the grouping if necessary, which requires only considering a few mutations
    if type_collapsing!="none": cmd += " --grouping_table %s"%grouping_df_file

    return cmd


def get_grouping_df_file(outdir_gwas_all, annot_df_all, type_vars_set, type_genes, type_mutations, type_collapsing, variants_with_domain_alteration, genes_with_goAnnotation, genes_with_reactomeAnnotation, genes_with_metacycAnnotation, domains_annot_df_all, df_GOterms_all, df_reactome_all, df_metacyc_all, min_nmuts_per_group=2):

    """Gets the grouping df file """

    # get the grouping df
    grouping_df_file_allmuts = "%s/grouping_df_all.all_muts.py"%(outdir_gwas_all)
    grouping_df_file = "%s/grouping_df_all.min_%imuts_per_group.py"%(outdir_gwas_all, min_nmuts_per_group)

    if file_is_empty(grouping_df_file):
        print("generating grouping df")

        # get the annotations of the important variants
        #print("filtering annot_df")
        annot_df = annot_df_all[(annot_df_all.type_var.isin(type_vars_set))]

        # filter only variants that affect type_genes
        if type_genes=="only_protein_coding": annot_df = annot_df[annot_df.is_protein_coding_gene]
        elif type_genes=="only_non_coding": annot_df = annot_df[~(annot_df.is_protein_coding_gene)]

        # filter the type of mutations
        if type_mutations=="syn_muts": annot_df = annot_df[annot_df.is_synonymous]
        elif type_mutations=="non_syn_muts": annot_df = annot_df[~(annot_df.is_synonymous)]
        elif type_mutations=="non_syn_non_truncating_muts": annot_df = annot_df[~(annot_df.is_synonymous) & ~(annot_df.is_truncating)]
        elif type_mutations=="truncating_muts": annot_df = annot_df[annot_df.is_truncating]

        # keep ìmportant fields
        annot_df = annot_df[["variantID_across_samples", "Gene"]]

        # depending on the collapsing you'd like to filter the annot_df accordingly
        if type_collapsing=="genes": annot_df = annot_df[annot_df.Gene!="-"]
        elif  type_collapsing=="domains": annot_df = annot_df[annot_df.variantID_across_samples.isin(variants_with_domain_alteration)]
        elif type_collapsing=="GO": annot_df = annot_df[annot_df.Gene.isin(genes_with_goAnnotation)]
        elif type_collapsing=="Reactome": annot_df = annot_df[annot_df.Gene.isin(genes_with_reactomeAnnotation)]
        elif type_collapsing=="MetaCyc": annot_df = annot_df[annot_df.Gene.isin(genes_with_metacycAnnotation)]
        else: raise ValueError("%s is not valid"%type_collapsing)

        # if there are no mutations, just continue
        if len(annot_df)==0: 
            #print("WARNING: There are no mutations in annot_df for %s"%outdir_gwas_all)
            grouping_df = pd.DataFrame(columns=["variantID_across_samples", "group"])

        # get the grouping df
        else:

            # get the grouping df
            if type_collapsing=="genes": grouping_df = annot_df[["variantID_across_samples", "Gene"]].drop_duplicates().rename(columns={"Gene":"group"})

            elif  type_collapsing=="domains": grouping_df = annot_df[["variantID_across_samples"]].drop_duplicates().merge(domains_annot_df_all[["domain_ID", "variantID_across_samples"]].drop_duplicates(), on="variantID_across_samples", how="left", validate="one_to_many").rename(columns={"domain_ID":"group"})[["variantID_across_samples", "group"]].drop_duplicates()

            elif type_collapsing=="GO": grouping_df = annot_df[["variantID_across_samples", "Gene"]].drop_duplicates().merge(df_GOterms_all, on="Gene", how="left").rename(columns={"GO":"group"})[["variantID_across_samples", "group"]].drop_duplicates()

            elif type_collapsing=="Reactome": grouping_df = annot_df[["variantID_across_samples", "Gene"]].drop_duplicates().merge(df_reactome_all, on="Gene", how="left").rename(columns={"pathway_ID":"group"})[["variantID_across_samples", "group"]].drop_duplicates()


            elif type_collapsing=="MetaCyc": grouping_df = annot_df[["variantID_across_samples", "Gene"]].drop_duplicates().merge(df_metacyc_all, on="Gene", how="left").rename(columns={"pathway_ID":"group"})[["variantID_across_samples", "group"]].drop_duplicates()

            else: raise ValueError("%s is not valid"%type_collapsing)

            # checks
            for f in ["variantID_across_samples", "group"]: 
                if any(pd.isna(grouping_df[f])): raise ValueError("There can't be nans in %s"%f)

            if len(grouping_df)==0: raise ValueError("the grouping df can't be empty")

            # change the index
            grouping_df = grouping_df.reset_index(drop=True)

            # save the grouping df with all mutations
            save_object(grouping_df, grouping_df_file_allmuts)

            # keep only groups that have >=min_nmuts_per_group mutations
            initial_len_gdf = len(grouping_df)
            group_to_nmutations = grouping_df.groupby("group").apply(len)
            valid_groups = set(group_to_nmutations[group_to_nmutations>=min_nmuts_per_group].index)
            grouping_df = grouping_df[grouping_df.group.isin(valid_groups)]
            #print("grouping_df kept %i/%i rows after filtering groups with <%i muts"%(len(grouping_df), initial_len_gdf, min_nmuts_per_group))

        # save
        save_object(grouping_df, grouping_df_file)

    return grouping_df_file


def get_pval_fields_gwas(df):

    """Returns the pval fields of a gwas df"""


    return [c for c in df.columns if c.startswith("pval_") and not c.endswith("_fdr") and not c.endswith("_maxT")]


def check_no_nans_series(x):

    """Raise value error if nans"""

    if any(pd.isna(x)): raise ValueError("There can't be nans in series %s"%x)


def get_gwas_df_with_corrected_pvals(gwas_df, pval_fields):

    """Gets the gwas df with the corrected pvals"""

    # module imports
    import statsmodels.stats.multitest as mt

    # init two gwas df
    merged_gwas_df = pd.DataFrame()
    for method in sorted(set(gwas_df.gwas_method)):

        # get the gwas df for this method
        gwas_df_method = gwas_df[gwas_df.gwas_method==method]
        if len(gwas_df_method)!=len(set(gwas_df_method.group_name)): raise ValueError("the gname should be unique")

        # go through each pval field
        for pval_f in pval_fields:

            # checck GWAS df
            check_no_nans_series(gwas_df_method[pval_f])

            # for different types of correction
            #for correction_method in ["bonferroni", "hommel", "fdr_bh", "fdr_by", "fdr_tsbh", "fdr_tsbky"]: # old way, many p vals
            for correction_method in ["bonferroni", "fdr_bh", "fdr_by"]:

                gwas_df_method["%s_%s"%(pval_f, correction_method)] = mt.multipletests(gwas_df_method[pval_f].values, alpha=0.05, method=correction_method)[1]

        # keep
        merged_gwas_df = merged_gwas_df.append(gwas_df_method).reset_index(drop=True)

    gwas_df = merged_gwas_df

    return gwas_df

def run_integrate_GWAS_jobs_one_combination_get_df(outdir_GWAS_jobs, type_collapsing, Igroup, type_vars, type_genes, type_mutations, ASR_methods_phenotypes, ASR_methods_mutations, min_support, njobs):

    """Returns a df with the integrated, filtered df (only comparisons that make sense)."""

    #print("integrating job %i/%i"%(Igroup, njobs))

    # get the gwas results df
    gwas_results_df = get_tab_as_df_or_empty_df("%s/integrated_GWAS_stats.tab"%outdir_GWAS_jobs)


    # define the pval fields
    pval_fields_complete_set = set(get_pval_fields_gwas(gwas_results_df)) # old way, all pvals
    pval_fields = sorted([f for f in pval_fields_complete_set if f in all_pval_fields])

    # filter to keep only rows with some convergent phenotypes (pvalue is not negative)
    gwas_results_df = gwas_results_df[gwas_results_df[pval_fields[0]]>=0]

    # add fields
    gwas_results_df["gwas_method"] = gwas_results_df["method"]

    # define fields to remove from the df
    fields_to_remove = pval_fields_complete_set.difference(set(pval_fields)) # unnecessary pal fields
    fields_to_remove.update({f for f in gwas_results_df.keys() if f.endswith("_fdr")}) # fdr fields, already calculated below in get_gwas_df_with_corrected_pvals
    for f in fields_to_remove: gwas_results_df.pop(f)

    # return the empty df
    if len(gwas_results_df)==0: return pd.DataFrame()

    # add the corrected p vals in different ways
    gwas_results_df = get_gwas_df_with_corrected_pvals(gwas_results_df, pval_fields)

    # add fields
    gwas_results_df["type_vars"] = type_vars
    gwas_results_df["type_genes"] = type_genes
    gwas_results_df["type_mutations"] = type_mutations
    gwas_results_df["type_collapsing"] = type_collapsing
    gwas_results_df["ASR_methods_phenotypes"] = ASR_methods_phenotypes
    gwas_results_df["ASR_methods_mutations"] = ASR_methods_mutations
    gwas_results_df["min_support"] = min_support

    # check no nans
    for k in gwas_results_df.keys():
        if any(pd.isna(gwas_results_df[k])): raise ValueError("There can't be nans in %s"%f)

    return gwas_results_df

def remove_file_if_html(outdir, f):

    """Removes a file if it is html"""

    if f.endswith(".html"): remove_file("%s/%s"%(outdir, f))


def get_representative_leafNames_from_tree_best_support_criterion(tree_node, nrepresentative_samples, min_support):

    """returns a list of the <nrepresentative_samples> leaf names of tree_node"""

    # get the samples
    sorted_samples = cp.deepcopy(sorted(tree_node.get_leaf_names()))

    # if there are few samples return them all 
    if len(sorted_samples)<=nrepresentative_samples: return sorted_samples

    # check that it is not a leaf
    if tree_node.is_leaf(): raise ValueError("tree_node can't be a leaf")

    # define the combinations of sorted_samples
    list_combinations_samples = sorted(map(tuple, map(sorted, itertools.combinations(sorted_samples, nrepresentative_samples))))

    # map each combination to the fraction of branches that pass min_support

    # function that calculates the distance between two nodes
    def get_distances_nodes(target_node, sA, sB):
        if sA==sB: return 0.0
        else: return float(target_node.get_distance(sA, sB))

    # get a function that calculates the information    
    def get_info_one_combination_prunning(combination_samples):

        # get a node with the pruned samples
        node = cp.deepcopy(tree_node)
        node.prune(combination_samples, preserve_branch_length=True)

        # get the fraction of nodes with good support
        list_nodes_are_supported = [n.support>=min_support for n in node.traverse() if not n.is_leaf()]
        fraction_good_support = sum(list_nodes_are_supported)/len(list_nodes_are_supported)

        # get the mean distance between all elements
        mean_distance_btw_leafs = np.mean(list(map(lambda x: get_distances_nodes(node, x[0], x[1]), itertools.combinations(combination_samples, 2))))

        return {"combination_samples":combination_samples, "node_support":node.support, "fraction_nodes_good_support":fraction_good_support, "mean_distance_btw_leafs":mean_distance_btw_leafs}

    df_branch_combinations = pd.DataFrame(dict(zip(list_combinations_samples, map(get_info_one_combination_prunning, list_combinations_samples)))).transpose()

    # get the best combination
    df_branch_combinations = df_branch_combinations.sort_values(by=["node_support", "fraction_nodes_good_support", "mean_distance_btw_leafs", "combination_samples"], ascending=[False, False, False, True])
    representative_samples = sorted(df_branch_combinations.iloc[0].combination_samples)
    if len(representative_samples)!=nrepresentative_samples: raise ValueError("There are too few rep samples")

    return representative_samples

def get_correct_tree_removing_braanches_with_low_support(tree, min_support=0):

    """Gets filtering out branches with low support"""

    # collapse low support bracnhes
    for n in tree.traverse():

        # for internal nodes, if the support is below the minimum, set each of the children as children of the parent
        if n.is_leaf() is False and len(n.get_ancestors())>0 and n.support<min_support: n.delete()

    return tree

def get_balanced_set_samples_for_GWAS_AF_resistance(tree, samples, sample_to_resistance, threads, nrepresentative_samples=3):

    """Takes a tree object and  a set of samples, returning a balanced set of representative samples for GWAS."""

    # check that they are the same
    if samples!=set(sample_to_resistance): raise ValueError("samples!=sample_to_resistance")

    # prune the tree
    tree = cp.deepcopy(tree)
    tree.prune(samples, preserve_branch_length=True)

    # add the resistance to the tree
    for l in tree.get_leaves(): l.resistance = sample_to_resistance[l.name]

    # define the monophyleric nodes for resistance values
    monophyletic_nodes = make_flat_listOflists([list(tree.get_monophyletic(values=[val], target_attr="resistance")) for val in set(sample_to_resistance.values())])

    # check that the nodes are monophyletic
    for n in monophyletic_nodes:
        resistance_values = {sample_to_resistance[l] for l in n.get_leaf_names()}
        if len(resistance_values)!=1: raise ValueError("there is a node that is not monophyletic")

    # get the representative samples in parallel
    inputs_fn = [(node, nrepresentative_samples) for node in monophyletic_nodes]
    with  multiproc.Pool(threads) as pool:

        #list_lists_repsamples = pool.starmap(get_representative_leafNames_from_tree_best_support_criterion, inputs_fn) # other way based on branch supports
        list_lists_repsamples = pool.starmap(get_representative_leafNames_from_tree,  inputs_fn) # old way, based on distances

        pool.close()
        pool.terminate()

    # define the representative samples
    representative_samples = set(make_flat_listOflists(list_lists_repsamples))

    return representative_samples


def plot_tree_AF_resistance_samples_for_GWAS(tree, sample_to_resistance, outdir_drug, min_support, species, drug):

    """Makes a tree with the representative samples"""

    #print("getting tree with samples")

    # prune the tree
    tree = cp.deepcopy(tree)
    check_that_tree_has_no_politomies(tree)

    # get the resistance to color dict
    resistance_type_to_color = {1:"red", 0:"blue"}

    # add the resistance to each node
    for n in tree.traverse():

        if n.is_leaf(): n.resistance = sample_to_resistance[n.name]
        else:

            resistance_leaves = {sample_to_resistance[l.name] for l in n.get_leaves()}
            if len(resistance_leaves)==1: n.resistance = next(iter(resistance_leaves))
            else: n.resistance = 0


    # init counters of resistance transitions
    n_transitions = 0
    n_transitions_with_correct_support = 0

    # go through each node
    for n in tree.traverse():

        # add whether the node is a transition node (phenotype is different than the parent)
        if n.is_root() is False and n.resistance!=(n.get_ancestors()[0].resistance): n.transition = True
        else: n.transition = False

        # add counters of transitions
        if n.transition is True:
            n_transitions += 1  
            if (n.support>=min_support or n.is_leaf()) and (n.get_ancestors()[0].support>=min_support): n_transitions_with_correct_support += 1

        # node
        nst = NodeStyle()
        nst["hz_line_width"] = 8
        nst["vt_line_width"] = 8
        nst["size"] = 0

        if n.support>=min_support or n.is_leaf(): 
            color = "black"
            label_text_color = "white"
        else: 
            color = "gray"
            label_text_color = "black"

        nst["hz_line_color"] = color
        nst["vt_line_color"] = color
        n.set_style(nst)

        # add circle
        if n.is_leaf() is False: n.add_face(CircleFace(10, color, style='sphere', label=dict(text=str(n.transition)[0], font="arial", color=label_text_color, fontsize=12)), column={True:1, False:0}[n.is_leaf()])

        # add the resistance
        if n.is_leaf(): 

            # add a box with the resistance
            color_leaf =  resistance_type_to_color[sample_to_resistance[n.name]]
            label = {0:"S", 1:"R"}[sample_to_resistance[n.name]]
            square_width = 20
            n.add_face(RectFace(square_width, square_width, fgcolor="black", bgcolor=color_leaf, label={"text":label, "color":get_annotationColor_on_bgcolor(color_leaf)}), position="aligned", column=0)

            n.add_face(CircleFace(10, color_leaf, style='sphere', label=dict(text=str(n.transition)[0], font="arial", color='black', fontsize=12)), column={True:1, False:0}[n.is_leaf()])

    print("%s-%s. There are %i/%i transitions with the current and parent nodes with supp>=%i"%(species, drug, n_transitions_with_correct_support, n_transitions, min_support))

    # init treestyle
    ts = TreeStyle()
    ts.title.add_face(TextFace("%s-%s GWAS samples. Black nodes have support>=%i\n"%(species, drug, min_support), bold=True, fsize=12), column=0)
    ts.show_branch_length = False
    ts.show_branch_support = False
    ts.show_leaf_name = True

    #print("saving into %s"%outdir_drug)
    if run_in_cluster is False: tree.render(file_name="%s/tree_resistance_min_support=%s.pdf"%(outdir_drug, min_support), tree_style=ts)#, dpi=500,  units='mm', h=nleafs*2) #w=20

def check_that_tree_has_no_politomies(tree):

    """Raises an error if tree has politomies"""

    for n in tree.traverse():
        if n.is_leaf() is False and len(n.get_children())!=2: raise ValueError("node %s has !=2 children"%(n.name))

def generate_tree_representative_samples_gwas(representative_samples, ploidy, reference_genome, small_vars_filt_file, outdir, df_sorted_bams, min_coverage_pos, threads):
    
    """Generates a tree for the representative samples in a way that is analogous to the system used in get_data.py """

    # prepare data
    make_folder(outdir)

    # keep only some samples
    representative_samples = sorted(map(int, representative_samples))
    df_sorted_bams = df_sorted_bams.loc[representative_samples]

    # depending on the ploidy work in one or the other system
    if ploidy==1:

        # generate a df with the SNPs that are in positions with coverage >12x in all samples, have no heterozygous SNPs and have no IN/DELs
        #print("generating haploid_snps_df_uniFormPositions_file...")
        haploid_snps_df_uniFormPositions_file = "%s/homoSNPs_positions_noINDELS_noHetSNPs_cov>%ix.py"%(outdir, min_coverage_pos)
        outdir_generateHaploidSNPs_df = "%s/generating_haploid_snps_df_uniFormPositions_file"%outdir
        generate_haploid_snps_df_uniFormPositions_file(small_vars_filt_file, df_sorted_bams, haploid_snps_df_uniFormPositions_file, min_coverage_pos, outdir_generateHaploidSNPs_df, ploidy, threads=threads)
        delete_folder(outdir_generateHaploidSNPs_df)

        # generate a fasta file with the haploid SNPs
        #print("generating haploid_snps_multifasta...")
        haploid_snps_multifasta = "%s/homoSNPs_positions_noINDELS_noHetSNPs_cov_atLeast_%ix_sequence.fasta"%(outdir, min_coverage_pos)
        if file_is_empty(haploid_snps_multifasta):
            snps_df = load_object(haploid_snps_df_uniFormPositions_file).set_index("sampleID", drop=False)
            generate_multifasta_from_snps_df(snps_df, haploid_snps_multifasta, representative_samples, threads=threads, generate_one_aln_each_chrom=True, pickRandomHetSNPs=False)

        # generate a concatenated alignment with only variable positions
        #print("generate concatenated alignment with only variable positions")
        haploid_snps_multifasta_onlyVariableSites = "%s.onlyVariableSites.fasta"%haploid_snps_multifasta
        get_multifasta_onlyVariableSites("%s.positions_df.py"%haploid_snps_multifasta, haploid_snps_multifasta_onlyVariableSites, representative_samples, replace=False)

        # generate tree
        outdir_tree = "%s/running_iqtree"%outdir
        make_folder(outdir_tree)

        # get unrooted tree form the aln
        #print("getting tree")
        outfileprefix= "%s/iqtree_unroted"%outdir_tree
        final_file = "%s.iqtree"%outfileprefix
        if file_is_empty(final_file):

            # remove previous files
            for f in os.listdir(outdir_tree):
                if f.startswith(get_file(outfileprefix)): remove_file("%s/%s"%(outdir_tree, f))

            # run iqtree
            run_cmd("iqtree -s '%s' -pre %s --mem 25G -m TEST+ASC -T AUTO -B 1000"%(haploid_snps_multifasta_onlyVariableSites, outfileprefix), env="Candida_mine_env") # -m  GTR+ASC would do ascertainment bias correction (actually the ASC does). ASC is necessary if you input an alignment that does not span the whole genome (just some positions)

        # get tree 
        tree = get_correct_tree_midpointRooted("%s/iqtree_unroted.treefile"%(outdir_tree))

    # for diploids, use the same technique to get random SNPs
    elif ploidy==2:

        # get a df with with diploid SNPs in positions that are covered and have no indels in all samples
        #print("generating homo_and_hetero_snps_df_correctPositions_file...")
        homo_and_hetero_snps_df_correctPositions_file = "%s/homo_and_hetero_SNPs_positions_noINDELS_cov_atLeast_%ix.py"%(outdir, min_coverage_pos)
        outdir_SNPs = "%s/generating_homo_and_hetero_snps_df_correctPositions_file"%outdir
        generate_homo_and_hetero_snps_df_correctPositions(small_vars_filt_file, df_sorted_bams, homo_and_hetero_snps_df_correctPositions_file, min_coverage_pos, outdir_SNPs, threads=threads)
        delete_folder(outdir_SNPs)

        # define a dir with all the trees
        outdir_tree_resampling = "%s/generate_tree_from_SNPs_resamplingHetSNPs"%outdir; make_folder(outdir_tree_resampling)
        outdir_resamplings = "%s/resamplings"%outdir_tree_resampling; make_folder(outdir_resamplings)

        # make a file with all samples
        samples_file = "%s/samples.tab"%outdir_tree_resampling
        df_sorted_bams[["sampleID"]].to_csv(samples_file, sep="\t", index=False, header=True)

        # generate 100 trees with bootstrapped data
        all_resampled_trees = []
        all_cmds_trees = []

        # generate 100 bootstraps
        for I in range(1, 100+1):
            print("running tree obtention on sample %i"%I)

            outdir_I = "%s/resample_%i"%(outdir_resamplings, I)
            final_file = "%s/tree_was_generated.txt"%outdir_I

            if file_is_empty(final_file):

                threads_per_job = 12
                all_cmds_trees.append("%s/CandidaMine_data_generation/v1/get_tree_from_snps_df_resamplingHeteroSNPs.py --outdir %s --ref %s --threads %i --snps_df_file %s --samples_file %s"%(ParentDir, outdir_I, reference_genome, threads_per_job, homo_and_hetero_snps_df_correctPositions_file, samples_file))
                continue

            # keep the resampled tree
            all_resampled_trees.append("%s/iqtree_unroted.treefile"%outdir_I)


        # if there are cnds to run, send them to the cluster
        if len(all_cmds_trees)>0:

            # write a file with all jobs
            cmd_prefix = "source /gpfs/projects/bsc40/mschikora/anaconda3/etc/profile.d/conda.sh && conda activate Candida_mine_env"
            stddir = "%s/STDfiles"%outdir_tree_resampling; delete_folder(stddir); make_folder(stddir)
            std_perJob_prefix = "%s/tree_generation"%(stddir)

            def get_redefined_cmd(x):
                Icmd, c = x
                redefined_cmd = "%s && %s > %s.%i.out 2>&1"%(cmd_prefix, c, std_perJob_prefix, Icmd+1)
                return redefined_cmd.replace(ParentDir, "/gpfs/projects/bsc40/mschikora")

            cmd_list = list(map(get_redefined_cmd, enumerate(all_cmds_trees)))
            jobs_filename = "%s/jobs.generate_individual_runs"%outdir_tree_resampling
            open(jobs_filename, "w").write("\n".join(cmd_list))

            # submit jobs
            run_jobarray_file_MN4_greasy_ListJobFiles([jobs_filename], 'running_het_trees', time="02:00:00", queue="debug", threads_per_job=threads_per_job, nodes=8, submit=True) # nodes=15 is the max. Each node will have 2GB RAM

            # exit 
            print("you need to finish the jobs before continuing. Check %s"%outdir_tree_resampling)
            sys.exit(0)


        # generate consensus tree
        print("get consensus tree...")
        generate_consensus_withBootstrap_from_resampledTrees(all_resampled_trees, outdir_tree_resampling, replace=False)

        # load tree
        tree = Tree("%s/tree_consensus_withBootstraps_and_branchLengths.nw"%(outdir_tree_resampling))

    else: raise ValueError("ploidy should be 1 or 2")

    # add a support of 100 to the children of the roor
    tree.support = 100
    for n in tree.get_children():
        if not n.is_leaf(): n.support = 100

    # check politomies
    check_that_tree_has_no_politomies(tree)

    return tree


def get_cmds_ancestral_GWAS_drugResistance_resampledPhenotypes(species, outdir_ancestralGWAS, drug_to_samplesForGWAS, threads_ancestral_GWAS=48, threads=48):

    """
    This function gets the cmds to run the GWAS on 50 resamples of the phenotypes
    """

    # init the cmds
    all_cmds = []

    # go through each drug, where you'll use a different set of samples
    for drug in sorted(drug_to_samplesForGWAS):
        print(species, drug, "resampled GWAS")

        # init cmd of this drug
        all_cmds_drug = []

        #if drug!="ANI": continue # debug

        # define dirs of the real GWAS
        outdir_drug_real = "%s/GWAS_%s_resistance"%(outdir_ancestralGWAS, drug) 
        outdir_ASR_mutations = "%s/ASR_mutations"%outdir_drug_real
        resampled_phenotypes_pastml_out = "%s/resampled_phenotypes_pastml_df.py"%outdir_drug_real

        """
        # verify that the gwas was ran
        for f in ["%s/gwas_results_df_all.py"%outdir_drug_real, resampled_phenotypes_pastml_out]:
            if file_is_empty(f): raise ValueError("file should exist")

        """

        # make the outdir
        outdir_drug_all_resamples = "%s_reshuffled_phenotypes"%outdir_drug_real; 
        #delete_folder(outdir_drug_all_resamples) # debug
        make_folder(outdir_drug_all_resamples)

        # load the real resistance phenotypes
        real_resistance_df = get_tab_as_df_or_empty_df("%s/resistance_df.tab"%outdir_drug_real)

        # define the file
        df_gwas_all_resamples_file = "%s/df_gwas_all_resamples.py"%outdir_drug_all_resamples

        if file_is_empty(df_gwas_all_resamples_file):
            print("getting df_gwas_all_resamples_file")

            # init the df gwas
            df_gwas_all_resamples = pd.DataFrame()
            no_results_resamples = set()

            # go through n resampled phenotypes
            n_resampled_phenotypes = 50
            for resampleI in range(1, n_resampled_phenotypes+1):

                # define the outdir for this drug
                outdir_drug = "%s/resample%i"%(outdir_drug_all_resamples, resampleI); make_folder(outdir_drug)

                # get the reshuffled phenotype
                resistance_df = cp.deepcopy(real_resistance_df)
                resistance_df["phenotype"] = np.random.choice(resistance_df.phenotype.values, size=len(resistance_df), replace=False, p=None)
                resistance_file = "%s/reshuffled_resistance_df.tab"%outdir_drug
                if file_is_empty(resistance_file): save_df_as_tab(resistance_df[["sampleID", "phenotype"]], resistance_file)

                # define the sorted grouping_strings
                def sort_type_collapsing(x):
                    if x.split("-")[-1]=="none": return 0
                    else: return 1
                sorted_grouping_str = sorted([x for x in os.listdir(outdir_drug_real) if len(x.split("-"))==4], key=sort_type_collapsing)

                # go through each type grouping
                for grouping_str in sorted_grouping_str:
      
                    # define things
                    type_vars, type_genes, type_mutations, type_collapsing = grouping_str.split("-")

                    # skip the not-none. Only do this for the single uncollapsed muts
                    if type_collapsing!="none": continue

                    # make the folder for this gwas
                    outdir_gwas_all = "%s/%s"%(outdir_drug, grouping_str)
                    make_folder(outdir_gwas_all)

                    # get the grouping df from the GWAS
                    if type_collapsing=="none": grouping_df_file = ""
                    else: grouping_df_file = "%s/%s/grouping_df_all.min_2muts_per_group.py"%(outdir_drug_real, grouping_str)

                    # go through each parameter
                    for ASR_methods in ["MPPA", "MPPA,DOWNPASS", "DOWNPASS"]:

                        # define the same method to ASR mutations
                        ASR_methods_mutations = ASR_methods_phenotypes = ASR_methods

                        # iterate through several min_support
                        for min_support in [50, 70]:
                            print(species, drug, resampleI, ASR_methods, min_support)

                            # define outdirs
                            outdir_gwas = "%s/%s-%s-min_support=%i"%(outdir_gwas_all, ASR_methods_phenotypes, ASR_methods_mutations, min_support)
                            make_folder(outdir_gwas)
                            outdir_GWAS_jobs = "%s/gwas_jobs"%outdir_gwas
                            make_folder(outdir_GWAS_jobs)

                            # get the cmd
                            cmd = get_cmd_ancestral_GWAS_drugResistance_specificMutations_and_grouping_runningFunction(outdir_gwas, type_collapsing, resistance_file, threads_ancestral_GWAS, outdir_ASR_mutations, ASR_methods_mutations, ASR_methods_phenotypes, min_support, outdir_GWAS_jobs, grouping_df_file, resampled_phenotypes_pastml_out, 0, 0)

                            # add the command
                            if cmd is not None: 
                                print("getting cmd")

                                # keep
                                all_cmds_drug.append(cmd)

                            # get the integrated df
                            else:
                                print('integrating', species, drug, resampleI)

                                df_gwas_all_resamples_I = run_integrate_GWAS_jobs_one_combination_get_df(outdir_GWAS_jobs, type_collapsing, 0, type_vars, type_genes, type_mutations, ASR_methods_phenotypes, ASR_methods_mutations, min_support, 0)

                                if len(df_gwas_all_resamples_I)==0: no_results_resamples.add(resampleI)

                                df_gwas_all_resamples_I["drug"] = drug
                                df_gwas_all_resamples_I["species"] = species
                                df_gwas_all_resamples_I["resampleI"] = resampleI
                                df_gwas_all_resamples = df_gwas_all_resamples.append(df_gwas_all_resamples_I)

            # if there are jobs to run
            if len(all_cmds_drug)>0: all_cmds += all_cmds_drug

            # save the df
            else:
                print("saving df all resamples")
                save_object(df_gwas_all_resamples, df_gwas_all_resamples_file)

    return all_cmds



def get_cmds_ancestral_GWAS_drugResistance(metadata_df, outdir_ancestralGWAS, taxID_dir, drug_to_samplesForGWAS, ploidy, species, gene_features_df, gff, obo_file, reactome_pathways_file, reactome_pathwayRelations_file, DataDir, genome, ancestral_GWAS_step, small_vars_filt_file, df_sorted_bams, min_coverage_pos, threads=48, replace=False, max_fraction_genes_pathway_toBeConsidered=0.05, threads_ancestral_GWAS=48):


    """This function gets commands to run ancestral_GWAS for different mutations and groups and methods. Each command is the ASR or GWAS run for one mutation / group"""

    print("getting cmds for GWAS...")

    # init the cmds
    all_cmds = []

    # check 
    if ancestral_GWAS_step not in {"run_ancestral_GWAS_muts", "run_ancestral_GWAS_gwas"}: raise ValueError("the ancestral_GWAS_step %s is incorrect"%ancestral_GWAS_step)

    # get interpro pathway data
    df_interpro = load_InterProAnnotation("%s/InterproScan_annotation/interproscan_annotation.out"%taxID_dir)
    df_interpro_pathways_all = get_df_pathways_from_df_interpro(df_interpro, "%s/interproscan_pathway_df.py"%taxID_dir)

    # get the df with the Metacyc annotations
    df_metacyc_all = df_interpro_pathways_all[df_interpro_pathways_all.pathway_type=="MetaCyc"]
    df_metacyc_all = get_df_metacyc_with_pathways_from_parents(df_metacyc_all, outdir_ancestralGWAS, max_fraction_genes_pathway_toBeConsidered)

    # add the names of the REACTOME data with transfered TERMS
    print("getting reactome data")
    df_reactome_all = df_interpro_pathways_all[df_interpro_pathways_all.pathway_type=="Reactome"]
    df_reactome_all = get_df_reactome_with_Parents_transferred(df_reactome_all, reactome_pathways_file, reactome_pathwayRelations_file, max_fraction_genes_pathway_toBeConsidered, outdir_ancestralGWAS)

    # get a df with the GO terms transferred from parents
    print("getting GO data")
    df_GOterms_per_gene = get_df_GOterms_all_species(DataDir, {species:gff}, outdir_ancestralGWAS) # each row is one GO term
    df_GOterms_all = get_df_GOterms_transfering_across_theGraph(df_GOterms_per_gene, outdir_ancestralGWAS, obo_file, max_fraction_genes_pathway_toBeConsidered)

    # define sets of genes that have annotations
    genes_with_reactomeAnnotation = set(df_reactome_all.Gene)
    genes_with_goAnnotation = set(df_GOterms_all.Gene)
    genes_with_metacycAnnotation = set(df_metacyc_all.Gene) # needs to be defined

    # go through each drug, where you'll use a different set of samples
    for drug in sorted(drug_to_samplesForGWAS):
        print(species, drug)

        # debug
        #if drug!="MIF": continue # debug
        #if drug!="ANI" or species!="Candida_auris": continue # debug

        # get the samples
        samples = drug_to_samplesForGWAS[drug]
        #samples = {'57', '90'} # one R one S MIF # debug

        # define an outdir for this drug
        outdir_drug = "%s/GWAS_%s_resistance"%(outdir_ancestralGWAS, drug) 
        make_folder(outdir_drug)

        # define the final file
        gwas_results_df_file = "%s/gwas_results_df_all.py"%outdir_drug
        #remove_file(gwas_results_df_file) # debug

        if file_is_empty(gwas_results_df_file):

            # get a rooted tree with the samples of this drug
            tree = get_strains_tree_object_ploidy1or2(taxID_dir, ploidy, species)
            check_that_tree_has_no_politomies(tree)

            # check that all samples are in the tree
            strange_samples = samples.difference(set(tree.get_leaf_names()))
            if len(strange_samples)>0: raise ValueError("There are weird samples: %s"%samples)

            # define a metadata df with the samples
            resistance_field = "%s_resistance"%drug
            metadata_df_drug = metadata_df[metadata_df.sampleID.apply(str).isin(samples)]
            if set(metadata_df_drug[resistance_field])!={"R", "S"}: raise ValueError("the resistance is not properly defined")
            resistance_to_phenotype = {"R":1, "S":0}
            metadata_df_drug["phenotype"] = metadata_df_drug[resistance_field].apply(lambda x: resistance_to_phenotype[x])

            # get a representative set of samples
            sample_to_resistance = dict(metadata_df_drug.set_index("sampleID").phenotype)
            representative_samples = get_balanced_set_samples_for_GWAS_AF_resistance(tree, samples, sample_to_resistance, threads)
            #print("There are %i/%i representative samples"%(len(representative_samples), len(samples)))

            # generate tree for representative samples
            #tree.prune(representative_samples, preserve_branch_length=True) # Old way: prunning tree!!
            tree = generate_tree_representative_samples_gwas(representative_samples, ploidy, genome, small_vars_filt_file, "%s/generating_tree"%outdir_drug, df_sorted_bams, min_coverage_pos, threads)
            if set(tree.get_leaf_names())!=set(representative_samples): raise ValueError("the leafs of the tree should be rep samples!")
            check_that_tree_has_no_politomies(tree)

            # write the tree
            treefile_rooted = "%s/rooted_tree.nw"%outdir_drug
            tree.write(outfile=treefile_rooted, format=2)

            # make a tree with the preserved samples (considering a branch support of 50)
            plot_tree_AF_resistance_samples_for_GWAS(tree, sample_to_resistance, outdir_drug, 50, species, drug)

            # define the sorted samples
            sorted_samples = sorted(representative_samples)

            # write the resistance files
            print("get resistance data")
            resistance_file = "%s/resistance_df.tab"%outdir_drug
            save_df_as_tab(metadata_df_drug[["sampleID", "phenotype"]].set_index("sampleID", drop=False).loc[sorted_samples], resistance_file)

            # get a df with all the variants and the annotations for these representative_samples
            variants_df_all, annot_df_all, variants_df_all_file = get_vars_df_only_subset_samples_for_GWAS_simplified(taxID_dir, representative_samples, outdir_drug, ploidy, replace, gff, threads, species, genome)


            # keep some fields
            print("loading variants")
            variants_df_all = variants_df_all[["variantID_across_samples", "sampleID"]]

            # get the ASR for each mutation with all the methods possible
            get_ASR_mutations_py = "%s/scripts/ancestral_GWAS/scripts/get_ASR_mutations.py"%ParentDir
            outdir_ASR_mutations = "%s/ASR_mutations"%outdir_drug
            resampled_phenotypes_pastml_out = "%s/resampled_phenotypes_pastml_df.py"%outdir_drug
            final_file_ASR_mutations = "%s/all_jobs_finished.txt"%outdir_ASR_mutations

            if file_is_empty(final_file_ASR_mutations):

                # keep the cmd
                cmd = "%s --variants_table %s --tree %s --phenotypes %s --outdir %s --varID_field variantID_across_samples --pastml_prediction_method ALL --threads %i --resampled_phenotypes_pastml_out %s --phenotypes_pastml_prediction_method ALL"%(get_ASR_mutations_py, variants_df_all_file, treefile_rooted, resistance_file, outdir_ASR_mutations, threads_ancestral_GWAS, resampled_phenotypes_pastml_out)
                all_cmds.append(cmd)

            else: print("The ASR for all mutations was already performed")

            # if you are running on run_ancestral_GWAS_muts, continue
            if ancestral_GWAS_step=="run_ancestral_GWAS_muts": continue
            elif ancestral_GWAS_step!="run_ancestral_GWAS_gwas": raise ValueError("ancestral_GWAS_step %s is invalid"%ancestral_GWAS_step)

            # make sure that all muts are done
            if file_is_empty(final_file_ASR_mutations): raise ValueError("there are some ASR of mutations missing.")

            # remove all the .html files
            """
            debug
            print("removing html files")
            out_stat = os.system("rm %s/ASR_mutations/*.html"%outdir_ASR_mutations) # debug
            if out_stat!=0: print("WARNING: There was an error in the removal of html files (likely due to the fact it has already been done)")
            print("html files removed")
            """

            # add fields of domains and other addings
            print("load domain annotations")
            domains_annot_df_all = get_domains_annotations_df(annot_df_all, outdir_drug, species, df_interpro, gff, gene_features_df)
            variants_with_domain_alteration = set(domains_annot_df_all.variantID_across_samples)

            ############# GET THE CMDS #################

            # define different iterables to subset data 
            all_types_variants = {'coverageCNV', 'SV', 'SNP', 'INDEL'}

            list_types_vars = [("all_vars", all_types_variants),
                               ("small_vars", {'SNP', 'INDEL'}), 
                               ("SVs", {'SV'}),
                               ("coverageCNVs", {'coverageCNV'}),
                               ("SVs_and_CNVs", {'coverageCNV', 'SV'}),
                               ("small_vars_and_SVs", {'SV', 'SNP', 'INDEL'}),
                               ("small_vars_and_CNVs", {'coverageCNV', 'SNP', 'INDEL'})]

            nonSyn_types_mutations = {"non_syn_muts", "non_syn_non_truncating_muts", "truncating_muts"}
            nonSyn_types_mutations_and_all = {"non_syn_muts", "non_syn_non_truncating_muts", "truncating_muts", "all_muts"}

            # prepare the inputs of the cmds
            print("preparing cmds for get_cmd_ancestral_GWAS_drugResistance_specificMutations_and_grouping_runningFunction...")

            inputs_fn_integration = []
            inputs_fn = [] 
            Igroup = 1

            for type_vars, type_vars_set in list_types_vars:
                for type_genes in ["all_genes", "only_protein_coding", "only_non_coding"]:
                    for type_mutations in ["all_muts", "syn_muts", "non_syn_muts", "non_syn_non_truncating_muts", "truncating_muts"]: # skipping all_muts
                        for type_collapsing in ["none", "genes", "domains", "GO", "MetaCyc", "Reactome"]: # 'Reactome' is missing
                            
                            # without collapsing, you just need the all genes
                            if type_collapsing=="none" and (type_vars!="all_vars" or type_genes!="all_genes" or type_mutations!="all_muts"): continue


                            # for genes run all genes together for different combinations of mutations (only the non synonymous and all ones)
                            if type_collapsing=="genes" and (type_genes!="all_genes" or type_mutations not in nonSyn_types_mutations_and_all): continue

                            # for domains and pathways only consider protein altering mutations
                            if type_collapsing in {"domains", "GO", "Reactome", "MetaCyc"} and (type_genes!="only_protein_coding" or type_mutations not in nonSyn_types_mutations): continue

                            # log
                            print(species, drug, type_vars, type_genes, type_mutations, type_collapsing)

                            # define the outdir gwas
                            outdir_gwas_all = "%s/%s-%s-%s-%s"%(outdir_drug, type_vars, type_genes, type_mutations, type_collapsing)
                            # if type_collapsing=="none": delete_folder(outdir_gwas_all) # debug
                            make_folder(outdir_gwas_all)

                            # get the grouping_df_file
                            if type_collapsing=="none": grouping_df_file = ""
                            else: 

                                # get the file
                                grouping_df_file = get_grouping_df_file(outdir_gwas_all, annot_df_all, type_vars_set, type_genes, type_mutations, type_collapsing, variants_with_domain_alteration, genes_with_goAnnotation, genes_with_reactomeAnnotation, genes_with_metacycAnnotation, domains_annot_df_all, df_GOterms_all, df_reactome_all, df_metacyc_all)

                                #if  len(load_object(grouping_df_file))==0: print("empty file. size:", os.path.getsize(grouping_df_file))

                                # skip if the len is 0
                                if (os.path.getsize(grouping_df_file))<2000 and len(load_object(grouping_df_file))==0: 
                                    delete_folder(outdir_gwas_all)
                                    continue

                            # iterate through different methods of the phenotypes reconstruction,
                            for ASR_methods in ["MPPA", "MPPA,DOWNPASS", "DOWNPASS"]:

                                # define the same method to ASR mutations
                                ASR_methods_mutations = ASR_methods_phenotypes = ASR_methods

                                # iterate through several min_support
                                for min_support in [50, 70]:
         
                                    # define the outdirs
                                    outdir_gwas = "%s/%s-%s-min_support=%i"%(outdir_gwas_all, ASR_methods_phenotypes, ASR_methods_mutations, min_support)
                                    make_folder(outdir_gwas)
                                    outdir_GWAS_jobs = "%s/gwas_jobs"%outdir_gwas
                                    make_folder(outdir_GWAS_jobs)

                                    # get the cmd
                                    inputs_fn.append([outdir_gwas, type_collapsing, resistance_file, threads_ancestral_GWAS, outdir_ASR_mutations, ASR_methods_mutations, ASR_methods_phenotypes, min_support, outdir_GWAS_jobs, grouping_df_file, resampled_phenotypes_pastml_out, Igroup]) # using half of the threads to have more memory
                                    
                                    # get the inputs for the integration
                                    inputs_fn_integration.append([outdir_GWAS_jobs, type_collapsing, Igroup, type_vars, type_genes, type_mutations, ASR_methods_phenotypes, ASR_methods_mutations, min_support])

                                    # update group
                                    Igroup += 1

            # add the number of jobs
            njobs = len(inputs_fn)
            inputs_fn = list(map(lambda x: tuple(x + [njobs]), inputs_fn))
            inputs_fn_integration = list(map(lambda x: tuple(x + [njobs]), inputs_fn_integration))
            
            # run jobs in parallel
            print("Running GWAS jobs in %i threads"%threads) # maybe change to 24
            with multiproc.Pool(threads) as pool:
                all_cmds_drug = pool.starmap(get_cmd_ancestral_GWAS_drugResistance_specificMutations_and_grouping_runningFunction, inputs_fn, chunksize=1)
                pool.close()
                pool.terminate()

            # get the cmds as a single list
            all_cmds_drug = [cmd for cmd in all_cmds_drug if cmd is not None]

            print("\n\n\n-------------------\nThere are %i jobs in %s\n-----------------\n\n\n"%(len(all_cmds_drug), drug))

            # integrate all the GWAS results
            if len(all_cmds_drug)==0:

                #inputs_fn_integration = inputs_fn_integration[0:1]

                # run the integration in parallel
                print("Integrating GWAS jobs in %i threads"%threads) # maybe change to 24
                with multiproc.Pool(threads) as pool:
                    gwas_results_df = pd.concat(pool.starmap(run_integrate_GWAS_jobs_one_combination_get_df, inputs_fn_integration, chunksize=1))
                    pool.close()
                    pool.terminate()

                # add fields to the df
                print("adding fields and keeping gwas_results_df")
                gwas_results_df["drug"] = drug
                gwas_results_df["species"] = species

                # check that the combinations are unique
                print("checking")
                if len(gwas_results_df)!=len(gwas_results_df.drop_duplicates(subset=["species", "drug", "type_vars", "type_genes", "type_mutations", "type_collapsing", "ASR_methods_phenotypes", "ASR_methods_mutations", "gwas_method", "min_support", "group_name"])): raise ValueError("The unique_ID_gwas is not unique")

                # save
                print("saving")
                save_object(gwas_results_df, gwas_results_df_file)

                # remove the mutations. Now I leave like this for future runs.
                """
                print("cleaning mutations")
                delete_folder(outdir_ASR_mutations)
                """

            # keep all the cmds
            all_cmds += all_cmds_drug

        ############################################

    # return all the cmds, independently of what they are
    return all_cmds

def get_metacyc_taxIDs_for_ID(ID, meta):

    """For one metacyc ID it returns a set with the taxIDs in which it was described"""

    # empty IDs
    if meta[ID] is None: 
        print("WARNING: metacyc %s is not in meta"%ID)
        return set()

    # get the data
    data_dict = meta[ID].get_frame_data().__dict__

    # if it is a class, get the IDs for all the instances
    if data_dict["_isclass"] is True: all_taxIDs = set.union(*[get_metacyc_taxIDs_for_ID(x["frameid"], meta) for x in data_dict["instances"]])

    else:

        # define the taxIDs (I checked that these are the only values possible)
        all_taxIDs = set.union(*[set(map(lambda x: int(x.split("|")[1].split("-")[1]), data_dict[f])) for f in ["species", "taxonomic_range"] if f in data_dict.keys()])

    return all_taxIDs


def get_metacyc_pathways_in_clade(input_pathways, target_clade, filename):

    """Gets pathways under 'clade' from input_pathways, and saves them under filename"""

    print("getting valid metacyc pathways")

    if file_is_empty(filename):
        
        # load ncbi taxonomy
        ncbi = NCBITaxa()
        #ncbi.update_taxonomy_database() # --> ran at 24/02/2022

        # get the meta object, which has all biocyc reactions
        print("You should run the pathway-tools software while running this. In the BSC desktop we do this with:\n\n/home/mschikora/software/pathway-tools/pathway-tools -lisp -python\n\n")

        import pythoncyc
        meta = pythoncyc.select_organism('meta')

        # get the sorted_metacyc IDs in a df with the taxIDs where it is found
        pathways_df = pd.DataFrame({"ID":sorted(input_pathways)})
        pathways_df["taxIDs"] = pathways_df.ID.apply(get_metacyc_taxIDs_for_ID, meta=meta)

        # check that they make sense (they are in ncbi)
        print("checking that the taxa make sense")
        for t in set.union(*pathways_df["taxIDs"]): ncbi.get_taxid_translator([t])

        # define taxIDs that are related to the target clade. This also includes the ancestors
        target_clade_taxIDs = ncbi.get_name_translator([target_clade])[target_clade]
        ancestral_taxIDs = set.union(*[set(ncbi.get_lineage(taxID)) for taxID in target_clade_taxIDs])
        descendant_taxIDs = set.union(*[set(ncbi.get_descendant_taxa(taxID, intermediate_nodes=True)) for taxID in target_clade_taxIDs])
        target_taxIDs = ancestral_taxIDs.union(descendant_taxIDs)
        print("There are %i ancestral and %i descendant taxIDs"%(len(ancestral_taxIDs), len(descendant_taxIDs)))


        # filter out pathways whose taxID does not overlap the target taxIDs
        initial_len_df = len(pathways_df)
        pathways_df = pathways_df[pathways_df.taxIDs.apply(lambda x: x.intersection(target_taxIDs)).apply(len)>0]
        print("There are %i/%i Metacyc pathways in %s"%(len(pathways_df), initial_len_df, target_clade))

        # define the valid pathways and save
        valid_pathways = set(pathways_df.ID)
        save_object(valid_pathways, filename)

    return load_object(filename)


def get_valid_groups_one_grouping(name_outdir_gwas_all, gwas_dir): 

    """ gets the outdir of one grouping df and returns a set with the valid name_outdir_gwas_all_and_group """

    print("working on %s"%name_outdir_gwas_all)

    # load the grouping df, and        
    group_to_nmuts = load_object("%s/%s/grouping_df_all.py"%(gwas_dir, name_outdir_gwas_all)).groupby("group").apply(len)

    # define as valid groups those that have >1 muts
    valid_groups = set(group_to_nmuts[group_to_nmuts>1].index)

    # return the group appended to the name_outdir_gwas_all, so that it matches name_outdir_gwas_all_and_group
    return set(map(lambda x: "%s_%s"%(name_outdir_gwas_all, x), valid_groups))


def check_no_nans_series(x):

    """Raise value error if nans"""

    if any(pd.isna(x)): raise ValueError("There can't be nans in series %s"%x)

def get_gwas_df_with_fisher_p_value_and_corrected_pvals_one_chunk(Ijob, df_gwas, njobs):

    """Runs get_gwas_df_with_fisher_p_value_and_corrected_pvals for one GWAS run"""

    #print("adding fisher p vals for chunk %i/%i"%(Ijob+1, njobs))

    # checks
    if len(df_gwas)!=len(df_gwas[["gwas_method", "group_name"]].drop_duplicates()): raise ValueError("df_gwas should be unique")
    initial_len = len(df_gwas)

    # add the fisher test
    def get_p_fisher_gwas_one_r(r):

        contingency_table = [[r.nodes_GenoAndPheno, r.nodes_GenoAndNoPheno], [r.nodes_noGenoAndPheno, r.nodes_noGenoAndNoPheno]]
        OR, p_fisher = stats.fisher_exact(contingency_table, alternative="greater")
        return p_fisher

    df_gwas["pval_fisher"] = df_gwas[["nodes_GenoAndPheno", "nodes_GenoAndNoPheno", "nodes_noGenoAndPheno", "nodes_noGenoAndNoPheno"]].apply(get_p_fisher_gwas_one_r, axis=1)
    check_no_nans_series(df_gwas["pval_fisher"])

    # add corrected pval
    df_gwas = get_gwas_df_with_corrected_pvals(df_gwas, ["pval_fisher"])

    # checks
    if len(df_gwas)!=initial_len: raise ValueError("len changed")
    return df_gwas

def get_gwas_df_with_fisher_p_value_and_corrected_pvals(gwas_df, threads, gwas_unique_id_fields=["type_vars", "type_genes", "type_mutations", "type_collapsing", "ASR_methods_phenotypes", "min_support"]):

    """Gets a gwas df with the fisher p value and the corrected pvalues"""
    #print("adding fisher pval")

    # define the inputs of a functoin that will add the fisher p values
    inputs_fn = [[I, df] for I, (idx, df) in enumerate(gwas_df.groupby(gwas_unique_id_fields))]
    njobs = len(inputs_fn)
    inputs_fn = [tuple(x + [njobs]) for x in inputs_fn]

    if threads>1:

        # run in parallel
        with multiproc.Pool(threads) as pool:
            gwas_df = pd.concat(pool.starmap(get_gwas_df_with_fisher_p_value_and_corrected_pvals_one_chunk, inputs_fn, chunksize=1))
            pool.close()
            pool.terminate()

    else:  gwas_df = pd.concat(list(map(lambda x: get_gwas_df_with_fisher_p_value_and_corrected_pvals_one_chunk(x[0], x[1], x[2]), inputs_fn)))

    return gwas_df


def get_df_gwas_af_filtering_integrated(DataDir, ProcessedDataDir, threads):

    """gets the filtered and integrated GWAS dfs with reduced fields"""

    print("getting reduced gwas dfs")

    # get outdir
    outdir = "%s/gwas_df_integrated_dir"%ProcessedDataDir; make_folder(outdir)

    # init a dict that maps each drug and species to the GWAS results
    spp_drug_to_gwas_df_file = {}

    # go through each species
    for species in ["Candida_glabrata", "Candida_albicans", "Candida_auris"]: # Candida_albicans

        # define the outdir gwas
        gwas_dir = "%s/%s_%i/ancestral_GWAS_drugResistance"%(DataDir, species, sciName_to_taxID[species])

        # keep df for each drug
        for drug_dir in [f for f in os.listdir(gwas_dir) if f.startswith("GWAS_") and f.endswith("resistance")]:
            drug = drug_dir.split("_")[1]
            print(species, drug)

            #if drug!="POS" or species!="Candida_glabrata": continue # debug to  load the first combination

            # define the reduced df
            gwas_df_file = "%s/%s-%s.tab"%(outdir, species, drug)

            # definine the file that maps variants
            varTypeID_group_vars_df_file = "%s/%s_%s_varTypeID_group_vars_df_GWAS.py"%(ProcessedDataDir, species, drug)

            #remove_file(varTypeID_group_vars_df_file)
            #remove_file(gwas_df_file)

            if file_is_empty(gwas_df_file):

                # load df
                print("loading df")
                gwas_df = load_object("%s/%s/gwas_results_df_all.py"%(gwas_dir, drug_dir))

                # keep valid fields (we are skipping some pvalue calculations and some correction methods)
                gwas_df = gwas_df[interesting_fields_gwas_df]

                # keep only one type of method
                gwas_df = gwas_df[gwas_df.gwas_method=="synchronous"]

                # checks
                if any(gwas_df.nodes_GenoAndPheno<2): raise ValueError("there are nodes with genoandpheno<2") 
                if any(gwas_df.ASR_methods_phenotypes!=gwas_df.ASR_methods_mutations): raise ValueError("The ASR method for mutatiobs and phenotypes is not always the same")

                # add the fisher pvalue with 
                gwas_df = get_gwas_df_with_fisher_p_value_and_corrected_pvals(gwas_df, threads, gwas_unique_id_fields=["type_vars", "type_genes", "type_mutations", "type_collapsing", "ASR_methods_phenotypes", "min_support"])


                ######### MAKE SURE THAT TYPE_VARS IS CORRECT #############

                # Add 'real_type_vars'

                # add the var_type_ID
                var_type_ID_fields = ["type_vars", "type_genes", "type_mutations", "type_collapsing"]
                def join_with_slash(r): return "-".join(r)
                gwas_df["var_type_ID"] = gwas_df[var_type_ID_fields].apply(join_with_slash, axis=1)

                # map each var_type_ID to the group to vars df 
                if file_is_empty(varTypeID_group_vars_df_file):
                    print("generating varTypeID_group_vars_df...")

                    #all_varTypeIDs_collapsing = sorted(set(gwas_df[gwas_df.type_collapsing!="none"]["var_type_ID"])) # old, some missing
                    all_varTypeIDs_collapsing = [x for x in os.listdir("%s/%s"%(gwas_dir, drug_dir)) if len(x.split("-"))==4 and x.split("-")[-1] in {"MetaCyc", "GO", "Reactome", "genes", "domains"}]# new all data

                    def get_group_to_vars_df(ID): 
                        group_to_vars = load_object("%s/%s/%s/grouping_df_all.all_muts.py"%(gwas_dir, drug_dir, ID)).groupby("group").apply(lambda df: set(df.variantID_across_samples))
                        df = pd.DataFrame({"all_vars":group_to_vars})
                        df["var_type_ID"] = ID
                        return df

                    varTypeID_group_vars_df = pd.concat(map(get_group_to_vars_df, all_varTypeIDs_collapsing))
                    varTypeID_group_vars_df["group_name"] = varTypeID_group_vars_df.index
                    varTypeID_group_vars_df = varTypeID_group_vars_df.reset_index(drop=True)

                    save_object(varTypeID_group_vars_df, varTypeID_group_vars_df_file)
                varTypeID_group_vars_df = load_object(varTypeID_group_vars_df_file)

                # add the expected variants to each group
                inital_len_df_gwas = len(gwas_df)
                gwas_df = gwas_df.merge(varTypeID_group_vars_df[["var_type_ID", "group_name", "all_vars"]], on=["var_type_ID", "group_name"], validate="many_to_one", how="left")
                if len(gwas_df)!=inital_len_df_gwas: raise ValueError("merge changed len")
                if any(pd.isna(gwas_df[gwas_df.type_collapsing!="none"].all_vars)): raise ValueError("nas in all_vars")

                def get_all_vars_set(x):
                    if pd.isna(x): return set()
                    else: return x
                gwas_df["all_vars"] = gwas_df.all_vars.apply(get_all_vars_set) # get all sets

                # define the variants
                varTypeID_group_vars_df["type_vars"] = varTypeID_group_vars_df.var_type_ID.apply(lambda x: x.split("-")[0])
                all_small_vars = set.union(*varTypeID_group_vars_df[varTypeID_group_vars_df.type_vars=="small_vars"].all_vars)
                #all_SVs = set.union(*(list(varTypeID_group_vars_df[varTypeID_group_vars_df.type_vars=="SVs"].all_vars) + [set()]))
                all_SVs = set.union(*varTypeID_group_vars_df[varTypeID_group_vars_df.type_vars=="SVs"].all_vars)
                all_coverageCNVs = set.union(*varTypeID_group_vars_df[varTypeID_group_vars_df.type_vars=="coverageCNVs"].all_vars)

                if len(all_small_vars.intersection(all_SVs))>0: raise ValueError("there can't be intersections between small vars and SVs")
                if len(all_small_vars.intersection(all_coverageCNVs))>0: raise ValueError("there can't be intersections between small vars and CNVs")
                if len(all_SVs.intersection(all_coverageCNVs))>0: raise ValueError("there can't be intersections between CNVs and SVs")

                # add the types of variants
                var_to_type = {}
                for type_var, all_vars in [("small_var", all_small_vars), ("SV", all_SVs), ("coverageCNV", all_coverageCNVs)]: var_to_type = {**var_to_type, **{v : type_var for v in all_vars}}
                tuple_vars_to_real_types_vars =  {('SV', 'coverageCNV', 'small_var'):"all_vars", ('small_var',):"small_vars", ('coverageCNV',):"coverageCNVs", ('SV',):"SVs", ('coverageCNV', 'small_var'):"small_vars_and_CNVs", ('SV', 'coverageCNV'):"SVs_and_CNVs", ('SV', 'small_var'):"small_vars_and_SVs"}

                def get_real_type_vars(tuple_vars):
                    if len(tuple_vars)==0: return "all_vars"
                    else: return tuple_vars_to_real_types_vars[tuple_vars]
                gwas_df["real_type_vars"] = gwas_df.all_vars.apply(lambda x: set(map(lambda v: var_to_type[v], x))).apply(sorted).apply(tuple).apply(get_real_type_vars)

                # check that the differences are expected
                observed_combinations_type_vars_real_type_vars = set(gwas_df[(gwas_df.type_collapsing!="none") & (gwas_df.type_vars!=gwas_df.real_type_vars)][["type_vars", "real_type_vars"]].apply(tuple, axis=1))
                allowed_combinations_type_vars_real_type_vars = {('all_vars', 'small_vars'), ('SVs_and_CNVs', 'coverageCNVs'), ('small_vars_and_CNVs', 'small_vars'), ('all_vars', 'small_vars_and_SVs'), ('small_vars_and_SVs', 'small_vars'), ('SVs_and_CNVs', 'SVs'), ('all_vars', 'SVs_and_CNVs'), ('small_vars_and_CNVs', 'coverageCNVs'), ('all_vars', 'small_vars_and_CNVs'), ('all_vars', 'coverageCNVs'), ('small_vars_and_SVs', 'SVs'), ('all_vars', 'SVs')}
                strange_combs = observed_combinations_type_vars_real_type_vars.difference(allowed_combinations_type_vars_real_type_vars)
                if len(strange_combs)>0: raise ValueError("ther are strange combinations: %s"%strange_combs)

                # remove cols
                gwas_df.pop("all_vars")

                #######################################################

                # define the number of phenotype transitions by all the 
                m_to_ntransitions = {}
                for asr_method in ["MPPA", "DOWNPASS", "MPPA,DOWNPASS"]:

                    df = gwas_df[(gwas_df.ASR_methods_phenotypes==asr_method) & (gwas_df.gwas_method=="synchronous") & (gwas_df.min_support==50)][["nodes_withPheno"]].drop_duplicates()
                    if len(df)==0: m_to_ntransitions[asr_method] =  0
                    elif len(df)==1: m_to_ntransitions[asr_method] = df.iloc[0].nodes_withPheno
                    else: raise ValueError("the df has to be len in 0,1")

                # skip this if there are too few samples
                log_text = "%s-%s. There are %i phenotype transitions by MPPA,DOWNPASS"%(species, drug_dir.split("_")[1], m_to_ntransitions["MPPA,DOWNPASS"])
                if m_to_ntransitions["MPPA,DOWNPASS"]<2: print("WARNING: %s"%log_text)
                else: print(log_text)

                # save
                print("saving")
                save_df_as_tab(gwas_df, gwas_df_file)

            # keep
            spp_drug_to_gwas_df_file[(species, drug)] = gwas_df_file

    # return
    return spp_drug_to_gwas_df_file

def get_df_gwas_af_filtering_pathways_and_genes(DataDir, ProcessedDataDir, threads):

    """Gets the filtered df of gwas"""

    # get file
    df_gwas_af_file = "%s/df_gwas_af_filtering_pathways_and_genes.py"%ProcessedDataDir

    if file_is_empty(df_gwas_af_file):
        print("getting gwas df")

        # int the df
        df_gwas_af = pd.DataFrame()

        # go through each species
        for species in ["Candida_albicans", "Candida_auris", "Candida_glabrata"]:
            print(species)

            gwas_df_species_file = "%s.%s.py"%(df_gwas_af_file, species)
            if file_is_empty(gwas_df_species_file):
            

                # define the fields
                gwas_df_fields = ['nodes_withGeno', 'nodes_withoutGeno', 'nodes_unkownGeno', 'nodes_withPheno', 'nodes_withoutPheno', 'nodes_unkownPheno', 'nodes_GenoAndPheno', 'epsilon', 'pval', 'group_name', 'ASR_methods_phenotypes', 'corrected_pval', 'type_vars', 'type_genes', 'type_mutations', 'type_collapsing', 'gwas_method', 'drug', 'species']

                # load all the results
                gwas_dir = "%s/%s_%i/ancestral_GWAS_drugResistance"%(DataDir, species, sciName_to_taxID[species])
                gwas_df = load_object("%s/gwas_results_df_all.py"%gwas_dir); print("gwas_df loaded")

                # record the initial len
                inital_len_gwas_df = len(gwas_df)

                # filter out the groups where it does not make sense to make a test (keep only instances where you have >=2 nodes with geno and pheno)
                print("filtering df")
                gwas_df = gwas_df[gwas_df.nodes_GenoAndPheno>=2]
                print("There are %i/%i rows left in gwas_df after filtering nodes_GenoAndPheno>=2."%(len(gwas_df), inital_len_gwas_df))


                # delet unneccessary cols
                print("deleting cols")
                for col in set(gwas_df.keys()).difference(set(gwas_df_fields)): del gwas_df[col]

                # checks
                print("checking")
                for f in set(gwas_df.keys()):
                    if any(pd.isna(gwas_df[f])): raise ValueError("there are nans in %s"%f)



                ########## filter out pathways that are not fungal #########
                print("filtering out pathways that are not fungal")

                # note that the GO terms considered are only those that are already defined in the curated set of CGD, so that we should not do any extra filtering

                # remove metacyc pathways that are not in ascomycota
                valid_metacyc_pathways_file = "%s/%s_GWAS_valid_metacyc_pathways_Ascomycota.py"%(ProcessedDataDir, species) 
                valid_metacyc_pathways = get_metacyc_pathways_in_clade(set(gwas_df[gwas_df.type_collapsing=="MetaCyc"].group_name), "Ascomycota", valid_metacyc_pathways_file)

                gwas_df = gwas_df[(gwas_df.type_collapsing!="MetaCyc") | (gwas_df.group_name.isin(valid_metacyc_pathways))]
                print("There are %i/%i rows left in gwas_df after filtering non-fungal metacyc pathways."%(len(gwas_df), inital_len_gwas_df))

                # remove Reactome pathways that are not based on S. cerevisiae or S. pombe
                df_reactome_info = pd.read_csv("%s/annotation_files/ReactomePathways.txt"%DataDir, sep="\t", header=None, names=["ID", "description", "species"])
                fungal_pathways_reactome = set(df_reactome_info[df_reactome_info.species.isin({"Saccharomyces cerevisiae", "Schizosaccharomyces pombe"})].ID)

                if len(set(gwas_df[gwas_df.type_collapsing=="Reactome"].group_name).difference(set(df_reactome_info.ID)))>0: raise ValueError("There are some pathways in the gwas df that are not in the defined reactome pathways")

                gwas_df = gwas_df[(gwas_df.type_collapsing!="Reactome") | (gwas_df.group_name.isin(fungal_pathways_reactome))]
                print("There are %i/%i rows left in gwas_df after filtering non-fungal Reactome pathways."%(len(gwas_df), inital_len_gwas_df))

                ############################################################

                ###### filter out the rows referring to groups that are mapped to less than 2 mutations ####

                # define a dir that reflects each unique grouping
                gwas_df["name_outdir_gwas_all"] = "GWAS_" + gwas_df.drug + "_resistance/" + gwas_df.type_vars + "-" + gwas_df.type_genes + "-" + gwas_df.type_mutations + "-" + gwas_df.type_collapsing

                # create a string that adds the group name to name_outdir_gwas_all
                gwas_df["name_outdir_gwas_all_and_group"] = gwas_df.name_outdir_gwas_all + "_" + gwas_df.group_name

                # keep only the name_outdir_gwas_all_and_group that have >1 mutation
                print("running get_valid_groups_one_grouping on %i threads"%threads)
                gwas_df_groups = gwas_df[gwas_df.type_collapsing!="none"]
                
                inputs_fn = list(map(lambda x: (x, gwas_dir), set(gwas_df_groups.name_outdir_gwas_all)))
                with multiproc.Pool(threads) as pool:
                        valid_name_outdir_gwas_all_and_group = set.union(*pool.starmap(get_valid_groups_one_grouping, inputs_fn))
                        pool.close()
                        pool.terminate()

                gwas_df = gwas_df[(gwas_df.type_collapsing=="none") | (gwas_df.name_outdir_gwas_all_and_group.isin(valid_name_outdir_gwas_all_and_group))]
                print("There are %i/%i rows left in gwas_df after removing groups for which there are <2 mutations."%(len(gwas_df), inital_len_gwas_df))

                ############################################################################################

                ######### ADD CORRECTED PVALS BASED ON THE FILTERED DF ############
                print("adding corrrected pvals")

                # define a dir that reflects each unique grouping
                gwas_df["gwas_run"] = gwas_df.drug + "_" + gwas_df.type_vars + "_" + gwas_df.type_genes + "_" + gwas_df.type_mutations + "_" + gwas_df.type_collapsing + "_" + gwas_df.ASR_methods_phenotypes + "_" + gwas_df.gwas_method
                gwas_df = gwas_df.reset_index(drop=True)

                # add a field that contains each gwas_run and the group_name
                print("adding run and name")
                gwas_df["gwas_run_and_group_name"] = gwas_df.gwas_run + "_" + gwas_df.group_name
                if len(gwas_df)!=len(set(gwas_df.gwas_run_and_group_name)): raise ValueError("gwas_run_and_group_name should be unique")

                # get the corrected pvalue for each group in the gwas run
                def get_group_name_to_corrected_pval_each_gwas_run(df_run): 
                    print(df_run.name)

                    # get the corrected pvals
                    df_run["corr_pval"] = multitest.fdrcorrection(df_run.pval.values)[1]

                    # return a series that maps each 
                    return df_run.set_index("group_name").corr_pval

                print("running get_group_name_to_corrected_pval_each_gwas_run")
                mapping_series = gwas_df[["gwas_run", "pval", "group_name"]].groupby("gwas_run").apply(get_group_name_to_corrected_pval_each_gwas_run)
                mapping_series.index = mapping_series.index.get_level_values(0).astype('str') + '_' + mapping_series.index.get_level_values(1).astype('str')

                gwas_df["corrected_pval"] = gwas_df.gwas_run_and_group_name.map(mapping_series); check_no_nans_series(gwas_df["corrected_pval"])

                ###################################################################

                # save
                print("saving")
                save_object(gwas_df[gwas_df_fields], gwas_df_species_file)

            # keep df (some fields)
            print("appending")
            df_gwas_af = df_gwas_af.append(load_object(gwas_df_species_file))

        # save
        print("saving")
        save_object(df_gwas_af, df_gwas_af_file)

    print("returning gwas df")
    return load_object(df_gwas_af_file)

def plot_qq_plot_subset_gwas_sets(df_gwas_af, PlotsDir, type_vars="small_vars", type_genes="all_genes", type_mutations="non_syn_non_truncating_muts", type_collapsing="genes"):

    """Plots the qq plot of raw pvalues for different drugs and species"""

    # get the filtered df with only the relevant data (genes and small variants)
    df_gwas_af = df_gwas_af[(df_gwas_af.type_vars==type_vars) & (df_gwas_af.type_genes==type_genes) & (df_gwas_af.type_mutations==type_mutations) & (df_gwas_af.type_collapsing==type_collapsing)]; print("filtered df obtained")
    if len(df_gwas_af)==0: raise ValueError("there has to be something in the df")

    # add fields
    df_gwas_af["species_and_drug"] = df_gwas_af.species.apply(lambda x: x.split("_")[1]) + "-" + df_gwas_af.drug
    dict_map = {"MPPA":"ML", "DOWNPASS":"MP", "MPPA,DOWNPASS":"ML&MP"}
    df_gwas_af["asr_phenotypes"] = df_gwas_af.ASR_methods_phenotypes.map(dict_map); check_no_nans_series(df_gwas_af["asr_phenotypes"])

    # add a unique ID 
    df_gwas_af["unique_gwas_ID"] = df_gwas_af.species_and_drug + df_gwas_af.asr_phenotypes + df_gwas_af.gwas_method

    # define a string with the type of gwas
    string_type_gwas = "-".join([type_vars, type_genes, type_mutations, type_collapsing])


    # go through each combination
    sorted_species_and_drug = sorted(set(df_gwas_af.species_and_drug))
    #sorted_species_and_drug = [x for x in sorted_species_and_drug if x=="glabrata-MIF"]

    # define the pseudocount of the pval
    pseudocount_pval = 1/10000

    # define the color
    pval_fields = ['pval_phenotypes', 'pval_RelToBranchLen', 'pval_notRelToBranchLen', 'pval_RelToBranchLen_wReplace', 'pval_notRelToBranchLen_wReplace', 'pval_fisher']
    pval_f_to_color = get_value_to_color(pval_fields, n=len(pval_fields), type_color="hex", palette="tab10")[0]

    # define the lims
    max_minus_log10pval_observed = max([max(-np.log10(df_gwas_af[f] + pseudocount_pval))  for f in pval_fields])

    unique_gwas_ID_to_min_expected_pval = 1 / df_gwas_af.groupby("unique_gwas_ID").apply(len)
    max_minus_log10pval_expected = max(-np.log10(unique_gwas_ID_to_min_expected_pval + pseudocount_pval))
    all_max_minus_log10pval = max([max_minus_log10pval_observed, max_minus_log10pval_expected])
    lims = [-0.1, all_max_minus_log10pval+0.1]

    # make one fig for each species_and_drug
    for species_and_drug in sorted_species_and_drug:
        print(species_and_drug)

        # get the df of the fig
        df_fig = df_gwas_af[df_gwas_af.species_and_drug==species_and_drug]

        # get the fig parms
        sorted_asr_phenotypes = sorted(set(df_fig.asr_phenotypes))
        sorted_gwas_methods = sorted(set(df_fig.gwas_method))

        nrows = len(sorted_asr_phenotypes)
        ncols = len(sorted_gwas_methods)
        fig = plt.figure(figsize=(ncols*2.2, nrows*2.2)); I=1

        for Ir, asr_phenotypes in enumerate(sorted_asr_phenotypes):
            for Ic, gwas_method in enumerate(sorted_gwas_methods):

                # get the df and check
                df_plot = df_fig[(df_fig.asr_phenotypes==asr_phenotypes) & (df_fig.gwas_method==gwas_method)]
                if len(df_plot)!=len(set(df_plot.group_name)): raise ValueError("there should be unique group names")

                # init subplot
                ax = plt.subplot(nrows, ncols, I); I+=1

                if len(df_plot)>0:

                    # create a long df with the expected and observed pvals
                    df_long = pd.DataFrame()
                    for f in pval_fields:
                        df = pd.DataFrame({"observed p-value" : df_plot[f].values + pseudocount_pval}).sort_values(by="observed p-value")
                        df["type p-value"] = f
                        df["expected p-value"] = np.linspace(1/len(df), 1, len(df))
                        df["minus_log10pval_observed"] = -np.log10(df["observed p-value"])
                        df["minus_log10pval_expected"] = -np.log10(df["expected p-value"])
                        df_long = df_long.append(df)

                    # plot
                    #ax = sns.scatterplot(x="minus_log10pval_expected", y="minus_log10pval_observed", data=df_long, hue="type p-value", edgecolor="none", alpha=.7, style="type p-value", palette="tab10")
                    ax = sns.lineplot(x="minus_log10pval_expected", y="minus_log10pval_observed", data=df_long, hue="type p-value", linewidth=2, palette=pval_f_to_color)

                    # add line
                    plt.plot([0, all_max_minus_log10pval],  [0, all_max_minus_log10pval], color="gray", linewidth=.7, linestyle="--")

                    alpha_line = -np.log10(0.05 + pseudocount_pval)
                    plt.axvline(alpha_line, color="gray", linewidth=.7, linestyle="--")
                    plt.axhline(alpha_line, color="gray", linewidth=.7, linestyle="--")

                # remove legend
                if Ir==1 and Ic==(ncols-1): 

                    def get_legend_element(color, label): return Line2D([0], [0], color=color, lw=4, label=label, alpha=1.0) 
                    legend_elements = [get_legend_element("white", "type p-value")] + [get_legend_element(color, pval_f) for pval_f, color in pval_f_to_color.items()]
                    ax.legend(handles=legend_elements, loc='lower left', bbox_to_anchor=(1.0, 0.3))

                elif len(df_plot)>0: ax.get_legend().remove()

                # axes
                if Ir!=(nrows-1): 
                    ax.set_xlabel("")
                    ax.set_xticklabels([])

                else: ax.set_xlabel("-log(expected p -val)\n%s"%gwas_method)

                if Ic!=0:
                    ax.set_ylabel("")
                    ax.set_yticklabels([])

                else: ax.set_ylabel("ASR by %s\n-log(observed p -val)"%asr_phenotypes)

                # title
                if Ic==0 and Ir==0: ax.set_title("%s (%s) QQ plot"%(species_and_drug, string_type_gwas))

                # get lims
                ax.set_xlim(lims)
                ax.set_ylim(lims)

        plt.subplots_adjust(wspace=0.02, hspace=0.02)
        plt.show()

        dirplot = "%s/GWAS_QQ_plot_genes_%s"%(PlotsDir, string_type_gwas); make_folder(dirplot)
        filename_plot = "%s/%s.pdf"%(dirplot, species_and_drug)
        print("saving", filename_plot)
        fig.savefig(filename_plot, bbox_inches='tight')

def plot_pval_distribution_gwas_genes(df_gwas_af, PlotsDir):

    """Plots the distribution of pvalues for genes and non-syn mutations. Each row would be a species-drug combination. Each col would be a ASR_methods_phenotypes-gwas_method combination"""
    print("running plot_pval_distribution_gwas_genes... ")

    # get the filtered df with only the relevant data (genes and small variants)
    df_gwas_af = df_gwas_af[(df_gwas_af.type_vars=="small_vars") & (df_gwas_af.type_genes=="all_genes") & (df_gwas_af.type_mutations=="non_syn_non_truncating_muts") & (df_gwas_af.type_collapsing=="genes")]; print("filtered df obtained")

    # add fields
    df_gwas_af["species_and_drug"] = df_gwas_af.species.apply(lambda x: x.split("_")[1]) + "-" + df_gwas_af.drug
    dict_map = {"MPPA":"ML", "DOWNPASS":"MP", "MPPA,DOWNPASS":"ML&MP"}
    df_gwas_af["asr_and_gwas_method"] = df_gwas_af.ASR_methods_phenotypes.map(dict_map) + "-" + df_gwas_af.gwas_method

    # go through each combination
    sorted_species_and_drug = sorted(set(df_gwas_af.species_and_drug))
    sorted_asr_and_gwas_method = sorted(set(df_gwas_af.asr_and_gwas_method))


    # init fig
    nrows = len(sorted_species_and_drug)
    ncols = len(sorted_asr_and_gwas_method)
    figsize = (ncols*1.5, nrows*1.5)
    print(figsize)
    fig = plt.figure(figsize=figsize); I=1

    for Ir, species_and_drug in enumerate(sorted_species_and_drug):

        for Ic, asr_and_gwas_method in enumerate(sorted_asr_and_gwas_method):

            # get  df
            df_plot = df_gwas_af[(df_gwas_af.species_and_drug==species_and_drug) & (df_gwas_af.asr_and_gwas_method==asr_and_gwas_method)]

            # plot the fraction of nodes that have the geno
            """
            df_plot["fraction_nodes_withGeno"] = df_plot.nodes_withGeno / (df_plot.nodes_withGeno + df_plot.nodes_withoutGeno + df_plot.nodes_unkownGeno)
            sns.scatterplot(data=df_plot, x="pval", y="epsilon", alpha=.1); plt.show()
            continue
            """

            # checks
            if len(df_plot)!=len(set(df_plot.group_name)): raise ValueError("the gname should be unique")
            check_no_nans_series(df_plot.pval)
            if any(df_plot.nodes_GenoAndPheno<2): raise ValueError("there are some testing instances that are not nodes_GenoAndPheno<2")

            # make the hist in a subplot
            ax = plt.subplot(nrows, ncols, I); I+=1
            ax = sns.distplot(df_plot.pval, color="black", kde=False, hist=True, hist_kws={"color":"black", "linewidth":2, "alpha":1.0})

            # define the axes
            #ax.set_yscale("log")
            ax.set_xlim([-0.05, 1.05])

            if len(df_plot)==0: 
                ax.set_yticklabels([])
                ax.set_yticks([])


            # labels
            if Ir==0: ax.set_title(asr_and_gwas_method)

            if Ic==0: ax.set_ylabel("%s\n# genes"%species_and_drug)
            else: ax.set_ylabel("")

            ax.set_yticks([])
            ax.set_yticklabels([])

            if Ir==(len(sorted_species_and_drug)-1): ax.set_xlabel("raw p-value")
            else:
                ax.set_xticks([])
                ax.set_xticklabels([])
                ax.set_xlabel("")   


    plt.subplots_adjust(wspace=0.01, hspace=0.01)
    
    filename = "%s/GWAS_raw_pval_distribution_genes.pdf"%(PlotsDir)
    print("saving %s"%filename)
    fig.savefig(filename, bbox_inches="tight")
    #plt.show()

    


def get_geneID_for_gName_from_gene_features_df(df, gName):

    """Find the geneID from gene_features_df_s for gName """

    for k in ["gene_name", "Scerevisiae_orthologs", "gff_upmost_parent"]:

        # get a df that matches the name
        df_k = df[df[k]==gName]

        # if there is only one match, return it
        if len(df_k)==1: return df_k.iloc[0].gff_upmost_parent

    raise ValueError("%s not found"%gName)

def get_target_grouping_df_file_fromDataDir_gwas_af_resistance(tmpdir, DataDir, species, drug, target_type_vars, target_type_genes, target_type_mutations, target_type_collapsing):

    """gets a grouping df from a target set of variants"""

    # define the file
    gdf_file = "%s/%s_%i/ancestral_GWAS_drugResistance/GWAS_%s_resistance/%s-%s-%s-%s/grouping_df_all.all_muts.py"%(DataDir, species, sciName_to_taxID[species], drug, target_type_vars, target_type_genes, target_type_mutations, target_type_collapsing)

    return gdf_file


def get_gwas_df_no_collapsing_with_collapsing_info_one_group(unique_gwas_ID, gwas_df, target_type_collapsing, target_type_vars, target_type_mutations, target_type_genes, DataDir, tmpdir):

    """Executes the get_gwas_df_no_collapsing_with_collapsing_info for one species-drug combination. returns teh gwas_df, where each row is one element of the target_* collapsing."""

    # define the ID
    species, drug = unique_gwas_ID.split("-")

    ###### GET THE GROUPING DF ########

    # get the grouping df
    target_grouping_df_file = get_target_grouping_df_file_fromDataDir_gwas_af_resistance(tmpdir, DataDir, species, drug, target_type_vars, target_type_genes, target_type_mutations, target_type_collapsing)

    # load
    target_grouping_df = load_object(target_grouping_df_file).rename(columns={"group":"target_group_name"})

    ###################################

    ######### EXPAND THE GWAS DF TO INCLUDE ONE ROW FOR EACH GROUP ##########

    # filter out the gwas_df
    gwas_df = gwas_df[gwas_df.group_name.isin(set(target_grouping_df.variantID_across_samples))]

    # iterate through each unique chunk of gwas df
    gwas_df_all = pd.DataFrame()
    gwas_df = gwas_df.set_index(["ASR_methods_phenotypes", "min_support", "gwas_method"], drop=False)

    for ASR_methods_phenotypes, min_support, gwas_method in gwas_df[["ASR_methods_phenotypes", "min_support", "gwas_method"]].drop_duplicates().values:

        # get unique df, and check that it is unique
        df = gwas_df.loc[{(ASR_methods_phenotypes, min_support, gwas_method)}]
        if len(df)!=len(set(df.group_name)): raise ValueError("group name shoul be unique")

        # filter target_grouping_df
        target_grouping_df_interesting = target_grouping_df[target_grouping_df.variantID_across_samples.isin(set(df.group_name))]
        if len(target_grouping_df_interesting)==0: raise ValueError("target_grouping_df_interesting can't be 0")

        # merge
        df = target_grouping_df_interesting.merge(df, left_on="variantID_across_samples", right_on="group_name", how="left", validate="many_to_one")
        if len(df)!=len(target_grouping_df_interesting): raise ValueError("df len changed over merge")

        # check no nans
        for k in ["group_name", "target_group_name"]: check_no_nans_series(df[k])

        # keep
        gwas_df_all = gwas_df_all.append(df)

    # add fields
    gwas_df_all["target_type_collapsing"] = target_type_collapsing
    gwas_df_all["target_type_vars"] = target_type_vars
    gwas_df_all["target_type_mutations"] = target_type_mutations
    gwas_df_all["target_type_genes"] = target_type_genes

    ######################################################################### 

    return gwas_df_all


def get_gwas_df_no_collapsing_with_collapsing_info(df_gwas_af, DataDir, threads, ProcessedDataDir, target_type_collapsing="genes", target_type_vars="all_vars", target_type_mutations="non_syn_muts", target_type_genes="all_genes", run_in_parallel=True):

    """Takes a df with gwas results and returns, for the type_collapsing==none, a gwas df where each row corresponds to a collapsing affected, and the type of alteration. This is based on DataDir"""

    make_folder(ProcessedDataDir)

    # filter the df to keep None variants
    df_gwas_af_none = df_gwas_af[(df_gwas_af.type_collapsing=="none") & (df_gwas_af.type_genes=="all_genes") & (df_gwas_af.type_vars=="all_vars") & (df_gwas_af.type_mutations=="all_muts")] 
    if len(df_gwas_af_none)==0: raise ValueError("empty df")

    # define the unique GWAS ID
    print("adding unique ID")
    df_gwas_af_none["unique_gwas_ID"] = df_gwas_af_none.species + "-" + df_gwas_af_none.drug

    # for each group method
    print("making tmpdir")
    tmpdir = "%s/gwas_df_transferring_groups_from_%s_%s_%s_%s_to_none"%(ProcessedDataDir, target_type_collapsing, target_type_vars, target_type_mutations, target_type_genes)
    make_folder(tmpdir)
    
    inputs_fn = [(gname, gdf, target_type_collapsing, target_type_vars, target_type_mutations, target_type_genes, DataDir, "%s/%s"%(tmpdir, gname)) for gname, gdf in df_gwas_af_none.groupby("unique_gwas_ID")]


    if run_in_parallel is True:
        print("running in parallel")
        with multiproc.Pool(threads) as pool:
            df_gwas_af_with_adds = pd.concat(pool.starmap(get_gwas_df_no_collapsing_with_collapsing_info_one_group, inputs_fn))
            pool.close()
            pool.terminate()
    else:

        print("getting df_gwas_af_with_adds")
        df_gwas_af_with_adds = pd.concat(map(lambda x: get_gwas_df_no_collapsing_with_collapsing_info_one_group(x[0], x[1], x[2], x[3], x[4], x[5], x[6], x[7]) , inputs_fn))



    return df_gwas_af_with_adds


def get_filtered_gwas_af_df(filters, gwas_df):

    """Takes a gwas results df and returns the filtered one, according to filters (a series)"""


    # filter the ASR_methods_phenotypes
    gwas_df = gwas_df[(gwas_df.ASR_methods_phenotypes==filters.ASR_methods_phenotypes)]

    # filter quantitative estimates
    gwas_df = gwas_df[(gwas_df.epsilon>=filters.min_epsilon) & (gwas_df.OR>=filters.min_OR)]

    # filter pvalues
    interesting_pval_fields = []
    if filters.pval_chi_square_RelToBranchLen: interesting_pval_fields.append("pval_chi_square_RelToBranchLen")
    if filters.pval_chi_square_phenotypes: interesting_pval_fields.append("pval_chi_square_phenotypes")
    if filters.pval_GenoAndPheno_RelToBranchLen: interesting_pval_fields.append("pval_GenoAndPheno_RelToBranchLen")
    if filters.pval_GenoAndPheno_phenotypes: interesting_pval_fields.append("pval_GenoAndPheno_phenotypes")

    if len(interesting_pval_fields)==0: raise ValueError("there has to be some pval filtering")

    # add the correction method
    if filters.correction_method!="none": interesting_pval_fields = ["%s_%s"%(pval_f, filters.correction_method) for pval_f in interesting_pval_fields]

    # filter
    gwas_df = gwas_df[(gwas_df[interesting_pval_fields]<0.05).apply(all, axis=1)]

    return gwas_df

def get_filtering_stats_df_one_species_drug_gwas_method(df_gwas_af_s, species, drug, gwas_method, filters_df, gene_features_df_s):


    """Takes a df for one test instance (species, drug, gwas_method) and returns a series with all the filtering stats. """

    print(species, drug, gwas_method)

    # reset the index
    filters_df = filters_df.reset_index(drop=True)

    # map each species and drugs to the interesting geness
    species_to_drug_to_interestingGenes = {"Candida_albicans": {"azoles":["ERG11"]},
                                           "Candida_auris": {"azoles":["ERG11", "TAC1b"], "echinocandins":["GSC2"]}, # GSC2 is the auris ortholog of FKS1
                                           "Candida_glabrata": {"azoles":["PDR1"], "echinocandins":["FKS1", "FKS2"]} }

    drug_to_type = {"FLC":"azoles", "POS":"azoles", "VRC":"azoles", "ITR":"azoles", "MIF":"echinocandins", "ANI":"echinocandins", "CAS":"echinocandins"}


    interesting_genes = species_to_drug_to_interestingGenes[species][drug_to_type[drug]]
    gID_to_gName = {get_geneID_for_gName_from_gene_features_df(gene_features_df_s, gName) : gName  for gName in interesting_genes}
    interesting_geneIDs = set(gID_to_gName.keys())

    # define a function that takes a filtered gwas df and returns stats about the filtering
    def get_filtering_stats_from_filtered_df(df_gwas_filt):

        # define the found genes
        if len(df_gwas_filt)>0: genes_found = set(df_gwas_filt.gene_name)
        else: genes_found = set()

        genes_expected_found = set(map(lambda gID: gID_to_gName[gID], genes_found.intersection(interesting_geneIDs)))

        # get the dict
        return pd.Series({"n_genes_found":len(genes_found), "n_genes_expected_found":len(genes_expected_found), "genes_expected_found":genes_expected_found})

    # get the filtered gwas df for each set of filters in filters_df
    filtering_stats_df = filters_df.apply(get_filtered_gwas_af_df, gwas_df=df_gwas_af_s, axis=1).apply(get_filtering_stats_from_filtered_df)

    # add general stats
    filtering_stats_df["genes_expected"] = [set(gID_to_gName.values())]*len(filtering_stats_df)
    filtering_stats_df["n_genes_expected"] = len(interesting_geneIDs)
    filtering_stats_df["total_n_genes"] = len(set(gene_features_df_s.gff_upmost_parent))

    filtering_stats_df["species"] = species 
    filtering_stats_df["drug"] = drug
    filtering_stats_df["gwas_method"] = gwas_method 

    # add the filters_df info
    for k in filters_df.keys(): filtering_stats_df[k] = filters_df[k]


    return filtering_stats_df

def check_no_nans_in_df(df):

    """Returns an error if there ar nans in a df"""

    if any(pd.isna(df).apply(any, axis=1)): 
        for k in df.keys(): check_no_nans_series(df[k])


def load_df_with_some_fields(tab_file, fields):

    """Load df with some fields"""

    print("loading %s"%tab_file)
    df = get_tab_as_df_or_empty_df(tab_file)[fields]
    print("df loaded. returning... df has a size of %ix%i"%(len(df), len(df.columns)))

    return df

def get_hmean_fraction_nodes_GwP_PwG(r):

    """Gets a row of the gwas_df and returns the harmonic mean"""


def get_filtered_gwas_af_df_consistency_btw_pvals(filters, gwas_df):

    """Takes a gwas results df and returns the filtered one, according to filters (a series). This is adapted to only work in synchronous for specific formats"""

    # filter the ASR_methods_phenotypes and min_support
    gwas_df = gwas_df[(gwas_df.ASR_methods_phenotypes==filters.ASR_methods_phenotypes) & (gwas_df.min_support==filters.min_support)]

    # filter quantitative estimates already computed
    #gwas_df = gwas_df[(gwas_df.epsilon>=filters.min_epsilon) & (gwas_df.OR>=filters.min_OR) & (gwas_df.nodes_GenoAndPheno>=filters.min_nodes_GenoAndPheno)]  
    gwas_df = gwas_df[(gwas_df.epsilon>=filters.min_epsilon) & (gwas_df.nodes_GenoAndPheno>=filters.min_nodes_GenoAndPheno)]  

    # if empty return 
    if len(gwas_df)==0: return gwas_df  

    """
    # this is from old runs
    # define fields
    gwas_df["fraction_nodes_Pheno_wGeno"] = gwas_df.nodes_GenoAndPheno / gwas_df.nodes_withPheno # fraction nodes with pheno that also have geno
    gwas_df["fraction_nodes_Geno_wPheno"] = gwas_df.nodes_GenoAndPheno / gwas_df.nodes_withGeno # fraction nodes with geno that also have pheno
    gwas_df["hmean_fraction_nodes_GwP_PwG"] = gwas_df[["fraction_nodes_Pheno_wGeno", "fraction_nodes_Geno_wPheno"]].apply(lambda r: get_harmonic_mean(r.fraction_nodes_Pheno_wGeno, r.fraction_nodes_Geno_wPheno), axis=1)

    # check fields
    for f in ["fraction_nodes_Pheno_wGeno", "fraction_nodes_Geno_wPheno", "hmean_fraction_nodes_GwP_PwG"]:
        if any(gwas_df[f]<0) or any(gwas_df[f]>1): raise ValueError("invalid %s"%f)
        check_no_nans_series(gwas_df[f])


    # filter by these combined fields
    gwas_df = gwas_df[(gwas_df.fraction_nodes_Pheno_wGeno>=filters.min_fraction_nodes_Pheno_wGeno) & (gwas_df.fraction_nodes_Geno_wPheno>=filters.min_fraction_nodes_Geno_wPheno) & (gwas_df.hmean_fraction_nodes_GwP_PwG>=filters.min_hmean_fraction_nodes_GwP_PwG)]    
    """

    # define the pval fields
    pval_fields = []
    if 'pval_chi_square_RelToBranchLen' in filters.index and filters.pval_chi_square_RelToBranchLen==True: pval_fields.append("pval_chi_square_RelToBranchLen")
    if filters.pval_chi_square_phenotypes==True: pval_fields.append("pval_chi_square_phenotypes")
    if 'pval_GenoAndPheno_RelToBranchLen' in filters.index and filters.pval_GenoAndPheno_RelToBranchLen==True: pval_fields.append("pval_GenoAndPheno_RelToBranchLen")
    if filters.pval_GenoAndPheno_phenotypes==True: pval_fields.append("pval_GenoAndPheno_phenotypes")
    if filters.pval_fisher==True: pval_fields.append("pval_fisher")

    # add the signifficant fields
    pval_fields_sig = []
    for pval_f in pval_fields:

        # define the field
        field_sig = "%s_is_significant"%pval_f

        # define significance
        if filters.correction_method=="none": gwas_df[field_sig] = gwas_df[pval_f]<filters.alpha_pval
        elif filters.correction_method=="bonferroni": gwas_df[field_sig] = gwas_df["%s_bonferroni"%pval_f]<filters.alpha_pval
        elif filters.correction_method in {"fdr_bh", "fdr_by"}: gwas_df[field_sig] = (gwas_df["%s_%s"%(pval_f, filters.correction_method)]<filters.fdr_threshold) & (gwas_df[pval_f]<filters.alpha_pval)
        else: raise ValueError("invalid filter name")

        # keep field
        pval_fields_sig.append(field_sig)

    # add the maxT filters to pval_fields_sig (independent of other corrections)
    for f in ["pval_epsilon_maxT", "pval_chi_square_maxT"]:
        if filters[f]==True: 

            field_sig = "%s_is_significant"%f
            gwas_df[field_sig] = gwas_df[f]<filters.alpha_pval
            pval_fields_sig.append(field_sig)

    # check that there is some significance field
    if len(pval_fields_sig)==0: raise ValueError("there are no sig fields")

    # filter
    gwas_df = gwas_df[(gwas_df[pval_fields_sig]).apply(all, axis=1)]

    return gwas_df

def get_n_groups_df(df):

    """returns number of groups in df"""

    if len(df)==0: return 0
    else: return len(set(df.group_name))

def get_series_n_sig_vars_one_resample(resampleI, df_gwas_af_no_collapsing_resampleI,  filters_df):

    """Returns a series with the number of variants by each filter in one resample"""

    print("resample %i"%resampleI, end="\r")

    return filters_df.apply(get_filtered_gwas_af_df_consistency_btw_pvals, gwas_df=df_gwas_af_no_collapsing_resampleI, axis=1).apply(get_n_groups_df)

def get_filtering_stats_df_one_species_drug_gwas_method_consistency_btw_pvals(Ic, nchunks, df_gwas_af_s, species, drug, gwas_method, filters_df, gene_features_df_s, df_gwas_af_no_collapsing, df_gwas_af_no_collapsing_resamples, threads):


    """Takes a df for one test instance (species, drug, gwas_method) and returns a series with all the filtering stats. """

    print(species, drug, gwas_method, len(filters_df), 'chunk %i/%i'%(Ic, nchunks))

    # check that the fileds are unique
    unique_fields = ['ASR_methods_phenotypes', 'min_support', 'group_name']
    if len(df_gwas_af_s)!=len(df_gwas_af_s[unique_fields + ["gene_name"]].drop_duplicates()): raise ValueError("not unique")
    if len(df_gwas_af_no_collapsing)!=len(df_gwas_af_no_collapsing[unique_fields].drop_duplicates()): raise ValueError("not unique")
    if len(df_gwas_af_no_collapsing_resamples)!=len(df_gwas_af_no_collapsing_resamples[unique_fields + ["resampleI"]].drop_duplicates()): raise ValueError("not unique")

    # keep only dfs that are in the filters df
    filters_ASR_methods_phenotypes = set(filters_df.ASR_methods_phenotypes)
    filters_min_support = set(filters_df.min_support)

    df_gwas_af_s = df_gwas_af_s[(df_gwas_af_s.ASR_methods_phenotypes.isin(filters_ASR_methods_phenotypes)) & (df_gwas_af_s.min_support.isin(filters_min_support))]
    df_gwas_af_no_collapsing = df_gwas_af_no_collapsing[(df_gwas_af_no_collapsing.ASR_methods_phenotypes.isin(filters_ASR_methods_phenotypes)) & (df_gwas_af_no_collapsing.min_support.isin(filters_min_support))]
    df_gwas_af_no_collapsing_resamples = df_gwas_af_no_collapsing_resamples[(df_gwas_af_no_collapsing_resamples.ASR_methods_phenotypes.isin(filters_ASR_methods_phenotypes)) & (df_gwas_af_no_collapsing_resamples.min_support.isin(filters_min_support))]

    # reset the index
    filters_df = filters_df.reset_index(drop=True)

    # map each species and drugs to the interesting geness
    species_to_drug_to_interestingGenes = {"Candida_albicans": {"azoles":["ERG11"]},
                                           "Candida_auris": {"azoles":["ERG11", "TAC1b"], "echinocandins":["GSC2"], "others":[]}, # GSC2 is the auris ortholog of FKS1
                                           "Candida_glabrata": {"azoles":["PDR1"], "echinocandins":["FKS1", "FKS2"], "others":[]} }

    drug_to_type = {"FLC":"azoles", "POS":"azoles", "VRC":"azoles", "ITR":"azoles", "MIF":"echinocandins", "ANI":"echinocandins", "CAS":"echinocandins", "5FC":"others", "AMB":"others"}


    interesting_genes = species_to_drug_to_interestingGenes[species][drug_to_type[drug]]
    gID_to_gName = {get_geneID_for_gName_from_gene_features_df(gene_features_df_s, gName) : gName  for gName in interesting_genes}
    interesting_geneIDs = set(gID_to_gName.keys())


    # define a function that takes a filtered gwas df and returns stats about the filtering
    def get_filtering_stats_from_filtered_df(df_gwas_filt):

        # define the found genes
        if len(df_gwas_filt)>0: genes_found = set(df_gwas_filt.gene_name)
        else: genes_found = set()

        genes_expected_found = set(map(lambda gID: gID_to_gName[gID], genes_found.intersection(interesting_geneIDs)))

        # get the dict
        return pd.Series({"n_genes_found":len(genes_found), "n_genes_expected_found":len(genes_expected_found), "genes_expected_found":genes_expected_found})


    # get the filtered gwas df for each set of filters in filters_df
    print("getting number sig genes")
    filtering_stats_df = filters_df.apply(get_filtered_gwas_af_df_consistency_btw_pvals, gwas_df=df_gwas_af_s, axis=1).apply(get_filtering_stats_from_filtered_df)


    # keep only gwas_method. Unnecessary because this should only be synchronous. This was already checked
    #df_gwas_af_no_collapsing = df_gwas_af_no_collapsing[df_gwas_af_no_collapsing.gwas_method==gwas_method]
    #df_gwas_af_no_collapsing_resamples = df_gwas_af_no_collapsing_resamples[df_gwas_af_no_collapsing_resamples.gwas_method==gwas_method]

    # map each filter to the number of uncollapsed significant vars
    print("calculating nsig vars")
    filterI_to_nsig_vars = filters_df.apply(get_filtered_gwas_af_df_consistency_btw_pvals, gwas_df=df_gwas_af_no_collapsing, axis=1).apply(get_n_groups_df)
    filtering_stats_df["nsignificant_vars"] = filterI_to_nsig_vars

    if len(df_gwas_af_no_collapsing_resamples)>0:

        # map each filter to the probability of observing nsig vars under resamples
        print("adding resamples...")
        start_time = time.time()
        df_gwas_af_no_collapsing_resamples = df_gwas_af_no_collapsing_resamples.set_index("resampleI", drop=False)
        nresamples_total = 50

        # slow way: takes 30s in C. auris ITR
        """
        df_resamples = pd.DataFrame(index=filterI_to_nsig_vars.index) # each row is one filter, indexed as filterI_to_nsig_vars. Each column is the number of sig vars in each resample
        for resampleI in range(1, nresamples_total+1):
            print(resampleI)
            df_gwas_af_no_collapsing_resampleI = df_gwas_af_no_collapsing_resamples.loc[{resampleI}]
            if len(df_gwas_af_no_collapsing_resampleI)==0: df_resamples[resampleI] = 0
            else: df_resamples[resampleI] = filters_df.apply(get_filtered_gwas_af_df_consistency_btw_pvals, gwas_df=df_gwas_af_no_collapsing_resampleI, axis=1).apply(get_n_groups_df)

        """


        # fast way: takes 9s in C. auris ITR
        resamples_w_results = set(df_gwas_af_no_collapsing_resamples.index)
        list_resamples = [rI for rI in range(1, nresamples_total+1) if rI in resamples_w_results]
        inputs_fn = [(resampleI, df_gwas_af_no_collapsing_resamples.loc[{resampleI}], cp.deepcopy(filters_df)) for resampleI in list_resamples]
        with multiproc.Pool(threads) as pool:
            df_resamples = pd.DataFrame(dict(zip(list_resamples, pool.starmap(get_series_n_sig_vars_one_resample, inputs_fn, chunksize=1))))
            pool.close()
            pool.terminate()

        print("%.4f seconds to calculated df resamples"%(time.time()-start_time))
        
        filterI_to_p_nsig_vars = df_resamples.apply(lambda r: sum(r>=filterI_to_nsig_vars[r.name]), axis=1)/nresamples_total
        filtering_stats_df["p_nsignificant_vars"] = filterI_to_p_nsig_vars

    else: filtering_stats_df["p_nsignificant_vars"] = 0.0

    # add general stats
    filtering_stats_df["genes_expected"] = [set(gID_to_gName.values())]*len(filtering_stats_df)
    filtering_stats_df["n_genes_expected"] = len(interesting_geneIDs)
    filtering_stats_df["total_n_genes"] = len(set(gene_features_df_s.gff_upmost_parent))

    filtering_stats_df["species"] = species 
    filtering_stats_df["drug"] = drug
    filtering_stats_df["gwas_method"] = gwas_method 

    # add the filters_df info
    for k in filters_df.keys(): filtering_stats_df[k] = filters_df[k]

    #print("returning df...")
    return filtering_stats_df

def get_varname(var):

    """gets the var name as a string"""

    # define callers
    callers_local_vars = inspect.currentframe().f_back.f_locals.items()

    # get list of vars
    list_names = [var_name for var_name, var_val in callers_local_vars if var_val is var]
    if len(list_names)!=1: raise ValueError(">1 var name")

    return list_names[0]



def plot_GWAS_AFresistance_heatmap_how_do_various_filtering_strategies_work_on_genes_consideringConsistencyBtwPvalues(spp_drug_to_gwas_df_file, ProcessedDataDir, PlotsDir, gene_features_df, DataDir, threads, species_to_gff, replace=False, min_npheno_transitions=5, figsize=(10, 10), only_no_pval_correction=False, correction_method_to_consider=None, max_ngenes_affected=10000, min_ngenes_affected=0, ASR_methods=None, max_fdr_threshold=0.2, plot_only_filters_yielding_all_expected_genes=False, set_min_epsilon=None, min_n_pvals=1, drugs_benchmarking={"MIF", "ANI", "CAS", "FLC"}, set_min_support=None, max_genes_to_plot_number_of_genes=10, set_alpha_pval=None, pval_fields_to_consider=None):

    """This plots , but only considering the """

    # define an outdir for this data
    outdir = "%s/processed_data_plot_GWAS_AFresistance_heatmap_how_do_various_filtering_strategies_consideringConsistencyBtwPvalues"%ProcessedDataDir
    make_folder(outdir)

    ######### GET DF #######

    # define the target pval fields
    all_pval_fields = ["%s_%s"%(pval_seed, pval_m) for pval_seed in ["pval_chi_square", "pval_GenoAndPheno"] for pval_m in  ["RelToBranchLen", "phenotypes"]] 

    # define the correction methods
    all_correction_methods = ["bonferroni", "fdr_bh", "fdr_by"]

    # define the interesting fields
    df_gwas_af_fields = ["species", "drug", "type_collapsing", "type_vars", "type_mutations", "type_genes", "ASR_methods_phenotypes", "min_support", "gwas_method", "group_name", "epsilon", "OR"]
    df_gwas_af_fields += ["%s_%s"%(pval_f, corr_method) for corr_method in all_correction_methods for pval_f in all_pval_fields]    
    df_gwas_af_fields += all_pval_fields

    # define the target drugs
    #target_drugs = {"FLC", "ANI", "MIF", "CAS"}
    #target_drugs = {"MIF"}
    target_drugs = {d for s,d in spp_drug_to_gwas_df_file.keys()}

    # get filtering df
    filtering_stats_df_file = "%s/df_trying_different_filter_combinations_gwas_af_resistance_stats_%s.py"%(outdir, "_".join(sorted(target_drugs)))
    if file_is_empty(filtering_stats_df_file) or replace is True:
        print("getting filtering_stats_df_file")

        # create the df_gwas_af with some target drugs and fields
        df_gwas_af_file = "%s/df_gwas_af_%s.py"%(outdir, "_".join(sorted(target_drugs)))
        if file_is_empty(df_gwas_af_file):

            all_tab_files = [tab for (spp, drug), tab in spp_drug_to_gwas_df_file.items() if drug in target_drugs]
            print("getting the gwas df for %i tab files"%(len(all_tab_files)))
            """
            with multiproc.Pool(1) as pool:

                df_gwas_af = pd.concat(pool.starmap(load_df_with_some_fields, [(t, df_gwas_af_fields) for t in all_tab_files]))
                pool.close()
                pool.terminate()
            """

            df_gwas_af = pd.concat(map(lambda x: load_df_with_some_fields(x[0], x[1]), [(t, df_gwas_af_fields) for t in all_tab_files]))

            save_object(df_gwas_af, df_gwas_af_file)
        print("loading df_gwas_af")
        df_gwas_af = load_object(df_gwas_af_file)

        # keep only some drugs to test
        if set(df_gwas_af.drug)!=target_drugs: raise ValueError("target drugs are not the same")

        # define the df that has mutations, adding the per-gene alteration
        print("getting df mutations")
        df_gwas_af_mutations = get_gwas_df_no_collapsing_with_collapsing_info(df_gwas_af, DataDir, threads, outdir, target_type_collapsing="genes", target_type_vars="all_vars", target_type_mutations="non_syn_muts", target_type_genes="all_genes").rename(columns={"target_group_name":"gene_name"})[df_gwas_af_fields + ["gene_name"]]

        # get the filtered df, with genes and domains
        df_gwas_af_genes_and_domains = df_gwas_af[(df_gwas_af.type_vars=="all_vars") & (df_gwas_af.type_mutations=="non_syn_muts")]

        df_gwas_af_genes = df_gwas_af_genes_and_domains[(df_gwas_af_genes_and_domains.type_collapsing=="genes") & (df_gwas_af_genes_and_domains.type_genes=="all_genes")]
        df_gwas_af_domains = df_gwas_af_genes_and_domains[(df_gwas_af_genes_and_domains.type_collapsing=="domains") & (df_gwas_af_genes_and_domains.type_genes=="only_protein_coding")]

        def get_gene_name(r):
            if r.type_collapsing=="genes": return r.group_name
            elif r.type_collapsing=="domains": return r.group_name.split("#")[1]
            else: raise ValueError("error in gene: %s"%r)

        df_gwas_af_genes["gene_name"] = df_gwas_af_genes[["type_collapsing", "group_name"]].apply(get_gene_name, axis=1)
        df_gwas_af_domains["gene_name"] = df_gwas_af_domains[["type_collapsing", "group_name"]].apply(get_gene_name, axis=1)

        # merge together the info from domains, genes and small variants, all centered around a gene
        df_gwas_af = pd.concat([d[df_gwas_af_fields + ["gene_name"]] for d in [df_gwas_af_genes, df_gwas_af_domains, df_gwas_af_mutations]]).reset_index(drop=True)

        # go through different combinations of filters and define a dataframe with them
        print("getting filters")
        filters_dict = {}; I = 0

        for ASR_methods_phenotypes in ['DOWNPASS', 'MPPA', 'MPPA,DOWNPASS']:
            #for min_n_sig_pvals in [1, 2, 3, 4]: # old way with n sig p vals

            for pval_chi_square_RelToBranchLen in [True, False]:
                for pval_chi_square_phenotypes in [True, False]:
                    for pval_GenoAndPheno_RelToBranchLen in [True, False]:
                        for pval_GenoAndPheno_phenotypes in [True, False]:

                            # require at least one p val field
                            if sum([pval_chi_square_RelToBranchLen, pval_chi_square_phenotypes, pval_GenoAndPheno_RelToBranchLen, pval_GenoAndPheno_phenotypes])==0: continue

                            for correction_method in (all_correction_methods + ["none"]):
                                for min_support in sorted(set(df_gwas_af.min_support)):

                                    array_fdr_tshds = [0.05, 0.1, 0.15, 0.2]
                                    for fdr_threshold in {"none":[0.05], "bonferroni":[0.05], "fdr_bh":array_fdr_tshds, "fdr_by":array_fdr_tshds}[correction_method]:
                                        for min_epsilon in [0.1, 0.2, 0.3]:

                                            for alpha_pval in {"none": [0.0001, 0.001, 0.01, 0.05], "bonferroni":[0.05], "fdr_bh":[0.05], "fdr_by":[0.05]}[correction_method]:

                                                # keep filters
                                                filters_dict[I] = {"ASR_methods_phenotypes":ASR_methods_phenotypes, "correction_method":correction_method, "min_epsilon":min_epsilon, "min_OR":1, "min_support":min_support, "fdr_threshold":fdr_threshold, "pval_chi_square_RelToBranchLen":pval_chi_square_RelToBranchLen, "pval_chi_square_phenotypes":pval_chi_square_phenotypes, "pval_GenoAndPheno_RelToBranchLen":pval_GenoAndPheno_RelToBranchLen, "pval_GenoAndPheno_phenotypes":pval_GenoAndPheno_phenotypes, "alpha_pval":alpha_pval, "filter_I":I}

                                                I+=1

        filters_df = pd.DataFrame(filters_dict).transpose()
        if len(filters_df)!=len(filters_df.drop_duplicates()): raise ValueError("df is not unique")

        # keep gene_features_df
        gene_features_df = gene_features_df[gene_features_df.species.isin({"Candida_albicans", "Candida_glabrata", "Candida_auris"})]


        # generate test dfs
        print("generating test dfs")
        inputs_fn = []
        test_df = pd.DataFrame()
        all_species_drug_method = sorted(set(df_gwas_af.species + "#" + df_gwas_af.drug + "#" + df_gwas_af.gwas_method))

        for species_drug_method in all_species_drug_method: 
            species, drug, gwas_method = species_drug_method.split("#")

            # get df and check that the tested unit is unique
            df_gwas_af_s = df_gwas_af[(df_gwas_af.species==species) & (df_gwas_af.drug==drug) & (df_gwas_af.gwas_method==gwas_method)]
            if len(df_gwas_af_s)==0: raise ValueError("df_gwas_af_s can' be 0")

            # keep only protein coding genes
            gff_df = load_gff3_intoDF(species_to_gff[species])
            gene_features_df_s = gene_features_df[gene_features_df.species==species]
            pseudogenes = set(gene_features_df_s[gene_features_df_s.feature_type.isin({"pseudogene", "pseudogene|Uncharacterized"})].gff_upmost_parent)
            protein_coding_genes = set(gff_df[gff_df.feature.isin({"CDS", "mRNA"})].upmost_parent).difference(pseudogenes)
            gene_features_df_s = gene_features_df_s[gene_features_df_s.gff_upmost_parent.isin(protein_coding_genes)]

            df_gwas_af_s = df_gwas_af_s[df_gwas_af_s.gene_name.isin(protein_coding_genes)]
            
            # keep
            inputs_fn.append((df_gwas_af_s, species, drug, gwas_method, cp.deepcopy(filters_df), cp.deepcopy(gene_features_df_s)))

        # run each filtering step in parallel
        print("getting the filtering stats")
        with multiproc.Pool(threads) as pool:

            filtering_stats_df = pd.concat(pool.starmap(get_filtering_stats_df_one_species_drug_gwas_method_consistency_btw_pvals, inputs_fn))
            pool.close()
            pool.terminate()

        # save
        print("saving")
        save_object(filtering_stats_df, filtering_stats_df_file)

    # load
    print("loading df")
    filtering_stats_df = load_object(filtering_stats_df_file)


    ########################

    ########## PLOT ###########
    print("plotting")

    # add spp and drug
    filtering_stats_df["spp_and_drug"] = filtering_stats_df.species + "-" + filtering_stats_df.drug
    all_spp_and_drug = sorted(set(filtering_stats_df.spp_and_drug))

    # only syncronous
    filtering_stats_df = filtering_stats_df[(filtering_stats_df.gwas_method=="synchronous")]

    # filter the fdr threshold and min_epsilon
    filtering_stats_df = filtering_stats_df[(filtering_stats_df.fdr_threshold<=max_fdr_threshold)]

    # filter the min_n_pvals
    filtering_stats_df = filtering_stats_df[(filtering_stats_df[["pval_chi_square_RelToBranchLen", "pval_chi_square_phenotypes", "pval_GenoAndPheno_RelToBranchLen", "pval_GenoAndPheno_phenotypes"]].apply(sum, axis=1)>=min_n_pvals)]
    
    
    # extra filters
    if only_no_pval_correction is True: filtering_stats_df = filtering_stats_df[filtering_stats_df.correction_method=="none"]
    if set_min_epsilon is not None: filtering_stats_df = filtering_stats_df[filtering_stats_df.min_epsilon==set_min_epsilon]
    if correction_method_to_consider is not None: filtering_stats_df = filtering_stats_df[filtering_stats_df.correction_method==correction_method_to_consider]
    if set_alpha_pval is not None: filtering_stats_df = filtering_stats_df[filtering_stats_df.alpha_pval==set_alpha_pval]
    
    if pval_fields_to_consider is not None: 

        pval_fields_to_not_consider = [f for f in all_pval_fields if f not in pval_fields_to_consider]
        filtering_stats_df = filtering_stats_df[((filtering_stats_df[pval_fields_to_consider]==True).apply(all, axis=1)) & ((filtering_stats_df[pval_fields_to_not_consider]==False).apply(all, axis=1))]

    if set_min_support is not None: 

        if type(set_min_support)==int: filtering_stats_df = filtering_stats_df[filtering_stats_df.min_support==set_min_support]
        elif type(set_min_support)==set: filtering_stats_df = filtering_stats_df[filtering_stats_df.min_support.isin(set_min_support)]
        else: raise ValueError("incorrect support")

    if ASR_methods is not None: 

        if type(ASR_methods)==str: filtering_stats_df = filtering_stats_df[filtering_stats_df.ASR_methods_phenotypes==ASR_methods]
        elif type(ASR_methods)==set: filtering_stats_df = filtering_stats_df[filtering_stats_df.ASR_methods_phenotypes.isin(ASR_methods)]
        else: raise ValueError("incorrect support")

    


    # add ad-hoc filters
    #filtering_stats_df = filtering_stats_df[ (filtering_stats_df.min_n_sig_pvals>=3) & (filtering_stats_df.fdr_threshold<=0.15) & (filtering_stats_df.min_support>=50)]

    # add row and col fileds for heatmap
    row_fields = ["gwas_method", "species", "drug"]
    col_fields = ["correction_method", "ASR_methods_phenotypes", "min_support", "fdr_threshold", "min_epsilon", "pval_chi_square_RelToBranchLen", "pval_chi_square_phenotypes", "pval_GenoAndPheno_RelToBranchLen", "pval_GenoAndPheno_phenotypes", "alpha_pval"]

    filtering_stats_df["rowID"] = filtering_stats_df[row_fields].applymap(str).agg("-".join, axis=1)
    filtering_stats_df["colID"] = filtering_stats_df[col_fields].applymap(str).agg("-".join, axis=1)

    # sort
    filtering_stats_df = filtering_stats_df.sort_values(by=col_fields, ascending=[True, True, True, False, True, True, True, True, True, False])

    # create a gwas df that has only some rows
    df_gwas_af_unique_file = "%s/df_gwas_af_unique_file_%s.py"%(outdir, "_".join(sorted(target_drugs)))
    if file_is_empty(df_gwas_af_unique_file):

        # loading df
        print("loading gwas df")
        df_gwas_af = load_object("%s/df_gwas_af_%s.py"%(outdir, "_".join(sorted(target_drugs))))
        
        # making unique df
        print("getting unique df and saving")
        df_gwas_af_unique = df_gwas_af[row_fields + ["ASR_methods_phenotypes", "nodes_withPheno", "min_support"]].drop_duplicates()
        df_gwas_af_unique["rowID"] = df_gwas_af_unique[row_fields].applymap(str).agg("-".join, axis=1)

        save_object(df_gwas_af_unique, df_gwas_af_unique_file)
    df_gwas_af_unique = load_object(df_gwas_af_unique_file)

    # define the number of phenotype nodes for each drug and sample
    asr_method_to_row_to_npheno = {m : dict(df_gwas_af_unique[(df_gwas_af_unique.ASR_methods_phenotypes==m) & (df_gwas_af_unique.min_support==0)].groupby("rowID").apply(lambda df_r: df_r.iloc[0].nodes_withPheno))  for m in ["MPPA", "DOWNPASS", "MPPA,DOWNPASS"]}
    for m, row_to_npheno in asr_method_to_row_to_npheno.items():
        for row in set(df_gwas_af_unique.rowID).difference(set(row_to_npheno)): asr_method_to_row_to_npheno[m][row] = 0 

    # discard data where you have <5 transitions
    def get_spp_drug(x): return (x.split("-")[1] , x.split("-")[2])
    valid_spp_drug = {get_spp_drug(row) for row, npheno in asr_method_to_row_to_npheno["MPPA,DOWNPASS"].items() if row.startswith("synchronous") and npheno>=min_npheno_transitions }
    valid_rows = {r for r in set(df_gwas_af_unique.rowID) if get_spp_drug(r) in valid_spp_drug}
    filtering_stats_df = filtering_stats_df[filtering_stats_df.rowID.isin(valid_rows)]


    # print, for each species-drugg, the number of genes and expected genes:
    for spp_and_drug in all_spp_and_drug:
        if tuple(spp_and_drug.split("-")) not in valid_spp_drug: continue
        df = filtering_stats_df[filtering_stats_df.spp_and_drug==spp_and_drug]
        if len(df)==0: 
            print("%s have 0 filters"%spp_and_drug)
            continue

        print("%s. filters yield %i-%i genes"%(spp_and_drug, min(df.n_genes_found), max(df.n_genes_found)))
        


    # checks
    if len(filtering_stats_df)!=(len(set(filtering_stats_df.rowID + filtering_stats_df.colID))): raise ValueError("combinations of row and col are not unique")

    if len(filtering_stats_df)==0: 
        print("WARNING: empty df")
        return None, None

    # row colors df
    drug_to_color= {"FLC":"c", "ITR":"blue", "POS":"aquamarine", "VRC":"cyan", "AMB":"gray", "ANI":"magenta", "CAS":"red", "MIF":"salmon", "AMB":"gray"}
    method_to_color = {"synchronous":"black", "phyC":"gray"}
    row_colors_df_all = pd.DataFrame({row : {name : color_dict[row.split("-")[I]] for I, (name, color_dict) in enumerate([("gwas_method", method_to_color), ("species", species_to_color), ("drug", drug_to_color)])} for row in sorted(set(filtering_stats_df.rowID))}).transpose()

    # col colors df
    corr_method_to_color = {"bonferroni":"black", "fdr_bh":"blue", "fdr_by":"cyan", "none":"white"}
    ASR_methods_phenotypes_to_color = {"DOWNPASS":"red", "MPPA":"blue", 'MPPA,DOWNPASS':"purple"}
    #min_n_sig_pvals_to_color = {str(x):c for x,c in get_value_to_color(sorted(set(filtering_stats_df.min_n_sig_pvals)), n=len(set(filtering_stats_df.min_n_sig_pvals)), type_color="hex", palette="rocket_r")[0].items()}
    fdr_threshold_to_color = {"0.05": "white", "0.1":"gray", "0.15":"red", "0.2":"black"}
    min_support_to_color = {"0":"black", "50":"salmon", "70":"red"}
    min_epsilon_to_color = {str(x):c for x,c in get_value_to_color(sorted(set(filtering_stats_df.min_epsilon)), n=len(set(filtering_stats_df.min_epsilon)), type_color="hex", palette="tab10")[0].items()}
    bool_to_color = {"True":"black", "False":"white"}
    alpha_pval_to_color = {"0.0001":"red", "0.001":"black", "0.01":"gray", "0.05":"white"}

    list_tuples_f_and_color_dict_cols = [('correction_method', corr_method_to_color), ('ASR_methods_phenotypes', ASR_methods_phenotypes_to_color), ("min_support", min_support_to_color), ("fdr_threshold", fdr_threshold_to_color), ("min_epsilon", min_epsilon_to_color), ("pval_chi_square_RelToBranchLen", bool_to_color), ("pval_chi_square_phenotypes", bool_to_color), ("pval_GenoAndPheno_RelToBranchLen", bool_to_color), ("pval_GenoAndPheno_phenotypes", bool_to_color), ("alpha_pval", alpha_pval_to_color)]
    col_colors_df_all = pd.DataFrame({col : {name : color_dict[col.split("-")[I]] for I, (name, color_dict) in enumerate(list_tuples_f_and_color_dict_cols)} for col in sorted(set(filtering_stats_df.colID))}).transpose()

    # define plots dir
    plots_dir = "%s/heatmaps_performance_gwas_filters_on_genes_consistency_btw_pvals"%PlotsDir
    #delete_folder(plots_dir)
    make_folder(plots_dir)

    # make square df
    square_df = filtering_stats_df.pivot(values="n_genes_found", columns="colID", index="rowID").sort_index(); check_no_nans_in_df(square_df)


    # keep only cols where the FLC and echinocaninds have some expected genes, except {('Candida_auris', 'ANI'), ('Candida_glabrata', 'MIF')}, as these have different types of phenotypes together
    print("filtering cols")
    #expected_species_drug_test_expected_genes = {(s,d) for s,d in valid_spp_drug if d in {"MIF", "ANI", "CAS", "FLC", "ITR", "POS", "VRC"}}.difference({('Candida_auris', 'ANI'), ('Candida_glabrata', 'MIF')})    
    expected_species_drug_test_expected_genes = {(s,d) for s,d in valid_spp_drug if d in drugs_benchmarking}.difference({('Candida_auris', 'ANI'), ('Candida_glabrata', 'MIF')})    

    row_col_to_n_genes_expected_found = filtering_stats_df[filtering_stats_df.rowID.apply(get_spp_drug).isin(expected_species_drug_test_expected_genes)].set_index(["rowID", "colID"]).n_genes_expected_found
    rowIDs_expected_drugs = set(row_col_to_n_genes_expected_found.index.get_level_values(0))
    
    def get_col_is_valid(c):
        if all((c>=min_ngenes_affected) & (c<=max_ngenes_affected)):
            
            indices = {(rID, c.name) for rID in rowIDs_expected_drugs}
            if {True:all, False:any}[plot_only_filters_yielding_all_expected_genes]((row_col_to_n_genes_expected_found[indices])>0) or len(indices)==0: return True 
            else: return False

        else: return False

    col_to_is_valid = square_df.apply(get_col_is_valid, axis=0)
    valid_cols = col_to_is_valid[col_to_is_valid].index

    # check
    if len(valid_cols)==0:
        print("WARNING: 0 cols")
        return None, None

    # define the sorted rows
    sorted_rows = sorted(rowIDs_expected_drugs) + sorted(set(square_df.index).difference(rowIDs_expected_drugs))

    # keep only these valid cols
    square_df = square_df.loc[sorted_rows, valid_cols]

    # define the annot df
    filtering_stats_df["fraction_n_genes_expected_found"] = filtering_stats_df.n_genes_expected_found / filtering_stats_df.n_genes_expected
    def get_fraction_n_genes_expected_found_str(r):
        if len(square_df.columns)<=max_genes_to_plot_number_of_genes: 

            if pd.isna(r.fraction_n_genes_expected_found): return str(r.n_genes_found)
            else: return "%s\n%s"%({0:"", 0.5:"~", 1:"*"}[r.fraction_n_genes_expected_found], r.n_genes_found)

        else: 
            if pd.isna(r.fraction_n_genes_expected_found): return ""
            else: return {0:"", 0.5:"~", 1:"*"}[r.fraction_n_genes_expected_found]

    filtering_stats_df["fraction_n_genes_expected_found_str"] = filtering_stats_df.apply(get_fraction_n_genes_expected_found_str, axis=1)
    annot_df = filtering_stats_df.pivot(values="fraction_n_genes_expected_found_str", columns="colID", index="rowID").loc[square_df.index, square_df.columns]

    # set 0s to nans
    def set_0_to_nan(x):
        if x==0: return np.nan
        else: return x

    square_df = square_df.applymap(set_0_to_nan)

    # make plot
    cmap = "rocket_r"
    cm = sns.clustermap(square_df, col_cluster=False, row_cluster=False, cmap=cmap,  col_colors=col_colors_df_all, row_colors=row_colors_df_all, cbar_kws={"label":"# sig. genes"}, annot=annot_df, annot_kws={"size": 12}, fmt="", figsize=figsize) # , xticklabels=1

    # ticks":[0, 0.5, 1]

    # set the ticklabels of the heatmap
    def get_yticklabel(row):
        method, species, drug = row.split("-")
        return "%s-C. %s-%s (%s phenotypes)"%(method, species.split("_")[1], drug, ",".join([str(asr_method_to_row_to_npheno[m][row]) for m in ["MPPA", "DOWNPASS", "MPPA,DOWNPASS"]]))

    cm.ax_heatmap.set_yticklabels([get_yticklabel(row) for row in square_df.index])
    cm.ax_heatmap.set_ylabel("<gwas method>-<species>-<drug>")

    cm.ax_heatmap.set_xlabel("filtering strategy")
    cm.ax_heatmap.set_xticklabels([])
    cm.ax_heatmap.set_xticks([])

    # add a legend for the row colors
    def get_lel(facecolor, label, edgecolor="gray"): return mpatches.Patch(facecolor=facecolor, edgecolor=edgecolor, label=label)
    legend_elements = make_flat_listOflists([([get_lel("white", f, edgecolor="white")] + [get_lel(color,label) for label,color in color_dict.items()]) for f, color_dict in [("gwas_method", method_to_color), ("species", species_to_color), ("drug", drug_to_color)]])
    cm.ax_row_colors.legend(handles=legend_elements, loc="upper right", bbox_to_anchor=(0, 1))

    # add a legend for the col colors
    legend_elements = make_flat_listOflists([([get_lel("white", f, edgecolor="white")] + [get_lel(color,label) for label,color in color_dict.items()]) for f, color_dict in list_tuples_f_and_color_dict_cols])
    
    cm.ax_cbar.legend(handles=legend_elements, loc="lower right", bbox_to_anchor=(0, 0))

    # set title
    title_str = "%i filter combinations (OR>=1)\nmin_pheno_transitions=%i, only_no_pval_correction=%s\n\nannotation:\n'*': 1/1 or 2/2 sig. genes\n'~':1/2 sig. genes"%(len(square_df.columns), min_npheno_transitions, only_no_pval_correction)
    cm.ax_col_colors.set_title(title_str)

    # show
    plt.show()

    # save
    filename = "%s/filter_effects_only_no_pval_correction=%s_correction_method_to_consider=%s.pdf"%(plots_dir, only_no_pval_correction, correction_method_to_consider)
    print("saving %s"%filename)
    cm.savefig(filename, bbox_inches="tight")

    return filtering_stats_df_file, valid_spp_drug

    ###########################

def plot_GWAS_AFresistance_heatmap_how_do_various_filtering_strategies_work_on_genes_small_vars(spp_drug_to_gwas_df_file, ProcessedDataDir, PlotsDir, gene_features_df, DataDir, threads, species_to_gff, replace=False, plots={"heatmap_all_filters_fractionKnownGenes"}, keep_only_filters_yielding_expected_genes=False, min_npheno_transitions=5, keep_only_pval_correction=False, min_min_epsilon=0, show_n_sig_genes=False, considering_only_all_GenoAndPheno_pvals=False, considering_only_all_phenotypes_pvals=False, figsize=(10, 10), min_fraction_keep_only_filters_yielding_expected_genes=0.5, considering_only_all_pvals=False):


    """Plots how different filtering strategies work on the AF data."""

    # define an outdir for this data
    outdir = "%s/processed_data_plot_GWAS_AFresistance_heatmap_how_do_various_filtering_strategies_work_on_genes_small_vars"%ProcessedDataDir; make_folder(outdir)

    ######### GET DF #######

    # define fields
    # define the target pval fields
    all_pval_fields = ["%s_%s"%(pval_seed, pval_m) for pval_seed in ["pval_chi_square", "pval_GenoAndPheno"] for pval_m in  ["RelToBranchLen", "phenotypes"]] # skip 'RelToBranchLen_wReplace', 'pval_OR', 'pval_epsilon'
    # ['pval_chi_square_RelToBranchLen', 'pval_chi_square_phenotypes', 'pval_GenoAndPheno_RelToBranchLen', 'pval_GenoAndPheno_phenotypes']

    # define the correction methods
    #all_correction_methods = ["bonferroni", "hommel", "fdr_bh", "fdr_by", "fdr_tsbh", "fdr_tsbky"] # old way
    all_correction_methods = ["bonferroni", "fdr_bh", "fdr_by"]

    # define the interesting fields
    df_gwas_af_fields = ["species", "drug", "type_collapsing", "type_vars", "type_mutations", "type_genes", "ASR_methods_phenotypes", "gwas_method", "group_name", "epsilon", "OR", "nodes_withPheno"]
    df_gwas_af_fields += ["%s_%s"%(pval_f, corr_method) for corr_method in all_correction_methods for pval_f in all_pval_fields]
    df_gwas_af_fields += all_pval_fields

    # define the target drugs
    target_drugs = {"FLC", "ANI", "MIF", "CAS"}
    #target_drugs = {"MIF"}

    # dfeine file
    filtering_stats_df_file = "%s/df_trying_different_filter_combinations_gwas_af_resistance_stats_%s.py"%(outdir, "_".join(sorted(target_drugs)))
    if file_is_empty(filtering_stats_df_file) or replace is True:

        # create the df_gwas_af with some target drugs and fields
        df_gwas_af_file = "%s/df_gwas_af_%s.py"%(outdir, "_".join(sorted(target_drugs)))
        if file_is_empty(df_gwas_af_file):

            all_tab_files = [tab for (spp, drug), tab in spp_drug_to_gwas_df_file.items() if drug in target_drugs]
            print("getting the gwas df for %i tab files"%(len(all_tab_files)))
            with multiproc.Pool(threads) as pool:

                df_gwas_af = pd.concat(pool.starmap(load_df_with_some_fields, [(t, df_gwas_af_fields) for t in all_tab_files]))
                pool.close()
                pool.terminate()

            save_object(df_gwas_af, df_gwas_af_file)
        print("loading df_gwas_af")
        df_gwas_af = load_object(df_gwas_af_file)


        # keep only some drugs to test
        if set(df_gwas_af.drug)!=target_drugs: raise ValueError("target drugs are not the same")

        # define the df that has mutations, adding the per-gene alteration
        print("getting df mutations")
        df_gwas_af_mutations = get_gwas_df_no_collapsing_with_collapsing_info(df_gwas_af, DataDir, threads, outdir, target_type_collapsing="genes", target_type_vars="all_vars", target_type_mutations="non_syn_muts", target_type_genes="all_genes").rename(columns={"target_group_name":"gene_name"})[df_gwas_af_fields + ["gene_name"]]

        # get the filtered df, with genes and domains
        df_gwas_af_genes_and_domains = df_gwas_af[(df_gwas_af.type_vars=="all_vars") & (df_gwas_af.type_mutations=="non_syn_muts")]

        df_gwas_af_genes = df_gwas_af_genes_and_domains[(df_gwas_af_genes_and_domains.type_collapsing=="genes") & (df_gwas_af_genes_and_domains.type_genes=="all_genes")]
        df_gwas_af_domains = df_gwas_af_genes_and_domains[(df_gwas_af_genes_and_domains.type_collapsing=="domains") & (df_gwas_af_genes_and_domains.type_genes=="only_protein_coding")]

        def get_gene_name(r):
            if r.type_collapsing=="genes": return r.group_name
            elif r.type_collapsing=="domains": return r.group_name.split("#")[1]
            else: raise ValueError("error in gene: %s"%r)

        df_gwas_af_genes["gene_name"] = df_gwas_af_genes[["type_collapsing", "group_name"]].apply(get_gene_name, axis=1)
        df_gwas_af_domains["gene_name"] = df_gwas_af_domains[["type_collapsing", "group_name"]].apply(get_gene_name, axis=1)

        # merge together the info from domains, genes and small variants, all centered around a gene
        df_gwas_af = pd.concat([d[df_gwas_af_fields + ["gene_name"]] for d in [df_gwas_af_genes, df_gwas_af_domains, df_gwas_af_mutations]]).reset_index(drop=True)

        # go through different combinations of filters and define a dataframe with them
        print("getting filters")
        filters_dict = {}; I = 0

        for ASR_methods_phenotypes in ['DOWNPASS', 'MPPA', 'MPPA,DOWNPASS']:

            for min_epsilon in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:
                for min_OR in [1, np.inf]:

                    for pval_chi_square_RelToBranchLen in [True, False]:
                        for pval_chi_square_phenotypes in [True, False]:
                            for pval_GenoAndPheno_RelToBranchLen in [True, False]:
                                for pval_GenoAndPheno_phenotypes in [True, False]:

                                    for correction_method in (all_correction_methods + ["none"]):

                                        # check that some pvals are true
                                        if not any([pval_chi_square_RelToBranchLen, pval_chi_square_phenotypes, pval_GenoAndPheno_RelToBranchLen, pval_GenoAndPheno_phenotypes]): continue

                                        # keep filters
                                        filters_dict[I] = {"ASR_methods_phenotypes":ASR_methods_phenotypes, "min_epsilon":min_epsilon, "min_OR":min_OR, "pval_chi_square_RelToBranchLen":pval_chi_square_RelToBranchLen, "pval_chi_square_phenotypes":pval_chi_square_phenotypes, "pval_GenoAndPheno_RelToBranchLen":pval_GenoAndPheno_RelToBranchLen, "pval_GenoAndPheno_phenotypes":pval_GenoAndPheno_phenotypes, "correction_method":correction_method, "filter_I":I}

                                        I+=1

        filters_df = pd.DataFrame(filters_dict).transpose()
        if len(filters_df)!=len(filters_df.drop_duplicates()): raise ValueError("df is not unique")

        # keep gene_features_df
        gene_features_df = gene_features_df[gene_features_df.species.isin({"Candida_albicans", "Candida_glabrata", "Candida_auris"})]

        # generate test dfs
        print("generating test dfs")
        inputs_fn = []
        test_df = pd.DataFrame()
        all_species_drug_method = sorted(set(df_gwas_af.species + "#" + df_gwas_af.drug + "#" + df_gwas_af.gwas_method))

        for species_drug_method in all_species_drug_method: 
            species, drug, gwas_method = species_drug_method.split("#")

            # get df and check that the tested unit is unique
            df_gwas_af_s = df_gwas_af[(df_gwas_af.species==species) & (df_gwas_af.drug==drug) & (df_gwas_af.gwas_method==gwas_method)]
            if len(df_gwas_af_s)==0: raise ValueError("df_gwas_af_s can' be 0")

            # keep only protein coding genes
            gff_df = load_gff3_intoDF(species_to_gff[species])
            gene_features_df_s = gene_features_df[gene_features_df.species==species]
            pseudogenes = set(gene_features_df_s[gene_features_df_s.feature_type.isin({"pseudogene", "pseudogene|Uncharacterized"})].gff_upmost_parent)
            protein_coding_genes = set(gff_df[gff_df.feature.isin({"CDS", "mRNA"})].upmost_parent).difference(pseudogenes)
            gene_features_df_s = gene_features_df_s[gene_features_df_s.gff_upmost_parent.isin(protein_coding_genes)]

            df_gwas_af_s = df_gwas_af_s[df_gwas_af_s.gene_name.isin(protein_coding_genes)]
            
            # keep
            inputs_fn.append((df_gwas_af_s, species, drug, gwas_method, cp.deepcopy(filters_df), cp.deepcopy(gene_features_df_s)))

        # run each filtering step in parallel
        print("getting the filtering stats")
        with multiproc.Pool(threads) as pool:

            filtering_stats_df = pd.concat(pool.starmap(get_filtering_stats_df_one_species_drug_gwas_method, inputs_fn))
            pool.close()
            pool.terminate()

        # save
        print("saving")
        save_object(filtering_stats_df, filtering_stats_df_file)

    # load
    print("loading df")
    filtering_stats_df = load_object(filtering_stats_df_file)

    ########################

    ########### GENERAL PLOTTING ##############

    """
    # C. auris FLC. Which genes are there?
    df = filtering_stats_df[(filtering_stats_df.species=="Candida_auris") & (filtering_stats_df.drug=="FLC") & (filtering_stats_df.n_genes_expected_found>0)]
    print(set.union(*df.genes_expected_found))

    # C. glabrata 
    df = filtering_stats_df[(filtering_stats_df.species=="Candida_glabrata") & (filtering_stats_df.drug=="FLC") & (filtering_stats_df.n_genes_expected_found>0) & (filtering_stats_df.gwas_method=="phyC")].reset_index(drop=True)

    # pval_chi_square_RelToBranchLen and pval_GenoAndPheno_RelToBranchLen all is False
    print(df[["ASR_methods_phenotypes", "min_epsilon", "min_OR", "pval_chi_square_phenotypes", "pval_GenoAndPheno_phenotypes", "correction_method"]])

    outdir_drug = "%s/Candida_glabrata_%i/ancestral_GWAS_drugResistance/GWAS_FLC_resistance"%(DataDir, sciName_to_taxID["Candida_glabrata"])
    tree = Tree("%s/rooted_tree.nw"%outdir_drug)
    resistance_df = get_tab_as_df_or_empty_df("%s/resistance_df.tab"%(outdir_drug))
    resistance_df["sampleID"] = resistance_df.sampleID.apply(str)
    samples = set(resistance_df["sampleID"])
    sample_to_resistance = dict(resistance_df.set_index("sampleID").phenotype)
    outdir_plot = "%s/Cglabrata_FLC_tree_resistance_dir"%PlotsDir; make_folder(outdir_plot)

    plot_tree_AF_resistance_samples_for_GWAS(tree, samples, sample_to_resistance, outdir_plot)
    """


    ###########################################

    ############ PLOTS ###############
    print("plotting")

    # discard the min_OR
    #filtering_stats_df = filtering_stats_df[filtering_stats_df.min_OR==1]

    # discard the none's
    if keep_only_pval_correction is True: filtering_stats_df = filtering_stats_df[filtering_stats_df.correction_method!="none"]

    # keep only the considering_only_all_GenoAndPheno_pvals
    if considering_only_all_GenoAndPheno_pvals is True: filtering_stats_df = filtering_stats_df[(filtering_stats_df.pval_chi_square_RelToBranchLen==False) & (filtering_stats_df.pval_chi_square_phenotypes==False) & (filtering_stats_df.pval_GenoAndPheno_RelToBranchLen) & (filtering_stats_df.pval_GenoAndPheno_phenotypes)]

    # keep only the considering_only_all_phenotypes_pvals
    if considering_only_all_phenotypes_pvals is True: filtering_stats_df = filtering_stats_df[(filtering_stats_df.pval_chi_square_RelToBranchLen==False) & (filtering_stats_df.pval_chi_square_phenotypes) & (filtering_stats_df.pval_GenoAndPheno_RelToBranchLen==False) & (filtering_stats_df.pval_GenoAndPheno_phenotypes)]

    if considering_only_all_pvals is True: filtering_stats_df = filtering_stats_df[(filtering_stats_df.pval_chi_square_RelToBranchLen) & (filtering_stats_df.pval_chi_square_phenotypes) & (filtering_stats_df.pval_GenoAndPheno_RelToBranchLen) & (filtering_stats_df.pval_GenoAndPheno_phenotypes)]


    if sum([considering_only_all_GenoAndPheno_pvals, considering_only_all_phenotypes_pvals, considering_only_all_pvals])>1: raise ValueError("there should be only one restriction")



    # map epsilon to color
    e_to_color = {str(x):c for x,c in get_value_to_color(sorted(set(filtering_stats_df.min_epsilon)), n=len(set(filtering_stats_df.min_epsilon)), type_color="hex", palette="tab10")[0].items()}


    # filter by min_min_epsilon
    filtering_stats_df = filtering_stats_df[filtering_stats_df.min_epsilon>=min_min_epsilon]


    # add row and col fileds for heatmap
    row_fields = ["gwas_method", "species", "drug"]
    col_fields = ["ASR_methods_phenotypes", "min_epsilon", "min_OR", "pval_chi_square_RelToBranchLen", "pval_chi_square_phenotypes", "pval_GenoAndPheno_RelToBranchLen", "pval_GenoAndPheno_phenotypes", "correction_method"]

    filtering_stats_df["rowID"] = filtering_stats_df[row_fields].applymap(str).agg("-".join, axis=1)
    filtering_stats_df["colID"] = filtering_stats_df[col_fields].applymap(str).agg("-".join, axis=1)
    filtering_stats_df["fraction_n_genes_expected_found"] = filtering_stats_df.n_genes_expected_found / filtering_stats_df.n_genes_expected

    # create a gwas df that has only some rows
    df_gwas_af_unique_file = "%s/df_gwas_af_unique_file_%s.py"%(outdir, "_".join(sorted(target_drugs)))
    if file_is_empty(df_gwas_af_unique_file):

        # loading df
        print("loading gwas df")
        df_gwas_af = load_object("%s/df_gwas_af_%s.py"%(outdir, "_".join(sorted(target_drugs))))
        
        # making unique df
        print("getting unique df and saving")
        df_gwas_af_unique = df_gwas_af[row_fields + ["ASR_methods_phenotypes", "nodes_withPheno"]].drop_duplicates()
        df_gwas_af_unique["rowID"] = df_gwas_af_unique[row_fields].applymap(str).agg("-".join, axis=1)

        save_object(df_gwas_af_unique, df_gwas_af_unique_file)
    df_gwas_af_unique = load_object(df_gwas_af_unique_file)

    # define the number of phenotype nodes for each drug and sample
    asr_method_to_row_to_npheno = {m : dict(df_gwas_af_unique[df_gwas_af_unique.ASR_methods_phenotypes==m].groupby("rowID").apply(lambda df_r: df_r.iloc[0].nodes_withPheno))  for m in ["MPPA", "DOWNPASS", "MPPA,DOWNPASS"]}
    for m, row_to_npheno in asr_method_to_row_to_npheno.items():
        for row in set(df_gwas_af_unique.rowID).difference(set(row_to_npheno)): asr_method_to_row_to_npheno[m][row] = 0 

    # discard data where you have <5 transitions. Also discard ('Candida_auris', 'ANI') because the data is weird!!
    def get_spp_drug(x): return (x.split("-")[1] , x.split("-")[2])
    valid_spp_drug = {get_spp_drug(row) for row, npheno in asr_method_to_row_to_npheno["MPPA,DOWNPASS"].items() if row.startswith("synchronous") and npheno>=min_npheno_transitions }.difference({('Candida_auris', 'ANI')})
    valid_rows = {r for r in set(df_gwas_af_unique.rowID) if get_spp_drug(r) in valid_spp_drug}
    filtering_stats_df = filtering_stats_df[filtering_stats_df.rowID.isin(valid_rows)]

    # checks
    if len(filtering_stats_df)!=(len(set(filtering_stats_df.rowID + filtering_stats_df.colID))): raise ValueError("combinations of row and col are not unique")

    # row colors df
    drug_to_color= {"FLC":"c", "ANI":"magenta", "CAS":"red", "MIF":"salmon"}
    method_to_color = {"synchronous":"black", "phyC":"gray"}
    row_colors_df_all = pd.DataFrame({row : {name : color_dict[row.split("-")[I]] for I, (name, color_dict) in enumerate([("gwas_method", method_to_color), ("species", species_to_color), ("drug", drug_to_color)])} for row in sorted(set(filtering_stats_df.rowID))}).transpose()

    # col colors df
    bool_to_color = {"True": "black", "False": "white"}
    corr_method_to_color = {"bonferroni":"black", "fdr_bh":"blue", "fdr_by":"cyan", "none":"white"}
    ASR_methods_phenotypes_to_color = {"DOWNPASS":"red", "MPPA":"blue", 'MPPA,DOWNPASS':"purple"}
    OR_to_color =  {"1":"white", "inf":"black"}
     
    col_colors_df_all = pd.DataFrame({col : {name : color_dict[col.split("-")[I]] for I, (name, color_dict) in enumerate([('ASR_methods_phenotypes', ASR_methods_phenotypes_to_color), ('min_epsilon', e_to_color), ('min_OR', OR_to_color), ('pval_chi_square_RelToBranchLen', bool_to_color), ('pval_chi_square_phenotypes', bool_to_color), ('pval_GenoAndPheno_RelToBranchLen', bool_to_color), ('pval_GenoAndPheno_phenotypes', bool_to_color), ('correction_method', corr_method_to_color)])} for col in sorted(set(filtering_stats_df.colID))}).transpose()

    # define plots
    plots_dir = "%s/heatmaps_performance_gwas_filters_on_genes"%PlotsDir;
    make_folder(plots_dir)

    # make heatmap with the fraction of known genes all filters
    if "heatmap_all_filters_fractionKnownGenes" in plots:
        print("heatmap_all_filters_fractionKnownGenes...")

        # make square df
        square_df = filtering_stats_df.pivot(values="fraction_n_genes_expected_found", columns="colID", index="rowID").sort_index(); check_no_nans_in_df(square_df)

        # filter df to keep only relevant filers (those that yield some of the important genes in some conditions)
        if keep_only_filters_yielding_expected_genes is True:
            all_expected_sppDrugs = set(map(get_spp_drug, square_df.index))
            filter_to_SppDrugCorrect = (square_df>=min_fraction_keep_only_filters_yielding_expected_genes).apply(lambda c: set(map(get_spp_drug, c[c].index)), axis=0)
            useful_filters = list(filter_to_SppDrugCorrect[filter_to_SppDrugCorrect==all_expected_sppDrugs].index)
            square_df = square_df[useful_filters]

        if len(square_df)==0 or len(square_df.columns)==0: raise ValueError("empty df")

        # define the annot_df
        if show_n_sig_genes:
            annot_df = filtering_stats_df.pivot(values="n_genes_found", columns="colID", index="rowID").loc[square_df.index, square_df.columns]
            check_no_nans_in_df(annot_df)

        else: annot_df = None

        # get clustermap
        #cmap = sns.color_palette("Greys", 3)

        cmap = "Greys"
        cm = sns.clustermap(square_df, col_cluster=True, row_cluster=False, cmap=cmap,  col_colors=col_colors_df_all, row_colors=row_colors_df_all, cbar_kws={"label":"fraction genes expected found", "ticks":[0, 0.5, 1]}, annot=annot_df, annot_kws={"size": 12}, fmt="", figsize=figsize) # , xticklabels=1

        # set the labels
        cm.ax_cbar.set_yticklabels(["0", "1/2", "1/1\nor\n2/2"])


        # set the yticklabels of the heatmap
        def get_yticklabel(row):
            method, species, drug = row.split("-")
            return "%s-C. %s-%s (%s phenotypes)"%(method, species.split("_")[1], drug, ",".join([str(asr_method_to_row_to_npheno[m][row]) for m in ["MPPA", "DOWNPASS", "MPPA,DOWNPASS"]]))

        cm.ax_heatmap.set_yticklabels([get_yticklabel(row) for row in square_df.index])
        cm.ax_heatmap.set_ylabel("<gwas method>-<species>-<drug>")
        cm.ax_heatmap.set_xlabel("filtering strategy")
        
        # set the yticklabels
        cm.ax_heatmap.set_xticklabels([])
        cm.ax_heatmap.set_xticks([])
        #plt.setp(cm.ax_heatmap.xaxis.get_majorticklabels(), rotation=90, size=7)


        # add a legend for the row colors
        def get_lel(facecolor, label, edgecolor="gray"): return mpatches.Patch(facecolor=facecolor, edgecolor=edgecolor, label=label)
        legend_elements = make_flat_listOflists([([get_lel("white", f, edgecolor="white")] + [get_lel(color,label) for label,color in color_dict.items()]) for f, color_dict in [("gwas_method", method_to_color), ("species", species_to_color), ("drug", drug_to_color)]])
        cm.ax_row_colors.legend(handles=legend_elements, loc="upper right", bbox_to_anchor=(0, 1))

        # add a legend for the col colors
        legend_elements = make_flat_listOflists([([get_lel("white", f, edgecolor="white")] + [get_lel(color,label) for label,color in color_dict.items()]) for f, color_dict in [('ASR_methods_phenotypes', ASR_methods_phenotypes_to_color), ('min_epsilon', e_to_color), ('min_OR', OR_to_color), ('pvals', bool_to_color), ('correction_method', corr_method_to_color)]])
        
        #hm_pos = cm.ax_heatmap.get_position()
        #print(hm_pos)
        #cm.ax_heatmap.legend(handles=legend_elements, loc="upper left", bbox_to_anchor=(hm_pos.x1+1.5, 1))
        cm.ax_cbar.legend(handles=legend_elements, loc="lower right", bbox_to_anchor=(0, 0))

        # set title
        title_str = "%i filter combinations\nmin_pheno_transitions=%i, only_pval_correction=%s, min_min_epsilon=%s"%(len(square_df.columns), min_npheno_transitions, keep_only_pval_correction, min_min_epsilon)
        if keep_only_filters_yielding_expected_genes is True: title_str += "\nfilters that yield at least %s significant genes in all spp-drugs"%min_fraction_keep_only_filters_yielding_expected_genes
        if show_n_sig_genes is True: title_str += "\nthe number is the # significant genes"
        if considering_only_all_GenoAndPheno_pvals is True: title_str += "\nonly considering all GenoAndPheno pvals"
        if considering_only_all_phenotypes_pvals is True: title_str += "\nonly considering all phenotypes pvals"
        if considering_only_all_pvals is True: title_str += "\nonly considering all pvals"

        cm.ax_col_dendrogram.set_title(title_str)

        # show
        plt.show()

        # save
        filename = "%s/filters_effect_onlyGoodFilters=%s_only_pval_correction=%s_min_min_e=%s_show_genes=%s_considering_only_all_GenoAndPheno=%s_min_fraction_keep_only_filters_yielding_expected_genes=%s_considering_only_all_phenotypes=%s_allps=%s.pdf"%(plots_dir, keep_only_filters_yielding_expected_genes, keep_only_pval_correction, min_min_epsilon, show_n_sig_genes, considering_only_all_GenoAndPheno_pvals, min_fraction_keep_only_filters_yielding_expected_genes, considering_only_all_phenotypes_pvals, considering_only_all_pvals)
        print("saving %s"%filename)
        cm.savefig(filename, bbox_inches="tight")



    ##########################################



    """
    filters_series = pd.Series(dict(zip(col_fields, square_df.columns[0].split("-"))))
    filters_series["min_epsilon"] = float(filters_series["min_epsilon"])
    filters_series["min_OR"] = float(filters_series["min_OR"])


    df_gwas_af_mutations = get_gwas_df_no_collapsing_with_collapsing_info(df_gwas_af, DataDir, threads, outdir, target_type_collapsing="genes", target_type_vars="all_vars", target_type_mutations="non_syn_muts", target_type_genes="all_genes").rename(columns={"target_group_name":"gene_name"})


    gwas_df_filt = get_filtered_gwas_af_df(filters_series, df_gwas_af_mutations[(df_gwas_af_mutations.species=="Candida_glabrata") & (df_gwas_af_mutations.drug=="CAS") & (df_gwas_af_mutations.gwas_method=="synchronous")])

    """
    
    """

    # filter out datasets where there is at least 
    rowID_to_bool = ((square_df>0).apply(any, axis=0))
    square_df = square_df[set(rowID_to_bool[rowID_to_bool].index)]

    print("There are %i filters"%len(square_df.columns))


    """


def get_df_gwas_af_with_fraction_filters_supporting_association_considering_ASR_methods_4pvals_and_no_correction_one_gwasID(gwasID, df_all):

    """Runs the function get_df_gwas_af_with_fraction_filters_supporting_association_considering_ASR_methods_4pvals_and_no_correction_one_gwasID for one unqique ID, returning a df where each row is one group name"""

    print("running function on %s"%gwasID)

    # check that there is only combinations of ASR_methods and group name

    if len(df_all)!=len(df_all[["ASR_methods_phenotypes", "group_name"]].drop_duplicates()): 
        print(df_all[["ASR_methods_phenotypes", "group_name"]])
        raise ValueError("the combination of ASR_methods and gname should be unique")

    # set the group name as index
    df_all = df_all.set_index("group_name", drop=False)

    # add whether it passes the filters for different pvals
    min_epsilon = 0.1
    min_OR = 1
    pval_threshold = 0.05
    interesting_pvals = ["pval_chi_square_RelToBranchLen", "pval_chi_square_phenotypes", "pval_GenoAndPheno_RelToBranchLen", "pval_GenoAndPheno_phenotypes"]

    # in each df, add the # filters that pass the data
    merging_fields = ["species", "drug", "type_collapsing", "type_vars", "type_mutations", "type_genes", "gwas_method", "gwasID", "group_name"]
    df_merged = df_all[merging_fields].drop_duplicates()
    initial_len_df_merged = len(df_merged)

    all_ASR_methods = ["MPPA", "DOWNPASS", "MPPA,DOWNPASS"]
    for ASR_methods_phenotypes in all_ASR_methods:

        # get df
        interesting_fields = interesting_pvals + ["epsilon", "OR"] 
        df = df_all[df_all.ASR_methods_phenotypes==ASR_methods_phenotypes][interesting_fields]

        # add whether it passes filters by each pval
        for pfield in interesting_pvals: df["pass_filters_%s"%pfield] = (df.epsilon>=min_epsilon) & (df.OR>=min_OR) & (df[pfield]<pval_threshold)
        df["number_supporting_filters_ASR_methods_4pvals_and_no_correction"] = df[["pass_filters_%s"%pfield for pfield in interesting_pvals]].apply(sum, axis=1)

        # rename fields
        df = df[interesting_fields + ["number_supporting_filters_ASR_methods_4pvals_and_no_correction"]].rename(columns={c : "%s_%s"%(c, ASR_methods_phenotypes) for c in df.columns})

        # add to df merged
        df_merged = df_merged.merge(df, left_index=True, right_index=True, validate="one_to_one", how="left")
        if len(df_merged)!=initial_len_df_merged: raise ValueError("len changed upon merge")

        # change the nans by 0s in number_supporting_filters_ASR_methods_4pvals_and_no_correction
        def get_nan_to_0(x):
            if pd.isna(x): return 0.0
            else: return x
        df_merged["number_supporting_filters_ASR_methods_4pvals_and_no_correction_%s"%ASR_methods_phenotypes] = df_merged["number_supporting_filters_ASR_methods_4pvals_and_no_correction_%s"%ASR_methods_phenotypes].apply(get_nan_to_0)

    # add the fractio of supporting fields
    df_merged["fraction_supporting_filters_ASR_methods_4pvals_and_no_correction"] = df_merged[["number_supporting_filters_ASR_methods_4pvals_and_no_correction_%s"%m for m in all_ASR_methods]].apply(sum, axis=1) / (len(interesting_pvals) * len(all_ASR_methods))
    check_no_nans_series(df_merged["fraction_supporting_filters_ASR_methods_4pvals_and_no_correction"])
    if any(df_merged.fraction_supporting_filters_ASR_methods_4pvals_and_no_correction>1): raise ValueError("fraction_supporting_filters_ASR_methods_4pvals_and_no_correction can't be above 1")

    return df_merged.reset_index(drop=True)

def get_df_gwas_af_with_fraction_filters_supporting_association_considering_ASR_methods_4pvals_and_no_correction(df_gwas_af, filename, threads):

    """Takes a gwas df and returns it with a field called 'fraction_supporting_filters_ASR_methods_4pvals_and_no_correction'"""

    if file_is_empty(filename):
        print("running get_df_gwas_af_with_fraction_filters_supporting_association_considering_ASR_methods_4pvals_and_no_correction...")

        # add a unique ID which will be treated independently
        def get_concatenated_data(r): return "-".join(r)
        df_gwas_af["gwasID"] = df_gwas_af[["species", "drug", "type_collapsing", "type_vars", "type_mutations", "type_genes", "gwas_method"]].apply(get_concatenated_data, axis=1)

        # for each gwasID return a unique df with the data for all ASR_mutations 
        inputs_fn = [(gwasID, df) for gwasID,df in df_gwas_af.groupby("gwasID")]
        with multiproc.Pool(threads) as pool:

            df_gwas_af = pd.concat(pool.starmap(get_df_gwas_af_with_fraction_filters_supporting_association_considering_ASR_methods_4pvals_and_no_correction_one_gwasID, inputs_fn)).reset_index(drop=True)
            pool.close()
            pool.terminate()

        # save
        print("saving")
        save_object(df_gwas_af, filename)

    print("returning %s"%filename)
    return load_object(filename)



def gwas_evaluate_how_different_fraction_pass_filters_work(ProcessedDataDir, min_npheno_transitions, spp_drug_to_gwas_df_file, threads, DataDir):

    """Tries different filtering srtaregies involving all three ASR methods, and the 4 types of p values (setting min_epsilon=0.1, min_OR=1 and correction_method=="none"). For each filtering strategy (i.e. only keep groups that pass filters in at least 50% of filter types) it calculates the # of genes (and expected) genes that pass the filter. It plots the relation between the fraction of filters that support a group and the total # of significant genes """

    # define an outdir for this data
    outdir = "%s/processed_data_plot_GWAS_AFresistance_heatmap_how_do_various_filtering_strategies_pct_filtersPASS_work_on_genes"%ProcessedDataDir; make_folder(outdir)

    ######### GET DF #######

    # define the target pval fields
    all_pval_fields = ["%s_%s"%(pval_seed, pval_m) for pval_seed in ["pval_chi_square", "pval_GenoAndPheno"] for pval_m in  ["RelToBranchLen", "phenotypes"]] 

    # define the correction methods
    all_correction_methods = ["bonferroni", "fdr_bh", "fdr_by"]


    # define the interesting fields
    df_gwas_af_fields = ["species", "drug", "type_collapsing", "type_vars", "type_mutations", "type_genes", "ASR_methods_phenotypes", "gwas_method", "group_name", "epsilon", "OR", "nodes_withPheno"]
    df_gwas_af_fields += ["%s_%s"%(pval_f, corr_method) for corr_method in all_correction_methods for pval_f in all_pval_fields]    
    df_gwas_af_fields += all_pval_fields

    # define the target drugs
    #target_drugs = {"FLC", "ANI", "MIF", "CAS"}
    target_drugs = {"MIF"}

    # get filtering df
    filtering_stats_df_file = "%s/df_trying_different_filter_combinations_gwas_af_resistance_stats_%s.py"%(outdir, "_".join(sorted(target_drugs)))
    if file_is_empty(filtering_stats_df_file) or replace is True:

        # create the df_gwas_af with some target drugs and fields
        df_gwas_af_file = "%s/df_gwas_af_%s.py"%(outdir, "_".join(sorted(target_drugs)))
        if file_is_empty(df_gwas_af_file):

            all_tab_files = [tab for (spp, drug), tab in spp_drug_to_gwas_df_file.items() if drug in target_drugs]
            print("getting the gwas df for %i tab files"%(len(all_tab_files)))
            with multiproc.Pool(threads) as pool:

                df_gwas_af = pd.concat(pool.starmap(load_df_with_some_fields, [(t, df_gwas_af_fields) for t in all_tab_files]))
                pool.close()
                pool.terminate()

            save_object(df_gwas_af, df_gwas_af_file)
        print("loading df_gwas_af")
        df_gwas_af = load_object(df_gwas_af_file)

        # keep only some drugs to test
        if set(df_gwas_af.drug)!=target_drugs: raise ValueError("target drugs are not the same")

        # define the df that has mutations, adding the per-gene alteration
        print("getting df mutations")
        df_gwas_af_mutations = get_gwas_df_no_collapsing_with_collapsing_info(df_gwas_af, DataDir, threads, outdir, target_type_collapsing="genes", target_type_vars="all_vars", target_type_mutations="non_syn_muts", target_type_genes="all_genes").rename(columns={"target_group_name":"gene_name"})[df_gwas_af_fields + ["gene_name"]]

        # get the filtered df, with genes and domains
        df_gwas_af_genes_and_domains = df_gwas_af[(df_gwas_af.type_vars=="all_vars") & (df_gwas_af.type_mutations=="non_syn_muts")]

        df_gwas_af_genes = df_gwas_af_genes_and_domains[(df_gwas_af_genes_and_domains.type_collapsing=="genes") & (df_gwas_af_genes_and_domains.type_genes=="all_genes")]
        df_gwas_af_domains = df_gwas_af_genes_and_domains[(df_gwas_af_genes_and_domains.type_collapsing=="domains") & (df_gwas_af_genes_and_domains.type_genes=="only_protein_coding")]

        def get_gene_name(r):
            if r.type_collapsing=="genes": return r.group_name
            elif r.type_collapsing=="domains": return r.group_name.split("#")[1]
            else: raise ValueError("error in gene: %s"%r)

        df_gwas_af_genes["gene_name"] = df_gwas_af_genes[["type_collapsing", "group_name"]].apply(get_gene_name, axis=1)
        df_gwas_af_domains["gene_name"] = df_gwas_af_domains[["type_collapsing", "group_name"]].apply(get_gene_name, axis=1)

        # merge together the info from domains, genes and small variants, all centered around a gene
        df_gwas_af = pd.concat([d[df_gwas_af_fields + ["gene_name"]] for d in [df_gwas_af_genes, df_gwas_af_domains, df_gwas_af_mutations]]).reset_index(drop=True)

        # add the 'fraction filters supporting association'

        check_how_you_can_add_gene_name
        
        df_gwas_af = get_df_gwas_af_with_fraction_filters_supporting_association_considering_ASR_methods_4pvals_and_no_correction(df_gwas_af, "%s.gwas_df_wfraction_filers.py"%outdir, threads)

        print(df_gwas_af)

        adjhdgajadggd


        addd_here_the_fraction_of_filters_passing




        print(df_gwas_af)

        adadkhgahghjadg




    # load
    print("loading df")
    filtering_stats_df = load_object(filtering_stats_df_file)


    ########################

def plot_known_genes_AF_resistance_GWAS(df_gwas_af, ProcessedDataDir, PlotsDir, gene_features_df, association_f="minus_log10pval_fdr"):

    """Plots the epsilon and pvaue comparing different GWAS results for different species and drugs. One plot for each species-drug"""

    # get the filtered df with only the relevant data (genes and small variants)
    df_gwas_af = df_gwas_af[(df_gwas_af.type_vars=="small_vars") & (df_gwas_af.type_genes=="all_genes") & (df_gwas_af.type_mutations=="non_syn_non_truncating_muts") & (df_gwas_af.type_collapsing=="genes")]; print("filtered df obtained")

    # keep gene_features_df
    gene_features_df = gene_features_df[gene_features_df.species.isin({"Candida_albicans", "Candida_glabrata", "Candida_auris"})]

    # map each species and drugs to the interesting geness
    species_to_drug_to_interestingGenes = {"Candida_albicans": {"azoles":["ERG11"]},
                                           "Candida_auris": {"azoles":["ERG11", "TAC1b"], "echinocandins":["GSC2"]}, # GSC2 is the auris ortholog of FKS1
                                           "Candida_glabrata": {"azoles":["ERG11", "PDR1", "CDR1"], "echinocandins":["FKS1", "FKS2"]} }

    drug_to_type = {"FLC":"azoles", "POS":"azoles", "VRC":"azoles", "ITR":"azoles", "MIF":"echinocandins", "ANI":"echinocandins", "CAS":"echinocandins"}

    # go through each combination of plots
    for species in set(df_gwas_af.species):
        gene_features_df_s = gene_features_df[gene_features_df.species==species]

        for drug in set(df_gwas_af.drug):

            # discard some drugs
            if drug in {"5FC", "AMB"}: continue
            #if drug not in {"FLC", "ANI", "MIF", "CAS"}: continue

            # get df
            df_gwas_af_s = df_gwas_af[(df_gwas_af.species==species) & (df_gwas_af.drug==drug)][["group_name", "gwas_method", "ASR_methods_phenotypes", "corrected_pval", "epsilon", "pval", "nodes_withPheno"]]
            if len(df_gwas_af_s)==0: continue

            # add fields
            pseudocount_pval = min(df_gwas_af_s[df_gwas_af_s.pval>0].pval)*0.1
            df_gwas_af_s["minus_log10pval"] = -np.log10(df_gwas_af_s.pval + pseudocount_pval)
            df_gwas_af_s["minus_log10pval_fdr"] = -np.log10(df_gwas_af_s.corrected_pval + pseudocount_pval)

            # define lims
            lims = [min(df_gwas_af_s[association_f])-0.05, max(df_gwas_af_s[association_f])+0.05]

            # log
            print(species, drug)

            # define the inetersting genes and map them to the df
            interesting_genes = species_to_drug_to_interestingGenes[species][drug_to_type[drug]]
            geneID_to_geneName = {get_geneID_for_gName_from_gene_features_df(gene_features_df_s, gName) : gName for gName in interesting_genes}
            geneName_to_color = get_value_to_color(geneID_to_geneName.values(), palette="tab10",  type_color="hex")[0]


            ############ FIGURE ##############

            # init fig
            nrows = 2
            ncols = 3
            fig = plt.figure(figsize=(ncols*2, nrows*2)); I=1

            # change names
            dict_map = {"MPPA":"ML", "DOWNPASS":"MP", "MPPA,DOWNPASS":"ML&MP"}
            df_gwas_af_s["ASR_methods_phenotypes"] = df_gwas_af_s.ASR_methods_phenotypes.map(dict_map); check_no_nans_series(df_gwas_af_s.ASR_methods_phenotypes)


            # each row is one gwas method
            for Ir, gwas_method in enumerate(["phyC", "synchronous"]):
                df_r = df_gwas_af_s[df_gwas_af_s.gwas_method==gwas_method].set_index("group_name")

                # each col is one comparison of the ASR_methods_phenotypes
                for Ic, (method_x, method_y) in enumerate([("ML", "MP"), ("ML", "ML&MP"), ("MP", "ML&MP")]):

                    # create a df with these methods, getting nans to 0s
                    def get_nans_to_0(x):
                        if pd.isna(x): return 0.0
                        else: return x

                    df_plot = pd.DataFrame({m : df_r[df_r.ASR_methods_phenotypes==m][association_f] for m in [method_x, method_y]}).applymap(get_nans_to_0)

                    # define the title
                    title = "%s, %s, %s"%(species, drug, association_f)


                    for m in [method_x, method_y]: check_no_nans_series(df_plot[m])
                    if len(df_plot)!=len(set(df_plot.index)): raise ValueError("the genes should be unique")

                    # create a scatter
                    ax = plt.subplot(nrows, ncols, I); I+=1
                    ax = sns.scatterplot(x=method_x, y=method_y, data=df_plot, color="gray", alpha=.1)

                    ax.set_ylim(lims)
                    ax.set_xlim(lims)


                    # add the interesting genes
                    for geneID, geneName in geneID_to_geneName.items(): 

                        if geneID in df_plot.index: sns.scatterplot(x=method_x, y=method_y, data=df_plot.loc[{geneID}], color=geneName_to_color[geneName], alpha=1)

                    # get labels, also including the number of phenotype transitions
                    for ax_fun, m in [(ax.set_xlabel, method_x), (ax.set_ylabel, method_y)]:

                        # define the number of nodes with phenotype
                        nodes_with_pheno = set(df_r[df_r.ASR_methods_phenotypes==m].nodes_withPheno)
                        if len(nodes_with_pheno)!=1: raise ValueError("there should be only one number of nodes with pheno")
                        nodes_with_pheno = next(iter(nodes_with_pheno))

                        # define axes
                        label = "%s ($n_{pheno}=%i$)"%(m, nodes_with_pheno)
                        if Ic==0 and m==method_y: label = "%s\n%s"%(gwas_method, label)
                        ax_fun(label)


                    # get labels
                    if Ir==0 and Ic==0: ax.set_title(title)

                    #if Ic==0: ax.set_ylabel("%s\n%s"%(gwas_method, method_y))

                    if Ic!=0: 
                        ax.set_yticklabels([])
                        ax.set_yticks([])

                    if Ir!=1: 
                        ax.set_xticklabels([])
                        ax.set_xticks([])

                    # add lims for pvals
                    if association_f in {"minus_log10pval", "minus_log10pval_fdr"}:

                        for val in [0.1, 0.05]:
                            val_plot = -np.log10(val + pseudocount_pval)

                            plt.axvline(val_plot, color="k", linestyle="--", linewidth=.7)
                            plt.axhline(val_plot, color="k", linestyle="--", linewidth=.7)


                    # set a legend
                    if Ir==0 and Ic==2:

                        def get_legend_element(facecolor, label): return mpatches.Patch(facecolor=facecolor, edgecolor="gray", label=label)
                        legend_elements = [get_legend_element("gray", "all data")] + [get_legend_element(color, gene) for gene, color in geneName_to_color.items()]
                        ax.legend(handles=legend_elements, loc=[1.1, 0])





            plt.subplots_adjust(wspace=0.3, hspace=0.3)
            plt.show()

            plots_dir = "%s/GWAS_correlations_btw_ASR_methods_phenotypes"%PlotsDir; make_folder(plots_dir)
            filename = "%s/%s_%s_%s.pdf"%(plots_dir, association_f, species, drug)
            print("saving %s"%filename)
            fig.savefig(filename, bbox_inches="tight")
            return 

            ##################################






def get_species_to_ref_genome(CurDir):

    """Map each species to the reference genome"""

    return {species : "%s/data/%s_%i/genome.fasta"%(CurDir, species, taxID) for taxID, species in taxID_to_sciName.items()}


def get_species_to_gff(CurDir):

    """Maps the species to each gff. Note that for Cpara and Ctrop I generated additional annotations"""

    # init
    species_to_gff = {species : "%s/data/%s_%i/annotations.gff"%(CurDir, species, taxID) for taxID, species in taxID_to_sciName.items()}
    
    # add other species
    for species in {"Candida_parapsilosis", "Candida_tropicalis"}: species_to_gff[species] = "%s.added_mtDNA_annotations.gff"%(species_to_gff[species]) 

    return species_to_gff


def  get_species_to_srr_to_sampleID(CurDir):

    """Map each species to the sampleID (numeric)"""

    return {species : dict(get_tab_as_df_or_empty_df("%s/data/%s_%i/samples_data.tab"%(CurDir, species, taxID)).set_index("srr")["sampleID"].apply(str)) for taxID, species in taxID_to_sciName.items()}

def get_species_to_tree(CurDir):

    """map each species to a tree (note that the choice is different for haploids and diploids)"""

    # get the dict for haploids
    species_to_tree = {species : get_correct_tree_midpointRooted("%s/data/%s_%i/generate_tree_from_SNPs/iqtree_unroted.treefile"%(CurDir, species, taxID)) for taxID, species in taxID_to_sciName.items() if taxID_to_ploidy[taxID]==1}

    # for diploids add another tree
    for taxID, species in taxID_to_sciName.items():
        if taxID_to_ploidy[taxID]==2: species_to_tree[species] = Tree("%s/data/%s_%i/generate_tree_from_SNPs_resamplingHetSNPs/tree_consensus_withBootstraps_and_branchLengths.nw"%(CurDir, species, taxID))

    return species_to_tree

# define syn / non-syn vars for the piN / piS analysis
type_var_to_nonSyn_Syn_consequences = {
                                        # in SNPs, only CDS-altering variants are considered as syn or non-syn if they alter the protein
                                        "SNP":
                                             {"non_synonymous": {"stop_lost", "start_lost", "missense_variant", "stop_gained"},
                                              "synonymous": {"synonymous_variant", "stop_retained_variant"},

                                              "others": {"incomplete_terminal_codon_variant", "5_prime_UTR_variant", "splice_donor_variant", "3_prime_UTR_variant", "non_coding_transcript_variant", "frameshift_variant", "downstream_gene_variant", "intergenic_variant", "splice_region_variant", "intron_variant", "coding_sequence_variant", "splice_acceptor_variant", "non_coding_transcript_exon_variant", "upstream_gene_variant"}},

                                        # in INDELs, only CDS-altering variants are considered, syn variants are the ones that produce some protein
                                        "IN/DEL":
                                             {"non_synonymous": {"frameshift_variant", "stop_lost", "start_lost", "stop_gained"},
                                              "synonymous": {"start_retained_variant", "inframe_deletion", "inframe_insertion"},
                                             
                                              "others": {"5_prime_UTR_variant", "3_prime_UTR_variant", "splice_donor_variant", "non_coding_transcript_variant", "downstream_gene_variant", "intergenic_variant", "splice_region_variant", "intron_variant", "coding_sequence_variant", "splice_acceptor_variant", "protein_altering_variant", "non_coding_transcript_exon_variant", "upstream_gene_variant"}},

                                        # in SVs anc CNVs, anything that overlaps the transcript is considered as nonsyn, and syn would be intergenic or regulatory SVs.
                                        "SV":
                                             {"non_synonymous": {"frameshift_variant", "start_lost_BND", "inframe_deletion", "start_retained_variant", "inframe_insertion", "transcript_ablation", "start_lost", "stop_lost", "coding_sequence_variant", "transcript_amplification", "coding_sequence_variant_BND", "protein_altering_variant", "stop_gained", "5_prime_UTR_variant", "3_prime_UTR_variant", "5_prime_UTR_variant_BND", "intron_variant_BND", "splice_region_variant", "intron_variant", "3_prime_UTR_variant_BND"},

                                              "synonymous": {"downstream_gene_variant_BND", "upstream_gene_variant_BND", "downstream_gene_variant", "intergenic_variant", "intergenic_variant_BND", "upstream_gene_variant", },

                                              "others": {"non_coding_transcript_variant", "non_coding_transcript_variant_BND", "non_coding_transcript_exon_variant_BND", "feature_elongation", "non_coding_transcript_exon_variant", "feature_truncation"}},

                                        # coverageCNVs is the same as SVs.
                                        "coverageCNV":
                                             {"non_synonymous": {"5_prime_UTR_variant", "3_prime_UTR_variant", "start_retained_variant", "inframe_deletion", "intron_variant", "inframe_insertion", "transcript_ablation", "start_lost", "coding_sequence_variant", "transcript_amplification"},

                                              "synonymous": {"downstream_gene_variant", "upstream_gene_variant", "intergenic_variant"},
                                              "others": { "feature_elongation", "non_coding_transcript_exon_variant", "feature_truncation"}}
                                        }



def get_is_synonymous_from_r_piN_piS(r):

    """Define whether a variant is syn or non-syn in the piN / piS analysis"""

    # if the gene is not protein coding, retrun nan
    if r.is_protein_coding_gene is False: return np.nan

    # define the muts
    non_syn_muts = type_var_to_nonSyn_Syn_consequences[r.type_var]["non_synonymous"]
    syn_muts = type_var_to_nonSyn_Syn_consequences[r.type_var]["synonymous"]
    other_muts = type_var_to_nonSyn_Syn_consequences[r.type_var]["others"]

    # get the type of variant
    if len(r.consequences_set.intersection(non_syn_muts))>0: return False # if any is non-syn, get it
    elif len(r.consequences_set.intersection(syn_muts))>0: return True # if any is syn, get it
    elif len(r.consequences_set.difference(other_muts))==0: return np.nan # if none of the others, set as non syn.
    else: 
        print(r)
        raise ValueError("%s contains non-described vars for %s"%(r.consequences_set, r.type_var))


# define small vars that can be truncating
SMALLVARS_TRUNCATING_MUTATIONS = {'stop_gained', 'protein_altering_variant', 'frameshift_variant', 'start_lost', 'coding_sequence_variant'}
SMALLVARS_NON_TRUNCATING_MUTATIONS = {'intron_variant', 'upstream_gene_variant', '5_prime_UTR_variant', 'inframe_insertion', 'synonymous_variant', 'non_coding_transcript_exon_variant', 'intergenic_variant', 'downstream_gene_variant', '3_prime_UTR_variant', 'missense_variant', 'splice_region_variant', 'splice_acceptor_variant', 'inframe_deletion', 'stop_lost', 'non_coding_transcript_variant', 'start_retained_variant', 'stop_retained_variant', 'incomplete_terminal_codon_variant', 'splice_donor_variant', '-'}

def get_is_truncating_small_variant_from_r_piN_piS(r):

    """For the piN / piS analysis, get whether a variant is truncating small variant"""

    # SVs and CNVs remove
    if r.type_var in {"SV", "coverageCNV"}: return False

    # get whether it is truncating
    if len(r.consequences_set.intersection(SMALLVARS_TRUNCATING_MUTATIONS))>0: return True
    elif len(r.consequences_set.difference(SMALLVARS_NON_TRUNCATING_MUTATIONS))==0: return False
    else: raise ValueError("%s contains non-described vars"%r.consequences_set)



def get_all_annotations_df_for_df_diversity_piN_piS(unique_SV_CNV_df, unique_small_vars_df, DataDir, ProcessedDataDir, species_to_gff, min_pct_overlap_CNV_simpleRepeats=25, threads=4, replace=False):

    """gets the annotations for the piN / piS calculations"""

    # define the file
    all_annotations_df_file = "%s/df_diversity_all_all_annotations_df.py"%ProcessedDataDir

    # get it
    if file_is_empty(all_annotations_df_file) or replace is True:
        print("getting all_annotations_df")

        # This df should contain, for each variant, the genes affected and whether it is synonymous or non-synonymous

        # get the variant annotations for filtered variants
        SV_CNV_annot_filt = get_annot_df_filt_SV_CNV_or_small_vars(DataDir, ProcessedDataDir, type_vars="SV_CNV", species_to_gff=species_to_gff, replace=False)
        small_vars_annot_filt = get_annot_df_filt_SV_CNV_or_small_vars(DataDir, ProcessedDataDir, type_vars="smallVars", species_to_gff=species_to_gff, replace=False)

        # add whether each of the variants overlaps repeats
        print("adding repeats")
        simple_repeats = {"Low_complexity", "Simple_repeat"}
        unique_SV_CNV_df = get_unique_SV_CNV_df_with_overlaps_simple_repeats(unique_SV_CNV_df, min_pct_overlap_CNV_simpleRepeats, simple_repeats)
        unique_small_vars_df = get_unique_small_vars_df_with_overlaps_simple_repeats(unique_small_vars_df, simple_repeats)
        if any(pd.isna(unique_SV_CNV_df.overlaps_simple_repeats)): raise ValueError("There should not be any NaNs in simple repeats")

        SV_CNV_annot_filt  = SV_CNV_annot_filt.merge(unique_SV_CNV_df[["species", "variantID_across_samples", "overlaps_simple_repeats"]].drop_duplicates(), on=["species", "variantID_across_samples"], how="left", validate="many_to_one")
        if any(pd.isna(SV_CNV_annot_filt.overlaps_simple_repeats)): raise ValueError("There should not be any NaNs in simple repeats")

        small_vars_annot_filt  = small_vars_annot_filt.merge(unique_small_vars_df[["species", "#Uploaded_variation", "overlaps_simple_repeats", "ISSNP"]].drop_duplicates(), on=["species", "#Uploaded_variation"], how="left", validate="many_to_one")
        if any(pd.isna(small_vars_annot_filt.overlaps_simple_repeats)): raise ValueError("There should not be any NaNs in simple repeats")

        # add fields and generate a df with the genes
        print("adding type of variant")
        def get_type_var_from_INFO_variantID(x):
            if x.startswith("coverage"): return "coverageCNV"
            else: return "SV"

        SV_CNV_annot_filt["type_var"] = SV_CNV_annot_filt.INFO_variantID.apply(get_type_var_from_INFO_variantID)
        small_vars_annot_filt["type_var"] = small_vars_annot_filt.ISSNP.map({True:"SNP", False:"IN/DEL"})

        # merge and get important fields
        fields = ["variantID_across_samples", "Gene", "species", "type_var", "overlaps_simple_repeats", "Consequence", "cDNA_position", "CDS_position", "Protein_position", "Amino_acids", "Codons", "CDS_length", "is_protein_coding_gene", "gene_length"]
        all_annotations_df = SV_CNV_annot_filt[fields].append(small_vars_annot_filt[fields]).drop_duplicates(subset=fields)

        # get a reduced set of annotations, where each line is the 
        print("getting reduced dataframe")
        unique_fields = ["type_var", "is_protein_coding_gene", "Consequence"]
        reduced_annotations_df = all_annotations_df[unique_fields].drop_duplicates(subset=unique_fields)

        # add fields
        print("adding consequences set")
        def get_consequences_list(x): return x.split(",")
        reduced_annotations_df["consequences_set"] = reduced_annotations_df.Consequence.apply(get_consequences_list).apply(set)

        # add whether a variant is truncating (meaning that it breaks the CDS)
        print("Getting types of variant")
        reduced_annotations_df["is_truncating_small_variant"] = reduced_annotations_df.apply(get_is_truncating_small_variant_from_r_piN_piS, axis=1)
        reduced_annotations_df["is_synonymous"] = reduced_annotations_df.apply(get_is_synonymous_from_r_piN_piS, axis=1)

        # add to 
        print("merging to all_annotations_df")
        initial_len_all_annotations_df = len(all_annotations_df)
        all_annotations_df = all_annotations_df.merge(reduced_annotations_df[unique_fields + ["is_truncating_small_variant", "is_synonymous"]], on=unique_fields, validate="many_to_one", how="left")

        # checks
        if len(all_annotations_df)!=initial_len_all_annotations_df: raise ValueError("The len changed in merge")
        if any(pd.isna(all_annotations_df.is_truncating_small_variant)): raise ValueError("There can't be nans in is_truncating_small_variant")

        print("saving")
        save_object(all_annotations_df, all_annotations_df_file)

    print("loading annotations")
    all_annotations_df = load_object(all_annotations_df_file)

    return all_annotations_df



def get_sample_to_genesCorrectCoverage_piN_piS(df_coverage_per_gene, species, protein_coding_genes, interesting_samples):


    """Maps each sample to the set of genes with correct coverage"""

    #print("defining genes with proper coverage")

    # get the coverage per gene for this species, so that it matches the gff and annotations notation
    df_coverage_per_gene_s = df_coverage_per_gene.set_index("species").loc[species]

    # change the ID for some species
    if species=="Candida_metapsilosis": df_coverage_per_gene_s["ID"] = "gene" + df_coverage_per_gene_s.ID.apply(lambda x: x.lstrip("g"))

    # keep only hemes that are protein coding
    df_coverage_per_gene_s = df_coverage_per_gene_s.set_index("ID", drop=False).loc[protein_coding_genes]

    # change the sample
    df_coverage_per_gene_s["sampleID"] = df_coverage_per_gene_s.sampleID.apply(int).apply(str)

    # keep interesting samples
    df_coverage_per_gene_s = df_coverage_per_gene_s.set_index("sampleID", drop=False).loc[interesting_samples]

    # define samples with enough coverage of each gene (median >24x and >95% of the gene covered)
    min_cov = 24
    min_pct_cov = 95
    df_coverage_per_gene_s["correct_coverage"] = (df_coverage_per_gene_s.median_reads_per_gene>=min_cov) & (df_coverage_per_gene_s.percentcovered_1>=min_pct_cov)

    def get_correct_genes(df_s): return set(df_s[df_s.correct_coverage].ID)
    sample_to_genesCorrectCoverage = df_coverage_per_gene_s[["sampleID", "correct_coverage", "ID"]].reset_index(drop=True).groupby("sampleID").apply(get_correct_genes)

    # plot
    #ax = sns.distplot((sample_to_genesCorrectCoverage.apply(len)/len(protein_coding_genes)), rug=True)
    #ax.set_title(species); ax.set_xlabel("fraction genes correct coverage (median>%ix & >%i%s covered)"%(min_cov, min_pct_cov, "%")); ax.set_ylabel("# samples"); plt.show()

    return sample_to_genesCorrectCoverage



def get_vars_df_file_for_ASR_mutations(taxID_dir, vars_df_file, taxID):

    """Gets the variants of this ploidy stached and simplified"""

    if file_is_empty(vars_df_file):
        print("getting vars")

        # define dirs
        integrated_varcallsDir = "%s/integrated_varcalls"%(taxID_dir)
        vars_fileds = ["variantID_across_samples", "sampleID", "common_GT"]

        # load vars
        small_vars_df = load_object("%s/smallVars_filt.py"%integrated_varcallsDir)
        small_vars_df = small_vars_df[small_vars_df.calling_ploidy==taxID_to_ploidy[taxID]]
        small_vars_df["variantID_across_samples"] = small_vars_df["#Uploaded_variation"]
        small_vars_df = small_vars_df[vars_fileds].drop_duplicates()
    
        SV_CNV_df = load_object("%s/SV_CNV_filt.py"%integrated_varcallsDir)
        SV_CNV_df["common_GT"] = "."
        SV_CNV_df = SV_CNV_df[vars_fileds].drop_duplicates()

        # merge and save
        vars_df = small_vars_df.append(SV_CNV_df)[vars_fileds]
        save_object(vars_df, vars_df_file)


def get_all_ASR_mutations_cmds(tree_object, vars_df_file, outdir, threads_ASRmutations):

    """This function gets a cmd to build the cmd the """

    # define the outdirs
    outdir_ASR_mutations = "%s/genearting_ASR_mutations_dir"%outdir
    final_file_ASR_mutations = "%s/all_jobs_finished.txt"%outdir_ASR_mutations

    # define outdir with chunks
    outdir_chunks_jobs = "%s/chunks_ASR_jobs"%outdir; 
    delete_folder(outdir_chunks_jobs); make_folder(outdir_chunks_jobs)

    # only continue if the final file of the mutations is not created
    if file_is_empty(final_file_ASR_mutations):
        print("running get_all_ASR_mutations_cmd...")

        # make the treefile
        treefile = "%s/raw_tree.nw"%(outdir_ASR_mutations)
        tree_object.write(outfile=treefile, format=2)

        # get the jobs file with all cmds, one for each mutation
        jobs_file = "%s/pastml_cmds.txt"%outdir_ASR_mutations 
        get_ASR_mutations_py = "%s/scripts/ancestral_GWAS/scripts/get_ASR_mutations.py"%ParentDir
        run_cmd("%s --variants_table %s --tree %s --phenotypes none --outdir %s --varID_field variantID_across_samples --pastml_prediction_method ALL --threads %i --phenotypes_pastml_prediction_method none --resampled_phenotypes_pastml_out none --add_pseudocount_dist_internal_nodes --only_write_pastml_cmds"%(get_ASR_mutations_py, vars_df_file, treefile, outdir_ASR_mutations, multiproc.cpu_count()), env="ancestral_GWAS_env")

        # if the jobs file was generated (meaning that some mutations have to be generated, split them to chunks and get as cmds)
        if not file_is_empty(jobs_file):
            print("preparing cmds chunks of jobs...")

            # get chunks of threads_ASRmutations
            cmds_return = []
            all_cmds = [l.strip() for l in open(jobs_file, "r").readlines()]
            for Ic, chunk_cmds in enumerate(chunks(all_cmds, int(threads_ASRmutations*4))): 
                jobs_file_chunk = "%s/jobs_ASRmuts.%i.txt"%(outdir_chunks_jobs, Ic+1)
                open(jobs_file_chunk, "w").write("\n".join(chunk_cmds))
                cmds_return.append("%s/CandidaMine_data_generation/v1/run_jobs_file_in_parallel.py %s ancestral_GWAS_env %i"%(ParentDir, jobs_file_chunk, threads_ASRmutations))

            return cmds_return
            
        else: 
            delete_folder(outdir_chunks_jobs)
            return None



    else: 
        delete_folder(outdir_chunks_jobs)
        return None # this will call to integrate the results of the ASR


def get_tree_with_internalNodeNames(tree):

    """Adds internal node names. The same as in ancstrel GWAS"""

    for I, n in enumerate(tree.traverse()): 
        if not n.is_leaf(): n.name = "node_%s_to_%s"%(n.get_leaf_names()[0], n.get_leaf_names()[-1])

    return tree

def get_tree_with_cladeID_info(tree, sampleID_to_clade):

    """Gets a tree and verifies that the clades are monophyletic"""

    # get misc data
    all_clades = {x for x in sampleID_to_clade.values() if not pd.isna(x)}
    clade_to_samples = {clade : {s for s,c in sampleID_to_clade.items() if c==clade} for clade in all_clades}

    # add the clade of each node and whether it is a branch clade
    for n in tree.traverse():

        # for leafs it is the clade
        if n.is_leaf(): 
            n.cladeID = sampleID_to_clade[n.name]
            n.clade_branch = False

        else:

            # get the clades
            clades_under_n = {sampleID_to_clade[s] for s in n.get_leaf_names() if not pd.isna(sampleID_to_clade[s])}
            if len(clades_under_n)==1: 
                n.cladeID = next(iter(clades_under_n))

                if set(n.get_leaf_names())==clade_to_samples[n.cladeID]: n.clade_branch = True
                else: n.clade_branch = False

            else: 
                n.cladeID = np.nan
                n.clade_branch = False






    # check that there is one branch clade for each expected clade
    if sorted([n.cladeID for n in tree.traverse() if n.clade_branch is True])!=sorted(all_clades): 

        for l in tree.get_leaves():

            l.add_face(TextFace(str(sampleID_to_clade[l.name]), bold=True, fsize=12), column=0)

        tree.show()

        raise ValueError("There is not only one clade_branch per clade")

    return tree


def convert_nan_to_0(x):

    """Gets a nan and returns a 0"""
    
    if pd.isna(x): return 0.0
    else: return x


def get_vars_df_piN_piS_plot_and_check_ASR_mutations(all_vars_df, sampleID_to_clade, all_clades, species, PlotsDir):

    """plots the number of variants vs the variants that were already in the ancestor"""
    
    print("checking that ASR mutations went well") # long

    # change the index
    all_vars_df = all_vars_df.set_index("sampleID", drop=False)[["sampleID", "variantID_across_samples", "variant_in_ancestral_cladeID"]]

    # only consider samples with clades and keep some clades
    print("keeping some clades")
    valid_samples = {s for s,c in sampleID_to_clade.items() if not pd.isna(c)}
    all_vars_df = all_vars_df.loc[valid_samples]
    valid_samples = sorted(valid_samples)

    # checks
    print("checking that each sample has a unique set of variants")
    if len(all_vars_df)!=len(all_vars_df.drop_duplicates(subset=["sampleID", "variantID_across_samples"])): raise ValueError("There are some samples that have twice the same variant")
    
    # create df_plot, which has the # vars and the fraction already in the ancestor
    print("getting df_plot")
    series_varInAncestralClade = all_vars_df["variant_in_ancestral_cladeID"]
    sample_to_nvars = series_varInAncestralClade.groupby(series_varInAncestralClade.index).apply(len)
    sample_to_fractionVarsInAncestor = series_varInAncestralClade.groupby(series_varInAncestralClade.index).apply(sum) / sample_to_nvars

    df_plot = pd.DataFrame({"# vars":sample_to_nvars, "fraction variants already in clade ancestor":sample_to_fractionVarsInAncestor})
    df_plot["sampleID"] = df_plot.index
    df_plot["cladeID"] = df_plot.sampleID.apply(lambda s: sampleID_to_clade[s])

    # define graphics
    print("plotting")
    nclades = len(all_clades)
    if nclades<=10: palette = "tab10"
    else: palette = "tab20"
    clade_to_color = get_value_to_color(list(range(1, nclades+1)), palette=palette, n=nclades, type_color="hex")[0]

    # calculate the spearman correlation
    r_spearman, p_spearman = stats.spearmanr(df_plot["# vars"], df_plot["fraction variants already in clade ancestor"], nan_policy="raise")

    # get the figure
    fig = plt.figure(figsize=(5, 5))
    ax = sns.scatterplot(x="# vars", y="fraction variants already in clade ancestor", data=df_plot, hue="cladeID", palette=clade_to_color)
    ax.set_title("%s; spearman r=%.4f; p=%.5f"%(species, r_spearman, p_spearman))
    ax.legend(bbox_to_anchor=(1.5, 1))
    plots_dir = "%s/correlation_btw_nvars_and_fractionVarsInAncestor"%PlotsDir; make_folder(plots_dir)
    fig.savefig("%s/%s.pdf"%(plots_dir, species), bbox_inches="tight")


def get_vars_df_piN_piS(DataDir, species, taxID, ProcessedDataDir, threads, all_annotations_df_species, sampleID_to_clade, PlotsDir):

    """Gets the all_vars_df from the ASR analysis, with information about whether the mutation appeared in the clade or not."""

    # define the final file
    all_vars_df_file = "%s/%s_all_vars_with_infoASR_piN_piS.py"%(ProcessedDataDir, species)
    if file_is_empty(all_vars_df_file):
        print("running get_vars_df_piN_piS ")

        # define outdir
        outdir_ASRmutations = "%s/%s_%s/generating_ASR_allMuts"%(DataDir, species, taxID)

        # load the tree used in the ASR of the mutations, so that it has cladeIDs, adding the information about the clade. clade_branch indicates 
        print("getting trees")
        tree = get_tree_with_internalNodeNames(Tree("%s/correct_tree.nw"%outdir_ASRmutations))
        interesting_samples = set(tree.get_leaf_names()).difference({str(x) for x in sciName_to_badSamples[species]})
        tree.prune(interesting_samples, preserve_branch_length=True) # remove bad samples
        tree = get_tree_with_cladeID_info(tree, sampleID_to_clade)
        
        # define dir and load dfs
        print("loading dfs")
        all_vars_df = load_object("%s/merged_vars_df.py"%outdir_ASRmutations)
        var_to_repVar_df = load_object("%s/var_to_representative_var.py"%outdir_ASRmutations)
        vars_df_asr = load_object("%s/variants_df_with_ASRdata.py"%outdir_ASRmutations).set_index("ASR_method")

        # checks
        if len(var_to_repVar_df)!=len(set(var_to_repVar_df.variant)): raise ValueError("variant should be unique in var_to_repVar_df")
        if any(pd.isna(var_to_repVar_df.rep_variant)): raise ValueError("rep_variant can't have nans")


        # remove samples that are bad
        print("filtering bad samples")
        all_vars_df["sampleID"] = all_vars_df.sampleID.apply(str)
        bad_samples = {str(s) for s in sciName_to_badSamples[species]}
        all_vars_df = all_vars_df[~all_vars_df.sampleID.isin(bad_samples)]

        # define the initial fields
        first_initial_len_all_vars_df = len(all_vars_df)
        initial_fields = list(cp.deepcopy(all_vars_df.keys()))

        # add the clade ID
        print("adding cladeID")
        all_vars_df["cladeID"] = all_vars_df.sampleID.apply(lambda s: sampleID_to_clade[s])


        # define variants that are in all samples
        print("filtering vars") # slow step (1 min)
        var_to_fractionSamples = all_vars_df[["variantID_across_samples", "sampleID"]].drop_duplicates().groupby("variantID_across_samples").apply(len) / len(sampleID_to_clade)
        interesting_vars = set(var_to_fractionSamples[var_to_fractionSamples<1].index)

        # define a df with the variants that are in all samples to be added at the end
        all_vars_df_VarsAllSamples = all_vars_df[~all_vars_df.variantID_across_samples.isin(interesting_vars)]

        # keep the variants that are not in all samples for further analysis (these are the only ones considered in ASR)
        all_vars_df = all_vars_df[all_vars_df.variantID_across_samples.isin(interesting_vars)]

        # add the representative var
        print("adding representative var")
        var_to_repVar = dict(var_to_repVar_df.set_index("variant")["rep_variant"])
        all_vars_df["rep_var"] = all_vars_df.variantID_across_samples.map(var_to_repVar)
        if any(pd.isna(all_vars_df["rep_var"])): raise ValueError("there can't be nans in rep_var")

        # map each node to the parent
        node_to_parent = {n.name : n.get_ancestors()[0].name for n in tree.traverse() if n.is_root() is False}
        clade_nodes = {n.name for n in tree.traverse() if n.clade_branch is True} # nodes that have the clades
        node_to_clade =  {n.name : n.cladeID for n in tree.traverse() if n.clade_branch is True}
        all_clades = set(node_to_clade.values())

        # go through different ASR methods, defining, for each mutation, the set of clades that have have this mutation in the ancestor
        method_to_var_to_cladesAncestralPresence = {}
        print("getting var_to_cladesAncestralPresence...")
        for ASR_method in sorted(set(vars_df_asr.index)): # I use all the methods because it is the most conservative
            print(ASR_method)

            # map each variant to the clades that don't have it
            def get_clades_with_ancestralPresence(r): return r[r==1.0].keys()
            method_to_var_to_cladesAncestralPresence[ASR_method] = vars_df_asr.loc[{ASR_method}].set_index("variantID_across_samples")[sorted(clade_nodes)].apply(get_clades_with_ancestralPresence, axis=1).apply(set)

        # map each variant to any clade that is in the ancestor by any of the methods
        print("integrating")
        var_to_cladesAncestralPresence = pd.DataFrame(method_to_var_to_cladesAncestralPresence).apply(lambda r: set.union(*r), axis=1).apply(sorted)
        var_to_cladesAncestralPresence = var_to_cladesAncestralPresence[var_to_cladesAncestralPresence.apply(len)>0]

        # create a df where each row has a combination of variant and clade that was present in the ancestor clade
        print("merging into one long df for variants that are in one single clade")
        var_to_cladesAncestralPresence_singleClade = var_to_cladesAncestralPresence[var_to_cladesAncestralPresence.apply(len)==1]
        def get_first_element(x): return x[0]
        df_ancestralVar_singleClade = pd.DataFrame({"rep_var":var_to_cladesAncestralPresence_singleClade.index, "cladeID_ancestral_presence": var_to_cladesAncestralPresence_singleClade.apply(get_first_element).values})

        print("merging into one long df for variants that are in >1 clade")
        var_to_cladesAncestralPresence_manyClades = var_to_cladesAncestralPresence[var_to_cladesAncestralPresence.apply(len)>1]
        def get_df_clades_one_var(r):
            df_one_var = pd.DataFrame({"cladeID_ancestral_presence":r.clades_ancestral_presence})
            df_one_var["rep_var"] = r.variantID_across_samples
            return df_one_var

        df_ancestralVar_manyClades = pd.concat(pd.DataFrame({"clades_ancestral_presence":var_to_cladesAncestralPresence_manyClades}).reset_index(drop=False).apply(get_df_clades_one_var, axis=1).values) # cladeID_ancestral_presence and rep_var

        print("merging both dfs")
        df_ancestralVar_fields = ["cladeID_ancestral_presence", "rep_var"]
        df_ancestralVar = df_ancestralVar_singleClade[df_ancestralVar_fields].append(df_ancestralVar_manyClades[df_ancestralVar_fields])

        # change the clade to have the clade notation of the variants
        print("changing clade to node")
        df_ancestralVar["cladeID_ancestral_presence"] = df_ancestralVar.cladeID_ancestral_presence.map(node_to_clade)
        if any(pd.isna(df_ancestralVar.cladeID_ancestral_presence)): raise ValueError("There can't be nans in cladeID_ancestral_presence")

        # add a field that specifies whether this variant appeared recently
        print("running merge")
        if set(all_vars_df[~pd.isna(all_vars_df.cladeID)]["cladeID"])!=set(df_ancestralVar.cladeID_ancestral_presence): raise ValueError("the clade IDs of the variants should be the same as the ancestral ones")

        df_ancestralVar["variant_in_ancestral_cladeID"] = True
        initial_len_all_vars_df = len(all_vars_df)
        all_vars_df = all_vars_df.merge(df_ancestralVar, how="left", left_on=["rep_var", "cladeID"], right_on=["rep_var", "cladeID_ancestral_presence"], validate="many_to_one")
        if len(all_vars_df)!=initial_len_all_vars_df: raise ValueError("the length of the df has changed upon merge")

        # check that the merge is meaningful
        print("checking...")
        if all(pd.isna(all_vars_df[all_vars_df.variantID_across_samples!=all_vars_df.rep_var].variant_in_ancestral_cladeID)): raise ValueError("all the non-rep variants are nan. This is suggestive that something went wrong with the steps above")

        # convert to True/False
        print("convering nans to False")
        def get_nan_to_false(x):
            if pd.isna(x): return False
            else: return x
        all_vars_df["variant_in_ancestral_cladeID"] = all_vars_df.variant_in_ancestral_cladeID.apply(get_nan_to_false)

        # additional checks and plots
        get_vars_df_piN_piS_plot_and_check_ASR_mutations(all_vars_df, sampleID_to_clade, all_clades, species, PlotsDir)

        # add the all_vars_df_VarsAllSamples
        print("adding all_vars_df_VarsAllSamples")
        final_fields = initial_fields + ["variant_in_ancestral_cladeID", "cladeID"]
        all_vars_df_VarsAllSamples["variant_in_ancestral_cladeID"] = True
        all_vars_df = all_vars_df[final_fields].append(all_vars_df_VarsAllSamples[final_fields])

        # checks
        if len(all_vars_df)!=first_initial_len_all_vars_df: 
            print(len(all_vars_df), first_initial_len_all_vars_df)
            raise ValueError("the len of all_vars_df changed")

        # save
        print("saving...")
        save_object(all_vars_df.reset_index(drop=True), all_vars_df_file)

    # return the vars df
    #print("loading var calls")
    all_vars_df = load_object(all_vars_df_file)

    # add the type of variant
    """
    print("adding type_var")
    var_to_typeVar = (all_annotations_df_species.set_index("variantID_across_samples").type_var)
    all_vars_df["type_var"] = all_vars_df.variantID_across_samples.map(var_to_typeVar)
    if any(pd.isna(all_vars_df["type_var"])): raise ValueError("there can't be nans in type_var")
    if set(all_vars_df.type_var)!={"SNP", "IN/DEL", "SV", "coverageCNV"}: raise ValueError("The type_var %s are incorrect"%set(all_vars_df.type_var))
    """

    # set as strings some vars
    #print("changing vars")
    for f in ["sampleID", "common_GT"]: all_vars_df[f] =  all_vars_df[f].apply(str)


    return all_vars_df


def get_sample_to_genes_NoTruncation_or_NoSV_piN_piS(vars_df, annotations_df, sorted_samples, protein_coding_genes, species, type_vars):

    """map each sample to genes that have a type of variant (truncating) or non-synonymous SVs."""

    #print("getting genes that have no %s"%type_vars)

    # define variants that are type_vars
    if type_vars=="small_vars_truncating": truncating_vars = set(annotations_df[(annotations_df.is_truncating_small_variant==True)].variantID_across_samples)

    elif type_vars=="nonSyn_SV": truncating_vars = set(annotations_df[(annotations_df.type_var.isin({"SV", "coverageCNV"})) & (annotations_df.is_synonymous==False)].variantID_across_samples)

    else: raise ValueError("not-valid type_vars: %s"%type_vars) 

    # keep annotations that are truncating
    annotations_df_trunc = annotations_df[annotations_df.variantID_across_samples.isin(truncating_vars)][["variantID_across_samples", "Gene"]].drop_duplicates()

    # create a merged df that has minimal info and check
    vars_df = vars_df[vars_df.variantID_across_samples.isin(truncating_vars)].drop_duplicates()
    vars_df = vars_df.merge(annotations_df_trunc, on="variantID_across_samples", how="left").drop_duplicates()
    vars_df["sampleID"] = vars_df.sampleID.apply(str)

    for f in vars_df.keys():
        if f=="cladeID": continue
        if any(pd.isna(vars_df[f])): raise ValueError("There are nans in %s"%f)

    # checks
    strange_samples = (set(vars_df.sampleID).difference({str(x) for x in sciName_to_badSamples[species]})).difference(set(sorted_samples))
    if len(strange_samples)>0: raise ValueError("There are strange samples: %s"%strange_samples)

    # map each sample to the genes that are truncated
    sample_to_truncatedGenes = dict(vars_df.groupby("sampleID").apply(lambda df_s: set(df_s.Gene)))

    # get the samples that have no truncated genes
    def get_non_trunc_genes(s):
        if s not in sample_to_truncatedGenes: return protein_coding_genes
        else: return protein_coding_genes.difference(sample_to_truncatedGenes[s])

    sample_to_genesNoTruncation = dict(zip(sorted_samples, map(get_non_trunc_genes, sorted_samples)))

    # plot
    #sample_to_fractionGenes_notTruncated = pd.Series(sample_to_genesNoTruncation).apply(len)/len(protein_coding_genes)
    #ax = sns.distplot(sample_to_fractionGenes_notTruncated, rug=True)
    #ax.set_title(species); ax.set_xlabel("fraction genes without %s"%type_vars); ax.set_ylabel("# samples"); plt.show()

    return sample_to_genesNoTruncation


def get_gene_to_length_features(gff_df, features, tmpdir, repeats_df, gene_f="upmost_parent"):

    """Maps each gene to a length of the features"""
    
    delete_folder(tmpdir); make_folder(tmpdir)

    # define a gff that has only the desired features with some bed format
    interesting_fields = ["chromosome", "start", "end", gene_f]
    for f in ["start", "end"]: gff_df[f] = gff_df[f].apply(int)
    gff_df = gff_df[gff_df.feature.isin(features)][interesting_fields].drop_duplicates().sort_values(by=interesting_fields).reset_index(drop=True)

    # define all the expected genes
    all_expected_genes = set(gff_df[gene_f])

    # add a unique ID
    gff_df["ID"] = list(map(str, range(len(gff_df))))
    gff_df["ID"] = gff_df[gene_f] + "###" + gff_df.ID

    # get as a bed the gff and the repeats
    gff_bed = "%s/gff.bed"%tmpdir
    gff_df[["chromosome", "start", "end", "ID"]].to_csv(gff_bed, sep="\t", index=False, header=False)

    repeats_bed = "%s/repeats.bed"%tmpdir
    repeats_df[["chromosome", "start", "end"]].to_csv(repeats_bed, sep="\t", index=False, header=False)

    # run bedtools subtract to remove the repeats from the gff
    bedtools_output = "%s/bedtools_out.bed"%tmpdir
    run_cmd("bedtools subtract -a %s -b %s > %s"%(gff_bed, repeats_bed, bedtools_output), env="Candida_mine_env")
    
    # get the gff_df as the one of the bedtools subtract
    gff_df = pd.read_csv(bedtools_output, sep="\t", header=None, names=["chromosome", "start", "end", "ID"])
    gff_df[gene_f] = gff_df.ID.apply(lambda x: x.split("###")[0])
    
    # run bedtools merge to merge overlapping regions of the same gene
    per_gene_gff_bed = "%s/per_gene_gff.bed"%tmpdir
    gff_df[[gene_f, "start", "end"]].drop_duplicates().sort_values(by=[gene_f, "start", "end"]).to_csv(per_gene_gff_bed, sep="\t", index=False, header=False)
    
    per_gene_gff_bed_merged = "%s.merged.bed"%per_gene_gff_bed
    run_cmd("bedtools merge -i %s > %s"%(per_gene_gff_bed, per_gene_gff_bed_merged), env="Candida_mine_env")
    
    # load as a gff
    gff_df = pd.read_csv(per_gene_gff_bed_merged, sep="\t", header=None, names=[gene_f, "start", "end"])
    if any(gff_df.end<gff_df.start): raise ValueError("There can't be end before start")
    gff_df["len_feature"] = gff_df.end - gff_df.start
    gene_to_len = gff_df.groupby(gene_f).apply(lambda df_g: sum(df_g.len_feature)).apply(int)
    
    # add the missing genes
    missing_genes = all_expected_genes.difference(set(gene_to_len.index))
    if len(missing_genes)>0: 
        gene_to_len = gene_to_len.append(pd.Series({g :0 for g in missing_genes}))
        print("WARNING: There are %i genes that are missing in the gff after filtering out repeats"%len(missing_genes))

    # check
    if all_expected_genes!=set(gene_to_len.index): raise ValueError("Not all genes got a length")
    delete_folder(tmpdir)

    return gene_to_len


def get_gene_to_positions_piN_piS(reference_genome, gff_df_all, protein_coding_genes, repeats_df, tmpdir, type_var):

    """Maps each gene to the positions of non-syn, syn mutations and also the genome size that may contain syn mutations."""

    print("running get_gene_and_genome_len_data...")

    # make the tmp
    delete_folder(tmpdir); make_folder(tmpdir)

    # map each chrom to len
    chr_to_len = get_chr_to_len(reference_genome)

    # keep only simple repeats and checks
    if len(repeats_df)==0: repeats_df = pd.DataFrame(columns=["chromosome", "start", "end"])
    else:

        repeats_df = repeats_df[repeats_df["type"].isin({"Low_complexity", "Simple_repeat"})].rename(columns={"begin_repeat_0based":"start", "end_repeat":"end"})[["chromosome", "start", "end"]]
        for f in ["start", "end"]: repeats_df[f] = repeats_df[f].apply(int)
        repeats_df = repeats_df.drop_duplicates().sort_values(by=["chromosome", "start", "end"])

    strange_chroms = set(repeats_df.chromosome).difference(set(chr_to_len))
    if len(strange_chroms)>0: raise ValueError("there are weird chroms: %s"%strange_chroms)

    # check that the gff has all the genes
    strange_genes = protein_coding_genes.difference(set(gff_df_all.upmost_parent))
    if len(strange_genes)>0: raise ValueError("There are strange genes in the gff: %s"%strange_genes)

    # keep only protein coding genes
    gff_df = gff_df_all[gff_df_all.upmost_parent.isin(protein_coding_genes)]
    if any(gff_df.end<gff_df.start): raise ValueError("The end has to be after start")

    # for SNPs, I consider 'positionsN=CDS_length*0.75' and 'positionsS=CDS_length*0.25', since there are ~75% / ~25% of positions in the codon that can yield non-syn and syn mutations, respecively.

    if type_var=="SNP":

        gene_to_lenCDS = get_gene_to_length_features(gff_df, {"CDS"}, tmpdir, repeats_df, gene_f="upmost_parent")
        gene_to_positionsN = gene_to_lenCDS*0.75
        gene_to_positionsS = gene_to_lenCDS*0.25
        sum_positionsS = sum(gene_to_positionsS.values)

    # for if INDELs, I consider positionsN and as the length of the CDS
    elif type_var=="if_INDEL":

        gene_to_positionsN = gene_to_positionsS = get_gene_to_length_features(gff_df, {"CDS"}, tmpdir, repeats_df, gene_f="upmost_parent")
        sum_positionsS = sum(gene_to_positionsS.values)

    # for SVs and CNVs, the positionsN is the length of all the gene/transcript, and positionsS is 0. However, we consider that the sum of positionsS is all the genome that has no annotations
    elif type_var in {"DEL", "DUP"}:

        # get the positions of each gene
        gene_to_positionsN = get_gene_to_length_features(gff_df, set(gff_df.feature), tmpdir, repeats_df, gene_f="upmost_parent")
        gene_to_positionsS = pd.Series({g : 0 for g in protein_coding_genes}) # no positionsS in protein-coding genes

        # get the positions that have no annotations nor repeats
        chrom_gff_df = pd.DataFrame({chrom : {"chromosome":chrom, "start":0, "end":length, "chromID":chrom, "feature":"chromosome"} for chrom, length in chr_to_len.items()}).transpose().reset_index(drop=True)

        features_to_subtract = {"exon", "snoRNA", "mRNA", "snRNA", "pseudogene", "ncRNA", "rRNA", "gene", "tRNA", "CDS", "intron"} # this comes from looking at all the gffs
        pos_fields = ["chromosome", "start", "end"]
        features_df_to_subtract = repeats_df[pos_fields].append(gff_df_all[gff_df_all.feature.isin(features_to_subtract)][pos_fields]).drop_duplicates().sort_values(by=pos_fields)

        chrom_to_positionsS = get_gene_to_length_features(chrom_gff_df, {"chromosome"}, tmpdir, features_df_to_subtract, gene_f="chromID")
        
        # define the synonymous positions 
        sum_positionsS = sum(chrom_to_positionsS.values)

    else: raise ValueError("%s is not valid"%type_var)

    # clean and return
    delete_folder(tmpdir)
    return gene_to_positionsN, gene_to_positionsS, sum_positionsS


def get_df_piN_piS_one_combination_types_vars(taxID, species, type_var, type_vars_appearance, type_vars_SimpleRepeats, all_annotations_df_species, all_vars_df, sorted_samples, sample_to_genesCorrectCoverage, sample_to_genesNoTruncation, sample_to_genesNoSVs, protein_coding_genes, ProcessedDataDir, species_to_ref_genome, gff_df, repeats_df_all, sample_to_genesNoTruncation_cladeAncestror, sample_to_genesNoSVs_cladeAncestror):

    """gets the piN_piS df for different filterings of the variants"""

    # define the file
    outdir = "%s/df_piN_piS_one_combination_types_vars_files"%(ProcessedDataDir); make_folder(outdir)
    df_piN_piS_file = "%s/%s_%s_%s_%s.py"%(outdir, species, type_var.replace("/","_"), type_vars_appearance, type_vars_SimpleRepeats)

    if file_is_empty(df_piN_piS_file):

        # log
        print("getting", species, type_vars_SimpleRepeats, type_var, type_vars_appearance)

        # filter var annot (only keep vars of this type_var and type_vars_SimpleRepeats)
        annotations_df = all_annotations_df_species[all_annotations_df_species.type_var_functional==type_var] # keep only variants of this type

        if type_var in {"DEL", "DUP", "if_INDEL"}: annotations_df["is_synonymous"] = False # all the variants are non syn
        elif type_var!="SNP": raise ValueError("invalid type_var")
        annotations_df = annotations_df[annotations_df.is_synonymous.isin({True, False})] # keep only syn or non syn
        
        if type_vars_SimpleRepeats=="only_vars_noSimpleRepeats": annotations_df = annotations_df[annotations_df.overlaps_simple_repeats==False]        
        if any(pd.isna(annotations_df.is_synonymous)): raise ValueError("there are nans in is_synonymous")


        # keep only variants that are in the annotations and also those that are not in the bad samples
        print("filtering vars")
        vars_df = all_vars_df[all_vars_df.variantID_across_samples.isin(set(annotations_df.variantID_across_samples)) & ~(all_vars_df.sampleID.apply(int).isin(sciName_to_badSamples[species])) & (all_vars_df.sampleID.isin(set(sorted_samples)))]

        # checks
        check_no_nans_series(vars_df.cladeID)

        # keep only variants that appeared recently
        if type_vars_appearance=="only_vars_recent":  vars_df = vars_df[(vars_df.variant_in_ancestral_cladeID==False)] # keep only samples that are 
            
        # merge annotations and vars
        vars_df = vars_df.merge(annotations_df[["variantID_across_samples", "Gene", "is_synonymous"]].drop_duplicates(), how="left", on="variantID_across_samples")
        
        for f in vars_df.keys():
            if f=="cladeID": continue
            if any(pd.isna(vars_df[f])): raise ValueError("nans in %s"%f)
            
        # define groups of variants to ease the parallelization
        print("preparing inputs of the data")

        somewhere_nonSyn_vars = set(annotations_df[annotations_df.is_synonymous==False].variantID_across_samples)
        everyehere_syn_vars = set(annotations_df[annotations_df.is_synonymous==True].variantID_across_samples).difference(somewhere_nonSyn_vars)

        # get the pN pS measurements for each gene and sample
        print("Running pN / pS measurements in %i threads for %i samples"%(threads, len(sorted_samples))) # maybe change to 24
        print("getting inputs_fn")
        nsamples = len(sorted_samples)
        inputs_fn = map(lambda s: (s, nsamples, vars_df[vars_df.sampleID==s], everyehere_syn_vars, sample_to_genesCorrectCoverage[s], sample_to_genesNoTruncation[s], sample_to_genesNoSVs[s], type_var, protein_coding_genes, species, taxID, sample_to_genesNoTruncation_cladeAncestror[s], sample_to_genesNoSVs_cladeAncestror[s], type_vars_appearance), sorted_samples)

        """
        print("running in parallel")
        with multiproc.Pool(threads) as pool:
            df_piN_piS = pd.concat(pool.starmap(get_df_pN_pS_one_sample, inputs_fn)) # chunksize=1
            pool.close()
            pool.terminate()

        """


        print("running not in parallel")
        df_piN_piS = pd.concat(map(lambda x: get_df_pN_pS_one_sample(x[0], x[1], x[2], x[3], x[4], x[5], x[6], x[7], x[8], x[9], x[10], x[11], x[12], x[13]), inputs_fn))

        # check
        if len(df_piN_piS)==0: raise ValueError("There can't to be some empty df")
     
        # define the positionsN and positionsS of each gene, which actually depends on the repeats and the type of variants
        if type_vars_SimpleRepeats=="all_vars": rep_df = pd.DataFrame()
        elif type_vars_SimpleRepeats=="only_vars_noSimpleRepeats": rep_df = repeats_df_all[repeats_df_all.species==species]
        else: raise ValueError("invalid type_vars_SimpleRepeats")

        gene_to_positionsN, gene_to_positionsS, sum_positionsS = get_gene_to_positions_piN_piS(species_to_ref_genome[species], gff_df, protein_coding_genes, rep_df, "%s/generating_genelen_data_tmp"%ProcessedDataDir, type_var)
            
        # add general things
        df_piN_piS["positionsS_all_genes"] = sum_positionsS
        df_piN_piS["positionsN"] = df_piN_piS.Gene.apply(lambda g: gene_to_positionsN[g])
        df_piN_piS["positionsS"] = df_piN_piS.Gene.apply(lambda g: gene_to_positionsS[g])
        df_piN_piS["species"] = species
        df_piN_piS["type_var"] = type_var
        df_piN_piS["type_vars_SimpleRepeats"] = type_vars_SimpleRepeats
        df_piN_piS["type_vars_appearance"] = type_vars_appearance
        df_piN_piS["gene_and_sample"] = df_piN_piS.Gene + "_" + df_piN_piS.sampleID

        # save
        print("saving")
        save_object(df_piN_piS, df_piN_piS_file)

    return df_piN_piS_file

def set_inf_to_nan(x):

    """Takes a float and, if inf, it is set to nan"""
    if x==np.inf: return np.nan
    else: return x

def get_cladeID_from_r_addingUnassignedClades(r):

    """Gets the row of a df with sampleID and clade ID and returns the clade as a string"""

    if pd.isna(r.cladeID): return "unassignedClade_sample%s"%r.sampleID
    else: return str(r.cladeID)

def get_df_diversity_row_piN_piS_old(df_g):

    """Takes a df for a gene and returns a series with the average values"""

    # init dict
    data_dict = {}

    # add the median of the non-syn diversity
    data_dict["median_piN"] = np.median(df_g.piN)
    data_dict["mad_piN"] = stats.median_absolute_deviation(df_g.piN, nan_policy="raise")
    data_dict["mean_piN"] = np.mean(df_g.piN)
    data_dict["std_piN"] = np.std(df_g.piN)

    # add the gene name
    data_dict["Gene"] = df_g.Gene.iloc[0]
    
    # for the different syn diversity, estimates, add the median values
    for pS_field, piS_field, piN_piS_field in [("pS", "piS", "piN_piS"), ("pS_all_genes", "piS_all_genes", "piN_piS_all_genes")]:

        # define a df that has no nans in the piS
        df_noNaN = df_g[~pd.isna(df_g[piS_field])]

        # get the median of the piS field, only considering no NaNs
        data_dict["median_%s"%piS_field] = np.median(df_noNaN[piS_field])
        data_dict["mad_%s"%piS_field] = stats.median_absolute_deviation(df_noNaN[piS_field], nan_policy="raise")
        data_dict["mean_%s"%piS_field] = np.mean(df_noNaN[piS_field])
        data_dict["std_%s"%piS_field] = np.std(df_noNaN[piS_field])

        # define a df to calculate median piN_piS. It should have a given number of pS's and no NaNs
        df_for_piN_piS = df_g[(df_g[pS_field]>=3) & ~(pd.isna(df_g[piN_piS_field]))]

        # get the median of the poN_piS_field
        data_dict["median_%s"%piN_piS_field] = np.median(df_for_piN_piS[piN_piS_field])
        data_dict["mad_%s"%piN_piS_field] = stats.median_absolute_deviation(df_for_piN_piS[piN_piS_field], nan_policy="raise")
        data_dict["mean_%s"%piN_piS_field] = np.mean(df_for_piN_piS[piN_piS_field])
        data_dict["std_%s"%piN_piS_field] = np.std(df_for_piN_piS[piN_piS_field])

        # calculate the number of samples and clades used for the piN_piS calculation
        data_dict["nsamples_for_median_%s"%piN_piS_field] = len(set(df_for_piN_piS.sampleID))
        data_dict["nclades_for_median_%s"%piN_piS_field] = len(set(df_for_piN_piS.cladeID))

    return pd.Series(data_dict)


def get_df_diversity_row_piN_piS(df_g):

    """Takes a df for a gene and returns a series with the average values"""

    # init dict
    data_dict = {}

    # add the mean of the non-syn diversity
    data_dict["mean_piN"] = np.mean(df_g.piN)
    data_dict["std_piN"] = np.std(df_g.piN)

    # add the gene name
    data_dict["Gene"] = df_g.Gene.iloc[0]
    
    # for the different syn diversity, estimates, add the mean values
    for pS_field, piS_field, piN_piS_field in [("pS", "piS", "piN_piS"), ("pS_all_genes", "piS_all_genes", "piN_piS_all_genes")]:

        # get the mean of the piS field, only considering no NaNs
        data_dict["mean_%s"%piS_field] = np.nanmean(df_g[piS_field])
        data_dict["std_%s"%piS_field] = np.nanstd(df_g[piS_field])

        # define a df to calculate mean piN_piS. It should have a given number of pS's and no NaNs
        df_for_piN_piS = df_g[(df_g[pS_field]>=3) & ~(pd.isna(df_g[piN_piS_field]))]

        # get the mean of the poN_piS_field
        data_dict["mean_%s"%piN_piS_field] = np.mean(df_for_piN_piS[piN_piS_field])
        data_dict["std_%s"%piN_piS_field] = np.std(df_for_piN_piS[piN_piS_field])

        # calculate the number of samples and clades used for the piN_piS calculation
        data_dict["nsamples_for_mean_%s"%piN_piS_field] = len(df_for_piN_piS.sampleID)
        data_dict["nclades_for_mean_%s"%piN_piS_field] = len(set(df_for_piN_piS.cladeID))

    return pd.Series(data_dict)


def get_harmonic_mean(x, y):

    """Gets the harmonic mean between two values"""

    if x==0 or y==0: return 0.0
    else: return (2*x*y)/(x+y)

def get_df_diversity_one_gene_piS_field_selection(Ig, gene, df_piN_piS_gene, selection, ngenes, nsamples, species, pS_rate, type_var, sampleID_to_clade):

    """Gets the df_piN_piS_gene for one gene and returns a df with the observed fraction of samples with that gene under the expected selection, and the probability that this happens under a modell of null evolution"""
    
    # check that samples are unique
    if len(df_piN_piS_gene)!=len(set(df_piN_piS_gene.sampleID)): raise ValueError("samples are not unique")

    # define the total number of clades
    nclades = len(set(sampleID_to_clade.values()))

    # if the type_var is SV or INDEL, set piS to 0
    if type_var in {"if_INDEL", "DEL", "DUP"}: df_piN_piS_gene["piS"] = 0
    elif type_var!="SNP": raise ValueError("type_var should be SNP")

    # define an array of min (or max) piN/piS to consider that an event is under selection
    """
    list_thresholds_piNpiS = {"SNP" : {"positive" : [1, 1.1, 1.2, 1.3, 1.4, 1.5, 2], "negative" : [0.01, 0.1, 0.5, 0.6, 0.7, 0.8, 0.9, 1]},
                              "if_INDEL" : {"positive" : [1]}, "DEL" : {"positive" : [1]}, "DUP" : {"positive" : [1]}}[type_var][selection]
    """
    list_thresholds_piNpiS = {"SNP" : {"positive" : [1], "negative" : [0.01, 0.1, 0.5, 0.6, 0.7, 0.8, 0.9, 1]},
                              "if_INDEL" : {"positive" : [1]}, "DEL" : {"positive" : [1]}, "DUP" : {"positive" : [1]}}[type_var][selection]


    # add the piN/piS ratio (there may be nans and infs)
    df_piN_piS_gene["piN_piS"] = df_piN_piS_gene.piN/df_piN_piS_gene.piS

    # init the data dict that will form a df
    data_dict_all = {threshold_piNpiS : {"Gene":gene, "nclades_total":nclades, "threshold_piNpiS":threshold_piNpiS, "species":species} for threshold_piNpiS in list_thresholds_piNpiS}

    # go through each threshold
    for threshold_piNpiS in list_thresholds_piNpiS:

        # depending on the selection, take one or another approach
        if selection=="positive": df_piN_piS_selection = df_piN_piS_gene[df_piN_piS_gene.piN_piS>threshold_piNpiS]
        elif selection=="negative": df_piN_piS_selection = df_piN_piS_gene[df_piN_piS_gene.piN_piS<threshold_piNpiS]

        # check that there are no nans
        check_no_nans_series(df_piN_piS_selection.piN_piS)

        # add the numbers and fraction of clades selected
        nsamples_selected = len(df_piN_piS_selection)
        fraction_samples_selected = nsamples_selected/nsamples
        nclades_selection = len(set(df_piN_piS_selection.cladeID))
        fraction_clades_selection = nclades_selection/nclades
        harmonicMean_fraction_samples_and_clades = get_harmonic_mean(fraction_samples_selected, fraction_clades_selection)

        data_dict_all[threshold_piNpiS]["nsamples_selected"] = nsamples_selected
        data_dict_all[threshold_piNpiS]["fraction_samples_selected"] = fraction_samples_selected
        data_dict_all[threshold_piNpiS]["nclades_selection"] = nclades_selection
        data_dict_all[threshold_piNpiS]["fraction_clades_selection"] = fraction_clades_selection
        data_dict_all[threshold_piNpiS]["harmonicMean_fraction_samples_and_clades"] = harmonicMean_fraction_samples_and_clades

    # get the df
    df_diversity_gene = pd.DataFrame(data_dict_all).transpose()

    # check vals
    for k in ["fraction_samples_selected", "fraction_clades_selection", "harmonicMean_fraction_samples_and_clades"]:
        check_no_nans_series(df_diversity_gene[k])
        if any(df_diversity_gene[k]<0) or any(df_diversity_gene[k]>1): raise ValueError("%s should be btw. 0 and 1."%k)

    # if there is selection in >1 clade (except C. tropicalis, which has too few clades) and at least 3 samples. Only for SNPs
    if type_var=="SNP" and any((df_diversity_gene.nclades_selection>=2) & (df_diversity_gene.nsamples_selected>=3)):

        # try different numbers of resamples to be faster
        for nresamples in [100, 1000, 10000]:
            #pct_progress = "%.2f"%(((Ig+1)/ngenes)*100)
            #if pct_progress.endswith("0"): print("get_df_diversity_one_gene_piS_field_selection already performed %s pct"%pct_progress)

            # pS_rate = probability of having a pS variant (of type var) in a gene for each pS SNP in all the genome

            # define the pS and pN rates for this gene. This would be the probability of having a pS or pN variant for each pS SNP in all the genome
            pS_rate_gene = pS_rate 
            pN_rate_gene = pS_rate*3 

            # check that the rates are above 0
            if pS_rate_gene<=0 or pN_rate_gene<=0 or pd.isna(pS_rate_gene) or pd.isna(pN_rate_gene) or pS_rate is None: 
                print(pS_rate_gene, pN_rate_gene)
                raise ValueError("pN_rate_gene and pS_rate_gene should be above 0")

            # for each type of vars (piN, piS), draw nresamples and add to df_resamples
            array_pS_all_genes_SNP = df_piN_piS_gene.pS_all_genes_SNP.apply(int).values # the number of SNPs all

            # define a function that takes a pS or pN rate and a positions field and returns the resampled pi values
            def get_resampled_piN_or_piS_values(rate, positions_val):

                return (pd.DataFrame(np.random.binomial(array_pS_all_genes_SNP, [rate]*len(array_pS_all_genes_SNP), size=[nresamples, len(array_pS_all_genes_SNP)]), index=list(range(nresamples)), columns=df_piN_piS_gene.sampleID).transpose()) / positions_val

            # get the resampled piN and piS values
            df_resampled_piN = get_resampled_piN_or_piS_values(pN_rate_gene, df_piN_piS_gene.iloc[0].positionsN) # the index are the samples
            df_resampled_piS = get_resampled_piN_or_piS_values(pS_rate_gene, df_piN_piS_gene.iloc[0].positionsS) # the index are the samples

            # define the piNpiS df
            df_resampled_piNpiS = df_resampled_piN/df_resampled_piS

            # for each type of threshold calculate the p value
            for threshold_piNpiS in list_thresholds_piNpiS:

                # get the df that indicates samples under selection
                if selection=="positive": df_resampled_selection_bools = df_resampled_piNpiS>threshold_piNpiS
                elif selection=="negative":  df_resampled_selection_bools = df_resampled_piNpiS<threshold_piNpiS
                else: raise ValueError("selection is invalid") 

                # map each resample to the fraction of samples under selection
                resample_to_fraction_samples_selected = df_resampled_selection_bools.apply(sum, axis=0) / nsamples

                # map each resample to the fraction of clades under selection
                df_resampled_selection_bools.index = list(map(lambda s: sampleID_to_clade[s], df_resampled_selection_bools.index))
                def get_nclades_selection_from_c_one_resample(c): return len(set(c[c==True].keys()))
                resample_to_fraction_clades_selection = df_resampled_selection_bools.apply(get_nclades_selection_from_c_one_resample, axis=0) / nclades
             
                # calculate the harmonic mean between clades and samples
                resample_to_harmonicMean_fraction_samples_and_clades = pd.DataFrame({"fraction_samples_selected": resample_to_fraction_samples_selected, "fraction_clades_selection": resample_to_fraction_clades_selection}).apply(lambda r: get_harmonic_mean(r.fraction_samples_selected, r.fraction_clades_selection), axis=1)

                # calculate p values (based on the data_dict_all)
                data_dict_obs = data_dict_all[threshold_piNpiS]
                df_diversity_gene.loc[threshold_piNpiS, "pval_fraction_samples"] = sum(resample_to_fraction_samples_selected>=data_dict_obs["fraction_samples_selected"]) / nresamples
                df_diversity_gene.loc[threshold_piNpiS, "pval_fraction_clades"] = sum(resample_to_fraction_clades_selection>=data_dict_obs["fraction_clades_selection"]) / nresamples
                df_diversity_gene.loc[threshold_piNpiS, "pval_harmonicMean_fraction_samples_and_clades"] = sum(resample_to_harmonicMean_fraction_samples_and_clades>=data_dict_obs["harmonicMean_fraction_samples_and_clades"]) / nresamples

            # checks
            check_no_nans_in_df(df_diversity_gene[["pval_fraction_samples", "pval_fraction_clades", "pval_harmonicMean_fraction_samples_and_clades"]])

            # calculate the probability that the observed piS (it could also be piS_all_genes) is found in the resampled distribution (df_resampled_piS)
            df_piN_piS_gene = df_piN_piS_gene.set_index("sampleID", drop=False)
            if list(df_piN_piS_gene.index)!=list(df_resampled_piS.index): raise ValueError("the df_piN_piS_gene index and df_resampled_piS should be the same")

            df_real_piSs = pd.DataFrame(list(map(lambda x: df_piN_piS_gene.piS, range(nresamples)))).transpose().loc[df_resampled_piS.index]
            df_real_piSs.columns = df_resampled_piS.columns

            df_piN_piS_gene["p_null_piS_above_real_piS"] = (df_resampled_piS>=df_real_piSs).apply(sum, axis=1) / nresamples
            df_piN_piS_gene["p_null_piS_below_real_piS"] = (df_resampled_piS<=df_real_piSs).apply(sum, axis=1) / nresamples
            df_piN_piS_gene["p_real_piS_not_extreme"] = df_piN_piS_gene[["p_null_piS_above_real_piS", "p_null_piS_below_real_piS"]].apply(min, axis=1)

            array_p_real_piS_not_extreme = df_piN_piS_gene["p_real_piS_not_extreme"].values

            # add to df
            df_diversity_gene["mean_p_real_piS_not_extreme"] = np.mean(array_p_real_piS_not_extreme)
            df_diversity_gene["std_p_real_piS_not_extreme"] = np.std(array_p_real_piS_not_extreme)
            df_diversity_gene["median_p_real_piS_not_extreme"] = np.median(array_p_real_piS_not_extreme)
            df_diversity_gene["mad_p_real_piS_not_extreme"] = stats.median_absolute_deviation(array_p_real_piS_not_extreme, nan_policy="raise")
            df_diversity_gene["fraction_samples_extreme_piS_p<0.05"] =  np.divide(sum(array_p_real_piS_not_extreme<0.05), len(array_p_real_piS_not_extreme))
            df_diversity_gene["fraction_samples_extreme_piS_p<0.1"] =  np.divide(sum(array_p_real_piS_not_extreme<0.1), len(array_p_real_piS_not_extreme))

            # clean
            del df_resampled_piN
            del df_resampled_selection_bools
            del df_resampled_piS
            del df_resampled_piNpiS
            del df_real_piSs

            # if the pval is very high, just skip it
            min_pval = (1/nresamples)*10
            #if all((df_diversity_gene[["pval_fraction_samples", "pval_fraction_clades", "pval_harmonicMean_fraction_samples_and_clades"]]>min_pval).apply(all)): break
            if all((df_diversity_gene[["pval_harmonicMean_fraction_samples_and_clades"]]>min_pval).apply(all)): break


    return df_diversity_gene.reset_index(drop=True)

def get_df_diversity_per_gene(df_piN_piS, species, type_var, type_vars_appearance, type_vars_SimpleRepeats, ProcessedDataDir, threads, plots_dir, sampleID_to_clade):

    """Gets the df_diversity (it has, for a given gene, the fraction of samples with positive or negative selection and the probability to see that fraction by a random model of evolution) in parallel"""

    # init df diversity
    df_diversity_all = pd.DataFrame()

    # load dfs of other types of variants that will be useful
    df_piN_piS_SNP = load_object("%s/df_piN_piS_one_combination_types_vars_files/%s_SNP_%s_%s.py"%(ProcessedDataDir, species, type_vars_appearance, type_vars_SimpleRepeats))

    # to debug, keep only CR_01270C_A in C. albicans, which gave errors.
    #df_piN_piS = df_piN_piS[df_piN_piS.Gene=="CR_01270C_A"]
    #df_piN_piS_SNP = df_piN_piS_SNP[df_piN_piS_SNP.Gene=="CR_01270C_A"]

    # keep only samples that have some SNPs in all_vars, in both piN_piS and piN_piS_SNPs
    df_piN_piS_SNP = df_piN_piS_SNP[df_piN_piS_SNP.pS_all_genes>0]
    samples_with_SNPs_all_vars = set(df_piN_piS_SNP.sampleID)
    print("There are %i/%i samples with some SNP in some gene"%(len(samples_with_SNPs_all_vars), len(set(df_piN_piS.sampleID))))
    df_piN_piS = df_piN_piS[df_piN_piS.sampleID.isin(samples_with_SNPs_all_vars)]

    # redefine the sampleID_to_clade based on df_piN_piS_SNP, so that all clades that are in df_piN_piS_SNP are considered
    df_piN_piS_SNP["cladeID"] = df_piN_piS_SNP.sampleID.apply(lambda s: sampleID_to_clade[s]) # samples with no clades are set as nan
    df_piN_piS_SNP["cladeID"] = df_piN_piS_SNP.apply(get_cladeID_from_r_addingUnassignedClades, axis=1); check_no_nans_series(df_piN_piS_SNP.cladeID)
    sampleID_to_clade = dict(df_piN_piS_SNP[["sampleID", "cladeID"]].drop_duplicates().set_index("sampleID").cladeID) # this already considers only samples with actual clades

    # add the cladeID to df_piN_piS
    df_piN_piS["cladeID"] = df_piN_piS.sampleID.map(sampleID_to_clade); check_no_nans_series(df_piN_piS.cladeID)

    # for SNPs, keep only genes that have a positive pS rate
    if type_var=="SNP":

        # get the pS rate of SNPs
        gene_to_pS_rate = get_syn_mutation_rates(ProcessedDataDir, species, type_var, type_vars_SimpleRepeats, set(df_piN_piS.Gene))

        # keep only genes with some pS rate
        genes_positive_pS_rate = set(gene_to_pS_rate[gene_to_pS_rate>0].index)
        df_piN_piS = df_piN_piS[df_piN_piS.Gene.isin(genes_positive_pS_rate)]

        # add the number of syn SNPs
        sample_to_pS_all_genes_SNP = dict(df_piN_piS_SNP[["sampleID", "pS_all_genes"]].drop_duplicates().set_index("sampleID").pS_all_genes)
        df_piN_piS["pS_all_genes_SNP"]  = df_piN_piS.sampleID.map(sample_to_pS_all_genes_SNP); check_no_nans_series(df_piN_piS.pS_all_genes_SNP)

        # checks
        for f in ["piS", "piN"]: check_no_nans_series(df_piN_piS[f])

    # for non-snps, the pS rate makes no sense
    else: gene_to_pS_rate = {g : None for g in set(df_piN_piS.Gene)}

    # define as all_samples those that have some SNP in this type of variants, and also those that have variants in these types of variants
    all_samples = set(df_piN_piS_SNP.sampleID).union(set(df_piN_piS.sampleID)); print("there are %i samples"%(len(all_samples)))
    nsamples = len(all_samples)

    # go through different types of selection and piN/piS fields
    for selection in ["positive", "negative"]:
        print(selection)

        # discard comparisons
        if type_var in {"DEL", "DUP", "if_INDEL"} and selection=="negative": continue

        # discard negative selection to ease measurements
        if selection=="negative": continue

        # get a df diversity row for each gene, in parallel
        ngenes = len(set(df_piN_piS.Gene))
        sorted_genes = sorted(set(df_piN_piS.Gene))
        df_piN_piS = df_piN_piS.set_index("Gene", drop=False)


        print("running get_df_diversity_one_gene_piS_field_selection on %i threads. Go through chunks of threads..."%threads)
        df_diversity = pd.DataFrame()
        chunk_size = threads*4
        for Ic, chunk_genes in enumerate(chunks(sorted_genes, chunk_size)):
            print("chunk %i/%i"%(Ic+1, int(ngenes/chunk_size)))

            # get the inputs
            chunk_inputs_fn = [(Ig, gene, df_piN_piS_gene, selection, ngenes, nsamples, species, gene_to_pS_rate[gene], type_var, sampleID_to_clade) for Ig, (gene, df_piN_piS_gene) in enumerate(df_piN_piS.loc[chunk_genes].reset_index(drop=True).groupby("Gene"))]

            # run in parallel
            with multiproc.Pool(threads) as pool:
                df_diversity = df_diversity.append(pd.concat(pool.starmap(get_df_diversity_one_gene_piS_field_selection, chunk_inputs_fn))).reset_index(drop=True)
                pool.close()
                pool.terminate()   

            # not in parallel
            #df_diversity = df_diversity.append(pd.concat(list(map(lambda x: get_df_diversity_one_gene_piS_field_selection(x[0], x[1], x[2], x[3], x[4], x[5], x[6], x[7], x[8], x[9]), chunk_inputs_fn)))).reset_index(drop=True)


        # checks
        df_diversity["gene_and_threshold_piNpiS"] = df_diversity.Gene + "_" + df_diversity.threshold_piNpiS.apply(str)
        if len(df_diversity)!=len(set(df_diversity.gene_and_threshold_piNpiS)): raise ValueError("gene_and_threshold_piNpiS is not unique")
        if set(df_diversity.Gene)!=set(df_piN_piS.Gene): raise ValueError("There should be one row for each gene in df_piN_piS")

        # add fields
        df_diversity["selection"] = selection
        df_diversity["ngenes"] = ngenes
        df_diversity["nsamples"] = nsamples

        df_diversity_all = df_diversity_all.append(df_diversity)

    return df_diversity_all


def get_syn_mutation_rates(ProcessedDataDir, species, type_var, type_vars_SimpleRepeats, all_genes):

    """For 'only_recent_vars', it calculates the synonymous mutation rates. Only for SNPs """

    #print("running get_df_piN_piS_resampled_pSs")

    # there are two syn_mut rates:
    # pS_rate = probability of having a pS variant (of type var) in a gene for each pS SNP in all the genome
    # pS_all_genes_rate = probability of having a pS variant (of type var) in any gene for each pS SNP in the genome

    print("calculating syn mutation rates...")
    if type_var!="SNP": raise ValueError("this only makes sense for SNPs")

    # define some thresholds
    min_n_samples = 3 # before 3
    min_n_SNPs_all_genes = 10 # before 10

    # load df of all_vars
    df_piN_piS_all_vars = load_object("%s/df_piN_piS_one_combination_types_vars_files/%s_%s_all_vars_%s.py"%(ProcessedDataDir, species, type_var.replace("/","_"), type_vars_SimpleRepeats))
    if len(df_piN_piS_all_vars)!=len(set(df_piN_piS_all_vars.gene_and_sample)): raise ValueError("gene_and_sample should be unique")

    # load df of all_vars SNPs
    df_piN_piS_all_vars_SNP = load_object("%s/df_piN_piS_one_combination_types_vars_files/%s_SNP_all_vars_%s.py"%(ProcessedDataDir, species, type_vars_SimpleRepeats))
    if len(df_piN_piS_all_vars_SNP)!=len(set(df_piN_piS_all_vars_SNP.gene_and_sample)): raise ValueError("gene_and_sample should be unique")

    # merge the two dfs. We keep only SNP genes and samples because they are the basis of the calculation
    merge_fields = ["pS", "pS_all_genes", "Gene", "sampleID"]
    df_piN_piS_merged = df_piN_piS_all_vars_SNP[merge_fields].merge(df_piN_piS_all_vars[merge_fields], how="left", on=["Gene", "sampleID"], validate="one_to_one", suffixes=("_SNP", "_otherVar"))

    check_no_nans_in_df(df_piN_piS_merged)
    if len(df_piN_piS_merged)!=len(df_piN_piS_all_vars_SNP): raise ValueError("The len changed")

    # checks
    if type_var=="SNP" and any(df_piN_piS_merged.pS_SNP!=df_piN_piS_merged.pS_otherVar): raise ValueError("for SNPs, the pS should be kept")

    # calculate pS_rate for each gene
    df_piN_piS_merged_pS_rate = df_piN_piS_merged[(df_piN_piS_merged.pS_otherVar>=1) & (df_piN_piS_merged.pS_all_genes_SNP>=min_n_SNPs_all_genes)] # keep rows w some SNP all genes, also some gene var
    df_piN_piS_merged_pS_rate["pS_rate"] = df_piN_piS_merged_pS_rate.pS_otherVar / df_piN_piS_merged_pS_rate.pS_all_genes_SNP

    def get_pS_rate_per_gene(df_g):
        if len(df_g)<min_n_samples: return -1 # you can't calculate the pS rate for this gene if there are not enough samples with mutations in this gene
        else: return np.mean(df_g.pS_rate)

    gene_to_pS_rate = df_piN_piS_merged_pS_rate.groupby("Gene").apply(get_pS_rate_per_gene)
    missing_genes = list(all_genes.difference(set(gene_to_pS_rate.index)))
    gene_to_pS_rate = gene_to_pS_rate.append(pd.Series([-1]*len(missing_genes), index=missing_genes))
    if len(gene_to_pS_rate)!=len(set(gene_to_pS_rate.index)): raise ValueError("the index should be unique")
    if any(gene_to_pS_rate>1): raise ValueError("there can't be genes with a rate above 1")

    return gene_to_pS_rate


def get_df_resampled_piS_one_sample(df_piN_piS_sample, Is, nsampleIDs, nresamples):

    """Runs the simulation of piS values for one sample"""

    # report
    print("working on sample %i/%i"%(Is+1, nsampleIDs))

    # change some fields
    df_piN_piS_sample["pS_all_genes_SNP"] = df_piN_piS_sample.pS_all_genes_SNP.apply(int)

    # get the resampled pS's
    pS_resamples_df = pd.DataFrame(np.random.binomial(df_piN_piS_sample.pS_all_genes_SNP.values, df_piN_piS_sample.pS_rate.values, size=[nresamples, len(df_piN_piS_sample)]), index=list(range(nresamples)), columns=df_piN_piS_sample.index).transpose()

    # get the resampled piS's (divide by positionsS)
    idx_to_positionsS = dict(df_piN_piS_sample.positionsS)
    df_resampled_piS_sample = pS_resamples_df.apply(lambda r: r/idx_to_positionsS[r.name], axis=1)

    # clean and return
    del pS_resamples_df

    return df_resampled_piS_sample


def get_df_resampled_piSs(df_piN_piS, gene_to_pS_rate, resamples, type_var, threads, plots_dir, tree, type_data_str, df_diversity_file):
    
    """Gets a df with the resampled piSs (for each gene), based on 1,000 resamples in each sample and gene"""

    print("running get_df_resampled_piSs")


    ########## GET RESAMPLED DF #############

    # define file
    df_resampled_piS_file = "%s.df_resampled_piS.py"%df_diversity_file
    if file_is_empty(df_resampled_piS_file):

        # checks
        if any(gene_to_pS_rate==0): raise ValueError("There can't be 0 in gene_to_pS_rate")
        check_no_nans_series(gene_to_pS_rate)
        if any(df_piN_piS.pS_all_genes_SNP<=0): raise ValueError("There have to be some pS_all_genes_SNP to work")

        # keep df
        print("copying")
        df_piN_piS = cp.deepcopy(df_piN_piS).set_index("gene_and_sample", drop=False)

        # debug
        #df_piN_piS = df_piN_piS[df_piN_piS.sampleID.isin(sorted(set(df_piN_piS.sampleID))[0:4])]

        # define the initial index
        initial_gene_and_sample = sorted(df_piN_piS.index)

        # add the pS_rate (probability of having a pS variant (of this type var) in a gene for each pS SNP in all the genome)
        print("addig pS_rate")
        df_piN_piS["pS_rate"] = df_piN_piS.Gene.map(gene_to_pS_rate); check_no_nans_series(df_piN_piS["pS_rate"])

        # get a df with no pS_rate
        idx_rows_no_pS_rate = ((df_piN_piS.pS_rate==-1) | (df_piN_piS.positionsS==0))
        gene_and_sample_no_pS_rate = df_piN_piS[idx_rows_no_pS_rate].index
        gene_and_sample_pS_rate = df_piN_piS[~idx_rows_no_pS_rate].index

        # for SVs and CNVs, check that this is the only one
        if type_var in {"coverageCNV", "SV"}:
            if len(gene_and_sample_pS_rate)>0: raise ValueError("there can't be gene_and_sample_pS_rate in coverageCNV or SV")
        elif type_var in {"SNP", "IN/DEL"}:
            if len(gene_and_sample_pS_rate)==0: raise ValueError("There has to be some gene_and_sample_pS_rate in SNPs and INDELs")
        else: raise ValueError("type var not considered")

        # define an index for the resamples
        columns_resamples = np.array(list(range(resamples)))

        # init df with the gene_and_sample_no_pS_rate
        df_resampled_piS = pd.DataFrame(-1, index=gene_and_sample_no_pS_rate, columns=columns_resamples)

        # add the actual resampled piSs
        if len(gene_and_sample_pS_rate)>0:
            print("getting resampled piS...")

            # get df and order
            df_piN_piS = df_piN_piS.loc[gene_and_sample_pS_rate]

            # go through chunks of the df (i.e. each sample) (in parallel)
            nsamples = len(set(df_piN_piS.sampleID))
            inputs_fn = [(df, Is, nsamples, resamples) for Is, (s, df) in enumerate(df_piN_piS.groupby("sampleID"))]
            print("getting resampled piSs in %i threads"%(threads))
            
            # run in parallel
            with multiproc.Pool(threads) as pool:
                df_resampled_piS_actual_resamples = pd.concat(pool.starmap(get_df_resampled_piS_one_sample, inputs_fn, chunksize=1))
                pool.close()
                pool.terminate()   

            # not in parallel
            #df_resampled_piS_actual_resamples = pd.concat(list(map(lambda x: get_df_resampled_piS_one_sample(x[0], x[1], x[2], x[3]), inputs_fn)))


            # keep
            df_resampled_piS = df_resampled_piS.append(df_resampled_piS_actual_resamples)

        # check
        if sorted(df_resampled_piS.index)!=initial_gene_and_sample: raise ValueError("not all gene and sample are included")

        print("saving df")
        save_object(df_resampled_piS, df_resampled_piS_file)

    print("loading df_resampled_piS")
    df_resampled_piS = load_object(df_resampled_piS_file)

    ###############

    ############# PLOT ##############
    print("plotting")

    # plot the p to find the actual value or more extreme in the syn distribution
    plot_filename = "%s/heatmap_observed_piS_fit_null_model.pdf"%plots_dir
    if file_is_empty(plot_filename):


        # get only genes with some syn variation
        interesting_genes = set(gene_to_pS_rate[gene_to_pS_rate>0].index)
        if len(interesting_genes)>0:

            # get the dfs
            df_piN_piS = df_piN_piS[df_piN_piS.Gene.isin(interesting_genes)].set_index("gene_and_sample", drop=False)
            df_resampled_piS = df_resampled_piS.loc[df_piN_piS.index]
            if list(df_resampled_piS.index)!=list(df_piN_piS.index): raise ValueError("indices are not the same")

            # calculate the probabilities
            print("calculating probabilities")
            df_real_piSs = pd.DataFrame(list(map(lambda x: df_piN_piS.real_piS, range(resamples)))).transpose().loc[df_resampled_piS.index]
            df_real_piSs.columns = df_resampled_piS.columns

            print("calculating p_null_piS_above_real_piS")
            df_piN_piS["p_null_piS_above_real_piS"] = (df_resampled_piS>=df_real_piSs).apply(sum, axis=1) / resamples

            print("calculating p_null_piS_below_real_piS")
            df_piN_piS["p_null_piS_below_real_piS"] = (df_resampled_piS<=df_real_piSs).apply(sum, axis=1) / resamples

            print("calculating p_real_piS_not_extreme")
            df_piN_piS["p_real_piS_not_extreme"] = df_piN_piS[["p_null_piS_above_real_piS", "p_null_piS_below_real_piS"]].apply(min, axis=1)

            # get the number
            n_gene_sample_with_low_p = sum(df_piN_piS.p_real_piS_not_extreme<0.05)

            # define the sorted samples
            tree = cp.deepcopy(tree); tree.prune(set(df_piN_piS.sampleID))
            sorted_samples = tree.get_leaf_names()

            # create a square df to plot
            print("creating square df")
            max_p = 0.1

            def convert_nan_to_half_and_top_to_half(x):
                if pd.isna(x): return max_p
                else: return min([x, max_p])

            df_plot = df_piN_piS.pivot(index="sampleID", columns="Gene", values="p_real_piS_not_extreme").applymap(convert_nan_to_half_and_top_to_half).loc[sorted_samples]
            df_plot = df_plot[sorted(df_plot.columns)]

            # create heatmap 
            print("generating plot")
            cm = sns.clustermap(df_plot, row_cluster=False, col_cluster=True, cbar_kws={'label':"p(real piS in null model (max at %s))"%max_p}, center=0.05, cmap="vlag") # cmap="rocket_r"


            hm_pos = cm.ax_heatmap.get_position()
            pixels_per_square = 0.002
            hm_height = len(df_plot)*pixels_per_square
            hm_y0 = (hm_pos.y0+hm_pos.height)-hm_height
            cm.ax_heatmap.set_position([hm_pos.x0, hm_y0, hm_pos.width, hm_height]); hm_pos = cm.ax_heatmap.get_position()

            cm.ax_heatmap.set_xticklabels([])
            cm.ax_heatmap.set_yticklabels([])
            cm.ax_heatmap.set_xlabel("genes")

            cm.ax_col_dendrogram.set_title("%s\n%i/%i (fraction %.3f) observed gene-sample piS don't fit null model (p<0.05, %i simulations)\n"%(type_data_str, n_gene_sample_with_low_p, len(df_piN_piS), n_gene_sample_with_low_p/len(df_piN_piS), resamples))

            #plt.show()

            # save
            cm.savefig(plot_filename, bbox_inches="tight")
            del df_real_piSs

    #################################

    return df_resampled_piS

def get_df_resampled_piS_all_genes(df_piN_piS, pS_all_genes_rate, nresamples, type_var, threads, plots_dir):

    """Gets a df with resampled piS_all_genes based on the SNP content. It adds related stats to df_piN_piS"""

    # check
    if pS_all_genes_rate<=0 or pS_all_genes_rate>1: raise ValueError("pS_all_genes_rate has to be btw 0 1nd 1")
    if type_var=="SNP" and pS_all_genes_rate!=1: raise ValueError("for SNPs, the pS_all_genes_rate should be 1")
    if len(set(df_piN_piS.positionsS_all_genes))!=1: raise ValueError("all positionsS_all_genes should be the same")

    # define the positionsS_all_genes
    positionsS_all_genes = df_piN_piS.positionsS_all_genes.iloc[0]
    if positionsS_all_genes<100: raise ValueError("there can't be so few positionsS_all_genes")

    # keep only one row per sample (note that all genes are the same)
    df_piN_piS_use = cp.deepcopy(df_piN_piS.set_index("sampleID", drop=False)[["pS_all_genes_SNP", "sampleID"]].drop_duplicates())

    # get the resampled pS's for all genes
    pS_all_genes_resamples_df = pd.DataFrame(np.random.binomial(df_piN_piS_use.pS_all_genes_SNP.apply(int).values, [pS_all_genes_rate]*len(df_piN_piS_use), size=[nresamples, len(df_piN_piS_use)]), index=list(range(nresamples)), columns=df_piN_piS_use.index).transpose()

    # get the resampled piS's (divide by positionsS)
    df_resampled_piS_all_genes = pS_all_genes_resamples_df / positionsS_all_genes
    if len(set(df_resampled_piS_all_genes.index))!=len(df_resampled_piS_all_genes): raise ValueError("index should be unique")
    
    # clean
    del pS_all_genes_resamples_df

    return df_resampled_piS_all_genes


def get_df_diversity_piN_piS_all_steps(ProcessedDataDir, species, type_var, type_vars_appearance, type_vars_SimpleRepeats, df_piN_piS, PlotsDir, sampleID_to_clade, threads):

    """Gets the df diversity for the piN piS analysis with all steps"""

    # log
    #print("Getting df_diversity")

    # define files
    outdir = "%s/df_diversities_piN_piS_analysis"%ProcessedDataDir
    make_folder(outdir)
    df_diversity_file = "%s/%s_%s_%s_%s.py"%(outdir, species, type_var.replace("/","_"), type_vars_appearance, type_vars_SimpleRepeats)

    # print the number of samples  that have no variants
    #nsamples_no_pS_all_genes = len(set(df_piN_piS[df_piN_piS.pS_all_genes==0].sampleID))
    #if nsamples_no_pS_all_genes>0: print(species, type_var, type_vars_appearance, type_vars_SimpleRepeats, nsamples_no_pS_all_genes, "samples with no pS_all_genes")

    if file_is_empty(df_diversity_file):

        # define plots dir
        plots_dir_all = "%s/get_df_diversity_piN_piS_all_steps_checks"%PlotsDir;
        #delete_folder(plots_dir_all)
        make_folder(plots_dir_all)
        plots_dir = "%s/%s_%s_%s_%s"%(plots_dir_all, species, type_var.replace("/","_"), type_vars_appearance, type_vars_SimpleRepeats); make_folder(plots_dir)

        # for if_INDEL, remove the positions S
        if type_var=="if_INDEL": df_piN_piS["positionsS"] = 0

        # checks 
        for f in ["positionsN", "positionsS_all_genes"]:
            if any(df_piN_piS[f]<=0): raise ValueError("There can't be <0 vals in %s"%f)

        if type_var in {"DEL", "DUP", "if_INDEL"} and not all((df_piN_piS.positionsS==0)): raise ValueError("the positionsS for each gene should be 0 in SVs")
        if type_var not in {"DEL", "DUP", "if_INDEL", "SNP"}: raise ValueError("type_var %s is invalid"%type_var)

        # add more fields
        df_piN_piS["piN"] = (df_piN_piS.pN / df_piN_piS.positionsN)
        if type_var=="SNP": df_piN_piS["piS"] = (df_piN_piS.pS / df_piN_piS.positionsS).apply(set_inf_to_nan)

        # check nans
        for f in df_piN_piS.keys():
            if any(pd.isna(df_piN_piS[f])): raise ValueError("There can't be nans in %s: %s"%(f, df_piN_piS[f]))

        # get a df with the per-gene diversity
        print("getting df_diversity per gene")
        df_diversity = get_df_diversity_per_gene(df_piN_piS, species, type_var, type_vars_appearance, type_vars_SimpleRepeats, ProcessedDataDir, threads, plots_dir, sampleID_to_clade)

        # add fields
        df_diversity["species"] = species
        df_diversity["type_var"] = type_var
        df_diversity["type_vars_SimpleRepeats"] = type_vars_SimpleRepeats
        df_diversity["type_vars_appearance"] = type_vars_appearance

        # save
        print("saving")
        save_object(df_diversity, df_diversity_file)

# define variables important for get_type_var_functional_from_annot_df_r
DUP_CONSEQUENCES = {"transcript_amplification"}
CNV_DEL_CONSEQUENCES = {"transcript_ablation"}

def get_type_var_functional_from_annot_df_r(r):

    """Takes an r from annot_df and return the type var functional.
    
    SNP: Anly SNP that is not truncating
    DEL: Any variant that breaks the protein (small vars) or that breaks the transcript (SVs and CNVs)
    DUP: Any variant that duplicated the whole transcript
    if_INDEL: INDELs that are not frameshifting


    """

    # define the consequences set
    consequences_set = set(r.Consequence.split(","))

    # SNPs that are syn or non syn and non truncating are SNPs
    if r.type_var=="SNP" and (r.is_truncating_small_variant is False and not pd.isna(r.is_synonymous)): return "SNP"

    # INDELs that are not truncating
    elif r.type_var=="IN/DEL" and r.is_synonymous is True: return "if_INDEL"

    # SVs and CNVs that are duplications
    elif r.type_var in {"SV", "coverageCNV"} and len(consequences_set.intersection(DUP_CONSEQUENCES))>0: return "DUP"

    # truncating variants
    elif r.type_var in {"SNP", "IN/DEL"} and r.is_truncating_small_variant is True: return "DEL" # small variants that are truncating    
    elif r.type_var in {"SV", "coverageCNV"} and len(consequences_set.intersection(CNV_DEL_CONSEQUENCES))>0: return "DEL" # SVs and CNVs  that are deletions of the whole gene
    elif r.type_var=="SV" and r.is_synonymous is False: return "DEL" # non-syn SVs are truncating (unless they are in DUP_CONSEQUENCES)

    # other things that we are not interested in the selection analysis
    elif r.is_protein_coding_gene is False: return "unclassified"
    elif r.type_var in {"SV", "coverageCNV"} and r.is_synonymous is True: return "unclassified" # SVs that are not overlapping any gene (synonymous)
    elif r.type_var in {"SNP", "IN/DEL"} and pd.isna(r.is_synonymous): return "unclassified" # small vars that have no syn-no-syn assignation (intergenic stuff)
    elif r.type_var=="coverageCNV" and r.is_synonymous is False and len(consequences_set.intersection(CNV_DEL_CONSEQUENCES))==0: return "unclassified" # CNVs  that are overlapping transcript but are not whole transcript deletions (we don't know what they are!!!!)
    elif r.type_var=="IN/DEL" and r.is_synonymous is False and r.is_truncating_small_variant is False: return "unclassified" # indels that are not truncating but also non syn


    # catch errors
    else: raise ValueError("invalid val: %s, %s"%(r, consequences_set))

def get_df_diversity_with_p_compared_to_other_genes(df_diversity, score_f):

    """Gets the df_diversity of a piN/piS run and adds the probability of """


    # checks
    if len(df_diversity)!=len(set(df_diversity.Gene)): raise ValueError("genes should be unique")
    check_no_nans_series(df_diversity[score_f])

    # get the empiric distribution
    print("calculate empiric distribution")
    all_vals = list(df_diversity[score_f])
    nbootstraps = 10000
    empriric_distribution_score = np.array(list(map(lambda x: random.choice(all_vals), range(nbootstraps))))

    print(len(empriric_distribution_score))

    # add the p value of being grater than the empiric distribution
    print("adding to df")
    df_diversity["p_high_observed_vals_%s"%score_f] = df_diversity[score_f].apply(lambda x: empriric_distribution_score>=x).apply(sum) / nbootstraps

    return df_diversity

def get_df_diversity_piN_piS(gene_features_df, unique_SV_CNV_df, unique_small_vars_df, DataDir, ProcessedDataDir, species_to_gff, df_coverage_per_gene, species_to_srr_to_sampleID, species_to_ref_genome, repeats_df_all, metadata_df, species_to_tree, PlotsDir, min_pct_overlap_CNV_simpleRepeats=25, threads=4, replace=False):

    """This function is useful to calculate the statisticts about piN and piS across genes in each sample and for different types of variants. Check get_df_variationRatePerGene to develop this, which is an earlier version. I will define some concepts:

    - pN: Number of non-synonymous mutations found in a gene in a given sample.
    - pS: Number of synonymous mutations found in a gene in a given sample.
    - positionsN: Number of positions where a non-syn mutation can appear.
    - positionsS: Number of positions where a syn mutation can appear.
    - piN = (pN / positionsN) --> non-syn diversity
    - piS = (pS / positionsS) --> syn diversity

    Other considerations:

    - For SNPs I will consider 'positionsN=CDS_length*0.75' and 'positionsS=CDS_length*0.25', since there are ~75% / ~25% of positions in the codon that can yield non-syn and syn mutations, respecively.

    - For INDELs I will consider as pN the number of stop-generating or frameshifting indels, as pS the number of non-frameshifting INDELs. THe positionsN and positionsS would be the CDS_length. I will also calculate a global piS.

    - For diploid SNPs and INDELs, I will resample the hetero. variants 100x for each sample and gene, and calculate the median pN and pS across resamples as the true values.

    - For SNPs and INDELs, I will consider only genes and samples were there is a median coverage >24x and >95% of the gene with coverage. In addition, I will consider genes and samples with no truncation or SV / CNV.

    - For coverage-based SVs and CNVs in a given gene, I will define as pN the number of SVs or CNVs that overlap the transcript, and as positionsN the length of the transcript. There is no easy way to define SVs and CNVs related to a gene but that are neutral (the pS). In addition, it is difficult to define positionsS, since we don't know which positions around a gene can yield neutral SVs. It is likely that this idea of 'neutral pS' or 'positionsS' does not make much sense in SVs-CNVs. Still, the idea of piN / piS is interesting to pinpoint genes that have higher or lower diversity of SVs.

    In SNPs, I understand that the idea of piN/piS is useful to correct for 1) inter-gene differences in intrinsic mutation rate and 2) divergence from the reference genome. We don't know if there are differences in intrinsic SV-CNV mutation rates (pr how they work), and we could assume that they don't exist for this analysis. However, it is still interesting to consider that there will be varying levels of divergence to the reference sequence in each sample, so we'd need to normalize that.  I propose to make piS just resemble this divergence. I will calculate, for each sample, a pS as the number of SVs or CNVs that do not affect any transcripts (considered synonymous), and the positionsS as the length of the genome that are not transcripts. This would assume that all genes have the same piS, which is the best we can do if we don't know how to define gene-specific pS.

    - For the average piN/piS calculations, I will only consider genes that have a piN/piS measurement in at least 2 clades and >=10 samples. This is to discard averaged numbers based on very few or very similar samples. Note that samples with 0 pN will not be considered for the piN/piS calculation.
    
    - I will do the analysis with or without filtering of variants overlapping simple repeats.

    """


    # define the files
    make_folder(ProcessedDataDir) # we'd pass a new ProcessedDataDir
    df_diversity_all_file = "%s/df_diversity_all_piN_piS_across_genes.py"%ProcessedDataDir

    # if the df_diversity_all_file is not existing
    if file_is_empty(df_diversity_all_file) or replace is True:
        print("getting the df_diversity_all_file")


        # keep only some fields of metadata_df to ensure that the clade is proerly taken
        metadata_df = cp.deepcopy(metadata_df[["cladeID_clonal_tshdSNPsKb_1", "species_name", "sampleID", "type"]])

        # init  the df_diversity
        df_diversity_all = pd.DataFrame()

        # define general lists
        #all_type_var = ["SNP", "coverageCNV", "SV", "IN/DEL"] # old
        all_type_var = ["SNP", "DEL", "DUP", "if_INDEL"] # grouping different types of vars (inframe INDELs are a different cathegory)
        all_type_vars_appearance = ["all_vars", "only_vars_recent"] 
        #all_type_vars_appearance = ["only_vars_recent"] 
        #all_type_vars_SimpleRepeats = ["all_vars", "only_vars_noSimpleRepeats"]
        all_type_vars_SimpleRepeats = ["only_vars_noSimpleRepeats"]

        # init the jobs of the diversity
        jobs_get_diversity_df = []

        # go through each type of overlaps_simple_repeats, type_var and species and calculate diversity measures per gene        
        for taxID, species in taxID_to_sciName.items():
            print(species)

            #if species!="Candida_parapsilosis": continue # skip

            # keep some dfs
            unique_SV_CNV_df = cp.deepcopy(unique_SV_CNV_df)
            unique_small_vars_df = cp.deepcopy(unique_small_vars_df)
            gene_features_df = cp.deepcopy(gene_features_df)
            repeats_df_all = cp.deepcopy(repeats_df_all)

            # map each sample to a clade
            sampleID_to_clade = dict(metadata_df[metadata_df.species_name==species].set_index("sampleID")["cladeID_clonal_tshdSNPsKb_1"])
    
            # generate the sampleID_to_clade_file
            sampleID_to_clade_file = "%s/%s_sampleID_to_clade.py"%(ProcessedDataDir, species)
            save_object(sampleID_to_clade, sampleID_to_clade_file)

            # load gff as a df
            print("getting gff")
            gff_df = load_gff3_intoDF(species_to_gff[species])

            # define the pseudogenes to remove them from the analysis. Note that we don't want to consider pseudogenes or nc regions because we don't know how selection works on them
            gene_features_df_s = gene_features_df[(gene_features_df.species==species)]
            pseudogenes = set(gene_features_df_s[gene_features_df_s.feature_type.isin({"pseudogene", "pseudogene|Uncharacterized"})].gff_upmost_parent)

            # check if all get_df_piN_piS_one_combination_types_vars have been ran
            all_get_df_piN_piS_one_combination_types_vars_ran = all([not file_is_empty("%s/df_piN_piS_one_combination_types_vars_files/%s_%s_%s_%s.py"%(ProcessedDataDir, species, type_var.replace("/","_"), type_vars_appearance, type_vars_SimpleRepeats)) for type_var in all_type_var for type_vars_appearance in all_type_vars_appearance for type_vars_SimpleRepeats in all_type_vars_SimpleRepeats])


            # only create heavy dfs if all_get_df_piN_piS_one_combination_types_vars_ran is False
            if all_get_df_piN_piS_one_combination_types_vars_ran is False:

                # get the annotations of this species (get intergenic annotations and remove the pseudogenes)
                #print("filtering annotations")
                all_annotations_df_species_file = "%s/%s_all_annotations_df_species_filt_piN_piS.py"%(ProcessedDataDir, species)

                if file_is_empty(all_annotations_df_species_file) or replace is True:

                    # get annotations for all species
                    all_annotations_df = get_all_annotations_df_for_df_diversity_piN_piS(unique_SV_CNV_df, unique_small_vars_df, DataDir, ProcessedDataDir, species_to_gff, min_pct_overlap_CNV_simpleRepeats=min_pct_overlap_CNV_simpleRepeats, threads=threads, replace=replace).set_index("species")

                    # keep only annotations for these species
                    all_annotations_df_species = all_annotations_df.loc[{species}].set_index("Gene", drop=False) # keep species
                    valid_genes = (set(all_annotations_df_species[all_annotations_df_species.is_protein_coding_gene==True].Gene).union({"-"})).difference(pseudogenes)
                    all_annotations_df_species = all_annotations_df_species.loc[valid_genes] # keep only genes that are protein coding or intergenic, not pseudogenes

                    # add the type_var_functional (groups into functionally relevant cathegories)
                    print("adding type_var_functional")

                    # get the reduced df with important fields
                    reduced_fields = ["Consequence", "is_synonymous", "is_truncating_small_variant", "type_var", "is_protein_coding_gene"]
                    reduced_all_annotations_df_species = all_annotations_df_species[reduced_fields].drop_duplicates().reset_index(drop=True)
                    reduced_all_annotations_df_species["type_var_functional"] = reduced_all_annotations_df_species.apply(get_type_var_functional_from_annot_df_r, axis=1)

                    # add to all_annotations_df_species
                    print("merging")
                    initial_len_all_annotations_df_species  = len(all_annotations_df_species)
                    all_annotations_df_species = all_annotations_df_species.merge(reduced_all_annotations_df_species, on=reduced_fields, how="left", validate="many_to_one")
                    if len(all_annotations_df_species)!=initial_len_all_annotations_df_species: raise ValueError("the len changed")
                    check_no_nans_series(all_annotations_df_species.type_var_functional)

                    # log
                    print(Counter(all_annotations_df_species["type_var_functional"]))
                    if set(all_annotations_df_species.type_var_functional)!=(set(all_type_var).union({"unclassified"})): raise ValueError("invalid type_var")

                    save_object(all_annotations_df_species, all_annotations_df_species_file)

                all_annotations_df_species = load_object(all_annotations_df_species_file) # it has whether it is syn

                # get the genes for this species. Only the protein coding ones (also checking that they make sense)
                #print("define the gene features df")
                protein_coding_genes = set(gff_df[gff_df.feature.isin({"CDS", "mRNA"})].upmost_parent).difference(pseudogenes)
                gene_features_df_s = gene_features_df_s[(gene_features_df_s.gff_upmost_parent.isin(protein_coding_genes))]

                strange_genes = set(all_annotations_df_species.Gene).difference(set(gene_features_df_s.gff_upmost_parent)).difference({"-"})
                if len(strange_genes)>0: raise ValueError("There are some weird genes: %s"%strange_genes)

                # define the treefile
                #print("writing tree")
                treefile = "%s/%s_tree.nw"%(ProcessedDataDir, species)
                species_to_tree[species].write(outfile=treefile, format=2)

                # load the variants
                all_vars_df = get_vars_df_piN_piS(DataDir, species, taxID, ProcessedDataDir, threads, all_annotations_df_species, sampleID_to_clade, PlotsDir)

                # keep only clinical isolates assignable to some clade
                clinical_samples = set(metadata_df[(metadata_df.species_name==species) & (metadata_df.type=="clinical") & (~pd.isna(metadata_df.cladeID_clonal_tshdSNPsKb_1))].sampleID) 

                #print("There are %i/%i clinical samples in %s"%(len(clinical_samples), sum(metadata_df.species_name==species), species))
                sorted_samples = sorted(clinical_samples)
                all_vars_df = all_vars_df[all_vars_df.sampleID.isin(clinical_samples)]

                # log the number of clades with clinical samples
                for cID in sorted(set(all_vars_df[~pd.isna(all_vars_df.cladeID)].cladeID)):
                    df_cID = all_vars_df[all_vars_df.cladeID==cID]
                    print("%s clade %s has %i clinical samples"%(species, cID, len(set(df_cID.sampleID))))

                # check the GTs
                for type_var in all_type_var: 
                    strange_GTSs = set(all_vars_df.common_GT).difference({"1", "0/1", "1/1", "."})
                    if len(strange_GTSs)>0: raise ValueError("There are weird gts: %s"%strange_GTSs)

                # get the genes with correct coverage in each sample, also useful to define the samples
                sample_to_genesCorrectCoverage = get_sample_to_genesCorrectCoverage_piN_piS(df_coverage_per_gene, species, protein_coding_genes, clinical_samples)
                    
                # get the genes with no truncation or SV
                print("getting sample to gene that has no truncation")
                sample_to_genesNoTruncation = get_sample_to_genes_NoTruncation_or_NoSV_piN_piS(all_vars_df, all_annotations_df_species, sorted_samples, protein_coding_genes, species, "small_vars_truncating")

                print("getting sample to gene that has no SVs")
                sample_to_genesNoSVs = get_sample_to_genes_NoTruncation_or_NoSV_piN_piS(all_vars_df, all_annotations_df_species, sorted_samples, protein_coding_genes, species, "nonSyn_SV")

                # For each sample, define genes that had no truncation or SV in the clade ancestor
                sample_to_genesNoTruncation_cladeAncestror = get_sample_to_genes_NoTruncation_or_NoSV_piN_piS(all_vars_df[all_vars_df.variant_in_ancestral_cladeID==True], all_annotations_df_species, sorted_samples, protein_coding_genes, species, "small_vars_truncating")

                sample_to_genesNoSVs_cladeAncestror = get_sample_to_genes_NoTruncation_or_NoSV_piN_piS(all_vars_df[all_vars_df.variant_in_ancestral_cladeID==True], all_annotations_df_species, sorted_samples, protein_coding_genes, species, "nonSyn_SV")

                # keep only important fields (remove type_var from annotations fields)
                all_annotations_df_species = all_annotations_df_species[[c for c in all_annotations_df_species.columns if c!="type_var"]]

            # create a bunch of empty objects
            else: all_annotations_df_species = all_vars_df = sorted_samples = sample_to_genesCorrectCoverage = sample_to_genesNoTruncation = sample_to_genesNoSVs = protein_coding_genes = sample_to_genesNoTruncation_cladeAncestror = sample_to_genesNoSVs_cladeAncestror = None

            # go through each type of variants
            for type_var in all_type_var: 

                # go through different types of variants, whether all of them
                for type_vars_appearance in all_type_vars_appearance:

                    # go through different filtering of the variants
                    for type_vars_SimpleRepeats in all_type_vars_SimpleRepeats:
                        print(species, type_var, type_vars_appearance, type_vars_SimpleRepeats)

                        # get the df_piN_piS for this combination of species and variants. This has only the pN, pS and positions
                        df_piN_piS_file = get_df_piN_piS_one_combination_types_vars(taxID, species, type_var, type_vars_appearance, type_vars_SimpleRepeats, all_annotations_df_species, all_vars_df, sorted_samples, sample_to_genesCorrectCoverage, sample_to_genesNoTruncation, sample_to_genesNoSVs, protein_coding_genes, ProcessedDataDir, species_to_ref_genome, gff_df, repeats_df_all, sample_to_genesNoTruncation_cladeAncestror, sample_to_genesNoSVs_cladeAncestror)

                        print(df_piN_piS_file)

                        # define the expected df_diversity_file
                        df_diversity_file = "%s/df_diversities_piN_piS_analysis/%s_%s_%s_%s.py"%(ProcessedDataDir, species, type_var.replace("/","_"), type_vars_appearance, type_vars_SimpleRepeats)

                        #if type_var!="SNP": remove_file(df_diversity_file)

                        # if it does not exist, call a job to generate it
                        if file_is_empty(df_diversity_file): 
                            print("getting job")

                            # get the cmd
                            run_get_df_diversity_piN_piS_all_steps_function_py = "%s/CandidaMine_data_generation/v1/run_get_df_diversity_piN_piS_all_steps_function.py"%ParentDir
                            cmd = "source /gpfs/projects/bsc40/mschikora/anaconda3/etc/profile.d/conda.sh && conda activate Candida_mine_env && %s %s %s %s %s %s %s %s %s"%(run_get_df_diversity_piN_piS_all_steps_function_py, ProcessedDataDir, species, type_var, type_vars_appearance, type_vars_SimpleRepeats, df_piN_piS_file, PlotsDir, sampleID_to_clade_file)

                            if run_in_cluster is False: 
                                run_cmd(cmd)
                                continue

                            jobs_get_diversity_df.append(cmd)

                        # if it was generated, keep
                        else: 
                            print("keeping df")

                            # get df
                            df_diversity = load_object_direct(df_diversity_file)

                            # if it is not SNP, add the probability of each 
                            if type_var!="SNP": 
                                print("adding empiric p value")
                                df_diversity = get_df_diversity_with_p_compared_to_other_genes(df_diversity, "harmonicMean_fraction_samples_and_clades")

                            # keep
                            print("appending")
                            df_diversity_all = df_diversity_all.append(df_diversity).reset_index(drop=True)
                            print("df kept")

        # debug
        if len(jobs_get_diversity_df)>0 and run_in_cluster is False: 
            print(jobs_get_diversity_df)
            error_no_run_local

        # run jobs
        if len(jobs_get_diversity_df)>0:
            print("running jobs in jobs_get_diversity_df")

            # get jobs filename
            jobs_filename = "%s/jobs.jobs_get_diversity_df"%ProcessedDataDir
            stddir = "%s/STDfiles"%(get_dir(jobs_filename))
            open(jobs_filename, "w").write("\n".join(["%s > %s/std.%i 2>&1"%(job, stddir, I+1) for I, job in enumerate(jobs_get_diversity_df)])+"\n")
            print(jobs_filename)

            # run array
            run_jobarray_file_MN4_greasy(jobs_filename, "jobs_get_diversity_df", time="02:00:00", queue="debug", threads_per_job=threads, nodes=16, submit=True)
            print("waiting until jobs of %s are completed, exiting..."%jobs_filename)
            sys.exit(0)

        print("saving df_diversity_all_file")
        save_object(df_diversity_all, df_diversity_all_file)

    # load
    print("loading df_diversity_all_file")
    df_diversity_all = load_object(df_diversity_all_file).reset_index(drop=True)

    return df_diversity_all


def get_df_diversity_filt_piN_piS(df_diversity_all, max_fraction_strains_extreme_piS):

    """Gets the filtered df"""

    # define the pval_f
    pval_f = "pval_harmonicMean_fraction_samples_and_clades"

    # define parms
    min_nclades_selection = 2
    min_nsamples_selected = 3

    # add whether the gene is relevant (there is some variation to be considered)
    df_diversity_all["gene_interesting_by_SNPs"] = (df_diversity_all.type_var=="SNP") & (df_diversity_all["fraction_samples_extreme_piS_p<0.05"]<=max_fraction_strains_extreme_piS) &  (df_diversity_all.nclades_selection>=min_nclades_selection) & (df_diversity_all.nsamples_selected>=min_nsamples_selected)
    df_diversity_all["gene_interesting_by_noSNPs"] = (df_diversity_all.type_var!="SNP") & (df_diversity_all.nclades_selection>=min_nclades_selection) & (df_diversity_all.nsamples_selected>=min_nsamples_selected)

    # go through different pval fields
    print("adding FDR corrected pvals for SNPs")

    # get df with the rows to calculate FDR corrections
    df_fdr = df_diversity_all[df_diversity_all.gene_interesting_by_SNPs]
    check_no_nans_series(df_fdr[pval_f])    

    # init a dict that maps each index to the fdr corrected pvalue (init with the nan fields)
    indices_no_pval = sorted(set(df_diversity_all.index).difference(set(df_fdr.index)))
    I_to_fdr_pval = dict(zip(indices_no_pval, [-1]*len(indices_no_pval)))

    # populate I_to_fdr_pval
    for species in set(df_fdr.species):
        for type_var in set(df_fdr.type_var):
            for type_vars_SimpleRepeats in set(df_fdr.type_vars_SimpleRepeats):
                for type_vars_appearance in set(df_fdr.type_vars_appearance):
                    for selection in set(df_fdr.selection):
                        for threshold_piNpiS in set(df_fdr.threshold_piNpiS):


                            # get df and check that gene is unique
                            df = df_fdr[(df_fdr.species==species) & (df_fdr.type_var==type_var) & (df_fdr.type_vars_SimpleRepeats==type_vars_SimpleRepeats) & (df_fdr.type_vars_appearance==type_vars_appearance) & (df_fdr.selection==selection) & (df_fdr.threshold_piNpiS==threshold_piNpiS)]
                            if len(df)!=len(set(df.Gene)): raise ValueError("Gene should be uniuque")
                            if len(df)==0: continue

                            # add to I_to_fdr_pval
                            I_to_fdr_pval = {**I_to_fdr_pval, **dict(zip(df.index, multitest.fdrcorrection(df[pval_f].values)[1]))}

    # add to df
    df_diversity_all["numeric_index"] = df_diversity_all.index
    df_diversity_all["%s_fdr"%pval_f] = df_diversity_all.numeric_index.map(I_to_fdr_pval); check_no_nans_series(df_diversity_all["%s_fdr"%pval_f])

    # adding fast fields
    print("adding -log10 pvals")
    pseudocount = 1/10000 # this is the minimum non-0 pval possible, since the resampling was up to 10000
    for pval_f in ["pval_harmonicMean_fraction_samples_and_clades_fdr"]: 

        df_diversity_all["minus_log10pval_%s"%pval_f] = -np.log10(df_diversity_all[pval_f] + pseudocount)
        df_diversity_all["minus_log10pval_%s"%pval_f] = df_diversity_all["minus_log10pval_%s"%pval_f].apply(lambda x: max([0, x]))


    # keep only the positive selection of recent variants that are interesting
    df_diversity_filt = df_diversity_all[(df_diversity_all.threshold_piNpiS==1) & (df_diversity_all.type_vars_SimpleRepeats=="only_vars_noSimpleRepeats")  & (df_diversity_all.type_vars_appearance=="only_vars_recent") & (df_diversity_all.selection=="positive") & ((df_diversity_all.gene_interesting_by_SNPs) | (df_diversity_all.gene_interesting_by_noSNPs))]


    # define the genes that are significant by SNPs
    df_diversity_filt["gene_significant_by_SNPs"] = (df_diversity_filt.gene_interesting_by_SNPs) & (df_diversity_filt.pval_harmonicMean_fraction_samples_and_clades_fdr<0.05)
    
    # define the genes that are significant by noSNPs
    var_to_spp_to_threshold_percentile = {}
    for type_var in ["if_INDEL", "DEL", "DUP"]:
        for spp in sorted_species_byPhylogeny:
            df = df_diversity_filt[(df_diversity_filt.type_var==type_var) & (df_diversity_filt.species==spp)]
            if any(~df.gene_interesting_by_noSNPs): raise ValueError("all should be interesting")
            if any(df.harmonicMean_fraction_samples_and_clades<=0): raise ValueError("all should be >0")
            check_no_nans_series(df.harmonicMean_fraction_samples_and_clades)
            if len(df)>0: var_to_spp_to_threshold_percentile.setdefault(type_var, {}).setdefault(spp, np.percentile(df['harmonicMean_fraction_samples_and_clades'], 90))

    def get_gene_significant_by_noSNPs(r):
        if r.type_var=="SNP": return False
        elif r.harmonicMean_fraction_samples_and_clades > var_to_spp_to_threshold_percentile[r.type_var][r.species]: return True
        else: return False

    df_diversity_filt["gene_significant_by_noSNPs"] = df_diversity_filt[["type_var", "species", "harmonicMean_fraction_samples_and_clades"]].apply(get_gene_significant_by_noSNPs, axis=1)

    # final checks
    check_no_nans_in_df(df_diversity_filt[["gene_interesting_by_SNPs", "gene_interesting_by_noSNPs", "gene_significant_by_SNPs", "gene_significant_by_noSNPs"]])
    if any(~((df_diversity_filt.gene_interesting_by_SNPs) | (df_diversity_filt.gene_interesting_by_noSNPs))): raise ValueError("There should be interesting genes")

  
    return df_diversity_filt


def get_gene_to_nvars_adding_0s(vars_df, expected_genes):

    """Gets a series that maps each gene to nvars, adding 0's for the expected genes"""

    # init the series
    gene_to_nvars = vars_df.groupby("Gene").apply(lambda df_g: set(df_g.variantID_across_samples)).apply(len)

    # add missing genes
    missing_genes = expected_genes.difference(set(gene_to_nvars.index))
    if len(missing_genes)>0: gene_to_nvars = gene_to_nvars.append(pd.Series({g:0 for g in missing_genes}))

    # check
    if set(gene_to_nvars.index)!=expected_genes: raise ValueError("The genes are not as expected")

    return gene_to_nvars


def get_df_pN_pS_one_sample(sampleID, nsamples, vars_df, everyehere_syn_vars, genesCorrectCoverage, genesNoTruncation, genesNoSVs, type_var, protein_coding_genes, species, taxID, genesNoTruncation_cladeAncestror, genesNoSVs_cladeAncestror, type_vars_appearance):

    """
    Calculates, for one sample, the pN, pS, pS_all_genes for each gene. These are the definitions:
    - pN: Number of non-syn variants per gene.
    - pS: Number of syn variants per genes. Note that the SVs don't have syn-per gene. They just have intergenic synonymous SVs.
    - pS_all_genes: The sum of all the synonymous variants across all the genome, but only considering variants that are in everyehere_syn_vars. This value is the same for all genes
    
    """
    
    # log
    #print("running get_df_piN_piS_one_sample on sample %s/%s"%(sampleID, nsamples))


    # get ploidy
    ploidy = taxID_to_ploidy[taxID]
    
    # check GT
    if ploidy==2 and type_var in {"SNP", "if_INDEL"}:        
        strange_GTs = set(vars_df.common_GT).difference({"1/1", "0/1", "."})
        if len(strange_GTs)>0: raise ValueError("strange GTs: %s"%strange_GTs)

    ################## DEFINE CORRECT GENES #####################

    # for SNPs and inframe INDELs, you nee to only consider samples where there is no truncation nor SV
    if type_var in {"SNP", "if_INDEL"}: correct_genes = (genesCorrectCoverage.intersection(genesNoTruncation)).intersection(genesNoSVs)

    # for other variants it depends on the type of variants
    elif type_var in {"DEL", "DUP"}: 

        # in general, we consider all genes as interesting for duplications and deletions
        correct_genes = protein_coding_genes

        # if you are considering recent variants, I want to discard genes that had a truncation or SV in the ancestor
        if type_vars_appearance=="only_vars_recent": correct_genes = correct_genes.intersection(genesNoTruncation_cladeAncestror).intersection(genesNoSVs_cladeAncestror)

    # keep only protein coding genes    
    correct_genes = correct_genes.intersection(protein_coding_genes)

    # check
    if len(correct_genes)==0: raise ValueError("this sample has no correct genes")
    
    #############################################################
    
    ### MAP EACH GENE TO THE PN AND THE PS ###
        
    # filter keeping only the correct genes
    vars_df_CorrectGenes = vars_df[vars_df.Gene.isin(correct_genes)]
    
    # if there are no variants, create a df with all 0s 
    if len(vars_df_CorrectGenes)==0:
        print("WARNING: Sample %s has no variants"%sampleID)
        gene_to_pN = {g:0 for g in correct_genes}
        gene_to_pS = {g:0 for g in correct_genes}

    else:
        
   
        # go through different types of mutations
        for is_synonymous in [True, False]:
            
            # filter the variants that are syn or not for a given gene
            vars_df_filt = vars_df_CorrectGenes[(vars_df_CorrectGenes.Gene.isin(correct_genes)) & (vars_df_CorrectGenes.is_synonymous==is_synonymous)][["Gene", "variantID_across_samples", "common_GT"]].drop_duplicates()

            # get the gene to # vars. For heterozygous SNPs you'd resample
            if type_var in {"SNP", "if_INDEL"} and ploidy==2: 

                # get the homo vars
                gene_to_nvars_homo = get_gene_to_nvars_adding_0s(vars_df_filt[vars_df_filt.common_GT=="1/1"], correct_genes)

                # get the hetero vars in the same way, but weighing by 2
                gene_to_nvars_hetero = get_gene_to_nvars_adding_0s(vars_df_filt[vars_df_filt.common_GT.isin({"0/1", "."})], correct_genes) / 2

                # sum both
                gene_to_nvars = gene_to_nvars_homo + gene_to_nvars_hetero

            elif type_var in {"DEL", "DUP"} or ploidy==1: gene_to_nvars = get_gene_to_nvars_adding_0s(vars_df_filt, correct_genes)
            else: raise ValueError("invalid type_var or ploidy")
            
            # set the dicts
            if is_synonymous==False: gene_to_pN = dict(gene_to_nvars)
            elif is_synonymous==True: gene_to_pS = dict(gene_to_nvars)
            
    ##########################################

    #### CALCULATE THE PS FOR ALL GENES ####
    
    # get the variants for the correct genes, that are in all syn
    vars_df_allSyn = vars_df[(vars_df.Gene.isin(correct_genes.union({"-"}))) & (vars_df.variantID_across_samples.isin(everyehere_syn_vars))]

    # define the fact that there are no vars Syn at all
    if len(vars_df_allSyn)==0: pS_all_genes = 0

    # else
    else:

        # check that only SNPs get here
        if type_var!="SNP": raise ValueError("only type_var=SNP can be here")
        
        # ploidy 1, this is calculated from variants that are only attributable to one gene 
        if ploidy==1: pS_all_genes = len(set(vars_df_allSyn.variantID_across_samples))
        
        # for small vars of ploidy 2, you have to resample as you did 
        elif ploidy==2: 

            # split into homozygous and heterozygous
            pS_all_genes_homo = len(set(vars_df_allSyn[vars_df_allSyn.common_GT=="1/1"].variantID_across_samples))
            pS_all_genes_hetero = len(set(vars_df_allSyn[vars_df_allSyn.common_GT.isin({"0/1", "."})].variantID_across_samples)) / 2
            pS_all_genes = pS_all_genes_homo + pS_all_genes_hetero
        
        else: raise ValueError("invalid type_var or ploidy")
    
    ########################################

    # get df and return
    df_pN_pS = pd.DataFrame({"pN":gene_to_pN, "pS":gene_to_pS})
    df_pN_pS["pS_all_genes"] = pS_all_genes
    df_pN_pS["sampleID"] = sampleID
    df_pN_pS["Gene"] = df_pN_pS.index

    return df_pN_pS


def plot_species_tree_Cmine(ProcessedDataDir, PlotsDir):

    """Plots the species tree generated by orthofinder"""

    # load the tree
    tree = Tree("%s/orthofinder_all_species/output_orthofinder/Results_Jun29/Species_Tree/SpeciesTree_rooted.txt"%ProcessedDataDir)

    # custimize
    for n in tree.traverse():

        # set style
        nst = NodeStyle()
        nst["hz_line_width"] = 12
        nst["vt_line_width"] = 12
        nst["hz_line_color"] = "black"
        nst["vt_line_color"] = "black"
        nst["size"] = 0
        n.set_style(nst)

        # add empty face
        n.add_face(RectFace(20, 100, fgcolor="white", bgcolor="white", label={"text":"", "color":"black"}), position="aligned", column=0)


    ts = TreeStyle()
    ts.show_branch_length = False
    ts.show_branch_support = False
    ts.show_leaf_name = False

    ts.draw_guiding_lines = True
    ts.guiding_lines_type = 2 # 0=solid, 1=dashed, 2=dotted.
    ts.guiding_lines_color = "gray" 
     
    # save
    tree.show(tree_style=ts)
    tree.render(file_name="%s/species_tree_orthofinder.pdf"%PlotsDir, tree_style=ts)


def plot_Cmetapsilosis_tree_homosnps(DataDir, min_support=95):

    """Plots the C. metapsilosis tree when only considering HomoSNPs"""

    species = "Candida_metapsilosis"
    taxID = sciName_to_taxID[species] 

    print("showing tree homo snps")
    tree = get_correct_tree_midpointRooted("%s/%s_%i/generate_tree_from_SNPs/iqtree_unroted.treefile"%(DataDir, species, taxID), min_support=min_support)
    tree.show()

    print("show tree hetero SNPs")
    tree = Tree("%s/%s_%i/generate_tree_from_SNPs_resamplingHetSNPs/tree_consensus_withBootstraps_and_branchLengths.nw"%(DataDir, species, taxID))
    for n in tree.traverse():
        if n.is_leaf() is False and len(n.get_ancestors())>0 and n.support<min_support: n.delete()

    tree.show()

    

def plot_clades_Cmine(species_to_tree, metadata_df, PlotsDir, root_opening_factor=1, show_clade=False, ts_mode="c"):

    """Plots the clades for each species"""

    # go through each species
    for taxID, species in taxID_to_sciName.items():
        print(species)

        #if species!="Candida_metapsilosis": continue

        # map each leaf to a clade
        metadata_df["sampleID"] = metadata_df.sampleID.apply(str) 
        leaf_to_clade = dict(metadata_df[metadata_df.species_name==species].set_index("sampleID").cladeID_Tree_and_BranchLen)

        # print stats
        metadata_df_s = metadata_df[metadata_df.species_name==species]
        nclades = len(set(metadata_df_s[~pd.isna(metadata_df_s.cladeID_Tree_and_BranchLen)].cladeID_Tree_and_BranchLen))
        print("%i clades"%(nclades))

        continue

        # get the number of clades
        nclades = len(set([x for x in leaf_to_clade.values() if not pd.isna(x)]))

        # map the clade to color
        if nclades<=10: palette = "tab10"
        else: palette = "tab20"
        clade_to_color = get_value_to_color(list(range(1, nclades+1)), palette=palette, n=nclades, type_color="hex")[0]

        # get tree
        tree = cp.deepcopy(species_to_tree[species])

        # remove bad samples
        good_samples = set(tree.get_leaf_names()).difference({str(x) for x in sciName_to_badSamples[species]})
        tree.prune(good_samples)

        # add the clade ID
        tree = get_tree_withcladeID_as_str(tree, leaf_to_clade)

        for n in tree.traverse():

            # define the bwidth    
            species_to_bwidth = {"Candida_albicans":40, "Candida_glabrata":80, "Candida_auris":20, "Candida_tropicalis":4, "Candida_metapsilosis":4, "Candida_parapsilosis":4, "Candida_orthopsilosis":4}
            bwidth = species_to_bwidth[species]

            #bwidth = 4

            nst = NodeStyle()
            nst["hz_line_width"] = bwidth
            nst["vt_line_width"] = bwidth
            nst["size"] = 0

            # define the color
            if pd.isna(n.cladeID): bgcolor = "white"
            else: bgcolor = clade_to_color[n.cladeID]
          
            nst["bgcolor"] = bgcolor
            nst["hz_line_color"] = "black"
            nst["vt_line_color"] = "black"
            
            n.set_style(nst)

            # add white face to the leafs
            if n.is_leaf(): n.add_face(RectFace(20, 40, fgcolor=bgcolor, bgcolor=bgcolor, label={"text":"", "color":"white"}), column=0 , position="aligned") #

            if show_clade is True and n.is_leaf(): n.add_face(RectFace(20, 40, fgcolor=bgcolor, bgcolor=bgcolor, label={"text":str(n.cladeID), "color":"black"}), column=1 , position="aligned") #


        ts = TreeStyle()
        ts.show_branch_length = False
        ts.show_branch_support = False
        ts.show_leaf_name = False

        # cicular orientation
        ts.mode = ts_mode
        ts.root_opening_factor = root_opening_factor
        #ts.arc_start = -180 # 0 degrees = 3 o'clock
        #ts.arc_span = 180    
        

        #tree.show(tree_style=ts); kdahgjhgad

        plots_dir= "%s/circular_trees_with_clades"%PlotsDir; make_folder(plots_dir)
        tree.render(file_name="%s/%s_tree.pdf"%(plots_dir, species), tree_style=ts)

        #tree.render(file_name=filename_tree, tree_style=ts, dpi=500,  units='mm', h=nleafs*2) #w=20

def plot_diversity_distribution_old(df_diversity_all, PlotsDir, diversity_f="median_piN_piS_each_varType", percentiles_thresholds=[10,90]):

    """Plots the distrubution of diversity sepparating species and types of variants"""

    logscale_x = True


    # iterate through different types of data
    for type_vars_SimpleRepeats in ["only_vars_noSimpleRepeats"]: # all_vars
        for type_vars_appearance in ["all_vars"]:

            # calculate the ylims

            # plot together
            fig = plt.figure(figsize=(6, 1.5))
            for Ic, type_variant in enumerate( ["SNP", "IN/DEL", "SV", "coverageCNV"]):

                # filter df
                df_diversity = df_diversity_all[(df_diversity_all.type_vars_SimpleRepeats==type_vars_SimpleRepeats) & (df_diversity_all.type_vars_appearance==type_vars_appearance) & ~(pd.isna(df_diversity_all[diversity_f])) & (df_diversity_all.type_var==type_variant)]

                # get subplot
                ax = plt.subplot(1, 4, Ic+1)
                for spp in sorted_species_byPhylogeny:
                    df_plot = df_diversity[(df_diversity.species==spp)]
                    
                    if logscale_x is True: df_plot[diversity_f] = (df_plot[diversity_f]+0.1).apply(np.log10)
                    ax = sns.distplot(df_plot[diversity_f], color=species_to_color[spp], hist=True, kde=False, kde_kws={"lw": 3, "label": spp}, hist_kws={"histtype": "step", "linewidth": 3, "alpha": 0.7, "color": species_to_color[spp], "label":spp})#, hue="species", palette=species_to_color)

                # reset the xticks
                if logscale_x is True:
                    xlim = ax.get_xlim()
                    xticks = [x for x in range(-3, 10) if x<=xlim[1] and x>=xlim[0]]
                    ax.set_xticks(xticks)
                    ax.set_xticklabels([str(get_float_or_int(10**x)) for x in xticks], rotation=90)

                # set the legend
                if Ic==3: 
                    legend_elements = [Line2D([0], [0], color=species_to_color[species], lw=4, label=species.split("_")[1], alpha=1.0) for species in sorted_species_byPhylogeny]
                    ax.legend(handles=legend_elements, loc='right', bbox_to_anchor=(2.8, 0))
                
                ax.set_yscale("log")
                ax.set_ylim([0.0, 8532.05])

                if Ic!=0: 
                    ax.set_ylabel("")
                    ax.set_yticks([])
                    ax.set_yticklabels([])

                # add lines
                if type_variant=="SNP":

                    for species in sorted_species_byPhylogeny:
                   
                        diversity_series = df_diversity[(df_diversity.species==species) & (df_diversity.type_var==type_variant) & ~(pd.isna(df_diversity[diversity_f]))][diversity_f]
                        lines_x = [np.log10(np.percentile(diversity_series, pct)+0.1) for pct in [90]]

                        for x in lines_x: plt.axvline(x, linewidth=.7, color=species_to_color[species], linestyle="--")

                else: plt.axvline(np.log10(1+0.1), linewidth=.7, color="k", linestyle="--")

                # xlabel
                if Ic!=1: ax.set_xlabel("")
                if Ic==0: ax.set_ylabel("# genes")

            plt.subplots_adjust(wspace=0.05, hspace=0.2)
            plots_dir = "%s/plots_distribution_diversity_allTogether"%PlotsDir; make_folder(plots_dir)
            filename = "%s/%s_%s_log%s.pdf"%(plots_dir, type_vars_SimpleRepeats, type_vars_appearance, logscale_x)
            fig.savefig(filename, bbox_inches='tight')

            # go through different groups of variants
            for type_vars_general, sorted_vartypes in [("all_vars",  ["SNP", "IN/DEL", "SV", "coverageCNV"]), ("onlySV_INDEL", ["IN/DEL", "SV", "coverageCNV"]), (["only_SNPs", ["SNP"]])]:

                # filter df
                df_diversity = df_diversity_all[(df_diversity_all.type_vars_SimpleRepeats==type_vars_SimpleRepeats) & (df_diversity_all.type_vars_appearance==type_vars_appearance) & ~(pd.isna(df_diversity_all[diversity_f])) & (df_diversity_all.type_var.isin(set(sorted_vartypes)))]

                # define lines to be drawn as the 95%
                species_to_vartype_to_xlines = {}

                for species in sorted_species_byPhylogeny:
                    for type_var in sorted_vartypes:

                        if type_var=="SNP" or True: 

                            diversity_series = df_diversity[(df_diversity.species==species) & (df_diversity.type_var==type_var) & ~(pd.isna(df_diversity[diversity_f]))][diversity_f]
                            lines_x = [np.percentile(diversity_series, pct) for pct in [percentiles_thresholds[1]]]


                            pritn(lines_x)

                        else: lines_x = [0.2, 1]

                        species_to_vartype_to_xlines.setdefault(species, {}).setdefault(type_var, lines_x)

                # cheks 
                if logscale_x is True and any(df_diversity[diversity_f]<0): raise ValueError("There can't be negatives if the field is true")


                # define the lims (manually set from another run)
                if diversity_f=="median_piN_piS_each_varType": ylim = [0.0, 8532.05] # this is for 

                # plot al sepparate
                xfield = diversity_f
                vartype_field = "type_var"
                plots_dir = "%s/plots_distribution_diversity"%PlotsDir; make_folder(plots_dir)
                filename = "%s/%s_%s_%s_log%s.pdf"%(plots_dir, type_vars_SimpleRepeats, type_vars_appearance, type_vars_general, logscale_x)
                plot_histograms_rows_in_species_vatypes_in_cols(df_diversity, sorted_vartypes, xfield, filename, vartype_field, logscale_x=logscale_x, logscale_y=True, pseudocount=0.1, xlabel=diversity_f, input_xticks=None, input_yticks=None, extended_xrange=0.1, ylabel="number genes (%s)"%type_vars_SimpleRepeats, species_to_vartype_to_xlines=species_to_vartype_to_xlines, add_xlim=[0.1,0], add_ylim=[0, 2000], xlim=None, ylim=ylim)


def plot_diversity_distribution(df_diversity_filt, PlotsDir, percentiles_thresholds, diversity_f, draw_lines_pct, figsize):

    """Plots the diversity distribution"""

    # define general things
    pseudocount = 0.001

    # iterate through different types of data
    for type_vars_SimpleRepeats in ["only_vars_noSimpleRepeats"]: # all_vars
        for type_vars_appearance in ["all_vars", "only_vars_recent"]:

            # plot together
            fig = plt.figure(figsize=figsize)
            for Ic, type_variant in enumerate( ["SNP", "IN/DEL", "SV", "coverageCNV"]):

                # filter df
                df_diversity = df_diversity_filt[(df_diversity_filt.type_vars_SimpleRepeats==type_vars_SimpleRepeats) & (df_diversity_filt.type_vars_appearance==type_vars_appearance) & ~(pd.isna(df_diversity_filt[diversity_f])) & (df_diversity_filt.type_var==type_variant)]

                # init the subplot
                ax = plt.subplot(1, 4, Ic+1)

  
                # set the legend
                if Ic==3: 
                    legend_elements = [Line2D([0], [0], color=species_to_color[species], lw=4, label=species.split("_")[1], alpha=1.0) for species in sorted_species_byPhylogeny]
                    ax.legend(handles=legend_elements, loc='right', bbox_to_anchor=(2.8, 0))
                
                ax.set_yscale("log")
                ax.set_ylim([0.0, 8532.05])

                if Ic!=0: 
                    ax.set_ylabel("")
                    ax.set_yticks([])
                    ax.set_yticklabels([])

                # add lines
                if draw_lines_pct is True:

                    # init title
                    title = ""

                    for species in sorted_species_byPhylogeny:
                            
                        # define percentiles
                        df_diversity_spp = df_diversity[(df_diversity.species==species)]
                        df_diversity_for_pcts = df_diversity_spp[(df_diversity_spp[diversity_f]>0)]
                        low_percentile = np.percentile(df_diversity_for_pcts[diversity_f], percentiles_thresholds[0])
                        high_percentile = np.percentile(df_diversity_for_pcts[diversity_f], percentiles_thresholds[1])


                        # get the lines
                        lines_x = [np.log10(pct+pseudocount) for pct in [high_percentile]] # low_percentile
                        for x in lines_x: plt.axvline(x, linewidth=.7, color=species_to_color[species], linestyle=":")

                        # add the numbers of genes in each cathegory (pct 5 and pct 95)
                        nlow_genes = sum(df_diversity_spp[diversity_f]<=low_percentile)
                        nhigh_genes = sum(df_diversity_spp[diversity_f]>=high_percentile)
            
                        title += "\n\n%s:\n#%i=%s\n#%i=%s"%(species.split("_")[1], percentiles_thresholds[0], nlow_genes, percentiles_thresholds[1], nhigh_genes)  
                    
                    ax.set_title(title + "\n\n%s"%type_variant)

                else: ax.set_title(type_variant)

                # get subplot
                
                for spp in sorted_species_byPhylogeny:
                    df_plot = df_diversity[(df_diversity.species==spp)]
                    
                    df_plot[diversity_f] = (df_plot[diversity_f]+pseudocount).apply(np.log10)
                    ax = sns.distplot(df_plot[diversity_f], color=species_to_color[spp], hist=True, kde=False, kde_kws={"lw": 3, "label": spp}, hist_kws={"histtype": "step", "linewidth": 1.5, "alpha": 0.9, "color": species_to_color[spp], "label":spp})#, hue="species", palette=species_to_color)

                # reset the xticks
                xlim = ax.get_xlim()
                xticks = [x for x in range(-3, 10) if x<=xlim[1] and x>=xlim[0]]
                ax.set_xticks(xticks)
                def get_pseudocount_to_0(x):
                    if x==pseudocount: return 0
                    else: return x
                ax.set_xticklabels([str(get_pseudocount_to_0(get_float_or_int(10**x))) for x in xticks], rotation=90)


                # xlabel
                if Ic!=1: ax.set_xlabel("")
                if Ic==0: ax.set_ylabel("# genes")

                # title
                

            plt.subplots_adjust(wspace=0.05, hspace=0.2)
            plots_dir = "%s/plots_distribution_diversity_allTogether"%PlotsDir; make_folder(plots_dir)
            filename = "%s/%s_%s_%s.pdf"%(plots_dir, type_vars_SimpleRepeats, type_vars_appearance, diversity_f)
            print("plotting %s"%filename)
            fig.savefig(filename, bbox_inches='tight')



def get_high_low_percentiles_df_diversity(df_diversity, percentiles_thresholds, diversity_f):

    """Returns the high and the low percentile diversity thresholds"""

    if len(df_diversity)==0: raise ValueError("The df_diversity can't be 0")

    # checks
    for f in ["type_vars_SimpleRepeats", "species", "type_vars_appearance", "type_var"]:
        if len(set(df_diversity[f]))!=1: raise ValueError("The percentiles can't be calculated. %s has these values: %s"%(f, set(df_diversity[f])))

    # define df for the percentiles
    #df_diversity_for_pcts = df_diversity # based on all
    df_diversity_for_pcts = df_diversity[df_diversity[diversity_f]>0]

    # get the percentiles
    low_percentile = np.percentile(df_diversity_for_pcts[diversity_f], percentiles_thresholds[0])
    high_percentile = np.percentile(df_diversity_for_pcts[diversity_f], percentiles_thresholds[1])

    return low_percentile, high_percentile

def plot_diversity_distribution_each_hist_sepparate(df_diversity_filt, PlotsDir, percentiles_thresholds, diversity_f, draw_lines_pct, title_prefix):

    """Plots one hist for each species and variant type for the diversity distribution"""

    # define general things
    pseudocount = 0.001

    # iterate through different types of data
    for type_vars_SimpleRepeats in ["only_vars_noSimpleRepeats"]: # all_vars
        for type_vars_appearance in ["only_vars_recent", "all_vars"]:

            # filter df
            df_diversity = df_diversity_filt[(df_diversity_filt.type_vars_SimpleRepeats==type_vars_SimpleRepeats) & (df_diversity_filt.type_vars_appearance==type_vars_appearance) & ~(pd.isna(df_diversity_filt[diversity_f]))]

            # define lines to be drawn 
            species_to_vartype_to_xlines = {}

            for species in sorted_species_byPhylogeny:
                for type_var in sorted(set(df_diversity.type_var)):
                    
                    df_div_pcts = df_diversity[(df_diversity.species==species) & (df_diversity.type_var==type_var)]
                    if len(df_div_pcts)==0: continue

                    low_percentile, high_percentile = get_high_low_percentiles_df_diversity(df_div_pcts, percentiles_thresholds, diversity_f)
                    species_to_vartype_to_xlines.setdefault(species, {}).setdefault(type_var, [low_percentile, high_percentile])

            # define fields for the histogram
            vartype_field = "type_var"
            plots_dir = "%s/plots_distribution_piN_piS"%PlotsDir; delete_folder(plots_dir); make_folder(plots_dir)


            # get the plot
            for logscale_x in [True]:

                filename = "%s/%s_%s_log%s.pdf"%(plots_dir, type_vars_SimpleRepeats, type_vars_appearance, logscale_x)
                title = "%s\n%s; %s"%(title_prefix, type_vars_SimpleRepeats, type_vars_appearance)

                # get the plot
                sorted_vartypes = ["SNP", "IN/DEL", "SV", "coverageCNV"]
                plot_histograms_rows_in_species_vatypes_in_cols(df_diversity, sorted_vartypes, diversity_f, filename, vartype_field, logscale_x=logscale_x, logscale_y=True, pseudocount=pseudocount, xlabel=diversity_f, input_yticks=None, extended_xrange=0.1, ylabel="number genes", species_to_vartype_to_xlines=species_to_vartype_to_xlines, title=title, size_multiplier=1, add_xlim=[0.1,0], input_xticks=[0.01, 1, 100, 10000])



def get_metacycID_to_description(list_metacyc_IDs, filename):

    """Maps a list of metacyc IDs to a description, saving into filename"""

    if file_is_empty(filename):

        # log activate the pathway tools
        print("You should run the pathway-tools software while running this. In the BSC desktop we do this with:\n\n/home/mschikora/software/pathway-tools/pathway-tools -lisp -python\n\n")

        # get the meta object, which has all biocyc
        import pythoncyc
        meta = pythoncyc.select_organism('meta')

        # get the description
        print("getting descriptions")
        npathways = len(list_metacyc_IDs)
        def get_description_one_ID(x):
            I, ID = x
            perc = "%.2f"%(((I+1)/npathways)*100 )
            if perc.endswith("00"): print("%s perc completed"%perc)
            return meta[ID].get_frame_data().common_name

        ID_to_description = dict(zip(list_metacyc_IDs, map(get_description_one_ID, enumerate(list_metacyc_IDs))))

        print("saving MetaCyc descr")
        save_object(ID_to_description, filename)

    print("loading metacyc descriptions")
    return load_object(filename)


def get_df_enrichment_particular_grouping(target_genes, all_genes, df_grouping, fileprefix):

    """Gets the enrichment data for each group in df_grouping in target_genes."""

    #  define the file prefix
    df_enrichment_file = "%s.enrichment_df.py"%fileprefix
    df_enrichment_file_excel = "%s.enrichment_df.xlsx"%fileprefix

    #print("removing")
    #for f in [df_enrichment_file, df_enrichment_file_excel]: remove_file(f)
    #return

    if file_is_empty(df_enrichment_file) or file_is_empty(df_enrichment_file_excel):

        # keep only genes that have a group
        genes_with_group = set(df_grouping.Gene)
        target_genes = target_genes.intersection(genes_with_group)
        all_genes = all_genes.intersection(genes_with_group)

        # define groups that are in target genes
        groups_in_target_genes = set(df_grouping[df_grouping.Gene.isin(target_genes)].ID)

        # define the fields in the enrichment df
        df_enrichment_fields = ['ID', 'ngenes_group_and_target', 'ngenes_no_group_target', 'ngenes_group_no_target', 'ngenes_no_group_no_target', 'OR', 'p_raw', 'p_fdr', "p_bonferroni", 'group_name', 'genes_group_and_target', 'genes_no_group_target', 'genes_group_no_target', 'genes_no_group_no_target']

        if len(groups_in_target_genes)>0:

            # map each group to the genes
            group_to_genes = df_grouping[df_grouping.ID.isin(groups_in_target_genes)].groupby("ID").apply(lambda df: set(df.Gene))
            sorted_groups = sorted(set(group_to_genes.keys()))

            # define a function that, for each group, obtains stats about the enrichment as a series
            def get_enrichment_stats_one_group(ID):

                # define the contingency table
                genes_In_group = group_to_genes[ID]
                genes_notIn_group = all_genes.difference(genes_In_group)
                not_target_genes = all_genes.difference(target_genes)

                genes_group_and_target = genes_In_group.intersection(target_genes)
                genes_no_group_no_target = genes_notIn_group.intersection(not_target_genes)
                genes_group_no_target = genes_In_group.intersection(not_target_genes)
                genes_no_group_target = genes_notIn_group.intersection(target_genes)


                contingency_table = [[len(genes_group_and_target), len(genes_no_group_target)], 
                                     [len(genes_group_no_target), len(genes_no_group_no_target)]]

                # check that all genes are somewhere
                if sum(make_flat_listOflists(contingency_table))!=len(all_genes): raise ValueError("the contingency_table should have all genes")
                if sum(contingency_table[0])!=len(target_genes): raise ValueError("the first row of table should be target genes")
                if sum(contingency_table[1])!=len(not_target_genes): raise ValueError("the second row of table should be not_target_genes genes")

                # make the fisher test
                OR, p_fisher = stats.fisher_exact(contingency_table, alternative="greater")

                # return stats
                return pd.Series({"ID":ID, "genes_group_and_target":sorted(genes_group_and_target), "genes_no_group_target":sorted(genes_no_group_target), "genes_group_no_target":sorted(genes_group_no_target), "genes_no_group_no_target":sorted(genes_no_group_no_target), "ngenes_group_and_target":len(genes_group_and_target), "ngenes_no_group_target":len(genes_no_group_target), "ngenes_group_no_target":len(genes_group_no_target), "ngenes_no_group_no_target":len(genes_no_group_no_target), "OR":OR, "p_raw":p_fisher})


            # get the enrichment stats df
            df_enrichment = pd.DataFrame(list(map(get_enrichment_stats_one_group, sorted_groups)))

            # add the other fields
            df_enrichment["p_fdr"] = multitest.fdrcorrection(df_enrichment.p_raw.values)[1] 
            df_enrichment["p_bonferroni"] = (df_enrichment.p_raw * len(df_enrichment)).apply(lambda x: min([1.0, x]))

            ID_to_description = dict(df_grouping[["ID", "group_name"]].drop_duplicates().set_index("ID").group_name)
            df_enrichment["group_name"] = df_enrichment.ID.apply(lambda x: ID_to_description[x])

            # keep only things that have an enrichment
            df_enrichment = df_enrichment[(df_enrichment.OR>=1) & (df_enrichment.p_raw<0.05)]

            # keep some fields
            df_enrichment = df_enrichment[df_enrichment_fields].sort_values(by=["p_fdr", "p_bonferroni", "p_raw", "OR"], ascending=[True, True, True, False])

        else: df_enrichment = pd.DataFrame(columns=df_enrichment_fields)

        


        # save files
        df_enrichment.to_excel(df_enrichment_file_excel)
        save_object(df_enrichment, df_enrichment_file)

    return load_object(df_enrichment_file)

def  plot_enriched_groups_n_overlapping_groups(df_enrichment, filename_plot, title, IDfield="ID", min_overlap_plot=0.05):

    """Plots a heatmap that has the Jaccard distance related to the number of overlapping groups."""

    if len(df_enrichment)==0: 
        print("WARNING: There are no enriched terms (%s)"%title)
        return 

    print("running plot_enriched_groups_n_overlapping_groups")

    # init general vars
    #fields = ["selection", "type_vars_appearance", "species", "type_var"]
    fields = ["species", "type_var"]

    field_to_color_dict = {"selection":{"positive":"magenta", "negative":"c"},
                           "type_vars_appearance":{"all_vars":"gold", "only_vars_recent":"salmon"},
                           "species":species_to_color,
                           #"type_var":{"SNP":"red", "IN/DEL":"magenta", "SV":"navy", "coverageCNV":"cyan"}
                           "type_var":{"SNP":"red", "if_INDEL":"magenta", "DEL":"grey", "DUP":"navy"}}

    field_to_sorted_items = {"selection":["positive", "negative"],
                             "type_vars_appearance":["all_vars", "only_vars_recent"],
                             "species":sorted_species_byPhylogeny,
                             "type_var":["SNP", "if_INDEL", "DUP", "DEL"]}



    # redefine field_to_sorted_items so that they appear in df
    field_to_sorted_items = {f : [x for x in sitems if x in set(df_enrichment[f])] for f, sitems in field_to_sorted_items.items() if f in fields}


    # only fields that are nonunique
    fields =  [f for f in fields if len(field_to_sorted_items[f])>1]

    # debug
    if len(fields)==0:
        print("WARNING: There are not more than 1 different term (%s). field_to_sorted_items is %s"%(title, field_to_sorted_items))
        return 



    # define the sorted combinations
    sorted_combinations = list(itertools.product(*[field_to_sorted_items[f] for f in fields]))


    # kepp valid combinations
    if len(fields)>1: all_combs_set = set(df_enrichment.set_index(fields).index)
    elif len(fields)==1: all_combs_set = {(x,) for x in set(df_enrichment[fields[0]])}
    sorted_combinations = [c  for c in sorted_combinations if c in all_combs_set] 


    # check that the groups of each fields are unique
    df_enrichment_check = df_enrichment.set_index(fields)[IDfield]
    for comb in set(df_enrichment_check.index):
        if len(set(df_enrichment_check[{comb}]))!=len(df_enrichment_check[{comb}]): raise ValueError("the IDs are not unique ")

    # map each combination to the groups
    comb_to_groups = dict(df_enrichment.groupby(fields).apply(lambda df_fs: set(df_fs[IDfield])))
    if len(fields)==1: comb_to_groups = {(c,):gs for c,gs in comb_to_groups.items()}

    missing_combs = set(sorted_combinations).difference(set(comb_to_groups))
    for missing_comb in missing_combs: comb_to_groups[missing_comb] = set()

    # define a function that takes one specific pair of combinations and returns the JC overlap
    def get_jaccard_index_overlap_two_combinations(comb1, comb2):

        # get data for each
        groups1 = comb_to_groups[comb1]
        groups2 = comb_to_groups[comb2]

        # return nan if there are no groups
        if len(groups1)==0 or len(groups2)==0: return 0.0

        # define the all and shared IDs
        all_IDs = groups1.union(groups2)
        shared_IDs = groups1.intersection(groups2)

        return len(shared_IDs)/len(all_IDs)

    def get_jaccard_index_overlap_two_combinations_annot(comb1, comb2):

        # get data for each
        groups1 = comb_to_groups[comb1]
        groups2 = comb_to_groups[comb2]

        # no groups, return no annot
        if len(groups1)==0 or len(groups2)==0: return ""

        # define the 
        ngroups = len(groups1.union(groups2))
        nshared_groups = len(groups1.intersection(groups2))

        if nshared_groups>0 and (nshared_groups/ngroups)>=min_overlap_plot: return str(nshared_groups)
        else: return ""


    # define the square df for each of the combinations
    df_square = pd.DataFrame({comb1 : {comb2 : get_jaccard_index_overlap_two_combinations(comb1, comb2) for comb2 in sorted_combinations} for comb1 in sorted_combinations}).loc[sorted_combinations, sorted_combinations]


    # define the square df for the annotations
    annot_df = pd.DataFrame({comb1 : {comb2 : get_jaccard_index_overlap_two_combinations_annot(comb1, comb2) for comb2 in sorted_combinations} for comb1 in sorted_combinations}).loc[sorted_combinations, sorted_combinations]

    # define the colors df
    row_colors_df = pd.DataFrame({comb : {f : field_to_color_dict[f][comb[I]]  for I,f in enumerate(fields)} for comb in sorted_combinations}).transpose()

    # the col colors in the same as row but inverted
    col_colors_df = row_colors_df[list(reversed(row_colors_df.columns))]


    # define the clustermap
    mask = df_square.isnull()
    mask = np.zeros_like(df_square)
    mask[np.triu_indices_from(mask)] = True


    colorbar_label = "fraction groups overlap"
    cm = sns.clustermap(df_square, col_cluster=False, row_cluster=False, yticklabels=False, xticklabels=False, col_colors=col_colors_df, row_colors=row_colors_df, linecolor="white", linewidth=.05, cbar_kws={'label': colorbar_label, "orientation":"horizontal"}, annot=annot_df, mask=mask, cmap="rocket_r", center=0.5, vmin=0, vmax=1, annot_kws={"size": 7}, fmt=""); # row_colors = row_colors_df

    # remove the row colors
    cm.ax_row_colors.set_xticklabels([])

    # add numbers
    cm.ax_row_colors.set_yticks([I+0.5 for I, comb in enumerate(df_square.index)])
    cm.ax_row_colors.set_yticklabels([str(len(comb_to_groups[comb])) for comb in df_square.index])

    cm.ax_col_colors.set_xticks([I+0.5 for I, comb in enumerate(df_square.index)])
    cm.ax_col_colors.set_xticklabels([str(len(comb_to_groups[comb])) for comb in df_square.columns], rotation=90)


    # labels
    cm.ax_heatmap.set_xlabel("")
    cm.ax_heatmap.set_ylabel("")

    # adjust
    square_w = 0.025
    distance_btw_boxes = 0.007
    adjust_cm_positions(cm, df_square, hm_height_multiplier=square_w, hm_width_multiplier=square_w, cc_height_multiplier=square_w, rc_width_multiplier=square_w, cbar_width=0.12, cbar_height=0.02, distance_btw_boxes=distance_btw_boxes)

    # def adjust_cm_positions(cm, df_square, hm_height_multiplier=0.0002, hm_width_multiplier=0.01, cc_height_multiplier=0.017, rc_width_multiplier=0.017, idx_delimiter="-", distance_btw_boxes=0.0025, cd_height=0.07, rd_width=0.07, cbar_width=0.08, cbar_height=0.015):

    # change the position of the col colors (be in the bottom)
    hm_pos = cm.ax_heatmap.get_position()
    cc_pos = cm.ax_col_colors.get_position()
    cm.ax_col_colors.set_position([hm_pos.x0, hm_pos.y0-cc_pos.height-distance_btw_boxes, cc_pos.width, cc_pos.height])

    # change the location of the bar
    cb_pos = cm.ax_cbar.get_position()
    cm.ax_cbar.set_position([cb_pos.x0-cb_pos.width-(distance_btw_boxes*4), cb_pos.y0-distance_btw_boxes, cb_pos.width, cb_pos.height])


    # add title
    cm.ax_heatmap.set_title(title+"\n%s"%("; ".join(["%s=%s"%(f, sitems[0]) for f, sitems in field_to_sorted_items.items() if len(sitems)==1])))

    # add the legend

    # add legend in the lft of the heatmap
    def get_empty_legend(label): return Line2D([0], [0], marker="o", label=label, markersize=0, lw=0)
    legend_elements = []
    for f in fields: 
        color_dict = field_to_color_dict[f]
        legend_elements.append(get_empty_legend(f))
        legend_elements += [mpatches.Patch(facecolor=color_dict[val], edgecolor="gray", label=str(val)) for val in field_to_sorted_items[f]]
        legend_elements.append(get_empty_legend(""))


    cm.ax_row_colors.legend(handles=legend_elements, bbox_to_anchor=[-4, 1], loc="upper right")

    #def adjust_cm_positions(cm, df_square, hm_height_multiplier=0.0002, hm_width_multiplier=0.01, cc_height_multiplier=0.017, idx_delimiter="-", distance_btw_boxes=0.0025, cd_height=0.07, rd_width=0.07, cbar_width=0.08, cbar_height=0.015):

    plt.show()

    # save
    print("saving %s"%filename_plot)
    cm.savefig(filename_plot, bbox_inches="tight")

def get_df_enrichment_without_redundant_GO_terms(df_enrichment, obo_file, species_to_gene_to_GOterms, filename_data, semantic_similarity_tsh, keep_oneTerm_per_group, row_fields=["species", "type_var"]):

    """Takes a df with enriched GO terms and returns it pruned to not include redundant GO terms (revigo-like)"""

    print("filtering GO terms")

    # keep
    df_GOEA = cp.deepcopy(df_enrichment).rename(columns={"ID":"GO", "group_name":"name"})
   
    # keep some fields
    df_GOEA["ngenes_pop_and_GO"] = df_GOEA[["ngenes_group_and_target", "ngenes_group_no_target"]].apply(sum, axis=1) # number of genes that have the GO term, in general
    df_GOEA["ngenes_pop"]  =  df_GOEA[["ngenes_group_and_target", "ngenes_group_no_target", "ngenes_no_group_target", "ngenes_no_group_no_target"]].apply(sum, axis=1) # number of genes in general
    df_GOEA["p_fdr_bh"] = df_GOEA.p_fdr

    df_GOEA = df_GOEA[["GO", "ngenes_pop_and_GO", "ngenes_pop", "p_fdr_bh", "name"] + row_fields]

    ########### GET A DF PLOT WITH THE SIMILARITY DISTANCES AND CLUSTER INFORMATION ###########

    # get the semantic similarity
    df_pairwise_semantic_similarity_file = "%s.semantic_similarity.py"%filename_data
    if file_is_empty(df_pairwise_semantic_similarity_file): save_object(get_df_pairwise_Lin_semantic_similarity(df_GOEA, obo_file, species_to_gene_to_GOterms), df_pairwise_semantic_similarity_file)

    df_pairwise_semantic_similarity = load_object(df_pairwise_semantic_similarity_file)

    # get the df_GOEA with the cluster info    
    if file_is_empty(filename_data):
        print("getting similarity")

        # checks
        if any(df_GOEA.ngenes_pop<=df_GOEA.ngenes_pop_and_GO): raise ValueError("the ngenes_pop should be above ngenes_pop_and_GO")

        # get the df_GOEA with a cluster name which is related to the most representative term of the cluster, by semantic similarity
        df_GOEA = get_df_GOEA_with_clusterInfo_bySemantic_similarity(df_GOEA, obo_file, df_pairwise_semantic_similarity, semantic_similarity_tsh=semantic_similarity_tsh)

        # add the sampleID
        df_GOEA["unique_sampleID"] = df_GOEA[row_fields].apply(tuple, axis=1) 

        # keep only one representative GO term for each group, the one that has the lowest pvalue for each sampleID
        if keep_oneTerm_per_group is True:
                        
            # keep one  GO term of each cluster_representative_GO for each unique_sampleID
            df_GOEA = df_GOEA.sort_values(by=["unique_sampleID", "cluster_representative_GO", "p_fdr_bh", "GO"]).drop_duplicates(subset=["unique_sampleID", "cluster_representative_GO"], keep="first")

        # save
        save_object(df_GOEA, filename_data)

    df_GOEA = load_object(filename_data)

    ###########################################################################################

    #### define the tuple_terms_to_similarity ######

    # add the inverse terms to df_pairwise_semantic_similarity
    df_pairwise_semantic_similarity = df_pairwise_semantic_similarity.append(df_pairwise_semantic_similarity.rename(columns={"termA":"termB", "termB":"termA"}))
    df_pairwise_semantic_similarity["terms_str"] = df_pairwise_semantic_similarity.termA + "_" + df_pairwise_semantic_similarity.termB
    check_no_nans_series(df_pairwise_semantic_similarity.semantic_similarity)

    # get only the series
    terms_str_to_similarity = dict(df_pairwise_semantic_similarity[["terms_str", "semantic_similarity"]].drop_duplicates().set_index("terms_str").semantic_similarity)

    ################################################


    # from the initial df_enrichment, only keep non redundant terms
    df_GOEA["unique_goID"] = df_GOEA[row_fields + ["GO"]].apply(tuple, axis=1)
    df_enrichment["unique_goID"] = df_enrichment[row_fields + ["ID"]].apply(tuple, axis=1)

    if len(df_GOEA)!=len(set(df_GOEA.unique_goID)): raise ValueError("not unique datasets")
    if len(df_enrichment)!=len(set(df_enrichment.unique_goID)): raise ValueError("not unique datasets")
    if len(set(df_GOEA.GO).difference(set(df_enrichment.ID)))>0: raise ValueError("there are GO terms that are in df_GOEA and not in df_enrichment")

    df_enrichment = df_enrichment[df_enrichment.unique_goID.isin(set(df_GOEA.unique_goID))]

    return df_enrichment, terms_str_to_similarity 

def plot_enriched_groups_clustermap(df_enrichment, filename_plot, title, groupID_to_orthogroups, obo_file, species_to_gene_to_GOterms,  filename_data, semantic_similarity_tsh, keep_oneTerm_per_group, zfield="-log(p)", col_cluster=True, groups_are_GO=False):

    """Makes a clustermap for all enriched groups. Each row is one group. Each col is a combination of species and type var"""

    # copy
    df_enrichment = cp.deepcopy(df_enrichment)

    # define fields
    df_enrichment["-log(p)"] = -np.log10(df_enrichment.p_raw)
    #sorted_type_var = ["SNP", "IN/DEL", "SV", "coverageCNV"]
    sorted_type_var = ["SNP", "if_INDEL", "DUP", "DEL"]
    #type_var_to_color = {"SNP":"red", "IN/DEL":"magenta", "SV":"navy", "coverageCNV":"cyan"}
    type_var_to_color = {"SNP":"red", "if_INDEL":"magenta", "DEL":"grey", "DUP":"navy"}

    #### MAKE SQUARE DF ######

    # make a df with the groups and the pval as z value
    all_groups = set(df_enrichment.ID)
    sorted_groups = sorted(all_groups)
    df_plot = pd.DataFrame(index=sorted_groups)

    sorted_cols = []
    for species in sorted_species_byPhylogeny:
        for type_var in sorted_type_var:

            # skip spp-v that have no selection
            if (species, type_var) in {("Candida_parapsilosis", "SNP"), ("Candida_parapsilosis", "DUP"), ("Candida_orthopsilosis", "DUP")}: continue

            # map each group to the zvalue
            df_g = df_enrichment[(df_enrichment.species==species) & (df_enrichment.type_var==type_var)]
            if len(df_g)!=len(set(df_g.ID)): raise ValueError("there are repeated IDs")


            ID_to_zval = dict(df_g.set_index("ID")[zfield])
            for ID in all_groups.difference(set(df_g.ID)): ID_to_zval[ID] = np.nan


            # skip empty cols
            #if len(df_g)==0: continue

            col = (species, type_var)
            df_plot[col] = [min(ID_to_zval[x], 7.0) for x in df_plot.index]
            sorted_cols.append(col)

    sorted_rows = list(df_plot.index)
    print("%i rows"%len(sorted_rows))

    ##########################

    ########## GET LINKAGE DATA ##########

    # define a function that returns the jaccard distance
    def get_jaccard_distance(groups1, groups2):

        all_IDs = groups1.union(groups2)
        shared_IDs = groups1.intersection(groups2)

        if len(all_IDs)==0: return 0.0
        else: return 1-(len(shared_IDs)/len(all_IDs))

    # define a function that takes two cols or rows and returns the distance between them
    def get_distance_cols_or_rows(x1, x2, type_data):

        # get the data of each row and col
        if type_data=="cols":
            groups1 = set(df_plot[~pd.isna(df_plot[x1])].index)
            groups2 = set(df_plot[~pd.isna(df_plot[x2])].index)

        elif type_data=="rows": error_undefined


        # get the jaccard distance
        return get_jaccard_distance(groups1, groups2)

    # define a function that takes two groupIDs and returns the distance based on the overlap of orthogroups
    def get_distance_orthogroups(ID1, ID2):

        # get the data of each row and col
        groups1 = groupID_to_orthogroups[ID1]
        groups2 = groupID_to_orthogroups[ID2]

        # get the jaccard distance
        return get_jaccard_distance(groups1, groups2) 

    # define a functtion that returns the semantic distance between GO terms
    def get_GO_semantic_distance(go1, go2, terms_str_to_similarity):

        # equal terms, return no distance
        if go1==go2: return 0.0
        else: 
            distance = 1 - terms_str_to_similarity["%s_%s"%(go1, go2)]
            return distance

    # define the linkage of cols
    df_cols_distance = pd.DataFrame({col1 : {col2 : get_distance_cols_or_rows(col1, col2, "cols") for col2 in sorted_cols} for col1 in sorted_cols}).loc[sorted_cols, sorted_cols]

    # define the linkage of cols (groups)
    if groups_are_GO is True:

        # get the enriched go terms removing redundancy
        df_enrichment, terms_str_to_similarity = get_df_enrichment_without_redundant_GO_terms(df_enrichment, obo_file, species_to_gene_to_GOterms, filename_data, semantic_similarity_tsh, keep_oneTerm_per_group)

        # redefine the sorted_rows
        all_GO_terms = set(df_enrichment.ID)
        sorted_rows = [r for r in sorted_rows if r in all_GO_terms]
        df_plot = df_plot.loc[sorted_rows]

        # define the row distance
        print("getting row distance")
        df_rows_distance = pd.DataFrame({row1 : {row2 : get_GO_semantic_distance(row1, row2, terms_str_to_similarity) for row2 in sorted_rows} for row1 in sorted_rows}).loc[sorted_rows, sorted_rows]

    else: df_rows_distance = pd.DataFrame({row1 : {row2 : get_distance_orthogroups(row1, row2) for row2 in sorted_rows} for row1 in sorted_rows}).loc[sorted_rows, sorted_rows]

    # get the linkage
    if len(df_plot.columns)>1 and col_cluster is True: 
        col_linkage = scipy.cluster.hierarchy.linkage(scipy.spatial.distance.squareform(df_cols_distance), method='average', metric="euclidean")
        col_cluster = True 

    else: 
        col_linkage = None
        col_cluster = False

    if len(df_plot.index)>1: 
        row_linkage = scipy.cluster.hierarchy.linkage(scipy.spatial.distance.squareform(df_rows_distance), method='average', metric="euclidean")
        row_cluster = True 

    else: 
        row_linkage = None
        row_cluster = False

    ######################################

    ######### PLOT #########

    # define the colors df
    col_colors_df = pd.DataFrame({col : {colfield : color_dict[col[I]] for I, (colfield, color_dict) in enumerate([("species", species_to_color), ("type_var", type_var_to_color)])} for col in sorted_cols}).transpose()

    # define the clustermap
    mask = df_plot.isnull()
    colorbar_label = zfield
    cm = sns.clustermap(df_plot, col_cluster=col_cluster, col_linkage=col_linkage, row_cluster=row_cluster, row_linkage=row_linkage, yticklabels=True, xticklabels=False, col_colors=col_colors_df, linecolor="gray", linewidth=.1, cbar_kws={'label': colorbar_label, "orientation":"horizontal"}, mask=mask, cmap="rocket_r", vmin=0, vmax=7); # row_colors = row_colors_df

    # adjust
    adjust_cm_positions(cm, df_plot, hm_height_multiplier=0.018, hm_width_multiplier=0.025, cc_height_multiplier=0.025, distance_btw_boxes=0.007) # hm_height_multiplier=0.015

    # add title
    cm.ax_col_dendrogram.set_title(title)

    # define a function that defines the number of species that an ID has
    def get_species_one_ID(ID):
        ID_series = df_plot.loc[ID]
        ID_series = ID_series[~pd.isna(ID_series)]
        return {x[0] for x in ID_series.index}
         
    # add a tick of the species color if the terms are only in one spp
    for I,ID_label in enumerate(cm.ax_heatmap.get_yticklabels()):
        ID = ID_label.get_text()
        for Is, spp in enumerate(sorted(get_species_one_ID(ID))):
            cm.ax_heatmap.plot([len(df_plot.columns)+0.5+(Is*0.5)], [I+0.5], marker="o", color=species_to_color[spp], markeredgecolor="black", markeredgewidth=.5)

    # redefine xlims
    max_nspp = max([len(get_species_one_ID(ID_label.get_text())) for ID_label in cm.ax_heatmap.get_yticklabels()])
    xlims = [0, len(df_plot.columns)+0.5+(max_nspp*0.5)]
    cm.ax_heatmap.set_xlim(xlims)
    cm.ax_col_colors.set_xlim(xlims)

    # add descrittion of the pathway as name
    ID_to_name = df_enrichment[["ID", "group_name"]].drop_duplicates().set_index("ID").group_name
    cm.ax_heatmap.tick_params(axis=u'both', which=u'both',length=0)
    cm.ax_heatmap.set_yticklabels([ID_to_name[ID.get_text()] for ID in cm.ax_heatmap.get_yticklabels()], rotation=0, fontsize=11)

    # create in bold the shared species ticks
    """
    shared_species_names = {ID_to_name[ID] for ID in df_plot.index if len(get_nspecies_one_ID(ID))>=2}

    for label in cm.ax_heatmap.get_yticklabels():
        if label.get_text() in shared_species_names:
            label.set_size(11)
            label.set_weight("bold")

        else:
            label.set_size(10)
    """

    # add legend in the lft of the heatmap
    def get_empty_legend(label): return [Line2D([0], [0], marker="o", label=label, markersize=0, lw=0)]

    legend_elements = get_empty_legend("species") + [mpatches.Patch(facecolor=species_to_color[s], edgecolor="gray", label=s) for s in sorted_species_byPhylogeny]  + get_empty_legend("") + get_empty_legend("type var") + [mpatches.Patch(facecolor=type_var_to_color[t], edgecolor="gray", label=t) for t in sorted_type_var]


    cm.ax_row_dendrogram.legend(handles=legend_elements, bbox_to_anchor=[0, 1], loc="upper right")

    #def adjust_cm_positions(cm, df_square, hm_height_multiplier=0.0002, hm_width_multiplier=0.01, cc_height_multiplier=0.017, idx_delimiter="-", distance_btw_boxes=0.0025, cd_height=0.07, rd_width=0.07, cbar_width=0.08, cbar_height=0.015):

    plt.show()

    # save
    print("saving %s"%filename_plot)
    cm.savefig(filename_plot, bbox_inches="tight")


    ########################

def get_target_and_all_genes_under_selection(species, type_var, selection, type_vars_SimpleRepeats, type_vars_appearance, df_diversity_all, gene_features_df, percentiles_thresholds, max_fraction_samples_extreme_piS_piS):

    """gets the target and all genes under selection as sets."""

    # get filtered df
    df_diversity = df_diversity_all[(df_diversity_all.type_vars_SimpleRepeats==type_vars_SimpleRepeats) & (df_diversity_all.type_vars_appearance==type_vars_appearance) & (df_diversity_all.type_var==type_var) & (df_diversity_all.species==species)] # at least 2 clades under selection

    # define all the genes
    strange_genes = set(df_diversity.Gene).difference(set(gene_features_df[gene_features_df.species==species].gff_upmost_parent))
    if len(strange_genes)>0: raise ValueError("There are some strange genes: %s"%strange_genes)
    all_genes = set(df_diversity.Gene)

    # define genes under selection
    if type_var in {"if_INDEL", "DEL", "DUP"}:

        # checks
        if len(df_diversity)!=len(set(df_diversity.Gene)): raise ValueError("There are non-unique genes")


        # define genes under selection, as done in visualize_distribution_SVs_INDELs_under_selection
        low_percentile, high_percentile = get_high_low_percentiles_df_diversity_INDELs_SVs(df_diversity, percentiles_thresholds, "harmonicMean_fraction_samples_and_clades")

        if selection=="positive": 
            target_genes = set(df_diversity[(df_diversity.harmonicMean_fraction_samples_and_clades>high_percentile) & (df_diversity.nclades_selection>=2)].Gene)

        elif selection=="negative": 
            target_genes = set(df_diversity[(df_diversity.harmonicMean_fraction_samples_and_clades<low_percentile) & (df_diversity.nclades_selection>=2)].Gene)

        else: raise ValueError("selection is invalid")

    elif type_var=="SNP": 

        # get genes under selection
        selection_to_genes = get_genes_under_selection_SNPs_union_all_thresholds_piN_piS(df_diversity, max_fraction_samples_extreme_piS_piS)
        target_genes = selection_to_genes[selection]

    else: raise ValueError("type var is invalid")

    return target_genes, all_genes


def plot_overlapping_northogroups_selection(gene_features_df, ProcessedDataDir, PlotsDir, df_diversity_all, percentiles_thresholds, max_fraction_samples_extreme_piS_piS=0.1):

    """This data plots the overlap between orthogroups by different selection sets"""

    ######## GET DF #########

    genes_df_all_file = "%s/genes_df_all_selection.py"%ProcessedDataDir

    if file_is_empty(genes_df_all_file):

        # load genes df from plot_distribution_and_GOEA_genes_selection
        plots_dir = "%s/genes_under_selection"%PlotsDir;  make_folder(plots_dir)
        genes_df_all = pd.DataFrame()

        for type_var in sorted(set(df_diversity_all.type_var)):
           for selection in ["positive", "negative"]:
                for type_vars_SimpleRepeats in ["only_vars_noSimpleRepeats"]:
                    for type_vars_appearance in ["only_vars_recent", "all_vars"]:
                        for species in sorted_species_byPhylogeny:
                            print(type_var, selection)

                            # skip
                            if selection!="positive": continue
                            if type_vars_appearance!="only_vars_recent": continue

                            # get filtered df
                            target_genes, all_genes = get_target_and_all_genes_under_selection(species, type_var, selection, type_vars_SimpleRepeats, type_vars_appearance, df_diversity_all, gene_features_df, percentiles_thresholds, max_fraction_samples_extreme_piS_piS)
                            if len(target_genes)==0: continue


                            # get the filtered df of diversity
                            df_diversity = df_diversity_all[(df_diversity_all.type_vars_SimpleRepeats==type_vars_SimpleRepeats) & (df_diversity_all.type_vars_appearance==type_vars_appearance) & (df_diversity_all.type_var==type_var) & (df_diversity_all.species==species)]
                            if type_var=="SNP": df_diversity = df_diversity[(df_diversity.threshold_piNpiS.isin(sel_to_valid_thresholds_piNpiS[selection])) & (df_diversity.selection==selection)]


                            # create a df with the genes and save under an excel
                            filename_genes = "%s/%s_%s_%s-selection_%s_%s.xlsx"%(plots_dir, species, type_var.replace("/", "-"), selection,  type_vars_SimpleRepeats, type_vars_appearance)
                            df_genes = gene_features_df[(gene_features_df.species==species) & (gene_features_df.gff_upmost_parent.isin(target_genes))]
                            if set(df_genes.gff_upmost_parent)!=target_genes: raise ValueError("df_genes is missing genes")

                            # add values
                            gene_to_min_hmean_FractionSamples_selection = df_diversity[["Gene", "harmonicMean_fraction_samples_and_clades"]].groupby("Gene").apply(lambda df: min(df.harmonicMean_fraction_samples_and_clades))

                            gene_to_max_pval = df_diversity[["Gene", "pval_harmonicMean_fraction_samples_and_clades_fdr"]].groupby("Gene").apply(lambda df: max(df.pval_harmonicMean_fraction_samples_and_clades_fdr))

                            df_genes["min_harmonicMean_fraction_samples_and_clades"] = df_genes.gff_upmost_parent.apply(lambda g: gene_to_min_hmean_FractionSamples_selection[g])
                            df_genes["max_pval_harmonicMean_fraction_samples_and_clades_fdr"] = df_genes.gff_upmost_parent.apply(lambda g: gene_to_max_pval[g])                        
                            print("saving %s"%filename_genes)
                            fields = ['species', 'chromosome', 'start', 'end', 'gff_upmost_parent', 'gene_name', 'Scerevisiae_orthologs', "min_harmonicMean_fraction_samples_and_clades", "max_pval_harmonicMean_fraction_samples_and_clades_fdr", 'orthofinder_orthocluster', 'description', 'GOterms']
                            df_genes[fields].sort_values(by=["max_pval_harmonicMean_fraction_samples_and_clades_fdr", "min_harmonicMean_fraction_samples_and_clades"], ascending=[True, False]).reset_index(drop=True).to_excel(filename_genes)

                            # check
                            if set(df_genes.gff_upmost_parent)!=target_genes: raise ValueError("the genes df is invalid")


                            # add fields and keep
                            df_genes["selection"] = selection
                            df_genes["type_vars_SimpleRepeats"] = type_vars_SimpleRepeats
                            df_genes["type_vars_appearance"] = type_vars_appearance
                            df_genes["type_var"] = type_var
                            df_genes["species"] = species

                            genes_df_all = genes_df_all.append(df_genes)

        save_object(genes_df_all, genes_df_all_file)

    genes_df_all = load_object(genes_df_all_file)

    #########################

    ###### plot the overlap in terms of orthogroups ##########

    # add the orthogroup
    genes_df_all_final =  pd.DataFrame()
    for spp in sorted_species_byPhylogeny:
        genes_df_spp = genes_df_all[genes_df_all.species==spp]
        if len(genes_df_spp)==0: continue

        gene_to_og = dict(gene_features_df[gene_features_df.species==spp].set_index("gff_upmost_parent").orthofinder_orthocluster)
        genes_df_spp["orthogroup"] = genes_df_spp.gff_upmost_parent.apply(lambda x: gene_to_og[x])
        genes_df_all_final = genes_df_all_final.append(genes_df_spp)

    genes_df_all = genes_df_all_final
    genes_df_all = genes_df_all[~pd.isna(genes_df_all.orthogroup)]

    # filter df
    interesting_selections = {"positive"} # could also be {"positive", "negative"}
    interesting_type_vars_appearances = {"only_vars_recent"} # ["all_vars", "only_vars_recent"]
    interesting_type_var =  {"SNP", "if_INDEL", "DEL", "DUP"}
    #interesting_type_var =  {"SNP", "if_INDEL", "DUP"}
    #interesting_type_var =  {"DUP"}

    genes_df = genes_df_all[(genes_df_all.selection.isin(interesting_selections)) & (genes_df_all.type_vars_SimpleRepeats=="only_vars_noSimpleRepeats") & (genes_df_all.type_vars_appearance.isin(interesting_type_vars_appearances)) & (genes_df_all.type_var.isin(interesting_type_var))]


    # get plot attributes
    plots_dir = "%s/plots_enriched_overlap_orthogroups_selection"%(PlotsDir); make_folder(plots_dir)
    filename_plot = "%s/overlaps_%s_%s_%s.pdf"%(plots_dir, "-".join(sorted(interesting_selections)), "-".join(sorted(interesting_type_vars_appearances)), "-".join(sorted(interesting_type_var)))

    # define the title of of the plot
    title = "Jaccard Distance by enriched groups (distance by orthogroups)"

    # keep only essential fields
    genes_df = genes_df[["species", "type_var", "selection", "type_vars_SimpleRepeats", "type_vars_appearance", "orthogroup"]].drop_duplicates()


    # make the plot
    plot_enriched_groups_n_overlapping_groups(genes_df, filename_plot, title, IDfield="orthogroup", min_overlap_plot=0)


    ##########################################################

def get_df_GOterms_all_with_NS_and_description(df_GOterms_all, filename, obo_file):

    """Gets the description and NS"""

    if file_is_empty(filename):
        print("adding description")

        # keep initial len
        initial_len = len(df_GOterms_all)

        # define all GOs
        sorted_GOs = sorted(set(df_GOterms_all.GO))

        # load obo
        obodag = GODag(obo_file,  optional_attrs={'consider', 'replaced_by'}, load_obsolete=True, prt=None)

        # get the df
        def get_attrs_dict_for_GO(GO):
            info_go = obodag[GO]
            return {"namespace":info_go.namespace, "description":info_go.name, "GO":GO}

        df_attrs = pd.DataFrame(dict(zip(sorted_GOs, map(get_attrs_dict_for_GO, sorted_GOs)))).transpose().reset_index(drop=True)

        # add to the df_GOterms_all
        df_GOterms_all = df_GOterms_all.merge(df_attrs, on="GO", validate="many_to_one", how="left")

        # checks
        check_no_nans_in_df(df_GOterms_all)
        if len(df_GOterms_all)!=initial_len: raise ValueError("invalid merge")

        # change
        namespace_to_NS = {"molecular_function":"MF", "biological_process":"BP", "cellular_component":"CC"}
        df_GOterms_all["NS"] = df_GOterms_all.namespace.apply(lambda x: namespace_to_NS[x])

        # save
        save_object(df_GOterms_all, filename)

    return load_object(filename)

def plot_enriched_domains_or_pathways_genes_selection(DataDir, gene_features_df, df_diversity_all, ProcessedDataDir, PlotsDir, percentiles_thresholds, species_to_gff, replace=False, figsize=(6, 1.5), min_OR=1, alpha = 0.05, factor_size_minus_log10pval=5, figsize_multiplier=2, max_fraction_samples_extreme_piS_piS=0.1, max_fraction_genes_pathway_toBeConsidered=0.25, plots={"overlap_btw_species", "clustermap_all_names"}, keep_only_repeated_groups=True, keep_oneTerm_per_group=True, interesting_type_grouping =  ["MetaCyc", "IP_domains", "Reactome", "GO_BP", "GO_CC", "GO_MF"]):


    """Performs enrichment for different types of groupings"""

    # define the parameters string, useful to not repeat things
    parms_string = "selection_pcts=%s_maxFractGenes=%s"%("-".join([str(x) for x in percentiles_thresholds]), max_fraction_genes_pathway_toBeConsidered)

    # define the outdir
    outdir = "%s/enriched_domains_or_pathways_genes_selection_files_%s"%(ProcessedDataDir, parms_string)
    make_folder(outdir)

    # define the obofile
    obo_file = "%s/annotation_files/go-basic_30062021.obo"%(DataDir) # this was got from http://purl.obolibrary.org/obo/go/go-basic.obo


    ### GET DF ###
    
    # define file
    df_enrichment_all_file = "%s/df_diversity_df_enrichment_file_%s.py"%(outdir, parms_string)

    # get the enrichments
    if file_is_empty(df_enrichment_all_file) or replace is True:

        # init dfs
        df_enrichment_all = pd.DataFrame()

        # go through each species
        for species in sorted_species_byPhylogeny:

            # define the dir of the ancestral GWAS, which is where I'd get the annotations
            taxID_dir = "%s/%s_%i"%(DataDir, species, sciName_to_taxID[species])
            outdir_species = "%s/%s"%(outdir, species); 
            make_folder(outdir_species)

            # go through each type of grouping
            for type_grouping in ["GO_BP", "GO_CC", "GO_MF", "Reactome", "IP_domains", "MetaCyc"]:
                print(species, type_grouping)

                # load the interpro df, seed for the other groupings
                df_interpro = load_InterProAnnotation("%s/InterproScan_annotation/interproscan_annotation.out"%taxID_dir)
                df_interpro_pathways_all = get_df_pathways_from_df_interpro(df_interpro, "%s/interproscan_pathway_df.py"%outdir_species)

                # load a df that has Gene, group_name, ID
                if type_grouping=="MetaCyc":

                    # load the metacyc pathways from interpro
                    df_metacyc_all = df_interpro_pathways_all[df_interpro_pathways_all.pathway_type=="MetaCyc"]
                    df_metacyc_all = get_df_metacyc_with_pathways_from_parents(df_metacyc_all, outdir_species, max_fraction_genes_pathway_toBeConsidered)

                    # define the grouping df
                    df_grouping = df_metacyc_all.rename(columns={"pathway_ID":"ID"})
                    df_grouping["group_name"] = df_grouping.ID.map(get_metacycID_to_description(list(df_grouping.ID), "%s/%s_metacyc_ID_to_description.py"%(outdir, species)))

                elif type_grouping=="Reactome":

                    # load the reactome pathways from interptro
                    reactome_pathways_file =  "%s/annotation_files/ReactomePathways.txt"%(DataDir) 
                    reactome_pathwayRelations_file = "%s/annotation_files/ReactomePathwaysRelation.txt"%DataDir

                    df_reactome_all = df_interpro_pathways_all[df_interpro_pathways_all.pathway_type=="Reactome"]
                    df_reactome_all = get_df_reactome_with_Parents_transferred(df_reactome_all, reactome_pathways_file, reactome_pathwayRelations_file, max_fraction_genes_pathway_toBeConsidered, outdir_species)

                    # get the grouping df
                    df_grouping = df_reactome_all.rename(columns={"pathway_ID":"ID"})

                    df_reactome_pathways = pd.read_csv(reactome_pathways_file, sep="\t", header=None, names=["ID", "group_name", "species"])
                    df_reactome_pathways["group_name"] = df_reactome_pathways.species.apply(lambda x: "%s. %s"%(x.split()[0][0], x.split()[1])) + " " + df_reactome_pathways.group_name
                    
                    df_grouping = df_grouping.merge(df_reactome_pathways[["ID", "group_name"]], on="ID", validate="many_to_one", how="left")

                elif type_grouping=="IP_domains":
                    df_grouping = df_interpro.rename(columns={"proteinID":"Gene", "signature_accession":"ID"})
                    df_grouping["group_name"] = df_grouping.type_analysis + "; " + df_grouping.signature_description + "; IPaccession=" + df_grouping.InterPro_accession + "(" + df_grouping.InterPro_annotation_description + ")"

                elif type_grouping in {"GO_MF", "GO_CC", "GO_BP"}:

                    # load GO terms
                    df_GOterms_per_gene = get_df_GOterms_all_species(DataDir, {species:species_to_gff[species]}, outdir_species) # each row is one GO term
                    df_GOterms_all = get_df_GOterms_transfering_across_theGraph(df_GOterms_per_gene, outdir_species, obo_file, max_fraction_genes_pathway_toBeConsidered)

                    # add the description and the namespace
                    df_GOterms_all = get_df_GOterms_all_with_NS_and_description(df_GOterms_all, "%s/df_GOterms_all_w_adds.py"%outdir_species, obo_file)

                    # keep final file
                    df_grouping = df_GOterms_all[df_GOterms_all.NS==(type_grouping.split("_")[1])].rename(columns={"GO":"ID", "description":"group_name"})


                else: raise ValueError("type grouping is invalid")

                if len(df_grouping)==0: raise ValueError("df_grouping can't be 0")
                df_grouping = df_grouping[["Gene", "ID", "group_name"]]

                # checks
                for f in ["Gene", "ID"]: check_no_nans_series(df_grouping[f])
                IDs_no_description = set(df_grouping[pd.isna(df_grouping.group_name)].ID)
                if len(IDs_no_description)>0: print( "WARNING: There are these IDs with no description: %s"%IDs_no_description )

                valid_genes = set(gene_features_df[gene_features_df.species==species].gff_upmost_parent)
                strange_genes = set(df_grouping.Gene).difference(valid_genes)
                if len(strange_genes)>0: raise ValueError("%s. There are %i strange genes: %s"%(species, len(strange_genes), strange_genes))

                # log
                print("%s. There are %i/%i genes with some %s group assigned"%(species, len(set(df_grouping.Gene)), len(valid_genes), type_grouping))


                # go through each set of variants
                for type_var in sorted(set(df_diversity_all.type_var)):
                   for selection in ["positive", "negative"]:
                        for type_vars_SimpleRepeats in ["only_vars_noSimpleRepeats"]:
                            for type_vars_appearance in ["only_vars_recent"]: # also all_vars
                                print(species, type_grouping, type_var, selection, type_vars_SimpleRepeats, type_vars_appearance)

                                # get the genes under selection
                                target_genes, all_genes = get_target_and_all_genes_under_selection(species, type_var, selection, type_vars_SimpleRepeats, type_vars_appearance, df_diversity_all, gene_features_df, percentiles_thresholds, max_fraction_samples_extreme_piS_piS)

                                # skip if there are no genes
                                if len(target_genes)==0:
                                    print("There are %i/%i genes with %s for %ss in %s. Considering %s %s"%(len(target_genes), len(all_genes), selection, type_var, species, type_vars_SimpleRepeats, type_vars_appearance))
                                    continue

                                # get the df with the results of the enrichment
                                df_enrichment = get_df_enrichment_particular_grouping(target_genes, all_genes, df_grouping, "%s/%s_%s_%s_%s_%s"%(outdir_species, selection, type_vars_SimpleRepeats, type_vars_appearance, type_var.replace("/", "_"), type_grouping))

                                # keep
                                df_enrichment["selection"] = selection
                                df_enrichment["type_vars_SimpleRepeats"] = type_vars_SimpleRepeats
                                df_enrichment["type_vars_appearance"] = type_vars_appearance
                                df_enrichment["type_var"] = type_var
                                df_enrichment["species"] = species
                                df_enrichment["type_grouping"] = type_grouping
                                df_enrichment_all = df_enrichment_all.append(df_enrichment).reset_index(drop=True)    

        # save df
        save_object(df_enrichment_all, df_enrichment_all_file)

    df_enrichment_all = load_object(df_enrichment_all_file)

    ##############

    ####### GENERAL DATA ########

    # redefine the parms_string to include alpha and OR
    parms_string = "%s_minOR=%s_alpha=%s"%(parms_string, min_OR, alpha)

    #############################

    ########## PLOT COMPARISON BTW SPECIES AND TYPES VAR ###########

    if "overlap_btw_species" in plots:
        print("PLOTTING")

        # filter df
        interesting_selections = {"positive"} # could also be {"positive", "negative"}
        interesting_type_vars_appearances = {"only_vars_recent"} # ["all_vars", "only_vars_recent"]
        #interesting_type_var = {"SNP", "IN/DEL", "SV", "coverageCNV"}
        interesting_type_var = {"SNP", "if_INDEL", "DEL", "DUP"}

        df_enrichment_all_groups = df_enrichment_all[(df_enrichment_all.selection.isin(interesting_selections)) & (df_enrichment_all.type_vars_SimpleRepeats=="only_vars_noSimpleRepeats") & (df_enrichment_all.type_vars_appearance.isin(interesting_type_vars_appearances)) & (df_enrichment_all.p_fdr<alpha) & (df_enrichment_all.OR>=min_OR) & (df_enrichment_all.type_var.isin(interesting_type_var))]


        # define the types of grouping. Onle plot for each
        for type_grouping in interesting_type_grouping:

            # get df
            df_enrichment = df_enrichment_all_groups[df_enrichment_all_groups.type_grouping==type_grouping]
            if len(df_enrichment)==0: continue

            # get plot attributes
            plots_dir = "%s/plots_enriched_domains_pathways_%s_overlaps_in_enriched_pathways"%(PlotsDir, parms_string); make_folder(plots_dir)
            filename_plot = "%s/%s.pdf"%(plots_dir, type_grouping)

            # define the title of of the plot
            title = "groups=%s\np(raw)<0.05, p(fdr)<%.4f, min_OR=%.4f\nJaccard Distance by enriched groups"%(type_grouping, alpha, min_OR)

            # make the plot
            print("saving %s"%filename_plot)
            plot_enriched_groups_n_overlapping_groups(df_enrichment, filename_plot, title, min_overlap_plot=0)

    ################################################################


    ########### PLOT RESULTS #############

    if "clustermap_all_names" in plots:

        # define combinations to plot
        interesting_selection = ["positive"] # could also be ["positive", "negative"]
        interesting_type_vars_appearance = ["only_vars_recent"] # ["all_vars", "only_vars_recent"]


        # define the interesting types of variants
        #interesting_type_var = {"SNP", "IN/DEL", "SV", "coverageCNV"}
        interesting_type_var = {"SNP", "if_INDEL", "DEL", "DUP"}

        # define the GO terms of each species
        species_to_gene_to_GOterms = dict(gene_features_df.groupby("species").apply(lambda df_s: dict(df_s.set_index("gff_upmost_parent").GOterms)))

        # make one plot for each combination (all species and types_var)
        for selection in interesting_selection: 
            for type_vars_SimpleRepeats in ["only_vars_noSimpleRepeats"]: 
                for type_vars_appearance in interesting_type_vars_appearance: # all_vars, only_vars_recent
                    for type_grouping in interesting_type_grouping:
                        print(selection, type_vars_SimpleRepeats, type_vars_appearance, type_grouping)

                        # get the enrichment df
                        df_enrichment = df_enrichment_all[(df_enrichment_all.type_grouping==type_grouping) & (df_enrichment_all.selection==selection) & (df_enrichment_all.type_vars_SimpleRepeats==type_vars_SimpleRepeats) & (df_enrichment_all.type_vars_appearance==type_vars_appearance) & (df_enrichment_all.p_fdr<alpha) & (df_enrichment_all.OR>=min_OR) & (df_enrichment_all.type_var.isin(interesting_type_var))]

                        if len(df_enrichment)==0: continue

                        # keep only groups that are overlapping something
                        if keep_only_repeated_groups is True:

                            ID_to_ncells = df_enrichment[["ID", "type_var", "species"]].drop_duplicates().groupby("ID").apply(len)
                            df_enrichment["ncells"] = df_enrichment.ID.apply(lambda x: ID_to_ncells[x])
                            df_enrichment = df_enrichment[df_enrichment.ncells>1]

                        # keep only pfam domains
                        #if type_grouping=="IP_domains": df_enrichment = df_enrichment[df_enrichment.group_name.apply(lambda x: (x.split(";")[0])=="Pfam")]

                        # for Reactome, only S. cerevisiae
                        if type_grouping=="Reactome": df_enrichment = df_enrichment[df_enrichment.group_name.apply(lambda x: x.split()[1])=="cerevisiae"]
                        if len(df_enrichment)==0: continue

                        # do not proubt if there are too many groups
                        ngroups = len(set(df_enrichment.ID))

                        # define if the group is a GO
                        groups_are_GO = type_grouping.startswith("GO_")

                        # discard if there are too many pathways
                        #if len(set(df_enrichment.ID))>500: continue

                        # map each groupID to the orthogroups
                        print("getting groupID_to_orthogroups")
                        df_enrichment["all_genes_group"] = (df_enrichment["genes_group_no_target"] + df_enrichment["genes_group_and_target"]).apply(set)

                        groupID_to_orthogroups = {ID:set() for ID in set(df_enrichment.ID)}
                        for spp in sorted_species_byPhylogeny:
                            df_enrichment_spp = df_enrichment[df_enrichment.species==spp]
                            if len(df_enrichment_spp)==0: continue

                            gene_to_og = dict(gene_features_df[gene_features_df.species==spp].set_index("gff_upmost_parent").orthofinder_orthocluster)
                            df_enrichment_spp["ogs"] = df_enrichment_spp.all_genes_group.apply(lambda genes: set(map(lambda g: gene_to_og[g], genes)))
                            df_enrichment_spp["ogs"] = df_enrichment_spp["ogs"].apply(lambda ogs: {og for og in ogs if not pd.isna(og)})

                            for ID, ogs in df_enrichment_spp[["ID", "ogs"]].values: groupID_to_orthogroups[ID].update(ogs)

                        # define the plot
                        plots_dir = "%s/plots_enriched_domains_pathways_%s"%(PlotsDir, parms_string); 
                        make_folder(plots_dir)
                        filename_plot = "%s/%s-selection_%s_%s_%s_onlyRepeated=%s.pdf"%(plots_dir, selection, type_vars_SimpleRepeats, type_vars_appearance, type_grouping, keep_only_repeated_groups)


                        # define the title of of the plot
                        title = "%s sel., %s, %s, %s\np(raw)<0.05, p(fdr)<%.4f, min_OR=%.4f\ngroups clustered by orthogroup overlap distance or GO Lin Semantic similarity"%(selection, type_vars_SimpleRepeats, type_vars_appearance, type_grouping, alpha, min_OR)


                        # if it is a GO term, add info
                        if groups_are_GO is True: 

                            # define the semantic_similarity_tsh
                            if len(set(df_enrichment.ID))<15: semantic_similarity_tsh = 1.0
                            else: semantic_similarity_tsh = 0.5


                            # add info to filename_plot
                            filename_plot += "_semSim=%.2f_onlyRepTerm%s.pdf"%(semantic_similarity_tsh, keep_oneTerm_per_group)

                            # define the filename data
                            filename_data = "%s.data.py"%filename_plot

                            # add to title
                            title += "\nsemantic_similarity_tsh=%s; keep_oneTerm_per_group=%s"%(semantic_similarity_tsh, keep_oneTerm_per_group)

                        else: semantic_similarity_tsh = filename_data = None

                        # make the plot
                        print("plotting")
                        plot_enriched_groups_clustermap(df_enrichment, filename_plot, title, groupID_to_orthogroups, obo_file, species_to_gene_to_GOterms, filename_data, semantic_similarity_tsh, keep_oneTerm_per_group, col_cluster=False, groups_are_GO=groups_are_GO)

    ######################################

    # _minLOR=%s_alpha=%s

def plot_distribution_and_GOEA_genes_selection(DataDir, gene_features_df, df_diversity_all, ProcessedDataDir, PlotsDir, percentiles_thresholds, replace=False, figsize=(6, 1.5), min_LOR=0, alpha = 0.3, factor_size_minus_log10pval=5, figsize_multiplier=2, max_fraction_samples_extreme_piS_piS=0.1):

    """draws the GO terms enriched in the genes that are under positive or negative selection. plot_distribution_and_GOEA_genes_high_or_low_piN_piS was used as a template"""

    # define the parameters string, useful to not repeat things
    parms_string = "selection_pcts=%s_minLOR=%s_alpha=%s"%("-".join([str(x) for x in percentiles_thresholds]), min_LOR, alpha)


    # define the obo file
    obo_file = "%s/annotation_files/go-basic_30062021.obo"%(DataDir) # this was got from http://purl.obolibrary.org/obo/go/go-basic.obo

    ##### GET THE DF ########


    # define the file
    df_GOEA_all_file = "%s/df_diversity_df_GOEA_file_%s.py"%(ProcessedDataDir, parms_string)


    if file_is_empty(df_GOEA_all_file) or replace is True:
        
        # init dfs
        df_GOEA_all = pd.DataFrame()
        df_GOEA_ngenes_all = pd.DataFrame()

        for type_var in sorted(set(df_diversity_all.type_var)):
           for selection in ["positive", "negative"]:
                for type_vars_SimpleRepeats in ["only_vars_noSimpleRepeats"]:
                    for type_vars_appearance in ["only_vars_recent", "all_vars"]:
                        for species in sorted_species_byPhylogeny:


                            #if type_var!="SNP": continue

                            # log
                            print("\nGOEA", type_var, selection, type_vars_SimpleRepeats, type_vars_appearance, species)

                            # get filtered df
                            df_diversity = df_diversity_all[(df_diversity_all.type_vars_SimpleRepeats==type_vars_SimpleRepeats) & (df_diversity_all.type_vars_appearance==type_vars_appearance) & (df_diversity_all.type_var==type_var) & (df_diversity_all.species==species)]

                            # define all the genes
                            strange_genes = set(df_diversity.Gene).difference(set(gene_features_df[gene_features_df.species==species].gff_upmost_parent))
                            if len(strange_genes)>0: raise ValueError("There are some strange genes: %s"%strange_genes)
                            all_genes = set(df_diversity.Gene)

                            # define genes under selection
                            if type_var in {"IN/DEL", "SV", "coverageCNV"}:

                                # checks
                                if len(df_diversity)!=len(set(df_diversity.Gene)): raise ValueError("There are non-unique genes")

                                # define genes under selection, as done in visualize_distribution_SVs_INDELs_under_selection
                                low_percentile, high_percentile = get_high_low_percentiles_df_diversity_INDELs_SVs(df_diversity, percentiles_thresholds, "harmonicMean_fraction_samples_and_clades")

                                if selection=="positive": 
                                    target_genes = set(df_diversity[df_diversity.harmonicMean_fraction_samples_and_clades>high_percentile].Gene)
                                    target_percentile = percentiles_thresholds[1]

                                elif selection=="negative": 
                                    target_genes = set(df_diversity[df_diversity.harmonicMean_fraction_samples_and_clades<low_percentile].Gene)
                                    target_percentile = percentiles_thresholds[0]

                                else: raise ValueError("selection is invalid")

                            elif type_var=="SNP": 

                                # get genes under selection
                                selection_to_genes = get_genes_under_selection_SNPs_union_all_thresholds_piN_piS(df_diversity, max_fraction_samples_extreme_piS_piS)
                                target_genes = selection_to_genes[selection]
                                target_percentile = -1

                                # keep only valid thresholds
                                df_diversity = df_diversity[(df_diversity.threshold_piNpiS.isin(sel_to_valid_thresholds_piNpiS[selection])) & (df_diversity.selection==selection)]

                            else: raise ValueError("type var is invalid")

                            # skip if there are no genes
                            if len(target_genes)==0:
                                print("There are %i/%i genes with %s for %ss in %s. Considering %s %s"%(len(target_genes), len(all_genes), selection, type_var, species, type_vars_SimpleRepeats, type_vars_appearance))
                                continue

                            # print the number of genes
                            print("GOEA",  "%i/%i genes"%(len(target_genes), len(all_genes)))

                            # get a df with the target genes and their description
                            
                            # define dir and df
                            plots_dir = "%s/genes_under_selection"%PlotsDir; make_folder(plots_dir)
                            filename_genes = "%s/%s_%s_%s-seletion_%s_%s.xlsx"%(plots_dir, species, type_var.replace("/", "-"), selection,  type_vars_SimpleRepeats, type_vars_appearance)
                            df_genes = gene_features_df[(gene_features_df.species==species) & (gene_features_df.gff_upmost_parent.isin(target_genes))]
                            if set(df_genes.gff_upmost_parent)!=target_genes: raise ValueError("df_genes is missing genes")

                            # add values
                            gene_to_min_hmean_FractionSamples_selection = df_diversity[["Gene", "harmonicMean_fraction_samples_and_clades"]].groupby("Gene").apply(lambda df: min(df.harmonicMean_fraction_samples_and_clades))

                            gene_to_max_pval = df_diversity[["Gene", "pval_harmonicMean_fraction_samples_and_clades_fdr"]].groupby("Gene").apply(lambda df: max(df.pval_harmonicMean_fraction_samples_and_clades_fdr))

   
                            df_genes["min_harmonicMean_fraction_samples_and_clades"] = df_genes.gff_upmost_parent.apply(lambda g: gene_to_min_hmean_FractionSamples_selection[g])
                            df_genes["max_pval_harmonicMean_fraction_samples_and_clades"] = df_genes.gff_upmost_parent.apply(lambda g: gene_to_max_pval[g])                        

                            print("saving %s"%filename_genes)
                            fields = ['species', 'chromosome', 'start', 'end', 'gff_upmost_parent', 'gene_name', 'Scerevisiae_orthologs', "min_harmonicMean_fraction_samples_and_clades", "max_pval_harmonicMean_fraction_samples_and_clades", 'description', 'GOterms']
                            df_genes[fields].sort_values(by=["max_pval_harmonicMean_fraction_samples_and_clades", "min_harmonicMean_fraction_samples_and_clades"], ascending=[True, False]).to_excel(filename_genes)


                            # keep
                            df_GOEA_ngenes_dict = {"selection":selection, "type_vars_SimpleRepeats":type_vars_SimpleRepeats, "type_vars_appearance":type_vars_appearance, "type_var":type_var, "species":species, "n_genes_target":len(target_genes), "n_genes_all":len(all_genes), "percentile":target_percentile}
                            df_GOEA_ngenes_all = df_GOEA_ngenes_all.append(pd.DataFrame({0 : df_GOEA_ngenes_dict}).transpose()).reset_index(drop=True)


                            # map each gene to the go terms 
                            gene_to_GOterms = dict(gene_features_df[gene_features_df.species==species].set_index("gff_upmost_parent").GOterms)


                            # get the df_GOEA and add to df
                            goatools_plots_dir = "%s/GOEA_goatools_enriched_diversity_perGene_plots_selection"%PlotsDir
                            make_folder(goatools_plots_dir)

                            plot_fileprefix = "%s/%s_%s_%s_%s_%s_%s"%(goatools_plots_dir, species, selection, type_var.replace("/", "-"), type_vars_SimpleRepeats, type_vars_appearance, parms_string)
                            df_GOEA = get_df_GOEA_target_on_all_genes(target_genes, all_genes, gene_to_GOterms, obo_file, plot_fileprefix, alpha=alpha, min_LOR=min_LOR)

                            # add things and keep
                            df_GOEA["selection"] = selection
                            df_GOEA["type_vars_SimpleRepeats"] = type_vars_SimpleRepeats
                            df_GOEA["type_vars_appearance"] = type_vars_appearance
                            df_GOEA["type_var"] = type_var
                            df_GOEA["species"] = species

                            df_GOEA_all = df_GOEA_all.append(df_GOEA).reset_index(drop=True)

        # save df with all genes
        print("saving excel")
        df_GOEA_ngenes_all.to_excel("%s.n_genes.xlsx"%df_GOEA_all_file)

        # save
        print("saving")
        save_object(df_GOEA_all, df_GOEA_all_file)

    # load
    print("loading df_GOEA all")
    df_GOEA_all = load_object(df_GOEA_all_file)

    #########################


    ########### PLOTS ############

    # get one plot for each namespace (NS), selection and type_vars_SimpleRepeats
    #for NS in ["CC", "MF", "BP"]:
    interesting_NS = ["CC", "MF", "BP"]
    interesting_selection = ["positive"] # could also be negative
    interesting_type_vars_appearance = ["all_vars", "only_vars_recent"]
    keep_oneTerm_per_group = True

    for selection in interesting_selection: 
        for type_vars_SimpleRepeats in ["only_vars_noSimpleRepeats"]: 
            for type_vars_appearance in interesting_type_vars_appearance: # all_vars, only_vars_recent
                for NS in interesting_NS:
                    #print("ploting", NS, selection, type_vars_SimpleRepeats, type_vars_appearance)

                    # define the df to plot
                    df_GOEA = df_GOEA_all[(df_GOEA_all.NS==NS) & (df_GOEA_all.selection==selection) & (df_GOEA_all.type_vars_SimpleRepeats==type_vars_SimpleRepeats) & (df_GOEA_all.type_vars_appearance==type_vars_appearance)]

                    if len(df_GOEA)==0: continue

                    # create a gene_to_GOterms that is unique
                    species_to_gene_to_GOterms = dict(gene_features_df.groupby("species").apply(lambda df_s: dict(df_s.set_index("gff_upmost_parent").GOterms)))

                    # add the short species
                    df_GOEA["short_species"] = df_GOEA.species.apply(lambda x: x.split("_")[1][0:3])
                    sorted_species_byPhylogeny_short = [s.split("_")[1][0:3] for s in sorted_species_byPhylogeny if s in set(df_GOEA.species)]

                    # define the semantic_similarity_tsh as a function of the  3 gos
                    if len(set(df_GOEA.GO))<15: semantic_similarity_tsh = 1.0
                    else: semantic_similarity_tsh = 0.5


                    # define the sorted cols
                    sorted_cols = [t for t in ["SNP", "IN/DEL", "SV", "coverageCNV"] if t in set(df_GOEA.type_var)]

                    # define a title for the plots
                    title_plots = "%s sel., %s, %s, %s\nGOEA p_fdr<%.4f, min_LOR=%.4f\nsimilarity_tshd=%.2f, onlyOneTermPerGroup=%s"%(selection, type_vars_SimpleRepeats, type_vars_appearance, NS, alpha, min_LOR, semantic_similarity_tsh, keep_oneTerm_per_group)


                    ########### CLUSTERING PLOT ############

                    # define a filename prefix
                    filename_prefix = "%s_%s_%s_%s_sim%.2f_onlyRepTerm%s_%s"%(selection, type_vars_SimpleRepeats, type_vars_appearance, NS, semantic_similarity_tsh, keep_oneTerm_per_group, parms_string)

                    # make the plot
                    plots_dir_cluster = "%s/plots_GOs_clustered_many_groups_perGeneDiversity_selection"%PlotsDir; make_folder(plots_dir_cluster)
                    filename_plot = "%s/%s.pdf"%(plots_dir_cluster, filename_prefix)

                    processed_data_cluster = "%s/GOs_clustered_many_groups_perGeneDiversity_selection"%ProcessedDataDir; make_folder(processed_data_cluster)
                    filename_data = "%s/%s_df_GOEA.py"%(processed_data_cluster, filename_prefix)

                    first_colfield_colors = {s.split("_")[1][0:3] : c for s,c in species_to_color.items()}
                    second_colfield_colors = {"SNP":"red", "IN/DEL":"magenta", "SV":"navy", "coverageCNV":"cyan"}

                    generate_clusteredGOterms_plot(df_GOEA, filename_plot, filename_data, obo_file, species_to_gene_to_GOterms, first_colfield="short_species", second_colfield="type_var", sorted_first_colfield=sorted_species_byPhylogeny_short, sorted_second_colfield=["SNP", "IN/DEL", "coverageCNV", "SV"], semantic_similarity_tsh=semantic_similarity_tsh, keep_oneTerm_per_group=keep_oneTerm_per_group, replace=False, title=title_plots, first_colfield_colors=first_colfield_colors, second_colfield_colors=second_colfield_colors)

                    ########################################


    ##############################

def plot_distribution_and_GOEA_genes_high_or_low_piN_piS(DataDir, gene_features_df, df_diversity_all, ProcessedDataDir, PlotsDir, percentiles_thresholds=[10, 90], replace=False, diversity_f="mean_piN_piS_each_varType", draw_lines_pct=True, figsize=(6, 1.5), min_LOR = 0, min_nsamples_for_mean_piN_piS_each_varType = 5, min_nclades_for_mean_piN_piS_each_varType = 2, alpha = 0.05, factor_size_minus_log10pval=5, figsize_multiplier=2, plot_hist=True):

    """Does GOEA on genes that have high or low piN_piS scores and also plots them
    increasing the semantic_similarity_tsh makes that there is more difference (less clustering of the terms)"""

    # define the parameters string, useful to not repeat things
    parms_string = "pcts=%s_divField=%s_minLOR=%s_alpha=%s_minSamples=%s_minClades=%s"%("-".join([str(x) for x in percentiles_thresholds]), diversity_f, min_LOR, alpha, min_nsamples_for_mean_piN_piS_each_varType, min_nclades_for_mean_piN_piS_each_varType)


    # filter the df to keep only samples with some piN_piS calculated
    df_diversity_all = df_diversity_all[(df_diversity_all.nsamples_for_mean_piN_piS_each_varType>=min_nsamples_for_mean_piN_piS_each_varType) & ((df_diversity_all.nclades_for_mean_piN_piS_each_varType>=min_nclades_for_mean_piN_piS_each_varType) | (df_diversity_all.species=="Candida_tropicalis"))] # note that for C. tropicalis we only have one clade with clinical isolates

    # plot all species in one
    if plot_hist is True:

        #plot_diversity_distribution(df_diversity_all, PlotsDir, percentiles_thresholds, diversity_f, draw_lines_pct, figsize)
        title_prefix = "diversity distribution; pcts=%s\nminSamples=%s; minClades=%s (except tropicalis)"%("-".join([str(x) for x in percentiles_thresholds]), min_nsamples_for_mean_piN_piS_each_varType, min_nclades_for_mean_piN_piS_each_varType)
        plot_diversity_distribution_each_hist_sepparate(df_diversity_all, PlotsDir, percentiles_thresholds, diversity_f, draw_lines_pct, title_prefix)
        return

    # define the obo file
    obo_file = "%s/annotation_files/go-basic_30062021.obo"%(DataDir) # this was got from http://purl.obolibrary.org/obo/go/go-basic.obo

    ##### GET THE DF ########

    # define the file
    df_GOEA_all_file = "%s/df_diversity_df_GOEA_file_%s.py"%(ProcessedDataDir, parms_string)


    if file_is_empty(df_GOEA_all_file) or replace is True or True:

        # init dfs
        df_GOEA_all = pd.DataFrame()
        df_GOEA_ngenes_all = pd.DataFrame()

        # checks
        if any((pd.isna(df_diversity_all[diversity_f]))): raise ValueError("There can't be nans in %s"%diversity_f)

        for type_genes in ["low_piN_piS", "high_piN_piS"]:
            for type_vars_SimpleRepeats in sorted(set(df_diversity_all.type_vars_SimpleRepeats)):
                for type_vars_appearance in sorted(set(df_diversity_all.type_vars_appearance)):
                    for type_var in sorted(set(df_diversity_all.type_var)):
                        for species in sorted_species_byPhylogeny:
                            print("GOEA", type_genes, type_vars_SimpleRepeats, type_vars_appearance, type_var, species)

                            # get filtered df
                            df_diversity = df_diversity_all[(df_diversity_all.type_vars_SimpleRepeats==type_vars_SimpleRepeats) & (df_diversity_all.type_vars_appearance==type_vars_appearance) & (df_diversity_all.type_var==type_var) & (df_diversity_all.species==species)]

                            # check that the genes are unique
                            if len(df_diversity)!=len(set(df_diversity.Gene)): raise ValueError("There are non-unique genes")

                            # define all the genes
                            strange_genes = set(df_diversity.Gene).difference(set(gene_features_df[gene_features_df.species==species].gff_upmost_parent))
                            if len(strange_genes)>0: raise ValueError("There are some strange genes: %s"%strange_genes)
                            all_genes = set(df_diversity.Gene)

                            # define the genes that are paticularly interesting
                            low_percentile, high_percentile = get_high_low_percentiles_df_diversity(df_diversity, percentiles_thresholds, diversity_f)

                            if type_genes=="low_piN_piS": 
                                target_genes = set(df_diversity[df_diversity[diversity_f]<=low_percentile].Gene)
                                target_percentile = percentiles_thresholds[0]

                            elif type_genes=="high_piN_piS": 
                                target_genes = set(df_diversity[df_diversity[diversity_f]>=high_percentile].Gene)
                                target_percentile = percentiles_thresholds[1]

                            else: raise ValueError("error in type_genes")

                            # skip if there are no genes
                            if len(target_genes)==0:
                                print("There are %i/%i genes with %s for %ss in %s. Considering %s %s"%(len(target_genes), len(all_genes), type_genes, type_var, species, type_vars_SimpleRepeats, type_vars_appearance))
                                continue

                            # get a df with the target genes and their description
 
                            if type_genes=="high_piN_piS" and type_vars_appearance=="only_vars_recent":

                                # define dir and df
                                plots_dir = "%s/genes_with_high_piN_piS_only_recent"%PlotsDir; make_folder(plots_dir)
                                filename_genes = "%s/%s_%s.xlsx"%(plots_dir, species, type_var.replace("/", "-"))
                                df_genes = gene_features_df[(gene_features_df.species==species) & (gene_features_df.gff_upmost_parent.isin(target_genes))]
                                if set(df_genes.gff_upmost_parent)!=target_genes: raise ValueError("df_genes is missing genes")

                                # add piN piS
                                gene_to_piNpiS = dict(df_diversity.set_index("Gene")[diversity_f])
                                df_genes[diversity_f] = df_genes.gff_upmost_parent.apply(lambda g: gene_to_piNpiS[g])                        

                                print("saving %s"%filename_genes)
                                fields = ['species', 'chromosome', 'start', 'end', 'gff_upmost_parent', 'gene_name', 'Scerevisiae_orthologs', diversity_f, 'description', 'GOterms']
                                df_genes[fields].sort_values(by=diversity_f, ascending=False).to_excel(filename_genes)

                            # print the number of genes
                            print("GOEA", type_vars_SimpleRepeats, type_vars_appearance, type_var, species, "%i/%i genes %s"%(len(target_genes), len(all_genes), type_genes))

                            # keep
                            df_GOEA_ngenes_dict = {"type_genes":type_genes, "type_vars_SimpleRepeats":type_vars_SimpleRepeats, "type_vars_appearance":type_vars_appearance, "type_var":type_var, "species":species, "n_genes_target":len(target_genes), "n_genes_all":len(all_genes), "percentile":target_percentile}
                            df_GOEA_ngenes_all = df_GOEA_ngenes_all.append(pd.DataFrame({0 : df_GOEA_ngenes_dict}).transpose()).reset_index(drop=True)

                            # map each gene to the go terms 
                            gene_to_GOterms = dict(gene_features_df[gene_features_df.species==species].set_index("gff_upmost_parent").GOterms)

                            # get the df_GOEA and add to df
                            goatools_plots_dir = "%s/GOEA_goatools_enriched_diversity_perGene_plots"%PlotsDir
                            make_folder(goatools_plots_dir)

                            plot_fileprefix = "%s/%s_%s_%s_%s_%s_%s"%(goatools_plots_dir, species, type_genes, type_var.replace("/", "-"), type_vars_SimpleRepeats, type_vars_appearance, parms_string)
                            df_GOEA = get_df_GOEA_target_on_all_genes(target_genes, all_genes, gene_to_GOterms, obo_file, plot_fileprefix, alpha=alpha, min_LOR=min_LOR)

                            # add things and keep
                            df_GOEA["type_genes"] = type_genes
                            df_GOEA["type_vars_SimpleRepeats"] = type_vars_SimpleRepeats
                            df_GOEA["type_vars_appearance"] = type_vars_appearance
                            df_GOEA["type_var"] = type_var
                            df_GOEA["species"] = species

                            df_GOEA_all = df_GOEA_all.append(df_GOEA).reset_index(drop=True)

        # save df with all genes
        print("saving excel")
        df_GOEA_ngenes_all.to_excel("%s.n_genes.xlsx"%df_GOEA_all_file)

        # save
        print("saving")
        save_object(df_GOEA_all, df_GOEA_all_file)

    # load
    print("loading df_GOEA all")
    df_GOEA_all = load_object(df_GOEA_all_file)

    #########################

    ########### PLOTS ############

    # define a semantic similarity threshold for each combination

    # get one revigo plot for each namespace (NS), type_genes and type_vars_SimpleRepeats
    #for NS in ["CC", "MF", "BP"]:
    interesting_NS = ["BP"] # MF
    interesting_type_genes = ["high_piN_piS", "low_piN_piS"]
    interesting_type_vars_appearance = ["all_vars", "only_vars_recent"]

    for NS in interesting_NS:
        for type_genes in interesting_type_genes: # low_piN_piS # low_piN_piS, high_piN_piS

            #for type_vars_SimpleRepeats in ["all_vars", "only_vars_noSimpleRepeats"]: 
            for type_vars_SimpleRepeats in ["only_vars_noSimpleRepeats"]: 
                for type_vars_appearance in interesting_type_vars_appearance: # all_vars, only_vars_recent

                    #for keep_oneTerm_per_group in [True, False]:
                    for keep_oneTerm_per_group in [True]:
                        print("ploting", NS, type_genes, type_vars_SimpleRepeats, type_vars_appearance)


                        # define the df to plot
                        df_GOEA = df_GOEA_all[(df_GOEA_all.NS==NS) & (df_GOEA_all.type_genes==type_genes) & (df_GOEA_all.type_vars_SimpleRepeats==type_vars_SimpleRepeats) & (df_GOEA_all.type_vars_appearance==type_vars_appearance)]

                        if len(df_GOEA)==0: 
                            continue
                            #raise ValueError("there should be some GOEA in %s"%NS)

                        # create a gene_to_GOterms that is unique
                        species_to_gene_to_GOterms = dict(gene_features_df.groupby("species").apply(lambda df_s: dict(df_s.set_index("gff_upmost_parent").GOterms)))

                        # add the short species
                        df_GOEA["short_species"] = df_GOEA.species.apply(lambda x: x.split("_")[1][0:3])
                        sorted_species_byPhylogeny_short = [s.split("_")[1][0:3] for s in sorted_species_byPhylogeny if s in set(df_GOEA.species)]

                        # define the semantic_similarity_tsh as a function of the  3 gos
                        if len(set(df_GOEA.GO))<15: semantic_similarity_tsh = 1.0
                        else: semantic_similarity_tsh = 0.5

                        # define the sorted cols
                        sorted_cols = [t for t in ["SNP", "IN/DEL", "SV", "coverageCNV"] if t in set(df_GOEA.type_var)]

                        # define a title for the plots
                        title_plots = "%s, %s, %s, %s\nGOEA p_fdr<%.4f, min_LOR=%.4f\nsimilarity_tshd=%.2f, onlyOneTermPerGroup=%s"%(type_genes, type_vars_SimpleRepeats, type_vars_appearance, NS, alpha, min_LOR, semantic_similarity_tsh, keep_oneTerm_per_group)

                        ##### CREATE THE REVIGO PLOT #####

                        """

                        # define a filename prefix with all the info
                        filename_prefix = "%s_%s_%s_%s_sim%.2f_onlyRepTerm%s_%s"%(type_genes, type_vars_SimpleRepeats, type_vars_appearance, NS, semantic_similarity_tsh, keep_oneTerm_per_group, parms_string)

                        # define the files
                        plots_dir_revigo = "%s/plots_revigo_many_groups_perGeneDiversity"%PlotsDir; 
                        filename_plot = "%s/%s.pdf"%(plots_dir_revigo, filename_prefix)
                        
                        processed_data_revigo = "%s/revigo_many_groups_perGeneDiversity"%ProcessedDataDir; 
                        filename_data = "%s/%s_df_GOEA_with_MDSdata.py"%(processed_data_revigo, filename_prefix)

                        # clean
                        #for f in [plots_dir_revigo, processed_data_revigo]: delete_folder(f) # clean

                        # make folders
                        make_folder(plots_dir_revigo); make_folder(processed_data_revigo)

                        # make plot
                        generate_REVIGOlike_plot_subplots(df_GOEA, filename_plot, filename_data, obo_file, species_to_gene_to_GOterms, rowfield="short_species", colfield="type_var", sorted_rows=sorted_species_byPhylogeny_short, sorted_cols=sorted_cols, semantic_similarity_tsh=semantic_similarity_tsh, keep_oneTerm_per_group=keep_oneTerm_per_group, replace=False, title="%s, %s, %s, %s\nGOEA p_fdr<%.4f, min_LOR=%.4f\nsimilarity_tshd=%.2f, onlyOneTermPerGroup=%s"%(type_genes, type_vars_SimpleRepeats, type_vars_appearance, NS, alpha, min_LOR, semantic_similarity_tsh, keep_oneTerm_per_group), figsize_multiplier=figsize_multiplier, remove_ticks=True, factor_size_minus_log10pval=factor_size_minus_log10pval)

                        """

                        ##################################

                        ########### CLUSTERING PLOT ############

                        # define a filename prefix
                        filename_prefix = "%s_%s_%s_%s_sim%.2f_onlyRepTerm%s_%s"%(type_genes, type_vars_SimpleRepeats, type_vars_appearance, NS, semantic_similarity_tsh, keep_oneTerm_per_group, parms_string)

                        # make the plot
                        plots_dir_cluster = "%s/plots_GOs_clustered_many_groups_perGeneDiversity"%PlotsDir; make_folder(plots_dir_cluster)
                        filename_plot = "%s/%s.pdf"%(plots_dir_cluster, filename_prefix)

                        processed_data_cluster = "%s/GOs_clustered_many_groups_perGeneDiversity"%ProcessedDataDir; make_folder(processed_data_cluster)
                        filename_data = "%s/%s_df_GOEA.py"%(processed_data_cluster, filename_prefix)

                        first_colfield_colors = {s.split("_")[1][0:3] : c for s,c in species_to_color.items()}
                        second_colfield_colors = {"SNP":"red", "IN/DEL":"magenta", "SV":"navy", "coverageCNV":"cyan"}

                        generate_clusteredGOterms_plot(df_GOEA, filename_plot, filename_data, obo_file, species_to_gene_to_GOterms, first_colfield="short_species", second_colfield="type_var", sorted_first_colfield=sorted_species_byPhylogeny_short, sorted_second_colfield=["SNP", "IN/DEL", "coverageCNV", "SV"], semantic_similarity_tsh=semantic_similarity_tsh, keep_oneTerm_per_group=keep_oneTerm_per_group, replace=False, title=title_plots, first_colfield_colors=first_colfield_colors, second_colfield_colors=second_colfield_colors)

                        ########################################

    ##############################


def get_second_element_in_str(x): return x.split("_")[1]

def get_node_ASR_state_one_mutation_df(mut, pct_progress, asr_dir):

    """Gets a df with all the stacked mutations info, just for one mutation"""

    # print the progress
    if pct_progress.endswith("0"): print(pct_progress, end="\r")

    # load the df as output of pastml
    df = pd.read_csv("%s/%s_pastml_reconstruction.out"%(asr_dir, mut), sep="\t").set_index("node")

    # change the names
    df.columns = list(map(get_second_element_in_str, df.columns))

    # transpose and add fields
    df = df.transpose()
    df["ASR_method"] = df.index
    df["numeric_mut"] = mut

    return df

def integrate_ASR_mutations(outdir_ASRmutations, integrated_file_ASRmutations, threads):

    """Integeares the results of the ASR of all the mutations. It generates a df where there is the variantID and the ASR"""
    print("running integrate_ASR_mutations...")

    # return if integrated_file_ASRmutations has been generated
    if not file_is_empty(integrated_file_ASRmutations): 
      print("integrated files already generated. returning...")
      return

    # clean the html files
    print("cleaning html files")
    os.system("rm %s/genearting_ASR_mutations_dir/ASR_mutations/*.html"%(outdir_ASRmutations))
    print("html files cleaned")

    # define the files to clean at the end
    files_to_clean = ["%s/%s"%(outdir_ASRmutations, f) for f in ["raw_tree.nw", "genearting_ASR_mutations_dir"]]
    if any([file_is_empty(f) and not os.path.isdir(f) for f in files_to_clean]): raise ValueError("There can't be empty files in files_to_clean")

    # define the df that maps mutations to rep mutations
    mut_to_repMut_df = get_tab_as_df_or_empty_df("%s/genearting_ASR_mutations_dir/mutation_to_representative_mutation.tab"%outdir_ASRmutations)

    # map each numeric mut to the variant
    mut_to_variant = dict(get_tab_as_df_or_empty_df("%s/genearting_ASR_mutations_dir/variants_IDmapping.tab"%outdir_ASRmutations).set_index("numeric_mutation").variantID_across_samples)

    # save a mut_to_repMut_df with variants
    print("generating var_to_repVar_df")
    var_to_repVar_df = pd.DataFrame({"%svariant"%x : mut_to_repMut_df["%smutation"%x].map(mut_to_variant) for x in ["", "rep_"]})

    for f in var_to_repVar_df.keys():
        if any(pd.isna(var_to_repVar_df[f])): raise ValueError("there can't be nans in %s"%f)

    save_object(var_to_repVar_df, "%s/var_to_representative_var.py"%outdir_ASRmutations)

    # define the inputs of get_node_ASR_state_one_mutation_df
    print("preparing inputs of get_node_ASR_state_one_mutation_df...")
    sorted_muts = sorted(set(mut_to_repMut_df.rep_mutation))
    nmuts = len(sorted_muts)
    asr_dir = "%s/genearting_ASR_mutations_dir/ASR_mutations"%outdir_ASRmutations
    inputs_fn = list(map(lambda x: (x[1], "%.2f"%(((x[0]+1) / nmuts)*100), asr_dir), enumerate(sorted_muts)))

    # run the df
    print("running get_node_ASR_state_one_mutation_df on %i threads"%threads)
    integrated_ASR_df = pd.DataFrame()
    for Ichunk, inputs_fn_chunk in enumerate(chunks(inputs_fn, int(threads*10))):
        #print(Ichunk)

        with multiproc.Pool(threads) as pool:
            integrated_ASR_df_chunk = pd.concat(pool.starmap(get_node_ASR_state_one_mutation_df, inputs_fn_chunk, chunksize=1)).reset_index(drop=True)
            pool.close()
            pool.terminate()

        integrated_ASR_df = integrated_ASR_df.append(integrated_ASR_df_chunk)

    # add the variant to integrated_ASR_df
    integrated_ASR_df["variantID_across_samples"] = integrated_ASR_df.numeric_mut.map(mut_to_variant)
    if any(pd.isna(integrated_ASR_df["variantID_across_samples"])): raise ValueError("There can't be nans in 'variantID_across_samples'")

    # save
    print("saving")
    save_object(integrated_ASR_df, integrated_file_ASRmutations)

    # clean
    print("cleaning %s"%outdir_ASRmutations)
    for f in ["correct_tree.nw"]: rsync_file("%s/genearting_ASR_mutations_dir/%s"%(outdir_ASRmutations, f), "%s/%s"%(outdir_ASRmutations, f))
    for f in files_to_clean: delete_file_or_folder(f)

def get_metadata_df_with_lat_and_lon(metadata_df):

    """Adds the lat and lon of the metadata df"""

    print("adding longitude and latitude data")

    # define missing_fields
    missing_fields = {"missing", "Missing", 'NOT APPLICABLE', 'Not applicable',  'not applicable',  'not available',  'not collected',  'unknown', 'cell culture', 'Unknown', 'United Kingdom: The National Collection of Pathogenic Fungi', 'not applicable', '37 C', 'Mouth', 'unknown'}


    # map each region to a latitude and longitude according to manual curation of google maps (from the location of the name of the place)
    region_to_lat_lon = {'Tuscany': (43.76, 11.25), 'Denmark':(56, 10), 'Australia: Canberra': (-35.26792812597631, 149.12901155241997), 'France: Nantes': (47.24106551841646, -1.5743885799472712), 'Brazil': (-8.538434926969312, -53.75416438636335), 'Belgium': (50.63971257785112, 4.406611946972468), 'USA: New York': (40.75460454129176, -73.99471873220475), 'United Kingdom: Portsmouth': (50.819184791486464, -1.0715035636392083), 'Ireland': (53.15598272030456, -7.687858433456018), 'French Guiana: Trois Sauts': (2.2580976273570403, -52.87354710462801), 'United Kingdom: Epsom': (51.33598513703005, -0.26733527758407516), 'Colombia': (3.6284572995052744, -73.40348557602601), 'United Kingdom: Crewe': (53.099911638225194, -2.445477712864697), 'Israel': (30.99524989618473, 34.86439324085356), 'France: Paris': (48.86210221554007, 2.3494226461516274), 'USA: Baltimore': (39.31111088440969, -76.62182358085903), 'United Kingdom: Ipswich': (52.05674048653221, 1.1481288232067033), 'United Kingdom: London': (51.506914212673, -0.1269157254963107), 'Canada: Manitoba': (56.10425478945556, -96.85284121301154), 'USA: Iowa': (42.09755960192686, -93.54434864034396), 'Senegal': (14.459679537960001, -14.627006700414265), 'Turkey': (39.34684655479633, 34.34716019191521), "United Kingdom: Oral Medicine Clinic at Guy's Hospital": (51.50380387761737, -0.08703816431967358), 'Belgium: St. Niklaas': (51.1766432283634, 4.136251763944037), 'Italy: Messina': (38.19204649757731, 15.542509615915092), 'USA: Florida': (27.474364991436477, -81.45044201153418), 'Madagascar': (-20.84751271919057, 46.32415710389879), 'Angola': (-12.037531779271252, 17.40574554319163), 'Germany': (51.211743546431734, 10.505096564197144), 'France: Palaiseau': (48.7136674728403, 2.2233516674488114), 'France: Strasbourg': (48.57255003774329, 7.75316081572526), 'Australia': (-25.066505485567795, 133.7939162400746), 'Morocco': (31.923234953648503, -6.124740119705653), 'France: Grenoble': (45.184011414362786, 5.71907541535044), 'USA: Arizona': (34.36227937500841, -112.02376385293276), 'USA: Omaha, Nebraska': (41.26779232804339, -95.99522574726386), 'Taiwan': (24.06671204417071, 121.29310156573625), 'USA': (39.85007108461509, -101.25466315772051), "Italy: L'Aquila": (42.362364148306, 13.382725997501616), 'United Kingdom: Oxford': (51.752318606607666, -1.2578487782257237), 'Germany:MIBi Univ Hosp. Cologne': (50.925337705870966, 6.9190895917915505), 'USA: College Station, TX': (30.590125109726362, -96.30677099636328), 'India': (23.25021532155768, 79.66568071342762), 'France': (46.808951261243244, 2.348436946757428), 'United Kingdom: Bristol': (51.45463165946981, -2.5871806689720236), 'Italy: Pisa': (43.7154303777668, 10.397668762381912),  'China': (34.863641077436114, 103.6338943587257), 'USA: Utah': (39.13389232183201, -111.84652043976027), 'Canada:Toronto': (43.65597822824901, -79.3868544177212), 'USA: Wisconsin': (44.75966779021038, -89.93252223107535), 'USA:NEW YORK,ROCHESTER': (43.185445073167685, -77.56439990524169), 'Spain': (39.34891668973952, -3.5648279748555654), 'United Kingdom: Wrexham': (53.04403093059917, -2.9936720381984276), 'United Kingdom: Bath': (51.38243893145635, -2.3597193616720094), 'Singapore': (1.3605665376485534, 103.84975163024808), 'USA: Michigan': (43.92094050894986, -84.85823587494794), 'Hong Kong': (22.310160029890767, 114.12203362927234), 'Netherlands': (52.02062849137703, 5.497740321238041), 'United Kingdom: Bangor': (53.22910176945942, -4.129903866161501), 'France: Lyon': (45.770387502733584, 4.841602030794112), 'USA: Rhode Island': (41.494695247383305, -71.33039266382015), 'Italy': (43.1491460488417, 12.062979344720581), 'United Kingdom: Stoke': (53.00232551801364, -2.180111020972052), 'France: Rouen': (49.43982416445809, 1.0937254050517873),  'USA: Indianapolis': (39.76860362019686, -86.15339703017824), 'South Korea': (36.504130155348626, 128.075492390978), 'USA: Atlanta': (33.7464762777678, -84.39869667450462),  'Pakistan': (29.8830624115238, 69.53896875529162), 'China: Beijing': (39.89185726218386, 116.41631847907085), 'Hungary: Szeged': (46.2496186205472, 20.139648500336786), 'France: Rennes': (48.10932367888122, -1.671294521805199), 'United Kingdom: Coventry': (52.4097620270444, -1.5197370424933632),  'Niger': (17.55639909936741, 9.204073263317936),  'Italy: Varese': (45.818684453044895, 8.826203129690272) , 'Canada: Ontario': (51.652568769604144, -86.82302346231907), 'United Kingdom: Birmingham': (52.490230111552776, -1.8951498232974502), 'Germany: MiBi Univ. Hosp. Cologne': (50.92572174880825, 6.918468570198779) , 'Puerto Rico': (18.14112089883755, -66.63064425395734), "Japan:Tokyo": (35.69403998879984, 139.75792296005173), "Japan:Iwate": (39.98758992946517, 141.21445732808567), "South Africa": (-30.688905707941572, 23.927380342723634), "Japan": (36.7548135531042, 139.15450398364703), "USA: San Jose": (37.334265298345024, -121.88574706060412), "USA:Detroit": (42.326510183160046, -83.04576206893782), "Venezuela": (6.86991649082784, -66.30295926946333)} 


    # define a function that has the r
    def get_r_with_lat_lon_one_sample(r):

        # if the latitude is provided, keep it:
        if not pd.isna(r.uncurated_lat_lon) and r.uncurated_lat_lon not in missing_fields:

            # get as strings
            latitude_str, ns, longitude_str, ew = r.uncurated_lat_lon.split()

            # define the values
            if ns=="N": latitude = float(latitude_str)
            elif ns=="S": latitude = -float(latitude_str)

            if ew=="E": longitude = float(longitude_str)
            elif ew=="W": longitude = -float(longitude_str)

        # go through the string locations
        elif not pd.isna(r.uncurated_location) and r.uncurated_location not in missing_fields: latitude, longitude = region_to_lat_lon[r.uncurated_location]

        # go through the uncurated_geographic location (country and/or sea)
        elif not pd.isna(r["uncurated_geographic location (country and/or sea)"]) and r["uncurated_geographic location (country and/or sea)"] not in missing_fields: latitude, longitude = region_to_lat_lon[r["uncurated_geographic location (country and/or sea)"]]

        # go through uncurated_geo_loc_name
        elif not pd.isna(r.uncurated_geo_loc_name) and r.uncurated_geo_loc_name not in missing_fields: latitude, longitude = region_to_lat_lon[r.uncurated_geo_loc_name]

        else:
            latitude = np.nan
            longitude = np.nan


        # add
        r["latitude"] = latitude
        r["longitude"] = longitude

        return r

    return metadata_df.apply(get_r_with_lat_lon_one_sample, axis=1)

def get_metadata_df_with_collection_date(metadata_df):

    """Returns the collection date as a float YYYYMMDD"""

    print("adding get_metadata_df_with_collection_date")

    # define missing_fields
    missing_fields = {"missing", "Cell Culture", "not applicable", "Singapore", "NOT APPLICABLE", "Homo sapiens", "not available", "unknown", "Missing", "not collected", "Not applicable", "-"}

    # map complex strings to dates
    str_to_date  = {"Aug-2013":(2013, 8, 1) }

    # define months
    month_to_number = {"Jan":1, "Feb":2, "Mar":3, "Apr":4, "May":5, "Jun":6, "Jul":7, "July":7, "Aug":8, "Sep":9, "Oct":10, "Nov":11, "Dec":12}


    # define check functions
    def is_valid_year(x): return (int(x)<2023 and int(x)>1915)
    def is_valid_day(x): return (int(x)<32 and int(x)>0)
    def is_valid_month(x): return (int(x)<13 and int(x)>0)

    # define a function that has the r
    def get_r_with_collection_date(r):

        # if the collection date is provided, keep it:
        if not pd.isna(r["uncurated_collection date"]) and r["uncurated_collection date"] not in missing_fields: collection_date = str_to_date[r["uncurated_collection date"]]

        # add with the most common date system
        elif not pd.isna(r.uncurated_collection_date) and r.uncurated_collection_date not in missing_fields: 

            # define the target value
            x = r.uncurated_collection_date

            # predefined situations
            if x in str_to_date: collection_date = str_to_date[x]

            # X-Y-Z format
            elif len(x.split("-"))==3:

                # get the split
                split_x = x.split("-")

                # 22-Nov-1995
                if split_x[1] in month_to_number and is_valid_day(split_x[0]) and is_valid_year(split_x[2]): collection_date = (int(split_x[2]), month_to_number[split_x[1]], int(split_x[0]))

                # 2012-04-17
                elif is_valid_year(split_x[0]) and is_valid_month(split_x[1]) and is_valid_day(split_x[2]): collection_date = tuple([int(y) for y in split_x])

            # X-Y format
            elif len(x.split("-"))==2:

                # get the split
                split_x = x.split("-")

                # May-2013
                if split_x[0] in month_to_number and is_valid_year(split_x[1]): collection_date = (int(split_x[1]), month_to_number[split_x[0]], 1)

                # 2009-12
                elif is_valid_year(split_x[0]) and is_valid_month(split_x[1]): collection_date = (int(split_x[0]), int(split_x[1]), 1)

            # 2018.0 format
            elif x in {"2018.0", "2016.0", "2017.0", "2019.0"}: collection_date = (int(float(x)), 1, 1)

            # '2013' format
            elif is_valid_year(x): collection_date = (int(x), 1, 1)

        # try with  uncurated_INSDC last update (format 2015-11-27T11:37:00Z)
        elif not pd.isna(r["uncurated_INSDC last update"]) and r["uncurated_INSDC last update"] not in missing_fields: collection_date = tuple([int(x) for x in r["uncurated_INSDC last update"].split("T")[0].split("-")])

        # uncurated_ENA-LAST-UPDATE
        elif not pd.isna(r["uncurated_ENA-LAST-UPDATE"]) and r["uncurated_ENA-LAST-UPDATE"] not in missing_fields: raise ValueError("this var has not been considered")

        elif not pd.isna(r["uncurated_ENA last update"]) and r["uncurated_ENA last update"] not in missing_fields: raise ValueError("this var has not been considered")

        else: collection_date = np.nan

        # add
        r["collection_date_tuple"] = collection_date

        # check
        if not pd.isna(collection_date):
            if any([not fn_valid(collection_date[I]) for I, fn_valid in enumerate([is_valid_year, is_valid_month, is_valid_day])]): raise ValueError("%s is invalid"%str(collection_date))

        return r

    return metadata_df.apply(get_r_with_collection_date, axis=1)


def get_km_distance_btw_samples(s1, s2, df_c):

    """Gets a df_c with latitude and longitude and returns the distance in km between s1 and s2. based on https://www.geeksforgeeks.org/program-distance-two-points-earth/#:~:text=For%20this%20divide%20the%20values,is%20the%20radius%20of%20Earth."""

    # check
    if s1==s2: raise ValueError("s1 and s2 can't be the same")

    # define things
    earth_radius_km = 6378.8 # earth radius in km
    lon1, lat1 = df_c.loc[s1][["longitude_radians", "latitude_radians"]].values
    lon2, lat2 = df_c.loc[s2][["longitude_radians", "latitude_radians"]].values

    # calculate harvesine formula
    dlon = lon2 - lon1
    dlat = lat2 - lat1
    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2
    c = 2 * asin(sqrt(a))
    return(c * earth_radius_km)


def get_cladeID_to_color_and_marker(metadata_df, species):

    """Maps each cladeID to the color"""

    # get the sorted clades
    metadata_df = metadata_df[~(pd.isna(metadata_df.cladeID_Tree_and_BranchLen)) & (metadata_df.species_name==species)]
    sorted_clades = sorted(set(metadata_df.cladeID_Tree_and_BranchLen.apply(int)))
    
    # get the graphics 
    if len(sorted_clades)<=10: palette="tab10"
    else: palette="tab20"

    cladeID_to_color = get_value_to_color(sorted_clades, palette=palette, n=len(sorted_clades), type_color="hex")[0]
    markers = "^ovs<P>HD"*1000
    cladeID_to_marker = dict(zip(sorted_clades, markers[:len(sorted_clades)]))

    return cladeID_to_color, cladeID_to_marker

def plot_pairwise_km_distance_across_years_clinical_isolates(metadata_df, PlotsDir, species_to_tree, ProcessedDataDir):

    """Plots the correlation btw time and geographic dispersion"""

    ############ GET DF PAIRWISE DIFERENCES ##########
    df_pairwiseDifferences_file = "%s/df_pairwiseDifferences_km_distances_clincal_isolates_also_time.py"%ProcessedDataDir
    if file_is_empty(df_pairwiseDifferences_file):

        # keep samples that are clinical and with clade info
        metadata_df = metadata_df[(metadata_df.type=="clinical") & ~(pd.isna(metadata_df.cladeID_Tree_and_BranchLen))]

        # get the metadata_df with the latitude and longitude of each sample
        metadata_df = get_metadata_df_with_lat_and_lon(metadata_df)
        metadata_df = metadata_df[~(pd.isna(metadata_df.latitude))]

        # get metadata df with time info
        metadata_df = get_metadata_df_with_collection_date(metadata_df)
        metadata_df = metadata_df[~(pd.isna(metadata_df.collection_date_tuple))]

        # add discrete years
        metadata_df["year"] = metadata_df.collection_date_tuple.apply(lambda x: x[0])

        # change
        metadata_df["sampleID"] = metadata_df.sampleID.apply(str) 

        # map each species to some time intervals
        species_to_t_intervals = {"Candida_glabrata": [[1984, 2000], [2001, 2005], [2006, 2010], [2011, 2015], [2016, 2017]],
                                  "Candida_auris": [[1997, 2005], [2006, 2010], [2011, 2015], [2016, 2018]], 
                                  "Candida_tropicalis": [[2016, 2016], [2018, 2018]],
                                  "Candida_albicans": [[1968, 1990], [1991, 1995], [1996, 2000], [2001, 2005], [2006, 2010], [2011, 2015], [2016, 2018]],
                                  "Candida_parapsilosis": [[2005, 2005], [2013, 2013], [2017, 2017]],
                                  "Candida_orthopsilosis": [[2004, 2007], [2008, 2010]]}

        species_to_t_intervals = {s : pd.Series([tuple(interval) for interval in ints]) for s, ints in species_to_t_intervals.items()}

        # define the df of pairwise differences
        df_pairwiseDifferences = pd.DataFrame()

        # go through each species and get the number of clades
        for species in sorted_species_byPhylogeny:
            print(species)

            # get dfs
            metadata_df_s = metadata_df[(metadata_df.species_name==species)]

            # add the time interval
            intervals = species_to_t_intervals[species]

            def get_year_interval(y): 
                correct_intervals = intervals[intervals.apply(lambda x: x[0]<=y and x[1]>=y)]
                if len(correct_intervals)!=1: raise ValueError("There should only be one time interval")
                return correct_intervals.iloc[0]

            metadata_df_s["year_interval"] = metadata_df_s.year.apply(get_year_interval)

            # check that all intervals were picked
            if set(intervals)!=set( metadata_df_s["year_interval"]): raise ValueError("not all intervals were picked at least once")

            # go through each interval
            for year_interval in  sorted(set(metadata_df_s["year_interval"])):
                metadata_df_s_year = metadata_df_s[metadata_df_s.year_interval==year_interval]

                # keep only clades with at least 2 samples, where it makes sense to count pairwise distances
                metadata_df_s_year["nsamples_in_clade"] = metadata_df_s_year.cladeID_Tree_and_BranchLen.map(Counter(metadata_df_s_year.cladeID_Tree_and_BranchLen))
                metadata_df_s_year = metadata_df_s_year[metadata_df_s_year.nsamples_in_clade>=2].set_index("cladeID_Tree_and_BranchLen")

                # debug
                if len(metadata_df_s_year)==0: continue

                # get the clades
                sorted_clades = sorted(set(metadata_df_s_year.index))

                # add the latitude and longitude in radians
                pi_val = 22/7
                metadata_df_s_year["longitude_radians"] = metadata_df_s_year.longitude / (180/pi_val)
                metadata_df_s_year["latitude_radians"] = metadata_df_s_year.latitude / (180/pi_val)

                # go through each clade
                for cladeID in sorted_clades:

                    # get df
                    df_c = metadata_df_s_year.loc[{cladeID}][["longitude_radians", "latitude_radians", "sampleID"]].set_index("sampleID")
                    if len(df_c)<2: raise ValueError("There should be at least 2 samples")

                    # calculate all pairwise differences
                    all_samples = set(df_c.index)
                    pairwise_differences = np.array(make_flat_listOflists(list(map(lambda s1: list(map(lambda s2: get_km_distance_btw_samples(s1, s2, df_c), all_samples.difference({s1}))), all_samples))))

                    df_pairwiseDifferences_c = pd.DataFrame({"distance_km":pairwise_differences})
                    df_pairwiseDifferences_c["species"] = species
                    df_pairwiseDifferences_c["cladeID"] = int(cladeID)
                    df_pairwiseDifferences_c["year_interval"] = [year_interval]*len(df_pairwiseDifferences_c)

                    # keep
                    df_pairwiseDifferences = df_pairwiseDifferences.append(df_pairwiseDifferences_c)

        # save
        save_object(df_pairwiseDifferences, df_pairwiseDifferences_file)

    df_pairwiseDifferences = load_object(df_pairwiseDifferences_file)

    ##################################################

    ########## PLOT #########

    # add data
    df_pairwiseDifferences["first_year"] = df_pairwiseDifferences.year_interval.apply(lambda x: x[0])        
    df_pairwiseDifferences["log_distance_km"] = np.log10(df_pairwiseDifferences.distance_km + 1)   

    # define the ylim
    ylim = [min(df_pairwiseDifferences.log_distance_km)-0.3, max(df_pairwiseDifferences.log_distance_km)+0.3]

    # define the interesting species
    interesting_species = [s for s in sorted_species_byPhylogeny if s!="Candida_parapsilosis"] # we saw that C. parapilosis has no variation in year and place

    # init figure
    nrows = 1
    ncols = len(interesting_species)
    fig = plt.figure(figsize=(ncols*4, nrows*4)); I=1

    for Ic, species in enumerate(interesting_species):
        print(species)

        # make subplot
        ax = plt.subplot(nrows, ncols, I); I+=1

        # define graphics
        cladeID_to_color, cladeID_to_marker = get_cladeID_to_color_and_marker(metadata_df, species)

        # define the plot df
        df_plot = df_pairwiseDifferences[df_pairwiseDifferences.species==species].sort_values(by=["year_interval", "cladeID"])

        clade_to_ndifferentYears = df_plot.groupby("cladeID").apply(lambda df_c: len(set(df_c.year_interval)))
        interesting_clades = set(clade_to_ndifferentYears[clade_to_ndifferentYears>=2].index)
        df_plot = df_plot[df_plot.cladeID.isin(interesting_clades)]

        if len(df_plot)==0: raise ValueError("plot can't be empty")

        # get lineplot
        ax = sns.lineplot(data=df_plot, x="first_year", y="log_distance_km", hue="cladeID", style="cladeID", palette=cladeID_to_color, markers=cladeID_to_marker, dashes=False, estimator="mean", ci=95, n_boot=100, seed=0, markeredgecolor="black", markeredgewidth=.1,  err_style="band", err_kws=dict(alpha=.05)); # ci=95err_kws

        # define the xlim
        ax.set_xlim([min(df_plot.first_year)-2, max(df_plot.first_year)+4])

        # add a text at the end of each clade
        """

        for cladeID in set(df_plot.cladeID):
            df_c = df_plot[df_plot.cladeID==cladeID]
            df_c = df_c[df_c.first_year==max(df_c.first_year)]
            point_x = df_c.first_year.iloc[0] 
            x = point_x + 1
            point_y = np.mean(df_c.log_distance_km)
            y = point_y + random.randrange(-10, 10)*0.06

            # plot label
            plt.text(x, y, str(cladeID), color=cladeID_to_color[cladeID])

            # plot line
            plt.plot([point_x, x], [point_y, y], color=cladeID_to_color[cladeID], linestyle="--", linewidth=.7)

        """

        # add axes
        first_year_to_years = {y_int[0] : y_int for y_int in sorted(set(df_plot.year_interval))}
        ax.set_xticks(list(first_year_to_years.keys()))
        def get_xlabel(y_int):
            if y_int[0]==y_int[1]: return str(y_int[0])
            else: return "-".join([str(x) for x in y_int])
        ax.set_xticklabels([get_xlabel(y_int) for y_int in first_year_to_years.values()], rotation=90)
        ax.set_xlabel("isolation year")

        ax.set_ylim(ylim)
        if Ic==0: ax.set_ylabel("log(pairwise km distance)")
        else: 
            ax.set_ylabel("")
            ax.set_yticklabels([])

        ax.set_title("C. %s (data in %i/%i clades)"%(species.split("_")[1], len(set(df_plot.cladeID)), len(cladeID_to_color)) )

        ax.legend(loc='lower left', bbox_to_anchor=(1.01, 0))



    plt.subplots_adjust(wspace=0.52,  hspace=0)
    plt.show()
    return 

    filename = "%s/pairwise_km_distance_across_years_lineplot.pdf"%(PlotsDir)
    print("plotting %s"%filename)
    fig.savefig(filename, bbox_inches='tight')



    #########################



    print(df_pairwiseDifferences)

    dajhgdjgda
   

def plot_pairwise_km_distance_clinical_isolates(metadata_df, PlotsDir, species_to_tree, ProcessedDataDir):

    """Plots the distribution of pairwise differences metadata df"""

    ############ GET DF PAIRWISE DIFERENCES ##########
    df_pairwiseDifferences_file = "%s/df_pairwiseDifferences_km_distances_clincal_isolates.py"%ProcessedDataDir
    if file_is_empty(df_pairwiseDifferences_file):

        # get the metadata_df with the latitude and longitude of each sample
        metadata_df = get_metadata_df_with_lat_and_lon(metadata_df)

        # change
        metadata_df["sampleID"] = metadata_df.sampleID.apply(str) 

        # define the expected clades, this is manually from the code below
        expected_species_to_nclades = {'Candida_albicans': 19, 'Candida_parapsilosis': 3, 'Candida_metapsilosis': 0, 'Candida_orthopsilosis': 4, 'Candida_tropicalis': 1, 'Candida_glabrata': 21, 'Candida_auris': 4}

        # define the df of pairwise differences
        df_pairwiseDifferences = pd.DataFrame()

        # go through each species and get the number of clades
        for species in sorted_species_byPhylogeny:

            # get dfs
            metadata_df_s = metadata_df[(metadata_df.species_name==species) & ~(pd.isna(metadata_df.latitude)) & (metadata_df.type=="clinical") & ~(pd.isna(metadata_df.cladeID_Tree_and_BranchLen))]

            # keep only clades with at least 2 samples, where it makes sense to count pairwise distances
            metadata_df_s["nsamples_in_clade"] = metadata_df_s.cladeID_Tree_and_BranchLen.map(Counter(metadata_df_s.cladeID_Tree_and_BranchLen))
            metadata_df_s = metadata_df_s[metadata_df_s.nsamples_in_clade>=2].set_index("cladeID_Tree_and_BranchLen")

            # get the clades
            sorted_clades = sorted(set(metadata_df_s.index))
            print(species, len(sorted_clades), "clades with at least 2 samples and geographic location")
            if len(sorted_clades)!=expected_species_to_nclades[species]: raise ValueError("You don't have the expected number of clades")
            if len(sorted_clades)==0: continue

            # add the latitude and longitude in radians
            pi_val = 22/7
            metadata_df_s["longitude_radians"] = metadata_df_s.longitude / (180/pi_val)
            metadata_df_s["latitude_radians"] = metadata_df_s.latitude / (180/pi_val)

            # go through each clade
            for cladeID in sorted_clades:
                print("clade %s"%cladeID)

                # get df
                df_c = metadata_df_s.loc[{cladeID}][["longitude_radians", "latitude_radians", "sampleID"]].set_index("sampleID")
                if len(df_c)<2: raise ValueError("There should be at least 2 samples")

                # calculate all pairwise differences
                all_samples = set(df_c.index)
                pairwise_differences = np.array(make_flat_listOflists(list(map(lambda s1: list(map(lambda s2: get_km_distance_btw_samples(s1, s2, df_c), all_samples.difference({s1}))), all_samples))))

                df_pairwiseDifferences_c = pd.DataFrame({"distance_km":pairwise_differences})
                df_pairwiseDifferences_c["species"] = species
                df_pairwiseDifferences_c["cladeID"] = int(cladeID)

                # keep
                df_pairwiseDifferences = df_pairwiseDifferences.append(df_pairwiseDifferences_c)

        # save
        save_object(df_pairwiseDifferences, df_pairwiseDifferences_file)

    df_pairwiseDifferences = load_object(df_pairwiseDifferences_file)

    ##################################################

    ######## PLOT ########

    # one plot for each species
    for species in sorted_species_byPhylogeny:

        # get df
        df_s = df_pairwiseDifferences[df_pairwiseDifferences.species==species].sort_values(by="cladeID")
        if len(df_s)==0: continue

        # define clade to color
        nclades = len(set(metadata_df[(metadata_df.species_name==species) & ~(pd.isna(metadata_df.cladeID_Tree_and_BranchLen))].cladeID_Tree_and_BranchLen))
        if nclades<=10: palette = "tab10"
        else: palette = "tab20"
        clade_to_color = get_value_to_color(list(range(1, nclades+1)), palette=palette, n=nclades, type_color="hex")[0]

        # transform to log
        #pseudocount = 0.1
        #df_s["distance_km"] = np.log10(df_s.distance_km + pseudocount)

        # get figure
        nclades = len(set(df_s.cladeID))
        fig = plt.figure(figsize=(nclades*1, 3))
        ax = sns.violinplot(x="cladeID", y="distance_km", data=df_s, color="gray") # color=[clade_to_color[c] for c in df_s.cladeID]
        ax = sns.stripplot(x="cladeID", y="distance_km", data=df_s, facecolors=[clade_to_color[c] for c in df_s.cladeID], alpha=0.15)

        # define graphics
        ax.set_ylim([-1, 20000])
        ax.set_ylabel("C. %s\npairwise distance (km)"%(species.split("_")[1]))
        ax.set_xlabel("clade")


        # change the xticks
        #yticks = [int(y) for y in [0, 2, 4, 6] if y<=ax.get_ylim()[1]]
        #ax.set_yticks(yticks)
        #ax.set_yticklabels([str(get_float_or_int(10**y)) for y in yticks])

        # save
        plots_dir = "%s/pairwise_distance_location"%PlotsDir; make_folder(plots_dir)
        fig.savefig("%s/%s.pdf"%(plots_dir, species), bbox_inches='tight')


    ######################


def get_countries_dict_for_reverse_geocoding():

    """Gets the countries dict for reverse geocoding"""

    # download data
    data = requests.get("https://raw.githubusercontent.com/datasets/geo-countries/master/data/countries.geojson").json()

    # create dict
    countries = {}
    for feature in data["features"]:
        geom = feature["geometry"]
        country = feature["properties"]["ADMIN"]
        countries[country] = prep(shape(geom))

    return countries


def get_country_from_lat_lon(lat, lon, countries):

    """gets the country for a lat, lon. Based on https://stackoverflow.com/questions/20169467/how-to-convert-from-longitude-and-latitude-to-country-or-city"""

    if pd.isna(lat) or pd.isna(lon): return np.nan

    # special cases (ocean)
    latLon_to_country = {(8.95, 79.53):"Indic Ocean", (-38.86, 151.20939999999996):"Tasmania Sea", (41.494695247383305, -71.33039266382015):"New York Sea", (22.310160029890767, 114.12203362927234):"Hong Kong Sea"}
    if (lat, lon) in latLon_to_country: return latLon_to_country[(lat, lon)]

    # get the country
    point = Point(lon, lat)
    for country, geom in countries.items():
        if geom.contains(point):
            return country

    # unkown
    raise ValueError("Unknown country for lat,lon (%s/%s)"%(lat, lon))



def get_sample_to_clade_and_clade_to_color(metadata_df, species):

    """Gets dicts of sample to clade and color"""

    # define the sample to clade
    sample_to_clade, sample_to_cladePreviousPaper, clade_to_cladePreviousPaper = get_relation_clades_with_previousPaper(metadata_df, species)

    # get the color
    sorted_numeric_clades = sorted(set(sample_to_clade[sample_to_clade.apply(type)==int]))
    if len(sorted_numeric_clades)<10: palette="tab10"
    else: palette="tab20"
    clade_to_color = get_value_to_color(sorted_numeric_clades, palette=palette, n=len(sorted_numeric_clades), type_color="hex")[0]

    # add color for clades that are not clades
    for cID in set(sample_to_clade[sample_to_clade.apply(type)==str]): clade_to_color[cID] = "gray"

    # get all as strings
    clade_to_color  = {str(cID):c for cID,c in clade_to_color.items()}
    sample_to_clade = {str(s):str(c) for s,c in sample_to_clade.items()}

    return sample_to_clade, clade_to_color

def plot_clades_with_location_and_date_heatmap(metadata_df, PlotsDir, ProcessedDataDir, type_nisolates="each_year"):

    """Plots a heatmap for each species with country in the y, clade, year in the x and # isolates"""

    #go through each species
    for taxID, species in taxID_to_sciName.items():
        print(species)

        #if species!="Candida_auris": continue

        # get the metadata df for this species
        metadata_df_s = metadata_df[metadata_df.species_name==species]
        metadata_df_s["sampleID"] = metadata_df_s.sampleID.apply(str)

        # get the color for the clades
        sorted_all_clades = sorted(set(metadata_df_s[~(pd.isna(metadata_df_s.cladeID_Tree_and_BranchLen))].cladeID_Tree_and_BranchLen.apply(int)))
        if len(sorted_all_clades)<10: palette="tab10"
        else: palette="tab20"
        clade_to_color = get_value_to_color(sorted_all_clades, palette=palette, n=len(sorted_all_clades), type_color="hex")[0]

        # get only clinical and with clade
        metadata_df_s = metadata_df_s[(metadata_df_s.type=="clinical") & ~(pd.isna(metadata_df_s.cladeID_Tree_and_BranchLen))]
        all_valid_samples = set(metadata_df_s.sampleID)
        cladeID_to_nsamplesAll = metadata_df_s.groupby("cladeID_Tree_and_BranchLen").apply(len)

        # get the metadata_df_s with the latitude and longitude of each sample
        metadata_df_s = get_metadata_df_with_lat_and_lon(metadata_df_s)
        metadata_df_s = metadata_df_s[~(pd.isna(metadata_df_s.latitude))]

        # get metadata with the collection date
        metadata_df_s = get_metadata_df_with_collection_date(metadata_df_s)
        metadata_df_s = metadata_df_s[~(pd.isna(metadata_df_s.collection_date_tuple))] 

        # add discrete years
        metadata_df_s["year"] = metadata_df_s.collection_date_tuple.apply(lambda x: x[0])

        # add the country name
        print("adding country")
        countries_dict = get_countries_dict_for_reverse_geocoding(); print("countries dict obtained. Adding to df...")
        metadata_df_s["country"] = metadata_df_s.apply(lambda r: get_country_from_lat_lon(r["latitude"], r["longitude"], countries_dict), axis=1)
       
        if len(metadata_df_s)==0:
            print("WARNING: There are no data for location & date for clinical strains of %s"%species)
            continue


        ########## GENERATE A DF THAT HAS, FOR EACH CLADE, THE NUMBERS OF SAMPLES OF EACH YEAR #######

        # setup general things
        all_years = np.array(sorted(set(metadata_df_s.year)))
        metadata_df_s["cladeID"] = metadata_df_s.cladeID_Tree_and_BranchLen.apply(int)
        all_clades = sorted(set(metadata_df_s["cladeID"] ))

        # map each clade to the country and year where it was first isolated
        clade_to_first_countryYear_isolated = dict(metadata_df_s[["cladeID", "country", "collection_date_tuple", "year"]].drop_duplicates().sort_values(by=["cladeID", "collection_date_tuple", "country"]).drop_duplicates(subset=["cladeID"], keep="first").set_index("cladeID").apply(lambda r: (r.country, r.year), axis=1))

        # init df
        df_plot_long = pd.DataFrame()

        for cladeID in all_clades:

            # define a df with info about all years
            if type_nisolates=="cumulative":

                def get_df_all_years(r):
                    df_all_years = pd.DataFrame({"year" : all_years[all_years>=r.year]})
                    df_all_years["country"] = r.country
                    df_all_years["sampleID"] = r.sampleID
                    return df_all_years

                df_all_years = pd.concat(metadata_df_s[metadata_df_s.cladeID==cladeID][["sampleID", "country", "year"]].apply(get_df_all_years, axis=1).values)

            elif type_nisolates=="each_year": df_all_years = metadata_df_s[metadata_df_s.cladeID==cladeID][["sampleID", "country", "year"]]

            # get the df with the n isolates
            df_nisolates = pd.DataFrame({"# isolates" : df_all_years.groupby(["year", "country"]).apply(lambda df: len(set(df.sampleID)))})
            df_nisolates["year"] = df_nisolates.index.get_level_values(0)
            df_nisolates["country"] = df_nisolates.index.get_level_values(1)
            df_nisolates["clade"] = cladeID

            df_plot_long = df_plot_long.append(df_nisolates).reset_index(drop=True)


        # get the df plot square, where each row is a country and each col is a year, clade combination
        def get_NaN_to_0(x):
            if pd.isna(x): return 0
            else: return int(x)

        df_plot_long["clade_and_year"] = df_plot_long.clade.apply(str) + "_" + df_plot_long.year.apply(str)
        df_plot = df_plot_long.pivot(index="country", columns="clade_and_year", values="# isolates").applymap(get_NaN_to_0)

        # add missing cols and sort
        missing_clade_year_combinations = {"%i_%i"%(c, y) for y in all_years for c in all_clades}.difference(set(df_plot.columns))
        for c in missing_clade_year_combinations: df_plot[c] = 0

        # define the sorted columns and countries
        sorted_cols = sorted(set(df_plot.columns), key=(lambda x: tuple([int(y) for y in x.split("_")]) ))
        sorted_countries = sorted(df_plot.index)
        df_plot = df_plot.loc[sorted_countries, sorted_cols]

        ##############################################################################################

        ###### GET HEATMAP #######

        # define the col colors
        year_to_color = get_value_to_color(all_years, palette="Greys", n=len(all_years), type_color="hex")[0]
        colItem_to_color_dict = {"clade":clade_to_color, "year":year_to_color}

        col_colors_df = pd.DataFrame()
        for I, (colItem, color_dict) in enumerate(colItem_to_color_dict.items()): col_colors_df[colItem] = pd.Series({col : color_dict[int(col.split("_")[I])]  for  col in df_plot.columns})


        # define the annotations df (get the first element)
        annot_df = pd.DataFrame()
        for clade_and_year in df_plot.columns:
            clade, year = [int(x) for x in clade_and_year.split("_")]

            def get_annot_cell(country):
                if (country, year)==clade_to_first_countryYear_isolated[clade]: return "*"
                else: return ""

            annot_df[clade_and_year] = pd.Series({country : get_annot_cell(country) for country in df_plot[clade_and_year].index})


        # get clustermap
        # annot_kws={"size": 6}
        def get_minusInf_to_0(x):
            if x==(-np.inf): return -1.0
            else: return x

        type_nisolates_to_cbar_label = {"cumulative":'log(cum. # isolates)', "each_year":'log(# isolates each year)'}
        cm = sns.clustermap(df_plot.applymap(float).applymap(np.log10).applymap(get_minusInf_to_0), row_cluster=True, col_cluster=False, col_colors=col_colors_df, cbar_kws={'label':type_nisolates_to_cbar_label[type_nisolates]}, mask=(df_plot.values==0), annot=annot_df, annot_kws={"size": 12, "color":"black"}, fmt="", cmap="rocket_r") # linewidths=.0025, linestyles="--", linecolor="dimgray"


        # add lines
        cm.ax_heatmap.hlines(list(range(len(df_plot)+1)), *cm.ax_heatmap.get_xlim(), color="gray", linestyle="--", linewidth=.5)
        cm.ax_heatmap.vlines(list(range(0, len(df_plot.columns)+1, len(all_years))), *cm.ax_heatmap.get_ylim(), color="gray", linestyle="--", linewidth=.5)



        # define a multiplier per species
        if species in {"Candida_parapsilosis", "Candida_orthopsilosis", "Candida_tropicalis"}: multiplier_w = 1
        elif species in {"Candida_albicans", "Candida_glabrata"}: multiplier_w = 0.1
        elif species=="Candida_auris": multiplier_w = 0.25


        #  define general things
        #pixels_per_square = 0.025
        pixels_per_square = 0.02

        hm_height = len(df_plot)*pixels_per_square
        hm_width = len(df_plot.columns)*pixels_per_square*multiplier_w
        #rc_width = len(row_colors_df.columns)*pixels_per_square
        rc_width = 0


        # adjust the heatmap positions
        hm_pos = cm.ax_heatmap.get_position()
        hm_y0 = (hm_pos.y0+hm_pos.height)-hm_height
        cm.ax_heatmap.set_position([hm_pos.x0, hm_y0, hm_width, hm_height]); hm_pos = cm.ax_heatmap.get_position()

        cc_pos = cm.ax_col_colors.get_position()
        cm.ax_col_colors.set_position([cc_pos.x0, cc_pos.y0+pixels_per_square/5, hm_width, len(col_colors_df.columns)*pixels_per_square])

        rd_width = pixels_per_square*2
        cm.ax_row_dendrogram.set_position([hm_pos.x0-rd_width, hm_pos.y0, rd_width, hm_height])

        #cm.ax_heatmap.set_yticklabels(df_plot.index, rotation=0)

        cm.ax_heatmap.set_yticklabels(cm.ax_heatmap.get_yticklabels(), rotation = 0)





        #cm.ax_heatmap.set_xticklabels(["c%s, %s"%(x.split("_")[0], x.split("_")[1]) for x in df_plot.columns], rotation=90)
        cm.ax_heatmap.set_xticklabels([])
        cm.ax_heatmap.set_xticks([])

        cm.ax_heatmap.set_xlabel("")
        cm.ax_col_colors.set_title(species)

        # calculate the clade to nsamples after filtering
        cladeID_to_nsamples = metadata_df_s.groupby("cladeID_Tree_and_BranchLen").apply(len)


        cladeID_to_nsamplesAll

        # add legend
        def get_legend_e(color, label): return Line2D([0], [0], color=color, lw=6, label=label, alpha=1.0)

        if len(all_years)>len(all_clades): 
            nextra_elements_years = 0
            nextra_elements_clades = len(all_years) - len(all_clades)

        elif  len(all_years)<len(all_clades): 
            nextra_elements_years = len(all_clades) - len(all_years)
            nextra_elements_clades = 0

        else:
            nextra_elements_years = 0
            nextra_elements_clades = 0

        legend_elements_years = [get_legend_e("white", "") for n in range(nextra_elements_years)] + [get_legend_e("white", "year")] + [get_legend_e(year_to_color[year], year) for year in all_years]
        legend_elements_clades = [get_legend_e("white", "") for n in range(nextra_elements_clades)] + [get_legend_e("white", "clade")] + [get_legend_e(clade_to_color[cladeID], "c%i (%i/%i)"%(cladeID, cladeID_to_nsamples[cladeID], cladeID_to_nsamplesAll[cladeID])) for cladeID in all_clades]
        legend_elements = legend_elements_years + legend_elements_clades

        cm.ax_col_dendrogram.legend(handles=legend_elements, loc='lower left', bbox_to_anchor=(0, 0.1), frameon=False, ncol=2) # x,y are the lower left coordinates of the legend

        plt.show()

        # save
        plots_dir = "%s/countries_with_isolates_heatmap"%PlotsDir; make_folder(plots_dir)
        print("saving %s"%plots_dir)
        cm.savefig("%s/%s.pdf"%(plots_dir, species), bbox_inches="tight")

        ##########################



def plot_clades_with_location_and_date(metadata_df, PlotsDir, species_to_tree, ProcessedDataDir):

    """Generates, for each species, one plot with the clades and the occupation on different countries"""

    # get info from plot_tree_with_fraction_recombinant_genome_eachClade_fastGEAR

    # go through each species
    for taxID, species in taxID_to_sciName.items():
        print(species)

        if species!="Candida_glabrata": continue

        # get the metadata df for this species
        metadata_df_s = metadata_df[metadata_df.species_name==species]
        metadata_df_s["sampleID"] = metadata_df_s.sampleID.apply(str)

        # define clades and clade-graphics
        sample_to_clade, clade_to_color = get_sample_to_clade_and_clade_to_color(metadata_df, species)

        ###### ADD COUNTRY INFO ########


        # get only clinical and with clade
        metadata_df_s = metadata_df_s[(metadata_df_s.type=="clinical") & ~(pd.isna(metadata_df_s.cladeID_Tree_and_BranchLen))]
        all_valid_samples = set(metadata_df_s.sampleID)
        cladeID_to_nsamplesAll = {clade : len({s for s,c in sample_to_clade.items() if c==clade and s in all_valid_samples}) for clade in set(sample_to_clade.values())}

        # get the metadata_df_s with the latitude and longitude of each sample
        metadata_df_s = get_metadata_df_with_lat_and_lon(metadata_df_s)
        metadata_df_s = metadata_df_s[~(pd.isna(metadata_df_s.latitude))]

        # map each 
        cladeID_to_nsamplesAll = {clade : len({s for s,c in sample_to_clade.items() if c==clade}) for clade in set(sample_to_clade.values())}


        # get metadata with the collection date
        metadata_df_s = get_metadata_df_with_collection_date(metadata_df_s)
        metadata_df_s = metadata_df_s[~(pd.isna(metadata_df_s.collection_date_tuple))] 

        # add discrete years
        metadata_df_s["year"] = metadata_df_s.collection_date_tuple.apply(lambda x: x[0])

        # add the country name
        print("adding country")
        countries_dict = get_countries_dict_for_reverse_geocoding(); print("countries dict obtained. Adding to df...")
        metadata_df_s["country"] = metadata_df_s.apply(lambda r: get_country_from_lat_lon(r["latitude"], r["longitude"], countries_dict), axis=1)
       
        # reset index 
        metadata_df_s.index = metadata_df_s.sampleID

        if len(metadata_df_s)==0:
            print("WARNING: There are no data for location & date for clinical strains of %s"%species)
            continue

        ################################

        ######### GENERATE PLOTS IN TREE #########

        # get the tree and keep correct samples
        tree = cp.deepcopy(species_to_tree[species])
        correct_samples = set(tree.get_leaf_names()).difference({str(x) for x in sciName_to_badSamples[species]}).intersection(set(metadata_df_s.sampleID))
        tree.prune(correct_samples)

        # remove low support branches
        tree = get_correct_tree(tree, min_support=95)

        # add the cladeID as a string
        tree = get_tree_withcladeID_as_str(tree, sample_to_clade)

        # init a map between each node and the number of leafs
        clade_to_leafs = {}

        # prune the tree to keep only ancestral nodes of each clade
        for n in tree.traverse():
            if n.is_root(): continue
            if pd.isna((n.get_ancestors()[0]).cladeID) and not pd.isna(n.cladeID): 

                clade_to_leafs[n.cladeID] = set(n.get_leaf_names())
                for child in n.get_children(): child.detach()
                n.name = n.cladeID

        # check that all the leafs where mentioned
        if set.union(*clade_to_leafs.values())!=set(metadata_df_s.sampleID): raise ValueError("not all samples in metadata_df_s were considered")

        # define some general things
        all_years = np.array(sorted(set(metadata_df_s.year)))
        all_countries = sorted(set(metadata_df_s.country))
        nnodes = len([x for x in tree.traverse()])
        all_clades = tree.get_leaf_names()

        # map each c to a color
        if len(all_countries)<10: palette="tab10"
        elif len(all_countries)<100: palette="tab20"
        else: raise ValueError("there are >100 countries in %s"%species)

        print(len(all_countries))

        country_to_color = get_value_to_color(all_countries, palette=palette,  type_color="hex")[0]


        # define the country & clade that has the maximum samples
        countryClade_to_nsamples = metadata_df_s.groupby(["country", "cladeID_Tree_and_BranchLen"]).apply(len)
        max_nsamples = max(countryClade_to_nsamples)

        # go through each clade
        for I, n in enumerate(tree.traverse()):

            # change the style of the node
            width = 19
            nst = NodeStyle()
            nst["hz_line_width"] = width
            nst["vt_line_width"] = width
            nst["size"] = 0
            nst["bgcolor"] = "white"
            nst["hz_line_color"] = "black"
            nst["vt_line_color"] = "black"
            
            n.set_style(nst)


            # for leafs (each one clade)
            if n.is_leaf() is True:


                ########### PLOT YEAR VS CUMULTiVE NUMBER OF ISOLATES ############

                # get a df with the year and country data, duplicating the initial isolates across posterior years
                df_eachisolate = metadata_df_s.loc[clade_to_leafs[n.name]][["year", "country", "sampleID"]]

                def get_df_all_years(r):
                    df_all_years = pd.DataFrame({"year" : all_years[all_years>=r.year]})
                    df_all_years["country"] = r.country
                    df_all_years["sampleID"] = r.sampleID
                    return df_all_years

                df_all_years = pd.concat(df_eachisolate.apply(get_df_all_years, axis=1).values)

                # get a df with the number of isolates in each year and country
                df_plot = pd.DataFrame({"# isolates" : df_all_years.groupby(["year", "country"]).apply(lambda df: len(set(df.sampleID)))})
                df_plot["year_country_tuple"] = df_plot.index

                # add 0s for years countries that are not found here
                not_found_combinations = {(y, c) for c in set(df_eachisolate.country) for y in all_years}.difference(set(df_plot.year_country_tuple))
                df_zeros = pd.DataFrame({"year_country_tuple" : list(not_found_combinations)})
                df_zeros["# isolates"] = 0

                df_plot = df_plot.append(df_zeros).reset_index(drop=True)
                df_plot["year"] = df_plot.year_country_tuple.apply(lambda x: x[0])
                df_plot["country"] = df_plot.year_country_tuple.apply(lambda x: x[1])
                df_plot = df_plot.sort_values(by=["year", "country"])


                # get a plot of the year vs number of isolates
                df_plot["year"] = df_plot.year.apply(str)
                fig = plt.figure(figsize=(3, 2))
                ax = sns.lineplot(data=df_plot, x="year", y="# isolates", hue="country", palette=country_to_color, linewidth=2)
                ax.set_xticklabels([str(x) for x in all_years], rotation=90)

                if n.name!=all_clades[-1]: 
                    ax.set_xticklabels([])
                    #ax.set_xticks([])
                    ax.set_xlabel("")

                ax.set_yscale("log")

                ax.get_legend().remove()
                ax.set_ylim([-0.1, max_nsamples + 0.5])
                ax.set_title("clade %s (%i/%i isolates)"%(n.name, len(clade_to_leafs[n.name]), cladeID_to_nsamplesAll[n.name]))

                if n.name==(all_clades[0]):
                    legend_elements = [Line2D([0], [0], color="white", lw=4, label="country", alpha=1.0)] + [Line2D([0], [0], color=color, lw=4, label=c, alpha=1.0) for c, color in country_to_color.items()]
                    #ax.legend(handles=legend_elements, loc='right', bbox_to_anchor=(2.0, -0.5))
                
                plots_dir = "%s/plots_location_and_date_per_clade_tree"%PlotsDir; make_folder(plots_dir)
                filename = "%s/%s_clade%s.png"%(plots_dir, species, n.name)
                fig.savefig(filename, bbox_inches="tight", dpi=400)
                plt.close(fig)

                ##################################################################

                # add face of the image
                n.add_face(RectFace(20, 200, fgcolor="white", bgcolor="white"), column=0 , position="aligned") #
                n.add_face(ImgFace(filename), column=1,  position="aligned")
                #remove_file(filename)

        # init tree style
        ts = TreeStyle()
        
        # add legend
        ts.legend.add_face(TextFace("country:", fgcolor="black", bold=False, fsize=35), column=0)
        for Ic, (c, color) in enumerate(country_to_color.items()):

            ts.legend.add_face(TextFace(c, fgcolor=color, bold=True, fsize=30), column=0)

        ts.legend_position=1


        # And, finally, Visualize the tree using my own layout function
        ts.show_branch_length = False
        ts.show_branch_support = False
        ts.show_leaf_name = False
        ts.draw_guiding_lines = True
        ts.guiding_lines_type = 2
        ts.guiding_lines_color = "gray"

        tree.show(tree_style=ts)


        plots_dir= "%s/plots_location_and_date_per_clade_treePlot"%PlotsDir; make_folder(plots_dir)
        print("plotting in %s"%plots_dir)
        tree.render(file_name="%s/%s_tree.pdf"%(plots_dir, species), tree_style=ts)

        #t.render("img_faces.png", w=600, tree_style = ts)


        ##########################################



def plot_location_clinical_isolates(metadata_df, PlotsDir, species_to_tree, consider_time=True):

    """Plots in a map the clinical isolates"""

    # get the metadata_df with the latitude and longitude of each sample
    metadata_df = get_metadata_df_with_lat_and_lon(metadata_df)

    # get metadata with the collection date
    if consider_time is True: metadata_df = get_metadata_df_with_collection_date(metadata_df)

    # change
    metadata_df["sampleID"] = metadata_df.sampleID.apply(str) 

    # define clades that are localized or you want to plot
    """
    species_to_localizedClades = {"Candida_glabrata":[4, 6, 13, 14, 18, 20, 22],
                                  "Candida_auris":[1, 2, 3, 4],
                                  "Candida_tropicalis":[3],
                                  "Candida_albicans":[2, 3, 4, 5, 7, 8, 9, 12, 15, 16, 18, 19, 20],
                                  "Candida_parapsilosis":[1, 2, 3],
                                  "Candida_orthopsilosis":[1, 2, 4, 5]}

    """

    # go through each species
    for taxID, species in taxID_to_sciName.items():

        #if species!="Candida_auris": continue

        # get dfs
        metadata_df_s = metadata_df[(metadata_df.species_name==species)]
        metadata_df_s_loc = metadata_df_s[~(pd.isna(metadata_df_s.latitude)) & (metadata_df_s.type=="clinical")] # ~(pd.isna(metadata_df_s.collection_date_tuple))

        # filter time
        if consider_time is True: metadata_df_s_loc = metadata_df_s_loc[~(pd.isna(metadata_df_s_loc.collection_date_tuple))]

        # prints
        nclades_s = len(set(metadata_df_s[~pd.isna(metadata_df_s.cladeID_Tree_and_BranchLen)].cladeID_Tree_and_BranchLen))
        nclades_s_loc = len(set(metadata_df_s_loc[~pd.isna(metadata_df_s_loc.cladeID_Tree_and_BranchLen)].cladeID_Tree_and_BranchLen))
        print("%s has %i/%i samples and %i/%i clades with geographic and time information and are clinical isolates"%(species, len(metadata_df_s_loc), len(metadata_df_s), nclades_s_loc, nclades_s))

        # get clade info
        leaf_to_clade = dict(metadata_df_s.set_index("sampleID").cladeID_Tree_and_BranchLen)
        nclades = len(set([x for x in leaf_to_clade.values() if not pd.isna(x)]))

        # map the clade to color
        if nclades<=10: palette = "tab10"
        else: palette = "tab20"
        clade_to_color = get_value_to_color(list(range(1, nclades+1)), palette=palette, n=nclades, type_color="hex")[0]

        # get tree
        tree = cp.deepcopy(species_to_tree[species])

        # define samples that have some loc
        good_samples = set(tree.get_leaf_names()).difference({str(x) for x in sciName_to_badSamples[species]})
        good_samples = good_samples.intersection(set(metadata_df_s_loc.sampleID))
        if len(good_samples)==0: continue

        # prune the tree and add the clade ID
        tree.prune(good_samples)
        leaf_to_clade = {l:c for l,c in leaf_to_clade.items() if l in good_samples}
        tree = get_tree_withcladeID_as_str(tree, leaf_to_clade)

        # add the location
        metadata_df_s_loc = metadata_df_s_loc.set_index("sampleID", drop=False)
        for l in tree.get_leaves(): l.location = tuple(metadata_df_s_loc.loc[l.name][["longitude", "latitude"]].values)
        
        # map each clade to a marker
        clade_to_marker = {}
        all_markers = ["triangle", "circle"]*100
        for Ic, clade in enumerate(clade_to_color): clade_to_marker[clade] = all_markers[Ic]

        # init fig
        fig = go.Figure()

        # add each clade as a leaf
        #for clade in species_to_localizedClades[species]:
        for clade in clade_to_color.keys():

            # define color
            color = clade_to_color[clade]

            # get clade
            df_c = metadata_df_s_loc[metadata_df_s_loc.cladeID_Tree_and_BranchLen==clade].sort_values(by="collection_date_tuple")
            df_c["sampleID"] = df_c.sampleID.apply(str)
            if len(df_c)==0: continue

            # add jittering
            df_c["longitude_jitter"] = df_c.longitude + list(map(lambda x: random.uniform(0, 1), range(len(df_c))))
            df_c["latitude_jitter"] = df_c.latitude + list(map(lambda x: random.uniform(0, 1), range(len(df_c))))

            # get fig
            sizes = [10]*len(df_c)
            fig.add_trace(go.Scattermapbox(mode="markers", lon=df_c.longitude_jitter, lat=df_c.latitude_jitter, marker={"color":color, "size":10}, name="clade_%i"%clade, opacity=0.8)) # you can't change the symbol and the color


            # calculate the  centroid, midle point, as the median if there is no time info
            if consider_time is False:

                centroid_longitude = np.mean(df_c.longitude_jitter)
                centroid_latitude = np.mean(df_c.latitude_jitter)

            else:

                first_isolate_r = df_c[~(pd.isna(df_c.collection_date_tuple))].sort_values(by="collection_date_tuple").iloc[0]
                centroid_longitude = first_isolate_r.longitude_jitter
                centroid_latitude = first_isolate_r.latitude_jitter

            # make one line from the centroid to the clades
            for sampleID in df_c.sampleID:

                longitudes = [centroid_longitude, df_c.loc[sampleID].longitude_jitter]
                latitudes = [centroid_latitude, df_c.loc[sampleID].latitude_jitter]
                fig.add_trace(go.Scattermapbox(mode="lines", lon=longitudes, lat=latitudes, line={"color":color, "width":1}, opacity=.8, showlegend=False)) 


        # write figure
        fig.update_layout(
            margin ={'l':0,'t':0,'b':0,'r':0},
            mapbox = {
                'center': {'lon': 10, 'lat': 10},
                'style': "stamen-terrain",
                'center': {'lon': -20, 'lat': -20},
                'zoom': 1})


        config={'editable': False}
        off_py.plot(fig, filename="%s/%s_location.html"%(PlotsDir, species), auto_open=False, config=config)

def get_node_state_all_ignoreNaN(r):

    """Gets a row with node states and returns the integrated states, but ignoring NaNs"""

    # define all states
    all_states = set(r[~pd.isna(r)].values)

    # only return a state if all the nodes are as in all states
    if len(all_states)==1: return next(iter(all_states))
    else: return np.nan



def get_ASR_interesting_variants_extra_info(interesting_variants_df, tmpdir_all, outdir_drug, ASR_methods_phenotypes, min_support):

    """Gets a df with the interesting vairants df"""

    # define final file
    asr_mutations_group_file = "%s/asr_only_muts_in_group.tab"%tmpdir_all
    if file_is_empty(asr_mutations_group_file):

        # map define var ID mapping
        print("getting individual gt states")

        ASR_mutations_dir = "%s/ASR_mutations"%outdir_drug
        mut_to_rep_mut = dict(get_tab_as_df_or_empty_df("%s/mutation_to_representative_mutation.tab"%ASR_mutations_dir).set_index("mutation").rep_mutation)
        var_to_mut = dict(get_tab_as_df_or_empty_df("%s/variants_IDmapping.tab"%ASR_mutations_dir).set_index("variantID_across_samples").numeric_mutation)

        # discard variants that are not in var_to_mut
        interesting_variants_df = interesting_variants_df[interesting_variants_df.variantID_across_samples.isin(set(var_to_mut))]
        if len(interesting_variants_df)==0: raise ValueError("there can't be 0 vars")

        # create a df that has a df for each mutation and node
        def get_df_GTstate_one_var(var):
            rep_mut = mut_to_rep_mut[var_to_mut[var]]
            df_pastml = get_tab_as_df_or_empty_df("%s/ASR_mutations/%s_pastml_reconstruction.out"%(ASR_mutations_dir, rep_mut)).rename(columns={"node":"node_name"})

            # add the genotype presence using the ASR_methods_phenotypes
            df_pastml["genotype_presence"] = df_pastml[["%s_%s"%(rep_mut, met) for met in ASR_methods_phenotypes.split(",")]].apply(get_node_state_all_ignoreNaN, axis=1)

            # add var
            df_pastml["variantID_across_samples"] = var
            return df_pastml

        asr_gt_states_df = pd.concat(list(map(get_df_GTstate_one_var, interesting_variants_df.variantID_across_samples)))
    
        # define tmp file
        asr_mutations_group_file_tmp = "%s.tmp"%asr_mutations_group_file

        # load the ASR data for these mutations
        print("getting  interesting variants")

        # define the file with all the mutations
        dir_all_mutations = "%s/all_vars-all_genes-all_muts-none/%s-%s-min_support=%i/gwas_jobs"%(outdir_drug, ASR_methods_phenotypes, ASR_methods_phenotypes, min_support)
        asr_mutations_all_file = "%s/integrated_ASR.tab"%(dir_all_mutations)

        # extract all the interesting mutations
        if len(interesting_variants_df)==0: raise ValueError("there can't be 0 vars")
        interesting_variants_file = "%s/list_interesting_vars.txt"%tmpdir_all
        interesting_variants_df[["variantID_across_samples"]].to_csv(interesting_variants_file, sep="\t", header=False, index=False)                
        run_cmd("egrep -f %s %s > %s"%(interesting_variants_file, asr_mutations_all_file, asr_mutations_group_file_tmp))
        remove_file(interesting_variants_file)

        # extract the header
        header_file = "%s/header.tab"%tmpdir_all
        run_cmd("head -1 %s > %s"%(asr_mutations_all_file, header_file))
        header = open(header_file, "r").readlines()[0].strip().split("\t")
        remove_file(header_file)

        # load the variants 
        asr_mutations_group_df = pd.read_csv(asr_mutations_group_file_tmp, sep="\t", header=None, names=header)

        # keep only interesting variants
        all_vars = set(interesting_variants_df.variantID_across_samples)
        asr_mutations_group_df = asr_mutations_group_df[asr_mutations_group_df.group_name.isin(all_vars)].rename(columns={"group_name":"variantID_across_samples"})

        if set(asr_mutations_group_df.variantID_across_samples)!=all_vars: raise ValueError("not all vars found")

        # add the individual GT states
        initial_len = len(asr_mutations_group_df)
        asr_mutations_group_df = asr_mutations_group_df.merge(asr_gt_states_df, on=["node_name", "variantID_across_samples"], validate="one_to_one", how="left")
        if len(asr_mutations_group_df)!=initial_len: raise ValueError("df changed upon merge")
 
        # save
        save_df_as_tab(asr_mutations_group_df, asr_mutations_group_file_tmp)

        # keep
        os.rename(asr_mutations_group_file_tmp, asr_mutations_group_file)

    return get_tab_as_df_or_empty_df(asr_mutations_group_file)


def get_tab_file_only_lines_matching_patterns(list_patterns, tab_file, dest_tab_file):

    """Generates a dest_tab_file that contains the patterns in list_patterns"""

    if file_is_empty(dest_tab_file):
        print("getting file for %i patterns"%len(list_patterns))

        # check
        if file_is_empty(tab_file): raise ValueError("the tab_file should exist")

        # generate a file with the patterns
        file_patterns = "%s.patterns_to_search.txt"%dest_tab_file
        open(file_patterns, "w").write("\n".join(list(list_patterns))+"\n")

        # extract all the patterns
        print("running grep")
        dest_tab_file_no_header = "%s.no_header.tab"%dest_tab_file
        run_cmd("egrep -f %s %s > %s || true > %s"%(file_patterns, tab_file, dest_tab_file_no_header, dest_tab_file_no_header))

        # extract the header
        print("creating file")
        header_file = "%s.header.tab"%dest_tab_file
        run_cmd("head -1 %s > %s"%(tab_file, header_file))

        # concatenate into the final file
        dest_tab_file_tmp = "%s.tmp"%dest_tab_file
        run_cmd("cat %s %s > %s"%(header_file, dest_tab_file_no_header, dest_tab_file_tmp))

        # clean and keep
        for f in [file_patterns, dest_tab_file_no_header, header_file]: remove_file(f)

        # save
        os.rename(dest_tab_file_tmp, dest_tab_file) 

    return dest_tab_file


def convert_py_pandas_df_to_tab(df_file_py):

    """Converts the .py file to .tab"""

    df_file_tab = "%s.tab"%df_file_py
    if file_is_empty(df_file_tab):

        print("loading df")
        df = load_object(df_file_py)
        
        print("saving to %s"%df_file_tab)
        save_df_as_tab(df, df_file_tab)



def get_string_representation_of_var_also_considering_SVs_and_CNVs(row):
    
    """Takes a row of a vars df such as the one provided in get_genesToSamplesToPrivateMutations_df. It returns a string with the meaning of the var."""

    if row["Gene"]=="-":
        if row["Consequence"]=="intergenic_variant": return "ig|%s|%s"%(row["#Uploaded_variation"], row.variantID_across_samples) 
        else: raise ValueError("The consequence should be exitsing")
    
    # define the most important consequence according to how you ranked the consequences in var_to_IDX
    cons = sorted(row["consequences_set"], key=lambda y: var_to_IDX[y])[0]

    # gget small vars
    if cons in protVar_to_info.keys():

        return "%s|%s.%s|%s"%(consequence_to_abbreviation[cons], protVar_to_info[cons][0], row[protVar_to_info[cons][1]], row[protVar_to_info[cons][2]]) 

    else:   

        # define the abbrebiation for BND feats
        if cons.endswith("BND"): abbreviation = consequence_to_abbreviation[cons.split("_BND")[0]]+"_BND"
        else: abbreviation = consequence_to_abbreviation[cons]

        return "%s|%s"%(abbreviation, row["#Uploaded_variation"])



def get_variant_effect_short_one_var_and_gene(df):

    """Takes a df with one gene and variant (df_annot-style), and returns a series with the short effect of the most impactfull variant."""

    # check
    variant, gene = df.name
    if set(df.variantID_across_samples)!={variant}: raise ValueError("the variant is not unique")
    if set(df.Gene)!={gene}: raise ValueError("the gene is not unique")

    # get consequences set
    df["consequences_set"] = df.Consequence.apply(lambda x: set(x.split(",")))

    # keep only the strongest consequence as series_annot (check that there is one that is clearly consequent. You may need to work on this for SVs.)
    df["consequences_set_strongestIDX"] = df.consequences_set.apply(lambda cons:  sorted(map(lambda c: var_to_IDX[c], cons))[0])
    df = df.sort_values(by="consequences_set_strongestIDX", ascending=True)
    #if len(df)>1 and df.consequences_set_strongestIDX.iloc[0]==df.consequences_set_strongestIDX.iloc[1]: raise ValueError("the strongest consquence should be unique to the first row. This is the df: %s "%(df[["consequences_set", "consequences_set_strongestIDX", "CDS_position", "#Uploaded_variation", "Gene"]]))

    series_annot = df.iloc[0]

    # return the short variant effect for this first row
    return "%s|||%s"%(series_annot.Consequence, get_string_representation_of_var_also_considering_SVs_and_CNVs(series_annot))


def get_df_vars_with_short_variant_effect(all_vars, DataDir, species, tmpdir_all, interesting_genes, allow_vars_no_annot=False):

    """Gets the variant effect short for many variants"""

    # define file
    make_folder(tmpdir_all)
    df_vars_weffect_file = "%s/df_vars_weffect.py"%tmpdir_all
    if file_is_empty(df_vars_weffect_file):
        print("adding short variant effect")

        # get all variants
        sorted_vars = sorted(all_vars)

        # define the taxID dir
        taxID_dir = "%s/%s_%i"%(DataDir, species, sciName_to_taxID[species])

        # generate tab files from the .py dfs (this is only done once for each species)
        print("converting to tab")
        for f in ["smallVars_filt.py", "SV_CNV_filt.py"]: convert_py_pandas_df_to_tab("%s/integrated_varcalls/%s"%(taxID_dir, f))

        # get a df with the variants df
        print("getting ID mapping df only interesting variants")  
        vars_df_allFields_smallVars = get_tab_as_df_or_empty_df(get_tab_file_only_lines_matching_patterns(sorted_vars, "%s/integrated_varcalls/smallVars_filt.py.tab"%taxID_dir, "%s.smallVars_filt.py.tab.vars_df_only_interesting_vars.tab"%df_vars_weffect_file)) 
        vars_df_allFields_smallVars["variantID_across_samples"] = vars_df_allFields_smallVars["#Uploaded_variation"]

        vars_df_allFields_SV_CNV = get_tab_as_df_or_empty_df(get_tab_file_only_lines_matching_patterns(sorted_vars, "%s/integrated_varcalls/SV_CNV_filt.py.tab"%taxID_dir, "%s.SV_CNV_filt.py.tab.vars_df_only_interesting_vars.tab"%df_vars_weffect_file)) 
        vars_df_allFields_SV_CNV["#Uploaded_variation"] = vars_df_allFields_SV_CNV["ID"]

        ID_fields = ["#Uploaded_variation", "variantID_across_samples"]
        vars_df_allFields = vars_df_allFields_smallVars[ID_fields].append(vars_df_allFields_SV_CNV[ID_fields]).drop_duplicates()

        # checks
        check_no_nans_in_df(vars_df_allFields)
        vars_not_found = all_vars.difference(set(vars_df_allFields.variantID_across_samples))
        if len(vars_not_found)>0: raise ValueError("there are %i variants that are not found: %s"%(len(vars_not_found), vars_not_found))

        # get the uploaded variation maps
        sorted_uploaded_variations = sorted(set(vars_df_allFields["#Uploaded_variation"]))
        uploaded_variation_to_var = dict(vars_df_allFields.set_index("#Uploaded_variation").variantID_across_samples)

        # get variant annotation only for these variants
        annot_df = pd.concat([get_tab_as_df_or_empty_df(get_tab_file_only_lines_matching_patterns(sorted_uploaded_variations, "%s/integrated_varcalls/%s"%(taxID_dir, f), "%s.%s.var_annot_only_interesting_vars.tab"%(df_vars_weffect_file, f))) for f in ["smallVars_annot.tab", "SV_CNV_annot.tab"]])
        annot_df = annot_df[(annot_df["#Uploaded_variation"].isin(set(sorted_uploaded_variations))) & (annot_df.Gene.isin(interesting_genes))].drop_duplicates()

        # add the variant ID
        annot_df["variantID_across_samples"] = annot_df["#Uploaded_variation"].map(uploaded_variation_to_var); check_no_nans_series(annot_df["variantID_across_samples"])

        # checks
        vars_no_annot = all_vars.difference(set(annot_df.variantID_across_samples))
        if allow_vars_no_annot is False:
            if len(vars_no_annot)>0: raise ValueError("There are %i/%i vars with no annotation: %s"%(len(vars_no_annot), len(all_vars), vars_no_annot))

        # get the short_variant effect for each gene
        print("getting short variant effects")
        df_vars_weffect = pd.DataFrame({"short_variant_effect" : annot_df.groupby(["variantID_across_samples", "Gene"]).apply(get_variant_effect_short_one_var_and_gene)}).reset_index()

        def get_r_df_vars_weffect(r):
            split_r = r.short_variant_effect.split("|||")
            r["Consequence"] = split_r[0]
            r["short_variant_effect"] = split_r[1]
            return r

        df_vars_weffect = df_vars_weffect.apply(get_r_df_vars_weffect, axis=1)

        print("saving")
        save_object(df_vars_weffect, df_vars_weffect_file)

    return load_object(df_vars_weffect_file)


def plot_tree_all_mutations_one_group_gwas_AF(outdir_drug, ASR_methods_phenotypes, target_type_vars, target_type_genes, target_type_mutations, target_type_collapsing, target_group_name, tree_plot, DataDir, species, drug, tmpdir_prefix, target_interesting_geneID_to_geneName, metadata_df, min_support, interesting_attributes=["sampleID"], only_mutations_associated_to_resistance=False, mic_fields_plot=[], rotation_mode="r"):

    """Plots a tree with all mutations for one group GWAS df.

    median_median_reads_per_gene,
    fraction_genes_covered_above95pct,

    """
    print("getting tree for group %s"%target_group_name)



    ####### GET PROCESSED DATA #########

    # make tmpdir
    tmpdir_all = "%s_%s_%s_%s_%s_%s"%(tmpdir_prefix, target_type_vars, target_type_genes, target_type_mutations, target_type_collapsing, only_mutations_associated_to_resistance)
    make_folder(tmpdir_all)

    # load the gwas results df of this group
    gwas_results_df = get_tab_as_df_or_empty_df("%s/%s_%i/ancestral_GWAS_drugResistance/GWAS_%s_resistance/%s-%s-%s-%s/%s-%s-min_support=%i/gwas_jobs/integrated_GWAS_stats.tab"%(DataDir, species, sciName_to_taxID[species], drug, target_type_vars, target_type_genes, target_type_mutations, target_type_collapsing, ASR_methods_phenotypes, ASR_methods_phenotypes, min_support))

    gwas_results_df["min_support"] = min_support
    gwas_results_df = gwas_results_df[gwas_results_df.group_name==target_group_name]
    if len(gwas_results_df)!=1: raise ValueError("There should be 1 rows for this group")

    # load the ASR gwas results of the group
    print("loading ASR results")
    gwas_asr_results_df_file = "%s/gwas_asr_results_df.py"%tmpdir_all
    if file_is_empty(gwas_asr_results_df_file):

        gwas_asr_results_df = get_tab_as_df_or_empty_df("%s/%s_%i/ancestral_GWAS_drugResistance/GWAS_%s_resistance/%s-%s-%s-%s/%s-%s-min_support=%i/gwas_jobs/integrated_ASR.tab"%(DataDir, species, sciName_to_taxID[species], drug, target_type_vars, target_type_genes, target_type_mutations, target_type_collapsing, ASR_methods_phenotypes, ASR_methods_phenotypes, min_support))
        gwas_asr_results_df = gwas_asr_results_df[gwas_asr_results_df.group_name==target_group_name].set_index("node_name")
        save_object(gwas_asr_results_df, gwas_asr_results_df_file)

    gwas_asr_results_df = load_object(gwas_asr_results_df_file)
    gwas_asr_results_df.index = list(map(str, gwas_asr_results_df.index))

    # get the grouping df
    if target_type_collapsing!="none":

        tmpdir = "%s/getting_gdf"%tmpdir_all
        target_grouping_df = load_object(get_target_grouping_df_file_fromDataDir_gwas_af_resistance(tmpdir, DataDir, species, drug, target_type_vars, target_type_genes, target_type_mutations, target_type_collapsing))

        # keep only mutations of the group name
        target_grouping_df = target_grouping_df[target_grouping_df.group==target_group_name].drop_duplicates()
        if len(target_grouping_df)==0: raise ValueError("there have to be some muts in the target_grouping_df")

    else: target_grouping_df = pd.DataFrame({"variantID_across_samples":[target_group_name], "group":[target_group_name]})

    # get an ASR df with all muts
    asr_df_vars = get_ASR_interesting_variants_extra_info(target_grouping_df, tmpdir_all, outdir_drug, ASR_methods_phenotypes, min_support)

    # keep only mutations that somehen associated to the resistance
    if only_mutations_associated_to_resistance is True:
        print("keeping only associated mutations")

        # load gwas results of the mutations
        gwas_results_df_muts = get_tab_as_df_or_empty_df("%s/%s_%i/ancestral_GWAS_drugResistance/GWAS_%s_resistance/all_vars-all_genes-all_muts-none/%s-%s-min_support=%i/gwas_jobs/integrated_GWAS_stats.tab"%(DataDir, species, sciName_to_taxID[species], drug, ASR_methods_phenotypes, ASR_methods_phenotypes, min_support))

        # check that the mutations are there
        unavailable_vars = set(asr_df_vars.variantID_across_samples).difference(set(gwas_results_df_muts.group_name))
        if len(unavailable_vars)>0: raise ValueError("there are %i mutations that were not run in GWAS: %s"%(len(unavailable_vars), unavailable_vars))

        # keep only vars associated to resistance at some point
        gwas_results_df_muts = gwas_results_df_muts[gwas_results_df_muts.group_name.isin(set(asr_df_vars.variantID_across_samples))]
        variants_associated_to_resistance = set(gwas_results_df_muts[(gwas_results_df_muts.nodes_GenoAndPheno>=1)].group_name)
        asr_df_vars = asr_df_vars[asr_df_vars.variantID_across_samples.isin(variants_associated_to_resistance)]


    # create a df that has all the variants
    if target_interesting_geneID_to_geneName is not None:

        df_vars_with_shortVarEffect = get_df_vars_with_short_variant_effect(set(asr_df_vars.variantID_across_samples), DataDir, species, tmpdir_all, set(target_interesting_geneID_to_geneName.keys()))
        df_vars_with_shortVarEffect = df_vars_with_shortVarEffect[df_vars_with_shortVarEffect.variantID_across_samples.isin(set(asr_df_vars.variantID_across_samples))]

        # keep target_interesting_geneID_to_geneName that are in df_vars_with_shortVarEffect
        target_interesting_geneID_to_geneName = {g:n for g,n in target_interesting_geneID_to_geneName.items() if g in set(df_vars_with_shortVarEffect.Gene)}

        # check that all vars and genes are considered
        if set(df_vars_with_shortVarEffect.variantID_across_samples)!=set(asr_df_vars.variantID_across_samples): raise ValueError("not all variants are in df_vars_with_shortVarEffect")
        if set(target_interesting_geneID_to_geneName.keys())!=set(df_vars_with_shortVarEffect.Gene): raise ValueError("not all genes are in df_vars_with_shortVarEffect")

    # add the ASR stats of each variant
    dir_all_mutations = "%s/all_vars-all_genes-all_muts-none/%s-%s-min_support=%i/gwas_jobs"%(outdir_drug, ASR_methods_phenotypes, ASR_methods_phenotypes, min_support)

    # load the tree
    dir_all_mutations = "%s/all_vars-all_genes-all_muts-none/%s-%s-min_support=%i/gwas_jobs"%(outdir_drug, ASR_methods_phenotypes, ASR_methods_phenotypes, min_support)
    tree = Tree("%s/correct_tree.nw"%dir_all_mutations)
    tree = get_tree_with_internalNodeNames(tree)
    tree.name = "root"

    # map each node to the phenotype
    asr_df_vars["node_name"] = asr_df_vars.node_name.apply(str)
    node_to_pheno = dict(asr_df_vars[["node_name", "phenotype"]].drop_duplicates().set_index("node_name").phenotype)
    all_nodes_tree = {n.name for n in tree.traverse()}
    if set(node_to_pheno.keys())!=all_nodes_tree: 
        print(set(node_to_pheno.keys()).difference(all_nodes_tree))
        print(all_nodes_tree.difference(set(node_to_pheno.keys())))
        raise ValueError("the gene names are not as in tree")

    # define the variants
    sorted_variants = sorted(set(asr_df_vars.variantID_across_samples))

    # change index
    if len(asr_df_vars)!=len(asr_df_vars[["variantID_across_samples", "node_name"]].drop_duplicates()): raise ValueError("var-node combs are not unique")
    asr_df_vars = asr_df_vars.set_index(["variantID_across_samples", "node_name"])

    # change the metadata_df
    if interesting_attributes is not None or len(mic_fields_plot)>0:
        metadata_df = cp.deepcopy(metadata_df)
        metadata_df["sampleID"] = metadata_df.sampleID.apply(str)
        metadata_df = metadata_df[metadata_df.species_name==species].set_index("sampleID", drop=False)

    # define all the mic values across all species
    if len(mic_fields_plot)>0: 
        all_log2_mic_vals = [np.log2(m) for m in sorted({float(x) for x in get_uniqueVals_df(metadata_df[["%s_MIC"%f for f in mic_fields_plot]])  if not pd.isna(x)})]
        log2_mic_to_color, log2_mic_palette = get_value_to_color(all_log2_mic_vals, n=10, palette='rocket_r', type_color="hex")


    ####################################

    ##### TREE #####
    print("getting tree")

    # define palette of each variant
    if len(sorted_variants)<=20: 
        if len(sorted_variants)<=10: palette = "tab10"
        elif len(sorted_variants)<=20: palette = "tab20"
        var_to_color = get_value_to_color(sorted_variants, n=len(sorted_variants), palette=palette, type_color="hex")[0]

    else:
        #var_to_color = get_value_to_color(sorted_variants, n=len(sorted_variants), palette="rocket_r", type_color="hex")[0]
        var_to_color = get_value_to_color(sorted_variants, n=len(sorted_variants), palette="Spectral", type_color="hex")[0]

    # map the edge color of each variant
    var_to_edge_color = {}
    all_edge_colors = ["black"]*100
    for Iv, var in enumerate(sorted_variants): var_to_edge_color[var] = all_edge_colors[Iv]


    # get a palette of shapes
    var_to_shape = {}
    all_shapes = ["o", "<", '[]', ">", "^", "v", '()', '<>']*100
    for Iv, var in enumerate(sorted_variants): var_to_shape[var] = all_shapes[Iv]

    # initialize the highest column in leafs
    max_col_used_leafs = 0

    # go trough each node
    for n in tree.traverse():

        #################### NODE STYLE BASED ON THE PHENOTYPE ###############

        # define the  phenotype's color
        pheno_to_color = {"1.0":"red", "0.0":"blue", "nan":"gray"}
        branch_color = pheno_to_color[str(node_to_pheno[n.name])]

        # set the lines
        nst = NodeStyle()
        nst["hz_line_width"] = 6
        nst["vt_line_width"] = 6
        nst["size"] = 0
        nst["hz_line_color"] = branch_color
        nst["vt_line_color"] = branch_color
        n.set_style(nst)

        # add a circle with the phenotype (with a label)
        asr_r = gwas_asr_results_df.loc[n.name]
        fontsize_text = 50
        if asr_r.genotype_transition==1 and asr_r.phenotype_transition==1: 
            label_text = "*"
            radius_circle = 25

        elif asr_r.genotype_transition==0 and asr_r.phenotype_transition==1: 
            label_text = ""
            radius_circle = 25

        else: 
            label_text = ""
            radius_circle = 9

        label_text_color = get_annotationColor_on_bgcolor(branch_color, threshold_gray=0.3) # the lower the threshold the more black colors
        n.add_face(CircleFace(radius_circle, branch_color, style='circle', label=dict(text=label_text, font="arial", color=label_text_color, fontsize=fontsize_text)), column=0)


        # if it is a leaf, add the interesting attributes from df_metadata
        if n.is_leaf() and interesting_attributes is not None: 

            def get_str_or_float_as_str(val):
                if type(val)==float or type(val)==np.float64: return "%.2f"%val
                else: return str(val)

            text = " " + "; ".join(["%s=%s"%(a, get_str_or_float_as_str(metadata_df.loc[n.name, a])) for a in interesting_attributes]) + " "
            n.add_face(TextFace(text, fgcolor=branch_color, bold=True, fsize=14 ), column=1)

            # add also attributes
            n.add_feature("attributes", text)

        ######################################################################

        ######## ADD DATA FOR EACH MUTATION ###########

        # map each var to the geno
        var_to_geno = {var : str(asr_df_vars.loc[(var, n.name)].genotype_presence) for var in sorted_variants}

        # define the sorted vars that are there or may be.
        sorted_variants_node = [var for var in sorted_variants if var_to_geno[var]!="0.0"]

        # go through each mut
        for Iv, var in enumerate(sorted_variants_node):

            # define the geno
            geno = var_to_geno[var]

            # dicts of the color
            var_color = var_to_color[var]
            geno_to_label = {"1.0":"", "nan":"?"}

            # define the color of the text
            label_color = get_annotationColor_on_bgcolor(var_color, threshold_gray=0.6)

            # define the face for this variant (see http://etetoolkit.org/docs/latest/reference/reference_treeview.html?highlight=rectface#ete3.SeqMotifFace)
            width_rect = 35
            shape = var_to_shape[var]
            width = width_rect
            height = width_rect
            label = "arial|18|%s|%s"%(label_color, geno_to_label[geno]) # (a text label in the format ‘FontType|FontSize|FontColor|Text’, for instance, arial|8|white|MotifName”“)
            motif = [1, width_rect, shape, width, height, var_to_edge_color[var], var_color, label] # [seq.start, seq.end, shape, width, height, fgcolor, bgcolor, text_label ]
            seq_motif = SeqMotifFace(seq=None, motifs=[motif])
            col_face = Iv+2

            # add motif differently for each type of node
            if n.is_leaf(): 
                n.add_face(seq_motif, column=col_face+1, position="aligned")
                max_col_used_leafs = max([max_col_used_leafs, col_face+1]) # update

            else: n.add_face(seq_motif, column=col_face)


        ###############################################

    #### ADD MIC DATA #####
    mic_f_to_col = {}
    if len(mic_fields_plot)>0:
        for l in tree.get_leaves():

            # add white box at the start
            rect_w = 35
            l.add_face(RectFace(rect_w*0.5, rect_w, bgcolor='white', fgcolor='white'), column=max_col_used_leafs+1, position="aligned")

            # add the MICs as boxes
            for Im, mic_f in enumerate(mic_fields_plot):

                # get color
                mic_val = metadata_df.loc[l.name, "%s_MIC"%mic_f]
                if pd.isna(mic_val): color_mic = "white"
                else: color_mic = log2_mic_to_color[np.log2(mic_val)]

                # define label
                """
                if pd.isna(mic_val):
                    label = None

                else:
                    label_color = get_annotationColor_on_bgcolor(color_mic, threshold_gray=0.6)

                    def format_mic_label(x):
                        if x<1: return "."+("%.2f"%x).split(".")[1]
                        else: return "%.1f"%x

                    label={"text":format_mic_label(mic_val), "color":label_color, "fontsize":15, "bold":True}
                """

                label= None

                # add box
                curr_col = max_col_used_leafs+Im+2
                mic_f_to_col[mic_f] = curr_col 
                l.add_face(RectFace(rect_w, rect_w, bgcolor=color_mic, fgcolor='black', label=label), column=curr_col, position="aligned")

    #######################

    ts = TreeStyle()

    # FOOTER
    if len(mic_fields_plot)>0:
        for mic_f in mic_fields_plot:
            text_face = TextFace(mic_f, bold=True, fsize=16)
            text_face.rotation = 90
            ts.aligned_foot.add_face(text_face, column= mic_f_to_col[mic_f])


    # LEGEND
    # empty spot
    for c in [0, 1]: ts.legend.add_face(TextFace(" \n "), column=c)

    # add phenotypes
    ts.legend.add_face(TextFace("phenotype", bold=True, fsize=14), column=0)
    ts.legend.add_face(TextFace("  "), column=1)

    for color, label in [("red", "resistant"), ("blue", "susceptible"), ("gray", "unknown")]:

        radius_circle = 20
        ts.legend.add_face(CircleFace(radius_circle, color, style='circle'), column=0)
        ts.legend.add_face(TextFace("C. %s %s %s"%(species.split("_")[1], drug, label),  fgcolor="black", bold=True, fsize=14), column=1)

    # empty
    for c in [0, 1]: ts.legend.add_face(TextFace(" \n  "), column=c)

    # add the mics
    if len(mic_fields_plot)>0:

        # title
        ts.legend.add_face(TextFace("MIC", bold=True, fsize=14), column=0)
        ts.legend.add_face(TextFace("  "), column=1)

        for I, (log2_mic, mic_color) in enumerate(log2_mic_palette.items()):

            # add face
            #label_color = get_annotationColor_on_bgcolor(mic_color, threshold_gray=0.6)


            ts.legend.add_face(RectFace(rect_w, rect_w*0.9, bgcolor=mic_color, fgcolor=mic_color), column=0)

            # add text
            def format_mic_label(x):
                if x<1: return "."+("%.2f"%x).split(".")[1]
                else: return "%.1f"%x

            ts.legend.add_face(TextFace(format_mic_label(2**log2_mic), bold=True, fsize=13), column=1)

        # empty spot
        for c in [0, 1]: ts.legend.add_face(TextFace(" \n  "), column=c)

    # add mutations header
    header_string = "variants"
    if only_mutations_associated_to_resistance: header_string += " (only associated to resistance)"
    ts.legend.add_face(TextFace(header_string, bold=True, fsize=14), column=0)
    ts.legend.add_face(TextFace("  "), column=1)

    # add mutations
    for Iv, var in enumerate(sorted_variants):

        # get string for this var
        if target_interesting_geneID_to_geneName is not None:

            # get the var
            df_var = df_vars_with_shortVarEffect[df_vars_with_shortVarEffect.variantID_across_samples==var].set_index("Gene")

            if len(target_interesting_geneID_to_geneName)>1: string_var = "; ".join(list(map(lambda g: "%s (%s)"%(df_var.loc[g].short_variant_effect, target_interesting_geneID_to_geneName[g]), sorted(set(target_interesting_geneID_to_geneName).intersection(set(df_var.index))))))

            else: string_var = "; ".join(list(map(lambda g: "%s"%(df_var.loc[g].short_variant_effect), sorted(set(target_interesting_geneID_to_geneName).intersection(set(df_var.index))))))

        else: string_var = var

        # define the seq motif for this variant
        var_color = var_to_color[var]
        width_rect = 40
        shape = var_to_shape[var]
        width = width_rect
        height = width_rect
        label = "arial|12|white|"
        motif = [1, width_rect, shape, width, height, var_to_edge_color[var], var_color, label] # [seq.start, seq.end, shape, width, height, fgcolor, bgcolor, text_label ]

        ts.legend.add_face(SeqMotifFace(seq=None, motifs=[motif]), column=0)

        # add to legend
        ts.legend.add_face(TextFace(string_var,  fgcolor="black", bold=True, fsize=14), column=1)

    for c in [0, 1]: ts.legend.add_face(TextFace(" \n  "), column=c)


    # add the title with information about the GWAS
    interesting_pval_fields = ["pval_chi_square_phenotypes", "pval_GenoAndPheno_phenotypes", "pval_chi_square_maxT", "pval_epsilon_maxT"]

    if target_interesting_geneID_to_geneName is not None: genes_str = " Genes: %s."%(",".join(sorted(target_interesting_geneID_to_geneName.values())))
    else: genes_str = " "

    title_string = "%s-%s.%sGroup %s. %s-%s-%s\nASR=%s, min_support=%s, epsilon=%s (explains %i/%i phenotype transitions)\n%s"%(species, drug, genes_str, target_type_vars, target_type_mutations, target_type_collapsing, target_group_name, ASR_methods_phenotypes, min_support, gwas_results_df.iloc[0].epsilon, gwas_results_df.iloc[0].nodes_GenoAndPheno, gwas_results_df.iloc[0].nodes_GenoAndPheno+gwas_results_df.iloc[0].nodes_noGenoAndPheno, ", ".join(["%s=%s"%(f, gwas_results_df.iloc[0][f]) for f in interesting_pval_fields]))

    ts.title.add_face(TextFace(title_string, bold=True, fsize=12), column=0)


    # get the treestyle
    ts.show_branch_length = False
    ts.show_branch_support = False
    ts.show_leaf_name = False
    ts.draw_guiding_lines = True
    ts.guiding_lines_type = 2 # 0=solid, 1=dashed, 2=dotted.
    ts.guiding_lines_color = "gray" 
    ts.legend_position = 2 # bottom-left (3)

    if rotation_mode=="c":

        
        ts.mode = "c"
        ts.root_opening_factor = 1
        ts.arc_start = -180 # 0 degrees = 3 o'clock
        ts.arc_span = 180
        ts.root_opening_factor = 1

    """
    if interesting_attributes is None:

        ts.mode = "c"
        ts.root_opening_factor = 1
        ts.arc_start = -180 # 0 degrees = 3 o'clock
        ts.arc_span = 180   
    """

    print("plotting tree")
    #tree.show(tree_style=ts); 
    #print("exiting"); sys.exit(0)

    # write
    print("rendering %s"%tree_plot)
    #tree.render(file_name=tree_plot+".png", tree_style=ts, units="px", dpi=30)#,  units='mm', h=len(tree.get_leaf_names())*5)
    tree.render(file_name=tree_plot, tree_style=ts)



    ###################


def get_figure_GWAS_main_distribution_resistance_across_clades(species_to_tree, metadata_df, spp_drug_to_gwas_df_file, PlotsDir):

    """Generates a tree that has the clades (collapsed) and the distribution of clades"""

    metadata_df = cp.deepcopy(metadata_df)
    
    for species in ["Candida_auris", "Candida_albicans", "Candida_glabrata"]:

        ###### GET TREE #####

        # define the sorted drugs for this species
        sorted_drugs = [d for d in ['FLC', 'ITR', 'POS', 'VRC', 'ANI', 'MIF', 'AMB'] if (species, d) in spp_drug_to_gwas_df_file]

        # remove bad samples
        tree = cp.deepcopy(species_to_tree[species])
        correct_samples = set(tree.get_leaf_names()).difference({str(x) for x in sciName_to_badSamples[species]})
        tree.prune(correct_samples)

        # define metadata df for this species
        metadata_df_s = metadata_df[metadata_df.species_name==species]
        metadata_df_s["sampleID"] = metadata_df_s.sampleID.apply(int)
        metadata_df_s = metadata_df_s.set_index("sampleID", drop=False)

        # define the sample_to_clade
        def get_correct_cladeID_Tree_and_BranchLen(r):
            if pd.isna(r.cladeID_Tree_and_BranchLen): return "unassignedSample_%i"%(r.sampleID)
            else: return int(r.cladeID_Tree_and_BranchLen)

        metadata_df_s["correct_cladeID_Tree_and_BranchLen"] = metadata_df_s.apply(get_correct_cladeID_Tree_and_BranchLen, axis=1)
        sample_to_clade = metadata_df_s[metadata_df_s.sampleID.apply(str).isin(correct_samples)].set_index("sampleID", drop=False).correct_cladeID_Tree_and_BranchLen

        # set clade to color
        sorted_numeric_clades = sorted(set(sample_to_clade[sample_to_clade.apply(type)==int]))
        if len(sorted_numeric_clades)<10: palette="tab10"
        else: palette="tab20"
        clade_to_color = get_value_to_color(sorted_numeric_clades, palette=palette, n=len(sorted_numeric_clades), type_color="hex")[0]

        # add unassigned clades colors
        for c in set(sample_to_clade):
            if str(c).startswith("unassignedSample_"):  clade_to_color[c] = "white"

        # convert to str
        clade_to_color  = {str(cID):c for cID,c in clade_to_color.items()}
        sample_to_clade = {str(s):str(c) for s,c in sample_to_clade.items()}

        # map each clade to the samples
        clade_to_samples = {}
        for s, c in sample_to_clade.items():
            if c not in clade_to_samples: clade_to_samples[c] = list()
            clade_to_samples[c].append(s)

        # add the cladeID
        tree = get_tree_withcladeID_as_str(tree, sample_to_clade)

        # init the clade_to_samples_sorted
        clade_to_samples_sorted = {}

        # keep only ancestral nodes of each clade
        for n in tree.traverse():
            if n.is_root(): continue
            if pd.isna((n.get_ancestors()[0]).cladeID) and not pd.isna(n.cladeID): 

                # define the sorten samples
                clade_to_samples_sorted[n.cladeID] = [int_n.name for int_n in n.traverse() if int_n.is_leaf()]

                # remove the children
                for child in n.get_children(): child.detach()
                n.name = n.cladeID

        # add children to the each clade in a collapsed manner (only leafs with some resistabce)
        for l in tree.get_leaves():    
            if not l.cladeID.startswith("unassignedSample_"):
                

                # only keep
                for s in clade_to_samples_sorted[l.name]: l.add_child(name=s, dist=0)


                # add child
                for child in l.get_children(): child.cladeID = l.cladeID


        # map each col to drug
        col_to_drug = {}

        # iterate through tree
        for n in tree.traverse("preorder"):

            # init style
            nst = NodeStyle()

            # define the bwidth    
            bwidth = 30
            nst["hz_line_width"] = bwidth
            nst["vt_line_width"] = bwidth

            # define the size of the node
            nst["size"] = 0

            # add the colors
            if pd.isna(n.cladeID) or n.cladeID.startswith("unassignedSample_"): clade_color = "black"
            else: clade_color = clade_to_color[n.cladeID]

            nst["fgcolor"] = "black"
            nst["hz_line_color"] = clade_color
            nst["vt_line_color"] = clade_color
            #nst["bgcolor"] = bgcolor

            # add style
            n.set_style(nst)


            # for leafs
            if n.is_leaf(): 

                rect_h = 15

                # add white space
                n.add_face(RectFace(20, rect_h, fgcolor="white", bgcolor="white"), column=0 , position="aligned")

                # add clade color
                n.add_face(RectFace(120, rect_h, fgcolor=clade_to_color[n.cladeID], bgcolor=clade_to_color[n.cladeID]), column=1 , position="aligned")

                # init col
                col = 2

                # add sep
                n.add_face(RectFace(20, rect_h, fgcolor="white", bgcolor="white"), column=col , position="aligned"); col+=1

                # for each drug print a square of the susceptibility
                for Id, drug in enumerate(sorted_drugs):

                    for color in ["white", "gray", "white"]:


                        n.add_face(RectFace(20, rect_h, fgcolor=color, bgcolor=color), column=col , position="aligned"); col+=1


                    # define sample name
                    if n.name.startswith("unassignedSample_"): sname = int(n.name.split("_")[-1])
                    else: sname = int(n.name)


                    # define color
                    r_val = metadata_df_s.loc[sname, "%s_resistance"%drug]
                    if pd.isna(r_val) or r_val=="I": color_resistance = "white"
                    else: color_resistance = {"R":"red", "S":"blue"}[r_val]

                    # add face
                    col_to_drug[col] = drug
                    n.add_face(RectFace(200, rect_h, fgcolor=color_resistance, bgcolor=color_resistance), column=col , position="aligned"); col+=1


        # init treestyle
        ts = TreeStyle()
        ts.show_branch_length = False
        ts.show_branch_support = False
        ts.show_leaf_name = False
        ts.mode = "r"

        # add header
        for col, drug in col_to_drug.items():

            drug_to_text = {'AMB': 'amphotericin B', 'ANI': 'anidulafungin', 'FLC': 'fluconazole', 'ITR': 'itraconazole', 'MIF': 'micafungin', 'POS': 'posaconazole', 'VRC': 'voriconazole'}
            text = "%s (%s)"%(drug_to_text[drug], drug)

            text_f = TextFace(text, fsize=150, fgcolor='black', bold=True)
            text_f.rotation = 270
            ts.aligned_foot.add_face(text_f, column=col)


        # show
        #tree.show(tree_style=ts)

        # save the tree
        plots_dir = "%s/Figure_GWAS_main_dataset_overview"%PlotsDir; make_folder(plots_dir)
        filename = "%s/%s.pdf"%(plots_dir, species)
        #tree.render(file_name=filename, tree_style=ts, dpi=500,  units='mm', h=len(sample_to_clade)*2) #w=20
        tree.render(file_name=filename, tree_style=ts) #w=20

        #####################





def plot_GWAS_AFresistance_ASR_phenotypes_tree_withGeneInfo_only_phenotypes(DataDir, PlotsDir, species_to_gff, gene_features_df, metadata_df, filters_df):


    """Plots the ASR of the phenotypes together with mutations of some expected genes"""

    # map each drug to some genes of interest

    # define plots dir
    plots_dir = "%s/figure_GWASmain_treesPhenotypes"%(PlotsDir); 
    make_folder(plots_dir)
    tmpdir = "%s/tmp_crating"%plots_dir; 

    #delete_folder(tmpdir)
    make_folder(tmpdir)


    # go through different species
    for taxID, species in taxID_to_sciName.items():
        if species not in {"Candida_albicans", "Candida_auris", "Candida_glabrata"}: continue

        # define the outdir of the gwas
        outdir_gwas = "%s/%s_%i/ancestral_GWAS_drugResistance"%(DataDir, species, taxID)

        # go through each drug
        for drug in [f.split("_")[1] for f in os.listdir(outdir_gwas) if f.startswith("GWAS_") and f.endswith("_resistance")]:
            print(species, drug)    

            #if (species=="Candida_albicans" and drug=="FLC"): continue

            #if not (species=="Candida_auris" and drug=="ANI"): continue
            if not (species=="Candida_glabrata" and drug=="VRC"): continue
            #if not (species=="Candida_auris" and drug=="FLC"): continue
            #if not (species=="Candida_albicans" and drug=="FLC"): continue

            # define the outdir of the drug
            outdir_drug = "%s/GWAS_%s_resistance"%(outdir_gwas, drug)


            ################ PLOT TREE WITH PHENOTYPES ##############

            # make tmpdir
            tmpdir_all = "%s/%s_%s_gwas_getting_files_for_phenotypes"%(tmpdir, species, drug)
            make_folder(tmpdir_all)

            # load the tree
            ASR_methods_phenotypes = filters_df.loc["%s-%s"%(species, drug)].ASR_methods_phenotypes
            min_support = filters_df.loc["%s-%s"%(species, drug)].min_support
            print(ASR_methods_phenotypes, min_support)
            dir_all_mutations = "%s/all_vars-all_genes-all_muts-none/%s-%s-min_support=%s/gwas_jobs"%(outdir_drug, ASR_methods_phenotypes, ASR_methods_phenotypes, min_support)

            tree = Tree("%s/correct_tree.nw"%dir_all_mutations)
            tree = get_tree_with_internalNodeNames(tree)
            tree.name = "root"
            all_nodes_tree = {n.name for n in tree.traverse()}

            # load the df with variants
            asr_df_vars_file = "%s/asr_df_vars_first_var.py"%tmpdir_all
            if file_is_empty(asr_df_vars_file):
                print("loading vars")

                # get a df with phenotype and phenotype transition
                asr_all_muts = "%s/integrated_ASR.tab"%(dir_all_mutations)
                first_mut = str(subprocess.check_output("head -2 %s | tail -1"%asr_all_muts, shell=True)).lstrip("b'").rstrip("'").split("\\t")[5].rstrip("\\n")
                asr_df_vars = get_tab_as_df_or_empty_df(get_tab_file_only_lines_matching_patterns([first_mut], asr_all_muts, "%s/first_mut_integrated_ASR.tab"%tmpdir_all))
                asr_df_vars["node_name"] = asr_df_vars.node_name.apply(str)
                asr_df_vars = asr_df_vars[["node_name", "phenotype", "phenotype_transition"]].drop_duplicates()

                # checks
                if set(asr_df_vars.node_name)!=all_nodes_tree: 
                    print(set(asr_df_vars.node_name).difference(all_nodes_tree))
                    print(all_nodes_tree.difference(set(asr_df_vars.node_name)))
                    raise ValueError("the gene names are not as in tree")

                if len(asr_df_vars)!=len(set(asr_df_vars.node_name)): raise ValueError("the node names are not unique")

                save_object(asr_df_vars, asr_df_vars_file)


            print("load object")
            asr_df_vars = load_object(asr_df_vars_file)

            # count the number of nodes that are a phenotype_transition
            n_pheno_transitions = len(set(asr_df_vars[asr_df_vars.phenotype_transition==1.0].node_name))

            # map each node to the phenotype
            node_to_pheno = dict(asr_df_vars[["node_name", "phenotype"]].drop_duplicates().set_index("node_name").phenotype)

            print("getting tree")

            # go trough each node
            n_transitions = 0
            for n in tree.traverse():

                # define the  phenotype's color
                pheno_to_color = {"1.0":"red", "0.0":"blue", "nan":"gray"}

                if n.support>=min_support or n.is_leaf(): 
                    n_transitions+=1
                    branch_color = pheno_to_color[str(node_to_pheno[n.name])]
                else: branch_color = 'gray'

                # set the lines
                nst = NodeStyle()
                nst["hz_line_width"] = 6
                nst["vt_line_width"] = 6
                nst["size"] = 0
                nst["hz_line_color"] = branch_color
                nst["vt_line_color"] = branch_color
                n.set_style(nst)

                # add the face
                radius_circle = 6
                n.add_face(CircleFace(radius_circle, branch_color, style='circle'), column=0)

            # legend
            ts = TreeStyle()
            print(n_transitions, "transitions")

            # add phenotypes
            ts.legend.add_face(TextFace("phenotype", bold=True, fsize=11), column=0)
            ts.legend.add_face(TextFace("  "), column=1)

            for color, label in [("red", "resistant"), ("blue", "susceptible"), ("gray", "unknown / support<%i"%min_support)]:

                ts.legend.add_face(CircleFace(radius_circle, color, style='circle'), column=0)
                ts.legend.add_face(TextFace("C. %s %s %s"%(species.split("_")[1], drug, label),  fgcolor="black", bold=True, fsize=11), column=1)

            
            # define the nodes of the 
            ts.title.add_face(TextFace("C. %s %s\n%i resistance transitions (by %s)\n"%(species.split("_")[1], drug, n_pheno_transitions, ASR_methods_phenotypes), bold=True, fsize=12), column=0)

            # get the treestyle
            ts.show_branch_length = False
            ts.show_branch_support = False
            ts.show_leaf_name = False
            ts.draw_guiding_lines = False
            #ts.guiding_lines_type = 2 # 0=solid, 1=dashed, 2=dotted.
            #ts.guiding_lines_color = "gray" 
            ts.legend_position = 2 # bottom-left (3)
            
            ts.mode = "c"
            ts.root_opening_factor = 1
            ts.arc_start = -180 # 0 degrees = 3 o'clock
            ts.arc_span = 360   
            
            # write
            tree_plot = "%s/%s_%s.pdf"%(plots_dir, species, drug)
            print("rendering %s"%tree_plot)
            #tree.show(tree_style=ts); sys.exit(0)
            tree.render(file_name=tree_plot, tree_style=ts)

            #########################################################





def plot_GWAS_AFresistance_ASR_phenotypes_tree_withGeneInfo(DataDir, PlotsDir, species_to_gff, gene_features_df, metadata_df, ASR_methods_phenotypes="MPPA,DOWNPASS", min_support=0):


    """Plots the ASR of the phenotypes together with mutations of some expected genes"""

    # map each drug to some genes of interest

    # define plots dir
    plots_dir = "%s/trees_ASR_gwas_%s_%i"%(PlotsDir, ASR_methods_phenotypes, min_support); 
    make_folder(plots_dir)
    tmpdir = "%s/tmp_crating"%plots_dir; 
    make_folder(tmpdir)

    # go through different species
    for taxID, species in taxID_to_sciName.items():
        if species not in {"Candida_albicans", "Candida_auris", "Candida_glabrata"}: continue

        # define the outdir of the gwas
        outdir_gwas = "%s/%s_%i/ancestral_GWAS_drugResistance"%(DataDir, species, taxID)

        # go through each drug
        for drug in [f.split("_")[1] for f in os.listdir(outdir_gwas) if f.startswith("GWAS_") and f.endswith("_resistance")]:
            #print(species, drug)    

            # skip ungeneral drugs
            if drug not in {"FLC", "CAS", "ANI", "MIF"}: continue

            #if not (species=="Candida_auris" and drug=="MIF"): continue
            #if not (species=="Candida_auris" and drug=="ANI"): continue
            #if not (species=="Candida_glabrata" and drug=="FLC"): continue
            #if not (species=="Candida_auris" and drug=="FLC"): continue
            #if not (species=="Candida_albicans" and drug=="FLC"): continue
            

            #if not (species=="Candida_glabrata" and drug=="MIF"): continue
            if not (species=="Candida_auris" and drug=="FLC"): continue

            # define the outdir of the drug
            outdir_drug = "%s/GWAS_%s_resistance"%(outdir_gwas, drug)

            # define the genes of interest
            species_to_drug_to_interestingGenes = {"Candida_albicans": {"azoles":["ERG11"]},
                                                   "Candida_auris": {"azoles":["TAC1b", "ERG11"], "echinocandins":["GSC2"]}, # GSC2 is the auris ortholog of FKS1
                                                   "Candida_glabrata": {"azoles":["PDR1", "ERG11", "ERG3"], "echinocandins":["FKS1", "FKS2"]} } #  "ERG11", "CDR1"

            drug_to_type = {"FLC":"azoles", "POS":"azoles", "VRC":"azoles", "ITR":"azoles", "MIF":"echinocandins", "ANI":"echinocandins", "CAS":"echinocandins"}



            interesting_genes = species_to_drug_to_interestingGenes[species][drug_to_type[drug]]
            gene_features_df_s = gene_features_df[gene_features_df.species==species]
            geneID_to_geneName = {get_geneID_for_gName_from_gene_features_df(gene_features_df_s, gName) : gName for gName in interesting_genes}


            # for each gene, get a tree with the interesting mutations
            for geneID, geneName in geneID_to_geneName.items():
                print(species, drug, geneName)    

                #if not (species=="Candida_auris" and drug=="CAS" and geneName=="GSC2"): continue
                #if not (species=="Candida_auris" and drug=="ANI" and geneName=="GSC2"): continue
                #if not (species=="Candida_auris" and drug=="MIF" and geneName=="GSC2"): continue

                #if not (species=="Candida_auris" and drug=="FLC" and geneName=="TAC1b"): continue
                #if not (species=="Candida_auris" and drug=="FLC" and geneName=="ERG11"): continue
                #if not (species=="Candida_glabrata" and drug=="CAS" and geneName=="FKS1"): continue
                #if not (species=="Candida_glabrata" and drug=="CAS" and geneName=="FKS2"): continue

                #if not (species=="Candida_glabrata" and drug=="MIF" and geneName=="FKS1"): continue

                #if not (species=="Candida_glabrata" and drug=="MIF" and geneName=="FKS2"): continue
                #if not (species=="Candida_glabrata" and drug=="FLC" and geneName=="PDR1"): continue
                #if not (species=="Candida_glabrata" and drug=="FLC" and geneName=="ERG11"): continue
                #if not (species=="Candida_glabrata" and drug=="FLC" and geneName=="ERG3"): continue

                #if not (species=="Candida_albicans" and drug=="FLC" and geneName=="ERG11"): continue

                # define the interesting attributes of each drug
                interesting_attributes = {("Candida_auris", "CAS", "GSC2") : ["CAS_MIC", "MIF_MIC", "ANI_MIC", "BioProject", "strain"],
                                          ("Candida_auris", "ANI", "GSC2") : ["CAS_MIC", "MIF_MIC", "ANI_MIC", "BioProject", "strain"],
                                          ("Candida_auris", "MIF", "GSC2") : None, # to do
                                          ("Candida_auris", "FLC", "TAC1b") : None,
                                          ("Candida_auris", "FLC", "ERG11") : None,

                                          ("Candida_glabrata", "CAS", "FKS1") : ["BioProject", "strain", "median_median_reads_per_gene", "fraction_genes_covered_above95pct", "sampleID", "Run"],
                                          ("Candida_glabrata", "CAS", "FKS2") : ["CAS_MIC", "MIF_MIC", "ANI_MIC", "BioProject", "strain"],
                                          ("Candida_glabrata", "MIF", "FKS1") : ["CAS_MIC", "MIF_MIC", "ANI_MIC", "BioProject", "strain"],
                                          ("Candida_glabrata", "MIF", "FKS2") : ["CAS_MIC", "MIF_MIC", "ANI_MIC", "BioProject", "strain"],
                                          #("Candida_glabrata", "MIF", "FKS1") : None,
                                          #("Candida_glabrata", "MIF", "FKS2") : None, 
                                          ("Candida_glabrata", "FLC", "PDR1") : None,
                                          ("Candida_glabrata", "FLC", "ERG11") : None,
                                          ("Candida_glabrata", "FLC", "ERG3") : None,

                                          ("Candida_albicans", "FLC", "ERG11") : None
                                          }[(species, drug, geneName)]

                # define only_mutations_associated_to_resistance
                #combinations_muts_assoc_to_R = {("Candida_auris", "FLC", "TAC1b")}
                combinations_muts_assoc_to_R = {("Candida_auris", "FLC", "TAC1b"), ("Candida_auris", "FLC", "ERG11"), ("Candida_glabrata", "FLC", "PDR1"), ("Candida_albicans", "FLC", "ERG11")}

                if (species, drug, geneName) in combinations_muts_assoc_to_R: only_mutations_associated_to_resistance = True
                else: only_mutations_associated_to_resistance = False

                # define data for the genID
                target_type_vars = "all_vars"
                target_type_genes = "all_genes"
                target_type_mutations = "non_syn_muts" # for type_mutations in ["all_muts", "syn_muts", "non_syn_muts", "non_syn_non_truncating_muts", "truncating_muts"]: # skipping all_muts

                target_type_collapsing = "genes"
                target_group_name = geneID
                target_interesting_geneID_to_geneName = {geneID : geneName}

                plot_tree_all_mutations_one_group_gwas_AF(outdir_drug, ASR_methods_phenotypes, target_type_vars, target_type_genes, target_type_mutations, target_type_collapsing, target_group_name, "%s/%s_%s_%s_%s_only_mutations_associated_to_resistance%s.pdf"%(plots_dir, species, drug, geneName, ASR_methods_phenotypes, only_mutations_associated_to_resistance), DataDir, species, drug, "%s/%s_%s_%s_%s_generateData"%(tmpdir, species, drug, geneName, ASR_methods_phenotypes), target_interesting_geneID_to_geneName, metadata_df, min_support, interesting_attributes=interesting_attributes, only_mutations_associated_to_resistance=only_mutations_associated_to_resistance)


    

def plot_diversity_how_does_null_model_work(df_diversity_all, xfield, yfield, PlotsDir, plots={"density_scatter", "histogram"}):

    """Plots the diversity df xfield vs yfield. This is useful to assess how the null model works"""

    # keep df that is useful
    df_diversity_all = df_diversity_all[(df_diversity_all.type_var=="SNP") & (df_diversity_all.threshold_piNpiS==1) & ~(pd.isna(df_diversity_all.mean_p_real_piS_not_extreme))][["species", "Gene", 'type_var', 'type_vars_SimpleRepeats', 'type_vars_appearance', "selection", xfield, yfield]]

    if "density_scatter" in plots:

        # go through type_vars_SimpleRepeats
        for type_vars_SimpleRepeats in sorted(set(df_diversity_all.type_vars_SimpleRepeats)):
            if type_vars_SimpleRepeats=="all_vars": continue

            for type_vars_appearance in sorted(set(df_diversity_all.type_vars_appearance)):
                print(type_vars_SimpleRepeats, type_vars_appearance)

                #if type_vars_appearance!="only_vars_recent": continue

                # get df
                df_diversity = df_diversity_all[(df_diversity_all.type_vars_SimpleRepeats==type_vars_SimpleRepeats) & (df_diversity_all.type_vars_appearance==type_vars_appearance)]

                # init figure for each combination of species and type_vars_appearance
                all_species = sorted_species_byPhylogeny
                all_selections = ["positive", "negative"]
                nrows = len(all_species)
                ncols = len(all_selections)
                fig = plt.figure(figsize=(ncols*2, nrows*2)); I=1

                # add subplots
                for Ir, species in enumerate(all_species):
                    for Ic, selection in enumerate(all_selections):

                        # get df and check that Gene is unique
                        df_plot = df_diversity[(df_diversity.species==species) & (df_diversity.selection==selection)]
                        if len(df_plot)!=len(set(df_plot.Gene)): raise ValueError("Gene should be unique ")
                        check_no_nans_in_df(df_plot[[xfield, yfield]])

                        # plot the density scatter
                        ax = plt.subplot(nrows, ncols, I); I+=1
                        try: density_scatter(df_plot[xfield].values , df_plot[yfield].values, ax, sort=True, bins=30, s=2)
                        except: sns.scatterplot(data=df_plot, x=xfield, y=yfield, alpha=.1, color=species_to_color[species], s=10)

                        # set lims
                        lims = [-0.05, 1.05]
                        ax.set_xlim(lims)
                        ax.set_ylim(lims)

                        # x axis
                        if species==(all_species[-1]): 
                            if Ic==0: ax.set_xlabel(xfield)
                            else: ax.set_xlabel("")
                        else:
                            ax.set_xticklabels([])
                            ax.set_xticks([])
                            ax.set_xlabel("")

                        # y axis
                        if Ic==0: 
                            if Ir==2: ax.set_ylabel("%s\nC. %s"%(yfield, species.split("_")[1]))                       
                            else: ax.set_ylabel("C. %s"%(species.split("_")[1]))

                        else:
                            ax.set_yticklabels([])
                            ax.set_yticks([])
                            ax.set_ylabel("")

                        # title
                        if Ir==0 and Ic==0: ax.set_title("type_vars_SimpleRepeats=%s; type_vars_appearance=%s\n%s selection"%(type_vars_SimpleRepeats, type_vars_appearance, selection))
                        elif Ir==0: ax.set_title("%s selection"%selection)



                plt.subplots_adjust(wspace=0.03,  hspace=0.03)
                plt.show()

                plots_dir = "%s/diversity_analysis_testing_how_does_null_model_work_density_scatters"%PlotsDir; make_folder(plots_dir)
                filename = "%s/%s_%s_%s-vs-%s.pdf"%(plots_dir, type_vars_SimpleRepeats, type_vars_appearance, xfield, yfield)
                print("saving %s"%filename)
                fig.savefig(filename, bbox_inches="tight")



    if "histogram" in plots:

        # go through type_vars_SimpleRepeats
        for type_vars_SimpleRepeats in sorted(set(df_diversity_all.type_vars_SimpleRepeats)):
            for type_vars_appearance in sorted(set(df_diversity_all.type_vars_appearance)):
                print(type_vars_SimpleRepeats, type_vars_appearance)

                # only 
                if type_vars_SimpleRepeats!="only_vars_noSimpleRepeats": continue
                #if type_vars_appearance!="only_vars_recent": continue

                # get df
                df_diversity = df_diversity_all[(df_diversity_all.type_vars_SimpleRepeats==type_vars_SimpleRepeats) & (df_diversity_all.type_vars_appearance==type_vars_appearance)]

                # init figure for each combination of species and type_vars_appearance
                all_species = sorted_species_byPhylogeny
                all_selections = ["positive", "negative"]
                nrows = len(all_species)
                ncols = len(all_selections)
                fig = plt.figure(figsize=(ncols*2, nrows*2)); I=1

                # add subplots
                for Ir, species in enumerate(all_species):
                    for Ic, selection in enumerate(all_selections):

                        # get df and check that Gene is unique
                        df_plot = df_diversity[(df_diversity.species==species) & (df_diversity.selection==selection)]
                        if len(df_plot)!=len(set(df_plot.Gene)): raise ValueError("Gene should be unique ")

                        # plot the hist
                        ax = plt.subplot(nrows, ncols, I); I+=1
                        sns.distplot(df_plot[xfield].values, kde=False, rug=False, hist=True, hist_kws=dict(alpha=1, color=species_to_color[species]))

                        # set lims
                        lims = [-0.05, 1.05]
                        ax.set_xlim(lims)

                        # x axis
                        if species==(all_species[-1]): 
                            if Ic==0: ax.set_xlabel(xfield)
                        else:
                            ax.set_xticklabels([])
                            ax.set_xticks([])
                            ax.set_xlabel("")

                        # y axis
                        if Ic==0: ax.set_ylabel("# genes\nC. %s"%(species.split("_")[1]))                       
                        else: ax.set_ylabel("")

                        # title
                        if Ir==0 and Ic==0: ax.set_title("type_vars_SimpleRepeats=%s; type_vars_appearance=%s\n%s selection"%(type_vars_SimpleRepeats, type_vars_appearance, selection))
                        elif Ir==0: ax.set_title("%s selection"%selection)



                plt.subplots_adjust(wspace=0.35,  hspace=0.04)
                plt.show()


                plots_dir = "%s/diversity_analysis_testing_how_does_null_model_work_histogram"%PlotsDir; make_folder(plots_dir)
                filename = "%s/%s_%s_%s.pdf"%(plots_dir, type_vars_SimpleRepeats, type_vars_appearance, xfield)
                print("saving %s"%filename)
                fig.savefig(filename, bbox_inches="tight")

def get_series_aggregated_str(df, delimiter):

    """gets a df and aggregates all the strings"""

    # get cols
    first_col = df.columns[0] 
    other_cols = df.columns[1:]

    # get the series
    agg_series = df[first_col]

    # add others
    for col in other_cols: agg_series += delimiter + df[col].apply(str)

    return agg_series



def adjust_cm_positions(cm, df_square, hm_height_multiplier=0.0002, hm_width_multiplier=0.01, cc_height_multiplier=0.017, rc_width_multiplier=0.017, idx_delimiter="-", distance_btw_boxes=0.0025, cd_height=0.07, rd_width=0.07, cbar_width=0.08, cbar_height=0.015):

    """ Adjusts cm positions """


    # define the hm width and height
    hm_height = len(df_square)*hm_height_multiplier
    hm_width = len(df_square.columns)*hm_width_multiplier

    # set heatmap position
    hm_pos = cm.ax_heatmap.get_position()
    hm_y0 = (hm_pos.y0+hm_pos.height)-hm_height
    cm.ax_heatmap.set_position([hm_pos.x0, hm_y0, hm_width, hm_height]); hm_pos = cm.ax_heatmap.get_position()

    # set the col colors position
    if cm.ax_col_colors is not None:
        
        if type(df_square.columns[0])==str: ncol_fields = len(df_square.columns[0].split(idx_delimiter))
        elif type(df_square.columns[0])==tuple: ncol_fields = len(df_square.columns[0])
        cc_height = ncol_fields*cc_height_multiplier


        cm.ax_col_colors.set_position([hm_pos.x0, hm_pos.y0+hm_height+distance_btw_boxes, hm_width, cc_height]); cc_pos = cm.ax_col_colors.get_position()
        cd_y0 = cc_pos.y0 + cc_height + distance_btw_boxes

    else: cd_y0 = hm_pos.y0 + hm_height + distance_btw_boxes

    # set the col dendrogram position
    if cm.ax_col_dendrogram is not None: cm.ax_col_dendrogram.set_position([hm_pos.x0, cd_y0, hm_width, cd_height])

    # set the row colors
    if cm.ax_row_colors is not None: 

        if type(df_square.index[0])==str: nrow_fields = len(df_square.index[0].split(idx_delimiter))
        elif type(df_square.index[0])==tuple: nrow_fields = len(df_square.index[0])
        else: 
            print(type(df_square.index[0]), df_square.index)
            raise ValueError("invalid ")

        rc_width = nrow_fields*rc_width_multiplier

        cm.ax_row_colors.set_position([hm_pos.x0-rc_width-distance_btw_boxes, hm_pos.y0, rc_width, hm_height]); rc_pos = cm.ax_row_colors.get_position()
        rd_x0 = rc_pos.x0 - rd_width - distance_btw_boxes

    else: rd_x0 = hm_pos.x0 - rd_width - distance_btw_boxes

    # set the row dendrogram
    if cm.ax_row_dendrogram is not None: 
        cm.ax_row_dendrogram.set_position([rd_x0, hm_pos.y0, rd_width, hm_pos.height]); rd_pos = cm.ax_row_dendrogram.get_position()


    # define the cbar position
    cm.ax_cbar.set_position([hm_pos.x0, hm_pos.y0 - cbar_height - distance_btw_boxes, cbar_width, cbar_height])


def get_genes_under_selection_SNPs_union_all_thresholds_piN_piS(df_diversity, max_fraction_samples_extreme_piS_piS):

    """Gets a df diversity (already filtered to include only one instance of positive and negative selection rows) and returns genes that are under selection according to any piN_piS measurement"""

    # go through each selection and define the selected genes
    selection_to_selected_genes = {}
    for selection in ["positive", "negative"]:

        # define the valid thresholds
        valid_thresholds_piNpiS = sel_to_valid_thresholds_piNpiS[selection]

        # get the df
        df = df_diversity[(df_diversity.type_var=="SNP") & ~(pd.isna(df_diversity.pval_harmonicMean_fraction_samples_and_clades)) & (df_diversity["fraction_samples_extreme_piS_p<0.05"]<=max_fraction_samples_extreme_piS_piS) & (df_diversity.threshold_piNpiS.isin(valid_thresholds_piNpiS)) & (df_diversity.pval_harmonicMean_fraction_samples_and_clades_fdr<0.05) & (df_diversity.selection==selection)]

        # check that the combination of gene and threshold is unique
        if len(df)!=len(set(df.Gene + df.threshold_piNpiS.apply(str))): raise ValueError("the combination of gene and threshold_piNpiS should be unique")

        # map each gene to the thresholds in which it appears as significant
        gene_to_sig_thresholds_piNpiS = df.groupby("Gene").apply(lambda df: set(df.threshold_piNpiS) )

        if len(gene_to_sig_thresholds_piNpiS)>0:

            # keep as selected genes those that appear by all thresholds
            selection_to_selected_genes[selection] = set(gene_to_sig_thresholds_piNpiS[gene_to_sig_thresholds_piNpiS==valid_thresholds_piNpiS].index)

        else: selection_to_selected_genes[selection]  = set()

    
    return selection_to_selected_genes


def get_tree_piN_piS_one_gene_SNPs(gene, ProcessedDataDir, plots_dir, df_diversity, gene_features_series, metadata_df_s, species, type_vars_SimpleRepeats, type_vars_appearance, tree):

    """Plots a tree for a gene with the piN and piS in all clades"""

    # keep df diversity of this gene and check that it is unique
    df_diversity = df_diversity[(df_diversity.Gene==gene) & (df_diversity.type_var=="SNP")]
    df_diversity = df_diversity[((df_diversity.selection=="positive") & (df_diversity.threshold_piNpiS.isin(sel_to_valid_thresholds_piNpiS["positive"]))) | ((df_diversity.selection=="negative") & (df_diversity.threshold_piNpiS.isin(sel_to_valid_thresholds_piNpiS["negative"])))]

    # log
    print("working on %s"%gene_features_series.final_name)

    # define all the samples that have some SNP
    all_samples = set(load_object("%s/getting_df_diversity_all_piN_piS/df_piN_piS_one_combination_types_vars_files/%s_SNP_all_vars_all_vars.py"%(ProcessedDataDir, species)).sampleID)

    # load the df from ProcessedDataDir that has all the individual piN, piS numbers
    df_piN_piS = load_object("%s/getting_df_diversity_all_piN_piS/df_piN_piS_one_combination_types_vars_files/%s_SNP_%s_%s.py"%(ProcessedDataDir, species, type_vars_appearance, type_vars_SimpleRepeats))

    # get the gene
    df_piN_piS  = df_piN_piS[df_piN_piS.Gene==gene]

    # add more fields
    df_piN_piS["piN"] = (df_piN_piS.pN / df_piN_piS.positionsN)
    df_piN_piS["piS"] = (df_piN_piS.pS / df_piN_piS.positionsS); check_no_nans_series(df_piN_piS["piS"])
    df_piN_piS = df_piN_piS.set_index("sampleID")

    # map each pN or pS value to a color
    all_pN_pS_vals = [str(x) for x in sorted(set(df_piN_piS.pN).union(set(df_piN_piS.pS)))]
    val_to_color = get_value_to_color(all_pN_pS_vals, n=len(all_pN_pS_vals), palette="rocket_r", type_color="hex")[0]
    val_to_color = {float(v):c for v,c in val_to_color.items()}

    # map each clade to a color (all clades)
    cladeID_to_color, cladeID_to_marker = get_cladeID_to_color_and_marker(metadata_df_s, species)

    # map each sample to a clade
    metadata_df_s = metadata_df_s[(metadata_df_s.sampleID.isin(all_samples))]; check_no_nans_series(metadata_df_s.cladeID_Tree_and_BranchLen)
    metadata_df_s["sampleID"] = metadata_df_s.sampleID.apply(str)
    metadata_df_s["cladeID_Tree_and_BranchLen"] = metadata_df_s.cladeID_Tree_and_BranchLen.apply(int)
    sample_to_clade = dict(metadata_df_s.set_index("sampleID").cladeID_Tree_and_BranchLen)
    if set(metadata_df_s.sampleID)!=all_samples: raise ValueError("not all samples in all_samples are in metadata_df_s")

    # keep tree
    tree = cp.deepcopy(tree); tree.prune(all_samples, preserve_branch_length=True)

    # add cladeID
    tree = get_tree_withcladeID_as_str(tree, sample_to_clade)

    # go through each node
    for n in tree.traverse():

        # define the bwidth    
        species_to_bwidth = {"Candida_albicans":40, "Candida_glabrata":80, "Candida_auris":20, "Candida_tropicalis":4, "Candida_metapsilosis":4, "Candida_parapsilosis":4, "Candida_orthopsilosis":4}
        bwidth = species_to_bwidth[species]
        #bwidth = 4

        nst = NodeStyle()
        nst["hz_line_width"] = bwidth
        nst["vt_line_width"] = bwidth
        nst["size"] = 0

        # define the color
        if pd.isna(n.cladeID): linecolor = "black"
        else: linecolor = cladeID_to_color[n.cladeID]
      
        nst["bgcolor"] = "white"
        nst["hz_line_color"] = linecolor
        nst["vt_line_color"] = linecolor
        
        n.set_style(nst)

        # map each selection type to the color
        type_sel_to_color = {"positive":"magenta", "negative":"cyan", "neutral":"purple", "no SNPs":"white", "truncated gene":"grey"}

        # add the piN and piS measurements
        if n.is_leaf():

            # create a square that indicates the selection type
            if n.name in df_piN_piS.index:

                # define values
                piN = df_piN_piS.loc[n.name, "piN"]
                piS = df_piN_piS.loc[n.name, "piS"]
                pN = df_piN_piS.loc[n.name, "pN"]
                pS = df_piN_piS.loc[n.name, "pS"]
                
                # deifne color
                if piN>piS: sel_color = type_sel_to_color["positive"]
                elif piN<piS: sel_color = type_sel_to_color["negative"]
                elif piN==piS and piN!=0: sel_color = type_sel_to_color["neutral"]
                elif  piN==0 and piS==0: sel_color =  type_sel_to_color["no SNPs"]
                else: raise ValueError("invalid")
                
                label = "%i/%i"%(pN, pS)

                # define a color of the pN and pS
                pN_color = val_to_color[float(pN)]
                pS_color = val_to_color[float(pS)]


            else: 
                sel_color = type_sel_to_color["truncated gene"]
                label = "X"
                pN_color = pS_color = "white"

            # define the label size
            #species_to_label_size = {"Candida_albicans":40, "Candida_glabrata":80, "Candida_auris":20, "Candida_tropicalis":4, "Candida_parapsilosis":4, "Candida_orthopsilosis":4}
            label_size = int(bwidth*1.5)
            


            # add the square with the selectoin
            rect_width = bwidth*5
            n.add_face(RectFace(rect_width, rect_width, fgcolor='gray', bgcolor=sel_color, label={"text":label, "color":"black", "fontsize":label_size}), column=0 , position="aligned") #

            # add a small spacer square
            n.add_face(RectFace(rect_width/2, rect_width/2, fgcolor='white', bgcolor="white"), column=1 , position="aligned") #


            # add squares for the pN and pS colors
            for Ic, color in enumerate([pN_color, pS_color]):  n.add_face(RectFace(rect_width, rect_width, fgcolor='gray', bgcolor=color), column=(Ic+2) , position="aligned")

        # add white face to the leafs
        #if n.is_leaf(): n.add_face(RectFace(20, 40, fgcolor=bgcolor, bgcolor=bgcolor, label={"text":"", "color":"white"}), column=0 , position="aligned") #

        #if show_clade is True and n.is_leaf(): n.add_face(RectFace(20, 40, fgcolor=bgcolor, bgcolor=bgcolor, label={"text":str(n.cladeID), "color":"black"}), column=1 , position="aligned") #


    ts = TreeStyle()
    ts.show_branch_length = False
    ts.show_branch_support = False
    ts.show_leaf_name = False

    # add the lables size
    label_size = label_size*5
    rect_width = rect_width*2

    # define title
    title_str = species + "-" + "\n".join(gene_features_series[["final_name", "description"]].values)
    title_str += "\n" + "1st col: selection; 2nd col: # ns SNPs; 3d col: # syn SNPs" + "\n\n"

    def format_title_item(x):
        if type(x)==float: return "%.3f"%x
        else: return x

    title_str += "diversity stats:\n" + "\n".join(["; ".join(["%s=%s"%(k, format_title_item(r[k])) for k in ["selection", "threshold_piNpiS", "fraction_samples_selected", "fraction_clades_selection", "harmonicMean_fraction_samples_and_clades", "pval_harmonicMean_fraction_samples_and_clades_fdr"]]) for I, r in df_diversity.iterrows()])



    ts.title.add_face(TextFace(title_str, fsize=label_size, bold=True), column=0)

    """
    Index(['', '', '', 'species',
           '', '', '',
           '', '',
           'pval_fraction_samples', 'pval_fraction_clades',
           'pval_harmonicMean_fraction_samples_and_clades',
           'mean_p_real_piS_not_extreme', 'std_p_real_piS_not_extreme',
           'median_p_real_piS_not_extreme', 'mad_p_real_piS_not_extreme',
           'fraction_samples_extreme_piS_p<0.05',
           'fraction_samples_extreme_piS_p<0.1', 'gene_and_threshold_piNpiS',
           'selection', 'ngenes', 'nsamples', 'type_var',
           'type_vars_SimpleRepeats', 'type_vars_appearance', 'numeric_index',
           'pval_harmonicMean_fraction_samples_and_clades_fdr',
           'minus_log10pval_pval_harmonicMean_fraction_samples_and_clades_fdr'],

    """




    # add legend values
    ts.legend.add_face(TextFace("# SNPs", bold=True, fsize=label_size), column=0)    
    ts.legend.add_face(TextFace("", bold=True, fsize=label_size), column=1)    

    for Ic, (val, color) in enumerate(val_to_color.items()): 
        ts.legend.add_face(RectFace(rect_width, rect_width, fgcolor="gray", bgcolor=color), column=0)
        ts.legend.add_face(TextFace(str(val), bold=True, fsize=label_size), column=1) 

    # add legend of the  selection type
    ts.legend.add_face(TextFace("\nselection", bold=True, fsize=label_size), column=0)    
    ts.legend.add_face(TextFace("\n", bold=True, fsize=label_size), column=1)   

    for type_sel, color in type_sel_to_color.items():
        ts.legend.add_face(RectFace(rect_width, rect_width, fgcolor="gray", bgcolor=color), column=0)
        ts.legend.add_face(TextFace(type_sel, bold=True, fsize=label_size), column=1) 

    # cicular orientation
    ts.mode = 'c'
    #ts.root_opening_factor = 1
    ts.arc_start = -180 # 0 degrees = 3 o'clock
    ts.arc_span = 180



    tree.show(tree_style=ts)


    filename = "%s/%s.pdf"%(plots_dir, gene_features_series.final_name)
    print("rendering %s"%filename)
    tree.render(file_name=filename, tree_style=ts)


def plot_piN_piS_on_tree_interesting_genes(ProcessedDataDir, df_diversity_all, PlotsDir, gene_features_df, metadata_df, species_to_tree, max_fraction_samples_extreme_piS_piS=0.1):

    """Plots a tree for each interesting genes with the piN piS values in all samples"""

    # add final name
    def get_final_name_gene_features_df(r):
        if not pd.isna(r.gene_name): return r.gene_name
        elif not pd.isna(r.Scerevisiae_orthologs): return "Scer_%s"%(r.Scerevisiae_orthologs)
        else: return r.gff_upmost_parent

    gene_features_df["final_name"] = gene_features_df.apply(get_final_name_gene_features_df, axis=1)

    # go through different types of variants
    for type_vars_appearance in ["only_vars_recent"]:

        # go through each soecies
        for species in sorted_species_byPhylogeny:
            if species!="Candida_albicans": continue

            # get df for this species
            metadata_df_s = metadata_df[metadata_df.species_name==species]

            # get diversity df
            type_vars_SimpleRepeats = "only_vars_noSimpleRepeats"
            df_diversity = df_diversity_all[(df_diversity_all.type_vars_appearance==type_vars_appearance) & (df_diversity_all.species==species) & (df_diversity_all.type_vars_SimpleRepeats==type_vars_SimpleRepeats)]

            # get genes under selection
            selection_to_genes = get_genes_under_selection_SNPs_union_all_thresholds_piN_piS(df_diversity, max_fraction_samples_extreme_piS_piS)
            print(type_vars_appearance, species, "# genes under selection: ", {s:len(g) for s,g in selection_to_genes.items()})

            # define the interesting genes
            genes_both_positive_negative = set.intersection(*selection_to_genes.values()) # define the genes that are both under positive and negative selection
            interesting_genes = sorted(genes_both_positive_negative)

            # define plots dir
            plots_dir_all = "%s/piN_piS_on_tree_interesting_genes_SNPs"%PlotsDir; make_folder(plots_dir_all)
            plots_dir = "%s/%s_%s"%(plots_dir_all, type_vars_appearance, species); make_folder(plots_dir)

            # for each gene generate a plot with piN and piS
            for gene in interesting_genes:
                gene_features_series = gene_features_df[(gene_features_df.species==species) & (gene_features_df.gff_upmost_parent==gene)].iloc[0]
                get_tree_piN_piS_one_gene_SNPs(gene, ProcessedDataDir, plots_dir, df_diversity, gene_features_series, metadata_df_s, species, type_vars_SimpleRepeats, type_vars_appearance, species_to_tree[species])

def get_high_low_percentiles_df_diversity_INDELs_SVs(df_diversity, percentiles_thresholds, diversity_f):

    """Returns the high and the low percentile diversity thresholds"""

    # checks
    if len(df_diversity)==0: raise ValueError("The df_diversity can't be 0")

    # checks
    for f in ["type_vars_SimpleRepeats", "species", "type_vars_appearance", "type_var"]:
        if len(set(df_diversity[f]))!=1: raise ValueError("The percentiles can't be calculated. %s has these values: %s"%(f, set(df_diversity[diversity_f])))

    # get the percentiles
    low_percentile = np.percentile(df_diversity[df_diversity[diversity_f]>0][diversity_f], percentiles_thresholds[0])
    high_percentile = np.percentile(df_diversity[df_diversity[diversity_f]>0][diversity_f], percentiles_thresholds[1])

    return low_percentile, high_percentile


def visualize_distribution_SVs_INDELs_under_selection(df_diversity_all, PlotsDir, percentiles_thresholds):

    """This function creates a plot with the fraction of samples and clades under selection for each gene """
    
    # filter df
    df_diversity_all = df_diversity_all[(df_diversity_all.type_var.isin({'if_INDEL', 'DEL', 'DUP'})) & (df_diversity_all.type_vars_SimpleRepeats=="only_vars_noSimpleRepeats")]
    check_no_nans_series(df_diversity_all["harmonicMean_fraction_samples_and_clades"])

    # one plot for each type of vars appearance
    for type_vars_appearance in ["only_vars_recent"]: # all_vars
            
        # filter df
        df_diversity = df_diversity_all[(df_diversity_all.type_vars_appearance==type_vars_appearance)]

        # define the diversity field
        diversity_f = "harmonicMean_fraction_samples_and_clades"

        # define lines to be drawn 
        species_to_vartype_to_xlines = {}

        for species in sorted_species_byPhylogeny:
            for type_var in sorted(set(df_diversity.type_var)):
                
                df_div_pcts = df_diversity[(df_diversity.species==species) & (df_diversity.type_var==type_var)]
                if len(df_div_pcts)==0: continue

                low_percentile, high_percentile = get_high_low_percentiles_df_diversity_INDELs_SVs(df_div_pcts, percentiles_thresholds, diversity_f)
                
                #low_percentile = 0; high_percentile = 0.1
                species_to_vartype_to_xlines.setdefault(species, {}).setdefault(type_var, [high_percentile])
        
        # define fields for the histogram
        vartype_field = "type_var"
        plots_dir = "%s/plots_distribution_fraction_samples_clades_affected_INDELs_SVs"%PlotsDir; delete_folder(plots_dir); make_folder(plots_dir)

        # get file
        filename = "%s/%s.pdf"%(plots_dir, type_vars_appearance)
        title = "distribution F-vaue(fration samples with variant, fraction clades with variant)\nonly_vars_noSimpleRepeats; %s"%(type_vars_appearance)

        # get the plot
        #sorted_vartypes = ["IN/DEL", "SV", "coverageCNV"]
        sorted_vartypes = ["if_INDEL", "DUP", "DEL"]

        plot_histograms_rows_in_species_vatypes_in_cols(df_diversity, sorted_vartypes, diversity_f, filename, vartype_field, logscale_x=False, logscale_y=True, pseudocount=0, xlabel="F-value (fraction clades or samples w/ variant)", input_yticks=None, extended_xrange=0.05, ylabel="number genes", species_to_vartype_to_xlines=species_to_vartype_to_xlines, title=title, size_multiplier=1, add_xlim=[0.05,0.05], input_xticks=[0, 0.5, 1], text_nelements_above_some_x_min_x=None, text_nelements_above_some_x_text_coords=[0.4, 500])



def clustermap_fraction_samples_clades_under_selection_SVs_INDELs(df_diversity_all, PlotsDir, gene_features_df,  col_cluster=True, type_vars_SimpleRepeats_values={"all_vars", "only_vars_noSimpleRepeats"}, type_vars_appearance_values={"all_vars", "only_vars_recent"}):


    """Draws in a clustermap the selection intensity of the """

    # define the genes for each species
    spp_to_ngenes = {spp : len(set(df_diversity_all[df_diversity_all.species==spp].Gene)) for spp in sorted_species_byPhylogeny}

    # filter 
    df_diversity_all = df_diversity_all[(df_diversity_all.type_var.isin({'IN/DEL', 'SV', 'coverageCNV'})) & (df_diversity_all.harmonicMean_fraction_samples_and_clades>0) & (df_diversity_all.type_vars_SimpleRepeats.isin(type_vars_SimpleRepeats_values)) & (df_diversity_all.type_vars_appearance.isin(type_vars_appearance_values))]

    # check vals
    check_no_nans_series(df_diversity_all["harmonicMean_fraction_samples_and_clades"])

    # define the column fields
    col_fields = ["type_var", "type_vars_SimpleRepeats", "type_vars_appearance"]
    colf_to_values = {c : set(df_diversity_all[c]) for c in col_fields}
    col_fields = [c for c in col_fields if len(colf_to_values[c])>1]
    df_diversity_all["colID"] = get_series_aggregated_str(df_diversity_all[col_fields], "-")


    # map each col field to the color dict
    colf_to_color_dict = {"type_var" : {"IN/DEL":"magenta", "SV":"navy", "coverageCNV":"cyan"},
                          "type_vars_SimpleRepeats" : {"all_vars":"olive", "only_vars_noSimpleRepeats":"r"},
                          "type_vars_appearance" : {"all_vars":"gold", "only_vars_recent":"salmon"}}

    # one plot for each species
    for species in sorted_species_byPhylogeny:
        print(species)
        #if species!="Candida_orthopsilosis": continue

        # make a square df
        def get_NaN_to_0(x):
            if pd.isna(x): return 0.0
            else: return x
        df_square = df_diversity_all[df_diversity_all.species==species][["colID", "Gene", "harmonicMean_fraction_samples_and_clades"]].pivot(index="Gene", columns="colID", values="harmonicMean_fraction_samples_and_clades").applymap(get_NaN_to_0)


        # define the row colors df
        col_colors_df = pd.DataFrame({colf : {col : colf_to_color_dict[colf][col.split("-")[Ic]] for col in df_square.columns} for Ic, colf in enumerate(col_fields)})

        # debug
        if len(df_square)==0: continue

        # plot the clustermap
        ticks_cbar = [0, 0.5, 1]
        cm = sns.clustermap(df_square, col_cluster=col_cluster, row_cluster=True, cmap="rocket_r", col_colors=col_colors_df, center=0.5, vmin=0, vmax=1, xticklabels=False, yticklabels=False, cbar_kws={'label': "F-value (fraction samples and clades w/ variant)", "orientation":'horizontal', "ticks":ticks_cbar}) #, linecolor="gray", linewidth=.01 )

        # add lines
        cm.ax_heatmap.vlines(list(range(0, len(df_square.columns)+1)), *cm.ax_heatmap.get_ylim(), color="gray", linestyle="--", linewidth=.5)
        cm.ax_col_colors.vlines(list(range(0, len(df_square.columns)+1)), *cm.ax_col_colors.get_ylim(), color="gray", linestyle="--", linewidth=.5)
        cm.ax_col_colors.hlines(list(range(0, 4+1)), *cm.ax_col_colors.get_xlim(), color="gray", linestyle="--", linewidth=.5)

        # labels
        cm.ax_heatmap.set_ylabel("")
        cm.ax_heatmap.set_xlabel("")

        # adjust
        adjust_cm_positions(cm, df_square, hm_height_multiplier=0.0001) # hm_height_multiplier=0.0002


        # legend
        def get_empty_legend(label): return Line2D([0], [0], marker="o", label=label, markersize=0, lw=0)
        def get_legend_element(color, label): return mpatches.Patch(facecolor=color, edgecolor="gray", label=label) 

        legend_elements = make_flat_listOflists([([get_empty_legend(colf)] + [get_legend_element(color, val)  for val, color in colf_to_color_dict[colf].items()] + [get_empty_legend("")])  for Ic, colf in enumerate(col_fields)])

        cm.ax_row_dendrogram.legend(handles=legend_elements, bbox_to_anchor=[-1, 1], loc="upper right")

        # add title
        cm.ax_col_dendrogram.set_title("C. %s INDEL/SV/CNV presence selection\n %i/%i genes\n%s"%(species.split("_")[1], len(df_square), spp_to_ngenes[species], "\n".join(["%s=%s"%(cf, next(iter(v))) for cf,v in colf_to_values.items() if len(v)==1])))
        plt.show()

        # save
        plots_dir = "%s/selection_signatures_INDELS_SVs_heatmap"%PlotsDir; make_folder(plots_dir)
        filename = "%s/%s_harmonicMean_colcluster=%s.pdf"%(plots_dir, species, col_cluster)
        print("saving %s"%filename)
        cm.savefig(filename, bbox_inches="tight")

        #plt.close(cm)


def clustermap_fraction_samples_clades_under_selection_SNP(df_diversity_all, PlotsDir, gene_features_df, max_fraction_samples_extreme_piS_piS=0.1, col_cluster=True, selection_values={"positive", "negative"}, type_vars_SimpleRepeats_values={"all_vars", "only_vars_noSimpleRepeats"}, type_vars_appearance_values={"all_vars", "only_vars_recent"}, add_gene_names=False, keep_only_significant_genes_all_rows=False):

    """Draws a clustermap for each species, where each row is one gene and each column is a combination of selection, type_vars_SimpleRepeats, type_vars_appearance and threshold of the piN/piS values"""

    # define the genes for each species
    spp_to_ngenes = {spp : len(set(df_diversity_all[df_diversity_all.species==spp].Gene)) for spp in sorted_species_byPhylogeny}

    # add final name
    def get_final_name_gene_features_df(r):
        if not pd.isna(r.gene_name): return r.gene_name
        elif not pd.isna(r.Scerevisiae_orthologs): return "Scer_%s"%(r.Scerevisiae_orthologs)
        else: return r.gff_upmost_parent

    gene_features_df["final_name"] = gene_features_df.apply(get_final_name_gene_features_df, axis=1)



    # keep only SNPs
    df_diversity_all = df_diversity_all[(df_diversity_all.type_var=="SNP") & ~(pd.isna(df_diversity_all.pval_harmonicMean_fraction_samples_and_clades)) & (df_diversity_all["fraction_samples_extreme_piS_p<0.05"]<=max_fraction_samples_extreme_piS_piS)  & (df_diversity_all.selection.isin(selection_values)) & (df_diversity_all.type_vars_SimpleRepeats.isin(type_vars_SimpleRepeats_values)) & (df_diversity_all.type_vars_appearance.isin(type_vars_appearance_values))]


    # keep only some thresholds
    df_diversity_all = df_diversity_all[(df_diversity_all.threshold_piNpiS.isin({0.01, 0.1, 0.5, 1, 1.5, 2}))]


    # keep only significant genes
    df_diversity_all = df_diversity_all[(df_diversity_all.pval_harmonicMean_fraction_samples_and_clades_fdr)<0.05]

    # check no nans
    check_no_nans_series(df_diversity_all["minus_log10pval_pval_harmonicMean_fraction_samples_and_clades_fdr"])

    # define the column fields
    col_fields = ["selection", "type_vars_SimpleRepeats", "type_vars_appearance", "threshold_piNpiS"]
    colf_to_values = {c : set(df_diversity_all[c]) for c in col_fields}
    col_fields = [c for c in col_fields if len(colf_to_values[c])>1]
    df_diversity_all["colID"] = get_series_aggregated_str(df_diversity_all[col_fields], "-")

    # map each col field to the color dict
    all_tshds = [str(x) for x in sorted(set(df_diversity_all.threshold_piNpiS))]
    tshd_piN_piS_to_color = get_value_to_color(all_tshds, n=len(all_tshds), palette="Greys", type_color="hex")[0]
    colf_to_color_dict = {"selection" : {"positive":"magenta", "negative":"c"},
                          "type_vars_SimpleRepeats" : {"all_vars":"olive", "only_vars_noSimpleRepeats":"r"},
                          "type_vars_appearance" : {"all_vars":"gold", "only_vars_recent":"salmon"},
                          "threshold_piNpiS" : tshd_piN_piS_to_color}

    # one plot for each species
    for species in sorted_species_byPhylogeny:
        print(species)
        #if species!="Candida_auris": continue

        # make a square df
        def get_NaN_to_0(x):
            if pd.isna(x): return 0.0
            else: return x
        df_square = df_diversity_all[df_diversity_all.species==species][["colID", "Gene", "minus_log10pval_pval_harmonicMean_fraction_samples_and_clades_fdr"]].pivot(index="Gene", columns="colID", values="minus_log10pval_pval_harmonicMean_fraction_samples_and_clades_fdr").applymap(get_NaN_to_0)

        # define the row colors df
        col_colors_df = pd.DataFrame({colf : {col : colf_to_color_dict[colf][col.split("-")[Ic]] for col in df_square.columns} for Ic, colf in enumerate(col_fields)})

        # define the significance threshold
        sig_threshold_logp = (-np.log10(0.05+1/10000))

        # kee only the genes that have significance in all rows
        if keep_only_significant_genes_all_rows is True: df_square = df_square[ (df_square>sig_threshold_logp).apply(all, axis=1) ]

        # debug
        if len(df_square)==0: continue

        # plot the clustermap
        ticks_cbar = [0, 1, 2, 3, 4]
        cm = sns.clustermap(df_square, col_cluster=col_cluster, row_cluster=True, cmap="coolwarm", col_colors=col_colors_df, center=sig_threshold_logp, xticklabels=False, yticklabels=add_gene_names, cbar_kws={'label': "p harmonic mean (FDR)", "orientation":'horizontal', "ticks":ticks_cbar}) #, linecolor="gray", linewidth=.01 )

        # set the tick labels
        cm.ax_cbar.set_xticklabels([str(10**(-t)) for t in ticks_cbar], rotation=90)

        # add lines
        cm.ax_heatmap.vlines(list(range(0, len(df_square.columns)+1)), *cm.ax_heatmap.get_ylim(), color="gray", linestyle="--", linewidth=.5)
        cm.ax_col_colors.vlines(list(range(0, len(df_square.columns)+1)), *cm.ax_col_colors.get_ylim(), color="gray", linestyle="--", linewidth=.5)
        cm.ax_col_colors.hlines(list(range(0, 4+1)), *cm.ax_col_colors.get_xlim(), color="gray", linestyle="--", linewidth=.5)


        # labels
        cm.ax_heatmap.set_ylabel("")
        cm.ax_heatmap.set_xlabel("")

        # add the gene names
        if add_gene_names is True: 
            geneID_to_name = dict(gene_features_df[gene_features_df.species==species].set_index("gff_upmost_parent").final_name)
            cm.ax_heatmap.set_yticklabels([geneID_to_name[y.get_text()] for y in cm.ax_heatmap.get_yticklabels()], fontsize=7, rotation=0)
            hm_height_multiplier = 0.01
            cm.ax_heatmap.hlines(list(range(0, len(df_square)+1)), *cm.ax_heatmap.get_xlim(), color="gray", linestyle="--", linewidth=.5)


        else: hm_height_multiplier = 0.0004


        # adjust
        adjust_cm_positions(cm, df_square, hm_height_multiplier=hm_height_multiplier) # hm_height_multiplier=0.0002


        # legend
        def get_empty_legend(label): return Line2D([0], [0], marker="o", label=label, markersize=0, lw=0)
        def get_legend_element(color, label): return mpatches.Patch(facecolor=color, edgecolor="gray", label=label) 

        legend_elements = make_flat_listOflists([([get_empty_legend(colf)] + [get_legend_element(color, val)  for val, color in colf_to_color_dict[colf].items()] + [get_empty_legend("")])  for Ic, colf in enumerate(col_fields)])

        cm.ax_row_dendrogram.legend(handles=legend_elements, bbox_to_anchor=[0, 1], loc="upper right")

        # add title
        cm.ax_col_dendrogram.set_title("C. %s SNP selection\n %i/%i genes p<0.05\nkeep_only_significant_genes_all_rows=%s\n%s"%(species.split("_")[1], len(df_square), spp_to_ngenes[species], keep_only_significant_genes_all_rows, "\n".join(["%s=%s"%(cf, next(iter(v))) for cf,v in colf_to_values.items() if len(v)==1])))
        plt.show()

        # save
        plots_dir = "%s/selection_signatures_SNPs_heatmap"%PlotsDir; make_folder(plots_dir)
        filename = "%s/%s_harmonicMean_colcluster=%s.pdf"%(plots_dir, species, col_cluster)
        print("saving %s"%filename)
        cm.savefig(filename, bbox_inches="tight")

        #plt.close(cm)

def SNPs_correlation_between_fraction_samples_clades_selection_and_clades(df_diversity_all, PlotsDir, max_fraction_samples_extreme_piS_piS=0.1):


    """Plots the correlation between the harmonic mean btw fraciton of samples and fraction of clades and the p value"""

    # keep SNPs with p val calculated
    df_diversity_all = df_diversity_all[(df_diversity_all.type_var=="SNP") & ~(pd.isna(df_diversity_all.pval_harmonicMean_fraction_samples_and_clades)) & (df_diversity_all["fraction_samples_extreme_piS_p<0.05"]<=max_fraction_samples_extreme_piS_piS)]

    # add fields
    """
    df_diversity_all["Gene_and_threshold"] = df_diversity_all.Gene + "_" + df_diversity_all.threshold_piNpiS.apply(str)
    df_diversity_all["selection_and_threshdold"] = df_diversity_all.selection + "_" + df_diversity_all.threshold_piNpiS.apply(str)
    """

    # go through type_vars_SimpleRepeats
    for type_vars_SimpleRepeats in sorted(set(df_diversity_all.type_vars_SimpleRepeats)):
        if type_vars_SimpleRepeats=="all_vars": continue

        for type_vars_appearance in sorted(set(df_diversity_all.type_vars_appearance)):
            for selection in ["positive", "negative"]:

                if selection!="positive": continue
                if type_vars_appearance!="only_vars_recent": continue

                print(type_vars_SimpleRepeats, type_vars_appearance)

                # get df
                df_diversity = df_diversity_all[(df_diversity_all.type_vars_SimpleRepeats==type_vars_SimpleRepeats) & (df_diversity_all.type_vars_appearance==type_vars_appearance) & (df_diversity_all.selection==selection) & (df_diversity_all.threshold_piNpiS.isin(sel_to_valid_thresholds_piNpiS[selection]))]


                # init figure for each combination of species and type_vars_appearance
                all_species = sorted_species_byPhylogeny
                all_thresholds = sorted(sel_to_valid_thresholds_piNpiS[selection])
                nrows = len(all_species)
                ncols = len(all_thresholds)
                fig = plt.figure(figsize=(ncols*1.5, nrows*1.5)); I=1


                # add subplots
                for Ir, species in enumerate(all_species):
                    for Ic, threshold_piNpiS in enumerate(all_thresholds):

                        # get df and check that Gene is unique
                        df_plot = df_diversity[(df_diversity.species==species) & (df_diversity.threshold_piNpiS==threshold_piNpiS)]
                        if len(df_plot)!=len(set(df_plot.Gene)): raise ValueError("Gene should be unique ")
                        check_no_nans_series(df_plot.minus_log10pval_pval_harmonicMean_fraction_samples_and_clades_fdr)


                        # plot the density scatter
                        ax = plt.subplot(nrows, ncols, I); I+=1

                        #density_scatter(df_plot.harmonicMean_fraction_samples_and_clades.values , df_plot.minus_log10pval_pval_harmonicMean_fraction_samples_and_clades_fdr.values, ax, sort=True, bins=3, s=3)
                        sns.scatterplot(data=df_plot, x="harmonicMean_fraction_samples_and_clades", y="minus_log10pval_pval_harmonicMean_fraction_samples_and_clades_fdr", color=species_to_color[species], alpha=.6, s=10)

                        # set lims
                        ax.set_xlim([-0.05, 1.05])
                        ax.set_ylim([-0.05, max(df_diversity_all[df_diversity_all.selection==selection]["minus_log10pval_pval_harmonicMean_fraction_samples_and_clades_fdr"])+0.05])

                        # add text with the number of sig genes
                        plt.text(0.01, 2.7, "%i/%i\np<0.05"%(sum(df_plot.pval_harmonicMean_fraction_samples_and_clades_fdr<0.05), len(df_plot)), fontsize=8)

                        # x axis
                        if species==(all_species[-1]): 
                            if Ic==1: ax.set_xlabel('harmonic mean (fraction samples w/ gene under selection, fraction caldes w/ sample w/ gene under selection)')
                            else: ax.set_xlabel("")
                        else:
                            ax.set_xticklabels([])
                            ax.set_xticks([])
                            ax.set_xlabel("")

                        # y axis
                        if Ic==0: 
                            if Ir==2: ax.set_ylabel("-log(p harmonic mean)\nC. %s"%(species.split("_")[1]))                       
                            else: ax.set_ylabel("C. %s"%(species.split("_")[1]))

                        else:
                            ax.set_yticklabels([])
                            ax.set_yticks([])
                            ax.set_ylabel("")



                        # title
                        tshd_str = "piN/piS%s%s"%({"positive":">", "negative":"<"}[selection], threshold_piNpiS)
                        if Ir==0 and Ic==1: ax.set_title("type_vars_SimpleRepeats=%s; type_vars_appearance=%s\nmax_fraction_samples_extreme_piS_piS (p<0.05)=%s; %s selection\n\n%s"%(type_vars_SimpleRepeats, type_vars_appearance, max_fraction_samples_extreme_piS_piS, selection, tshd_str))
                        elif Ir==0: ax.set_title(tshd_str)

                        # legend
                        #ax.get_legend().remove()

                        # add the line for the pval
                        for pval in [0.05]: plt.axhline(-np.log10(pval+1/10000), color="gray", linestyle="--", linewidth=.7)
                        for y in [0.5]: plt.axvline(y, color="gray", linestyle="--", linewidth=.7)

                    
                    


                plt.subplots_adjust(wspace=0.03,  hspace=0.1)
                plt.show()
            

                plots_dir = "%s/correlation_selection_vs_pval_SNPs"%PlotsDir; make_folder(plots_dir)
                filename = "%s/%s_%s_%s_harmonicMean.pdf"%(plots_dir, type_vars_SimpleRepeats, type_vars_appearance, selection)
                print("saving %s"%filename)
                fig.savefig(filename, bbox_inches="tight")


def get_df_pvals_QQ_plot_one_spp_and_drug(spp, drug, tab_file, min_npheno_transitions, DataDir, optimal_filters_series):

    """Returns data for one spp-drug for gwas_QQ_plot_all_data. The df should have species, drug, """

    # log
    print("getting df for %s-%s"%(spp, drug))

    # load gwas
    print("loading gwas data")
    df_gwas = get_tab_as_df_or_empty_df(tab_file)

    # check the min_npheno_transitions
    n_pheno_transitions_set = set(df_gwas[(df_gwas.ASR_methods_phenotypes=="MPPA,DOWNPASS") & (df_gwas.gwas_method=="synchronous") & (df_gwas.min_support==0)].nodes_withPheno)

    if len(n_pheno_transitions_set)==0: 
        print("WARNING: Skipping %s-%s because it has 0 transitions (there is nothing in the df)"%(spp, drug))
        return pd.DataFrame()

    if len(n_pheno_transitions_set)!=1: raise ValueError("there are more than one phenotype transitions")
    if next(iter(n_pheno_transitions_set))<min_npheno_transitions: 
        print("WARNING: Skipping %s-%s because it has %i transitions"%(spp, drug, next(iter(n_pheno_transitions_set))))
        return pd.DataFrame()

    # define fields
    print("getting the long df format")
    all_pval_fields = ["%s_%s"%(pval_seed, pval_m) for pval_seed in ["pval_chi_square", "pval_GenoAndPheno"] for pval_m in  ["RelToBranchLen", "phenotypes"]] 
    interesting_pval_fields = [f for f in all_pval_fields if optimal_filters_series[f] is True]
    ID_fields = ["species", "drug", "gwas_method", "type_collapsing", "type_vars", "type_mutations", "type_genes"]
    contingency_fields = ["nodes_GenoAndPheno", "nodes_noGenoAndPheno", "nodes_GenoAndNoPheno", "nodes_noGenoAndNoPheno"]

    # keep only data for ASR_methods_phenotypes and min_support and synchronous
    df_gwas = df_gwas[(df_gwas.ASR_methods_phenotypes==optimal_filters_series.ASR_methods_phenotypes) & (df_gwas.min_support==optimal_filters_series.min_support) & (df_gwas.gwas_method=="synchronous")][interesting_pval_fields + ID_fields + contingency_fields + ["group_name", "nodes_withPheno", "nodes_withoutPheno"]]

    # add the numeric ID and pct progress
    print("adding unique ID")
    df_gwas["unique_ID"] =  df_gwas[ID_fields].apply(lambda r: "-".join(r), axis=1)
    all_unique_IDs = sorted(set(df_gwas.unique_ID))
    ID_to_num = dict(zip(all_unique_IDs, range(1, len(all_unique_IDs)+1)))
    df_gwas["pct_progress"] = (df_gwas.unique_ID.map(ID_to_num) / len(all_unique_IDs))*100

    df_gwas = df_gwas.sort_values(by="pct_progress")
    check_no_nans_series(df_gwas.pct_progress)

    # define a function that calculates a random 2x2 contigency table keeping the same pheno / no-pheno proportions but randomizing the geno assignation 
    def get_pval_fisher_random_from_r(r):

        # define the number of nodes with and without phenotype
        nodes_pheno = r.nodes_GenoAndPheno + r.nodes_noGenoAndPheno
        nodes_no_pheno = r.nodes_GenoAndNoPheno + r.nodes_noGenoAndNoPheno

        # define the pheno
        pheno_geno = random.randint(2, nodes_pheno)
        pheno_no_geno = nodes_pheno-pheno_geno

        # define the no_pheno_no_geno so that the OR>1
        for min_no_pheno_no_geno in range(1, nodes_no_pheno+1):
            no_pheno_geno_test = nodes_no_pheno-min_no_pheno_no_geno
            OR = np.divide((pheno_geno*min_no_pheno_no_geno),(pheno_no_geno*no_pheno_geno_test))
            if OR>1: break

        # define the no_pheno_no_geno so that the OR is above 1
        no_pheno_no_geno = random.randint(min_no_pheno_no_geno, nodes_no_pheno)
        no_pheno_geno = nodes_no_pheno-no_pheno_no_geno

        # define the table
        contingency_table = [[pheno_geno, no_pheno_geno], 
                             [pheno_no_geno, no_pheno_no_geno]]

        # calculate fisher p value
        OR, p_fisher = stats.fisher_exact(contingency_table, alternative="greater")

        # checks to see that this is only a biased subset of the p values
        if not pheno_geno>=2: raise ValueError("pheno_geno should be 2 or more")
        if not no_pheno_no_geno>0: raise ValueError("no_pheno_no_geno should be >0")
        if not OR>1: raise ValueError("The OR should be above 1. OR=%s"%OR)

        return p_fisher

    # get a long df
    def get_long_gwas_df_oneID(df_all):

        # print the progress
        #print("alrady traversed %s pct"%df_all.pct_progress.iloc[0])


        # check
        if len(df_all)!=len(set(df_all.group_name)): raise ValueError("each df should have one group name")

        # get the sorted the p value of a fisher test keeping the same numbers of pheno / no-pheno but changing the distribution
        sorted_pvals_fisher_random = sorted(df_all.apply(get_pval_fisher_random_from_r, axis=1))

        # define the total number of groups
        rID = df_all[ID_fields].iloc[0]
        gwas_results_file = "%s/%s_%i/ancestral_GWAS_drugResistance/GWAS_%s_resistance/%s-%s-%s-%s/%s-%s-min_support=%i/gwas_jobs/integrated_GWAS_stats.tab"%(DataDir, spp, sciName_to_taxID[spp], drug, rID.type_vars, rID.type_genes, rID.type_mutations, rID.type_collapsing, optimal_filters_series.ASR_methods_phenotypes, optimal_filters_series.ASR_methods_phenotypes, optimal_filters_series.min_support) # get the dir of this group
        total_n_groups = (int(str(subprocess.check_output("wc -l '%s'"%gwas_results_file, shell=True)).split("'")[1].split()[0]) - 1) / 2
        if total_n_groups!=int(total_n_groups): raise ValueError("The wc -l should yield a number that is divisible by 2")
        total_n_groups = int(total_n_groups)
        if total_n_groups<len(df_all): raise ValueError("there are less groups in the individual file than in the df_all")

        # define a set of sorted p values assuming that the observed ones are the top ones
        sorted_pvals_all_groups_uniformly_distributed = list(np.linspace(1/total_n_groups, 1, total_n_groups))

        # init df
        df_long = pd.DataFrame()
        for pval_f in interesting_pval_fields:

            # sort df
            df_pval = df_all.sort_values(by=[pval_f, "group_name"])[ID_fields  + [pval_f, "group_name"]].rename(columns={pval_f:"pvalue"})

            # add the expecte p value under a uniform distribution
            df_pval["expected_pvalue_uniformly_distributed"] = np.linspace(1/len(df_pval), 1, len(df_pval))

            # add the expecte p value under a uniform distribution considering all groups
            df_pval["expected_pvalue_uniformly_distributed_all_groups"] = sorted_pvals_all_groups_uniformly_distributed[0:len(df_pval)]


            # add the expected p value from sorted_pvals_fisher_random
            df_pval["expected_pvalue"] = sorted_pvals_fisher_random


            # add the number of groups
            df_pval["n_groups_wpvalue"] = len(df_pval)

            # add to df
            df_pval["pval_field"] = pval_f
            df_long = df_long.append(df_pval)

        return df_long

    df_gwas_long = pd.concat(map(get_long_gwas_df_oneID, map(lambda x: x[1], df_gwas.groupby(ID_fields))))


    return df_gwas_long

def gwas_QQ_plot_all_data(spp_drug_to_gwas_df_file, ProcessedDataDir, PlotsDir, threads, min_npheno_transitions, DataDir, expected_pval_field, optimal_filters_df, type_filters):

    """Plots the QQ plot for all GWAS. Only for ASR_methods_phenotypes and the species-drug with enough data (min_npheno_transitions). Each row would be a species-drug and the columns would be combinations of pval type and gwas method """

    # define the pval fields
    all_pval_fields = ["%s_%s"%(pval_seed, pval_m) for pval_seed in ["pval_chi_square", "pval_GenoAndPheno"] for pval_m in  ["RelToBranchLen", "phenotypes"]] 

    ####### GET DATA #########

    # define a pseudocount for the pval
    pseudocount = 1/1e5

    # get file
    df_pvals_file = "%s/QQ_plot_pvals_df_%i_each_spp_drug_optimal_filters_%s.py"%(ProcessedDataDir, min_npheno_transitions, type_filters)

    if file_is_empty(df_pvals_file):

        # debug
        #spp_drug_to_gwas_df_file = {x:y for x,y in spp_drug_to_gwas_df_file.items() if x==("Candida_auris", "VRC")}

        # get the concatenated data for each df
        inputs_fn = [(spp, drug, tab_file, min_npheno_transitions, DataDir, optimal_filters_df.loc["%s-%s"%(spp, drug)]) for (spp,drug), tab_file in spp_drug_to_gwas_df_file.items() if "%s-%s"%(spp, drug) in set(optimal_filters_df.index)]
        print("running %i jobs"%(len(inputs_fn)))

        with multiproc.Pool(threads) as pool:
            df_pvals = pd.concat(pool.starmap(get_df_pvals_QQ_plot_one_spp_and_drug, inputs_fn)).reset_index(drop=True)
            pool.close()
            pool.terminate()

        # add fields
        print("adding fields")
        df_pvals["spp_and_drug"] = df_pvals.species + "-" + df_pvals.drug
        df_pvals["gwas_method_and_pvalf"] = df_pvals.gwas_method + "-" + df_pvals.pval_field
        df_pvals["types_vars_and_collapsing"] = df_pvals.type_collapsing + "-" + df_pvals.type_vars + "-" + df_pvals.type_mutations + "-" + df_pvals.type_genes

        df_pvals["minus_log10pval"] = -np.log10(df_pvals.pvalue + pseudocount)

        for f in ["expected_pvalue", "expected_pvalue_uniformly_distributed", "expected_pvalue_uniformly_distributed_all_groups"]:
            df_pvals["%s_minus_log10pval"%f] = -np.log10(df_pvals[f] + pseudocount)


        # save
        print("saving")
        save_object(df_pvals, df_pvals_file)

    print("loading df_pvals")
    df_pvals = load_object(df_pvals_file)

    ##########################

    ############ PLOT ##############

    # debug
    #df_pvals = df_pvals.set_index("species").loc["Candida_albicans"]

    # define the plots dir
    plots_dir = "%s/QQ_plots_optimal_filters_%s"%(PlotsDir, type_filters)
    make_folder(plots_dir)

    # go through different types of collapsing
    #for type_collapsing in ["Reactome", "GO", "MetaCyc", "domains", "none", "genes"]:
    for type_collapsing in ["none"]:
        print(type_collapsing)

        # filter
        print("filtering")
        df_plot = df_pvals[df_pvals.type_collapsing==type_collapsing]

        # keep only data with some calculated p vals
        print("filtering min_n_groups_wpvalue")
        min_n_groups_wpvalue = 100
        df_plot = df_plot[df_plot.n_groups_wpvalue>=min_n_groups_wpvalue]
        if len(df_plot)==0: raise ValueError("no groups in df_plot")

        # get plot
        print("getting plot")
        g = sns.relplot(data=df_plot, kind="scatter", row="spp_and_drug", col="gwas_method_and_pvalf", x="%s_minus_log10pval"%expected_pval_field, y="minus_log10pval", hue="types_vars_and_collapsing", alpha=0.3, palette="tab20", s=4, edgecolor="none", height=1.8, aspect=1.6) # aspect=1, height=1

        # change the legend location
        #leg = g._legend
        #leg.set_bbox_to_anchor([0, 0])  # coordinates of lower left of bounding box
        #leg._loc = 1  # if required you can set the loc (upper right)
        #plt.legend([],[], frameon=False)
        g._legend.remove()

        # define the maxium
        max_minus_logp = max([max(df_plot[f]) for f in ["%s_minus_log10pval"%expected_pval_field, "minus_log10pval"]])
        ncols = len(set(df_plot.gwas_method_and_pvalf))
        nrows = len(set(df_plot.spp_and_drug))

        # edit each subplot
        print("editing subplots")
        for ax in g.axes.flat:

 
            # define the values
            row_val, col_val = [v.split("=")[1].rstrip().lstrip() for v in ax.get_title().split("|")]
            gwas_m, pval_f = col_val.split("-")

            # deffine the filters series
            filters_series = optimal_filters_df.loc[row_val]


            # ylabel
            if ax.colNum==0:
                rowID = "%s-%s"%(row_val.split("-")[0].split("_")[1][0:3], row_val.split("-")[1])
                if ax.rowNum==5: ax.set_ylabel("observed -log(p)\n%s"%rowID)
                else: ax.set_ylabel(rowID)

            # xlabel
            if ax.rowNum==(nrows-1):

                gwasm_to_str = {"phyC":"pC", "synchronous":"sy"}
                pval_f_to_str = {"pval_chi_square_RelToBranchLen":"X2_gt", "pval_chi_square_phenotypes":"X2_ph", "pval_GenoAndPheno_RelToBranchLen":"g&p_gt", "pval_GenoAndPheno_phenotypes":"g&p_ph"}
                colID = "%s-%s"%(gwasm_to_str[col_val.split("-")[0]], pval_f_to_str[pval_f])
                if ax.colNum==3: ax.set_xlabel("%s\nexpected -log(p) (%s)"%(colID, expected_pval_field))
                else: ax.set_xlabel(colID)

            if ax.rowNum==0 and ax.colNum==3: ax.set_title("collapsing=%s; species-drug (rows) & gwas method-p value type (columns); %s"%(type_collapsing, expected_pval_field))
            else: ax.set_title("")

            ax.set_xlim([0, max_minus_logp+0.2])
            ax.set_ylim([0, max_minus_logp+0.2])

            # add a text box if this pvalue was not considered
            center_pos = (max_minus_logp+0.2)/2
            if filters_series[pval_f] is False: ax.text(center_pos, center_pos, "X", size=16)
            else:

                # add a straight line
                ax.plot([0, max_minus_logp], [0, max_minus_logp], color="red", linestyle="--", linewidth=.7)

                # add lines for the pvalue threshold
                ax.axhline(-np.log10(filters_series.alpha_pval+pseudocount), color="gray", linestyle="--", linewidth=.7)

            # add a text with the used metadata
            if ax.colNum==(ncols-1):

                text_corr = {"none":"no cor", "bonferroni":"bonf", "fdr_bh":"fdr"}[filters_series.correction_method]
                if filters_series.correction_method=="fdr_bh": text_corr += " (%s)"%filters_series.fdr_threshold

                text_asr = {"DOWNPASS":"MP", "MPPA,DOWNPASS":"MP&ML", "MPPA":"ML"}[filters_series.ASR_methods_phenotypes]
                text_str = "\n".join([text_corr, "ASR by %s"%text_asr, "sup>=%i; p<%s; e>=%s"%(filters_series.min_support, filters_series.alpha_pval, filters_series.min_epsilon)])


                ax.text(max_minus_logp+0.6, center_pos, text_str, ha="left",va="center")

        plt.show()

        filename = "%s/QQ_plots_collapsing=%s_%s.pdf"%(plots_dir, type_collapsing, expected_pval_field)
        print("saving %s"%filename)
        g.savefig(filename)

    ################################

def one_set_is_subset_of_the_other(set_1, set_2):

    """Takes two sets and checks if one of the sets is a subset of the other"""

    # checks
    if len(set_1)==0 or len(set_2)==0: raise ValueError("some empty sets")
    if set_1==set_2: raise ValueError("sets can't be the same")


    # deefine and return
    set_1_has_unique_data = len(set_1.difference(set_2))>0
    set_2_has_unique_data = len(set_2.difference(set_1))>0

    if sum([set_1_has_unique_data, set_2_has_unique_data])==1: return True
    else: return False


def get_whether_string_is_subset_of_another(x, y):

    """Returns the string that is a subset of the other"""

    if x in y or y in x: return True
    else: return False

def get_get_df_gwas_filtered_and_redundancy_removed_one_spp_and_drug(spp, drug, tab_file, min_npheno_transitions, min_OR, all_pval_fields, DataDir, ProcessedDataDir, optimal_filters_series, type_filters, type_muts, gff, gene_features_df_s):

    """Runs get_df_gwas_filtered_and_redundancy_removed for one species and drug. These are the types of data. 
    for type_vars, type_vars_set in list_types_vars:
        for type_genes in ["all_genes", "only_protein_coding", "only_non_coding"]:
           for type_mutations in ["all_muts", "syn_muts", "non_syn_muts", "non_syn_non_truncating_muts", "truncating_muts"]
               for type_collapsing in ["none", "genes", "domains", "GO", "MetaCyc", "Reactome"]:                         
    """

    print("\n", spp, drug)

    # load gwas
    df_gwas = get_tab_as_df_or_empty_df(tab_file)
    print("gwas loaded")

    # define the GWAS dir
    gwas_dir = "%s/%s_%i/ancestral_GWAS_drugResistance/GWAS_%s_resistance"%(DataDir, spp, sciName_to_taxID[spp], drug)

    ####### FILTERING #######

    # check the min_npheno_transitions
    n_pheno_transitions_set = set(df_gwas[(df_gwas.ASR_methods_phenotypes=="MPPA,DOWNPASS") & (df_gwas.gwas_method=="synchronous") & (df_gwas.min_support==0)].nodes_withPheno)
    
    if len(n_pheno_transitions_set)==0: 
        print("WARNING: Skipping %s-%s because it has 0 transitions"%(spp, drug))
        print("returning df")
        return pd.DataFrame()

    if len(n_pheno_transitions_set)!=1: raise ValueError("there are more than one phenotype transitions")
    if next(iter(n_pheno_transitions_set))<min_npheno_transitions: 
        print("WARNING: Skipping %s-%s because it has %i transitions"%(spp, drug, next(iter(n_pheno_transitions_set))))
        print("returning df")
        return pd.DataFrame()


    # keep only synchronous
    df_gwas = df_gwas[df_gwas.gwas_method=="synchronous"]

    # get the filtered gwas as done in the function for benchmaring get_filtering_stats_df_one_species_drug_gwas_method_consistency_btw_pvals
    optimal_filters_series["min_OR"]  = min_OR
    df_gwas =  get_filtered_gwas_af_df_consistency_btw_pvals(optimal_filters_series, df_gwas)
    print("There are %i total lines in df_gwas"%(len(df_gwas)))

    # keep only non-syn vars and genes
    if type_muts=="only_non_syn":
        print("keeping only prot-altering vars")

        # define the protein coding genes
        """
        gff_df = load_gff3_intoDF(gff)
        pseudogenes = set(gene_features_df_s[gene_features_df_s.feature_type.isin({"pseudogene", "pseudogene|Uncharacterized"})].gff_upmost_parent)
        protein_coding_genes = set(gff_df[gff_df.feature.isin({"CDS", "mRNA"})].upmost_parent).difference(pseudogenes)
        """

        # get the tab file that greps to none,
        tab_file_no_collapsing = "%s/gwas_df_no_collapsing_%s_%s.tab"%(ProcessedDataDir, spp, drug)
        get_tab_file_only_lines_matching_patterns(['none'], tab_file, tab_file_no_collapsing)
        df_gwas_no_collapsing = get_tab_as_df_or_empty_df(tab_file_no_collapsing); df_gwas_no_collapsing = df_gwas_no_collapsing[df_gwas_no_collapsing.type_collapsing=="none"]

        # define the variants that are protein altering
        non_syn_vars = set()
        for target_type_mutations in ["non_syn_muts", "truncating_muts", "non_syn_non_truncating_muts"]:

            df_gene_alteration = get_gwas_df_no_collapsing_with_collapsing_info(df_gwas_no_collapsing, DataDir, 1, '%s/%s_%s_%s_gwas_mapping_muts_to_genes_nonsyn_considering_all_genes'%(ProcessedDataDir, spp, drug, target_type_mutations), target_type_collapsing="genes", target_type_vars="all_vars", target_type_mutations=target_type_mutations, target_type_genes="all_genes", run_in_parallel=False)
            non_syn_vars.update(set(df_gene_alteration.group_name))

        # define a function that says if the row is nonsyn
        def row_is_non_syn(r):

            if r.type_collapsing=="none":


                # define based on the mutations df
                if r.group_name in non_syn_vars: return True
                else: return False

            else:

                # only keep non-syn vars
                if r.type_mutations in {"non_syn_muts", "truncating_muts", "non_syn_non_truncating_muts"}: return True
                elif r.type_mutations in {"all_muts"}: return False
                else: raise ValueError("invalid type_mutations")


        print("filtering out non coding alterations")
        df_gwas = df_gwas[df_gwas.apply(row_is_non_syn, axis=1)]
        print("%s-%s. There are %i associated groups after filtering non-syn alterations"%(spp, drug, len(df_gwas)))

    elif type_muts=="all": pass
    else: raise ValueError("invalid type_muts")


    # debug
    if len(df_gwas)==0: 
        print("WARNING: %s-%s has 0 significant results"%(spp, drug))
        print("returning df")
        return pd.DataFrame()

    # define the used pval fields
    used_pval_fields = [f for f in all_pval_fields if optimal_filters_series[f] is True]
    if len(used_pval_fields)==0: raise ValueError("there have to be some pval fields")

    # add the max pval
    df_gwas["max_pval"] = df_gwas[used_pval_fields].apply(max, axis=1)

    # define the raw filtered df
    df_gwas_filt_raw = cp.deepcopy(df_gwas)

    #########################

    ######## MAP EACH GROUP TO THE VARIANTS ########

    # define dicts that define the level of specificity of the variants
    type_vars_to_level_specificity = {"all_vars": 3,
                                      "SVs_and_CNVs":2, "small_vars_and_SVs":2, "small_vars_and_CNVs":2,
                                      "small_vars":1, "SVs":1, "coverageCNVs":1}

    type_collapsing_to_level_specificity = {"MetaCyc":4, "Reactome":4, "GO":4,
                                            "genes":3, 
                                            "domains":2,
                                            "none":1}

    type_mutations_to_level_specificity = {"all_muts": 3,
                                           "syn_muts": 2, "non_syn_muts": 2,
                                           "non_syn_non_truncating_muts": 1.5, "truncating_muts": 1} # we put 1.5 in non_syn_non_truncating_muts because truncating are more relevant

    # add the variant type ID
    var_type_ID_fields = ["type_vars", "type_genes", "type_mutations", "type_collapsing"]
    df_gwas["var_type_ID"] = df_gwas[var_type_ID_fields].apply(lambda r: "-".join(r), axis=1)

    # load the variant type
    varTypeID_group_vars_df_file = "%s/%s_%s_varTypeID_group_vars_df_GWAS.py"%(ProcessedDataDir, spp, drug)
    varTypeID_group_vars_df = load_object(varTypeID_group_vars_df_file)

    varTypeID_group_vars_df = varTypeID_group_vars_df[(varTypeID_group_vars_df.var_type_ID.isin(set(df_gwas.var_type_ID))) & (varTypeID_group_vars_df.group_name.isin(set(df_gwas.group_name)))] # keep important lines
    varTypeID_group_vars_df["type_vars"] = varTypeID_group_vars_df.var_type_ID.apply(lambda x: x.split("-")[0])
    varTypeID_group_vars_df["type_genes"] = varTypeID_group_vars_df.var_type_ID.apply(lambda x: x.split("-")[1])
    varTypeID_group_vars_df["type_mutations"] = varTypeID_group_vars_df.var_type_ID.apply(lambda x: x.split("-")[2])
    varTypeID_group_vars_df["type_collapsing"] = varTypeID_group_vars_df.var_type_ID.apply(lambda x: x.split("-")[3])
    varTypeID_group_vars_df["all_vars_tuple"] = varTypeID_group_vars_df.all_vars.apply(sorted).apply(tuple)

    # create the level of specificity
    varTypeID_group_vars_df["type_mutations_level_spec"] = varTypeID_group_vars_df.type_mutations.map(type_mutations_to_level_specificity); check_no_nans_series(varTypeID_group_vars_df.type_mutations_level_spec)

    # if there are only none vars
    if set(df_gwas.var_type_ID)=={'all_vars-all_genes-all_muts-none'}:

        df_gwas["all_vars"] = [set()]*len(df_gwas)
        df_gwas["real_type_mutations"] = df_gwas.type_mutations

    else:

        # add the real_type_mutations
        def get_varTypeID_group_vars_df_one_type_vars_and_collapsing_and_group_wTypeMutations(df):

            # if there is only one df
            if len(df)==1: 
                df["real_type_mutations"] = df.type_mutations
                return df

            # if there are different levels of specificity and unique
            elif len(df)==len(set(df.type_mutations_level_spec)) and len(df)<=4: 
                df["real_type_mutations"] = df.sort_values(by="type_mutations_level_spec", ascending=True).iloc[0].type_mutations
                return df

            else:

                print(spp, drug)
                print(df[["type_mutations", "all_vars_tuple", "group_name"]])
                raise ValueError("something went wrong in get_varTypeID_group_vars_df_one_type_vars_and_collapsing_and_group_wTypeMutations")

        varTypeID_group_vars_df = pd.concat(map(lambda x: get_varTypeID_group_vars_df_one_type_vars_and_collapsing_and_group_wTypeMutations(x[1]), varTypeID_group_vars_df.groupby(["type_vars", "type_genes", "type_collapsing", "group_name", "all_vars_tuple"])))
        check_no_nans_series(varTypeID_group_vars_df.real_type_mutations)

        # add the all_vars
        inital_len_df_gwas = len(df_gwas)
        df_gwas = df_gwas.merge(varTypeID_group_vars_df[["var_type_ID", "group_name", "all_vars", "real_type_mutations"]], on=["var_type_ID", "group_name"], validate="many_to_one", how="left")
        if len(df_gwas)!=inital_len_df_gwas: raise ValueError("merge changed len")
        if any(pd.isna(df_gwas[df_gwas.type_collapsing!="none"].all_vars)): raise ValueError("nas in all_vars")

        def get_all_vars_set(x):
            if pd.isna(x): return set()
            else: return x
        df_gwas["all_vars"] = df_gwas.all_vars.apply(get_all_vars_set) # get all sets


    ################################################

    # add the variant type w no collapsing
    df_gwas["var_type_ID_without_collapsing"] = df_gwas[["type_vars", "type_genes", "type_mutations"]].apply(lambda r: "-".join(r), axis=1)

    # init the final df with redundancy removed
    df_gwas_NR_all = pd.DataFrame()

    # process each gwas method differently
    for gwas_method in sorted(set(df_gwas.gwas_method)):
        ID_gwas_m = "%s-%s-%s"%(spp, drug, gwas_method)

        # get df with this method, which will be constantly trimmed
        df_gwas_m = df_gwas[df_gwas.gwas_method==gwas_method]
        print("%s. There are %i associated groups"%(ID_gwas_m, len(df_gwas_m)))

        # if there are no results, continue
        if len(df_gwas_m)==0: continue

        # checks
        if len(df_gwas_m[["group_name", "var_type_ID"]].drop_duplicates())!=len(df_gwas_m): raise ValueError("the combination of group_name and varID should be unique")


        #### REMOVE GROUPS WHERE THERE IS AN INTERNAL SUBGROUP THAT IS SIGNIFICANT ##########

        # Genes where there is a domain of that gene or pathways where there is a gene or a domain siginificant should be discarded

        # map each pathway collapsing to the group names and the genes
        df_MetaCyc = load_object("%s/%s_%i/ancestral_GWAS_drugResistance/df_metacyc_per_gene_with_all_parents.py"%(DataDir, spp, sciName_to_taxID[spp]))
        df_Reactome = load_object("%s/%s_%i/ancestral_GWAS_drugResistance/df_reactome_per_gene_with_all_parents.py"%(DataDir, spp, sciName_to_taxID[spp]))
        df_GO = load_object("%s/%s_%i/ancestral_GWAS_drugResistance/df_GOterms_per_gene.py"%(DataDir, spp, sciName_to_taxID[spp]))
        type_collapsing_to_group_to_genes = {type_col : dict(df.groupby("pathway_ID").apply(lambda x: set(x.Gene))) for type_col, df in {"MetaCyc":df_MetaCyc, "Reactome":df_Reactome, "GO":df_GO.rename(columns={"GO":"pathway_ID"})}.items()}

        # map each gene to the domains
        gene_to_domains = dict(load_object("%s/domains_annot_df.py"%gwas_dir).groupby("Gene").apply(lambda x: set(x.domain_ID)))
        missing_genes = set(df_gwas_m[df_gwas_m.type_collapsing=="genes"].group_name).difference(set(gene_to_domains))
        missing_genes.update(set.union(*[set.union(*g_to_gs.values()) for g_to_gs in type_collapsing_to_group_to_genes.values()]).difference(set(gene_to_domains))) # some genes have no domains annotation because they had no variants. They are added as missing but will never be considered
        gene_to_domains = {**gene_to_domains, **{g:set() for g in missing_genes}}
        
        # add to the gwas df the expected genes and domains
        def get_genes_in_group(r):
            if r.type_collapsing in {"none", "domains", "genes"}: return set()
            elif r.type_collapsing in {"GO", "MetaCyc", "Reactome"}: return type_collapsing_to_group_to_genes[r.type_collapsing][r.group_name]
            else: raise ValueError("invalid %s"%r)
        df_gwas_m["all_genes_in_group"] = df_gwas_m[["type_collapsing", "group_name"]].apply(get_genes_in_group, axis=1)

        def get_domains_in_group(r):
            if r.type_collapsing in {"none", "domains"}: return set()
            elif r.type_collapsing=="genes": return gene_to_domains[r.group_name]
            elif r.type_collapsing in {"GO", "MetaCyc", "Reactome"}: return set.union(*map(lambda g: gene_to_domains[g], r.all_genes_in_group))
            else: raise ValueError("invalid %s"%r)
        df_gwas_m["all_domains_in_group"] = df_gwas_m[["type_collapsing", "group_name", "all_genes_in_group"]].apply(get_domains_in_group, axis=1)

        # discard pathways that have associated genes
        associated_genes = set(df_gwas_m[df_gwas_m.type_collapsing=="genes"].group_name)
        df_gwas_m = df_gwas_m[df_gwas_m.all_genes_in_group.apply(lambda x: x.intersection(associated_genes)).apply(len)==0]
        print("%s. There are %i associated groups after filtering lower genes"%(ID_gwas_m, len(df_gwas_m)))

        # discard genes and pathways that have significant domains
        associated_domains = set(df_gwas_m[df_gwas_m.type_collapsing=="domains"].group_name)
        df_gwas_m = df_gwas_m[df_gwas_m.all_domains_in_group.apply(lambda x: x.intersection(associated_domains)).apply(len)==0]
        print("%s. There are %i associated groups after filtering lower domains"%(ID_gwas_m, len(df_gwas_m)))

        # remove some fields and return
        df_gwas_m.pop("all_domains_in_group")
        df_gwas_m.pop("all_genes_in_group")


        # debug
        #df_print = df_gwas_m[(df_gwas_m.group_name=='mobidb-lite#gene-B9J08_000401#301_324')]
        #print("mobidb-lite#gene-B9J08_000401#301_324 before vars", "\n", df_print[["type_vars", "type_mutations", "type_genes", "epsilon", "n_sig_pvals"]])

        #####################################################################################

        ######## REMOVE GROUPS WHERE THE VARIANT IS ALREADY SIGNIFICANT ###########

        # remove the collapsing groups where there is a variant already associated
        associated_vars = set(df_gwas_m[df_gwas_m.type_collapsing=="none"].group_name)
        df_gwas_m = df_gwas_m[(df_gwas_m.type_collapsing=="none") | (df_gwas_m.all_vars.apply(lambda x: x.intersection(associated_vars)).apply(len)==0)]
        print("%s. There are %i associated groups after filtering groups w/ lower vars"%(ID_gwas_m, len(df_gwas_m)))

        # debug
        #df_print = df_gwas_m[(df_gwas_m.group_name=='mobidb-lite#gene-B9J08_000401#301_324')]
        #print("mobidb-lite#gene-B9J08_000401#301_324 after vars", "\n", df_print[["type_vars", "type_mutations", "type_genes", "epsilon", "n_sig_pvals", "max_pval"]])

        ###########################################################################

        ####### REMOVE REDUNDANT GROUP NAMES ###########

        # create the level of specificity for type_vars
        df_gwas_m["type_vars_level_spec"] = df_gwas_m.type_vars.map(type_vars_to_level_specificity); check_no_nans_series(df_gwas_m.type_vars_level_spec)
        df_gwas_m["type_mutations_level_spec"] = df_gwas_m.type_mutations.map(type_mutations_to_level_specificity); check_no_nans_series(df_gwas_m.type_mutations_level_spec)

        # define the sorting fields
        sorting_fields = ["epsilon", "OR", "type_vars_level_spec", "type_mutations_level_spec"] + used_pval_fields
        sorting_fields_ascending = [False, False, True, True] + [True]*len(used_pval_fields)


        # keep the fewest possible number of rows for repeated groups
        def get_df_gwas_m_one_combination_type_collapsing_group_name_NR(df):

            # if there is only one row return it
            if len(df)==1: return df

            # sort by epsilon, OR and p values
            df = df.sort_values(by=sorting_fields, ascending=sorting_fields_ascending)

            # get the df that has the sorting values of the first row (this could have more than 1 row)
            first_r = df.iloc[0]
            df = df[df[sorting_fields].apply(lambda r: all(map(lambda f: r[f]==first_r[f], sorting_fields)), axis=1)]

            # if there is already only one row, return it
            if len(df)==1: return df

            else: 
                print(df[["epsilon", "max_pval", "type_vars", "type_genes", "type_mutations", "group_name", "real_type_vars"]])
                #for x in df.all_vars: print("\n", "vars_row", sorted(x), "\n")
                vars1 = df.all_vars.iloc[0].difference(df.all_vars.iloc[1])
                vars2 = df.all_vars.iloc[1].difference(df.all_vars.iloc[0])
                print(vars1, "\n", vars2)

                raise ValueError("some unconsidered situations here")

        df_gwas_m = pd.concat(map(lambda x: get_df_gwas_m_one_combination_type_collapsing_group_name_NR(x[1]), df_gwas_m.groupby(["type_collapsing", "group_name"]))).reset_index(drop=True)
        print("%s. There are %i associated groups after filtering redundant groups"%(ID_gwas_m, len(df_gwas_m)))

        ################################################

        ######## REMOVE DOMAINS WITH OVERLAPPING MUTATIONS ################

        # define the gwas dfs with
        df_gwas_m_domains = df_gwas_m[df_gwas_m.type_collapsing=="domains"]
        if len(df_gwas_m_domains)>0:
            print("filtering out redundant domains")

            # add the domain name
            df_gwas_m_domains["domain_name"] = df_gwas_m_domains.group_name.apply(lambda x: x.split("#")[0])

            # load interpro df
            taxID_dir =  "%s/%s_%i"%(DataDir, spp, sciName_to_taxID[spp])
            df_interpro = load_InterProAnnotation("%s/InterproScan_annotation/interproscan_annotation.out"%taxID_dir)
            df_interpro = df_interpro[df_interpro.signature_accession.isin(set(df_gwas_m_domains.domain_name))]

            # add fields
            df_gwas_m_domains["altered_gene"] = df_gwas_m_domains.group_name.apply(lambda x: x.split("#")[1])
            df_gwas_m_domains["list_range"] = df_gwas_m_domains.group_name.apply(lambda x: list(map(int, x.split("#")[2].split("_"))))
            df_gwas_m_domains["domain_start"] = df_gwas_m_domains.list_range.apply(lambda x: x[0])
            df_gwas_m_domains["range_protein_covered"] = df_gwas_m_domains.list_range.apply(lambda x: x[1]-x[0])
            if any(df_gwas_m_domains.range_protein_covered<=0): raise ValueError("there should be no proteins with no range covered")

            # add the type analysis field
            sigAcc_to_type_analysis = dict(df_interpro[["type_analysis", "signature_accession"]].drop_duplicates().set_index("signature_accession").type_analysis)
            sorted_types_analysis =  ["chunk", "PIRSF", "CDD", "MobiDBLite", "Hamap", "Coils", "PRINTS", "Gene3D", "SUPERFAMILY", "ProSiteProfiles", "ProSitePatterns", "SMART", "SFLD", "TIGRFAM", "Pfam" , "PANTHER"]
            type_analysis_to_importance_idx = dict(zip(sorted_types_analysis, range(0, len(sorted_types_analysis))))

            def get_domain_type(gname):
                if gname.startswith("pChunk_"): return 'chunk'
                else: return sigAcc_to_type_analysis[gname.split("#")[0]]

            df_gwas_m_domains["domain_type"] = df_gwas_m_domains.group_name.apply(get_domain_type) # the higher the number the better
            df_gwas_m_domains["domain_type_importance_idx"] = df_gwas_m_domains.domain_type.apply(lambda x: type_analysis_to_importance_idx[x])

            # define the sorting fields. Take the domains with least importance 
            sorting_fields_domains = sorting_fields + ["domain_type_importance_idx", "range_protein_covered", "domain_start"]
            sorting_fields_ascending_domains = sorting_fields + [False, False, True]

            # define a mapping between known domain names and the result to take
            tuple_domains_to_domain = {}

            # get a set with the non-redundant domains
            def get_set_nr_domains_one_gene(df_g):

                # change index
                df_g = df_g.set_index("group_name", drop=False)

                # check
                if len(df_g)!=len(set(df_g.index)): raise ValueError("the group names should be unique")

                # map each ID to the overlapping IDs
                all_domains = sorted(set(df_g.index))
                ID_to_overlappingIDs = dict(zip(all_domains, map(lambda d : set(df_g[df_g.all_vars.apply(lambda x: x.intersection(df_g.loc[d, "all_vars"])).apply(len)>0].index), all_domains)))

                # go through each cluster of overlapping domains to keep only one
                g_nr_domains = set()
                for cluster_IDs in get_list_clusters_from_dict(ID_to_overlappingIDs):

                    df_ID = df_g.loc[cluster_IDs].sort_values(by=sorting_fields_domains, ascending=sorting_fields_ascending_domains)
                    first_r = df_ID.iloc[0]
                    df_ID = df_ID[df_ID.apply(lambda r: all([r[f]==first_r[f] for f in sorting_fields_domains]), axis=1)]

                    # if you already have only one domain
                    if len(df_ID)==1: g_nr_domains.add(df_ID.group_name.iloc[0])

                    else:  

                        # define the tuple of domains and the types analyses
                        tuple_domains = tuple(sorted(df_ID.group_name))
                        all_types_analyses = set(df_ID.domain_type)


                        # if you have two domains manually curated
                        if tuple_domains in tuple_domains_to_domain: g_nr_domains.add(tuple_domains_to_domain[tuple_domains])

                        # PANTHER analyses where there are two types of domains and one is the subset of the other. This type: PTHR23423#gene-B9J08_002446#12_369', 'PTHR23423:SF10#gene-B9J08_002446#12_369
                        elif all_types_analyses=={'PANTHER'} and len(tuple_domains)==2 and  len(tuple_domains[1])>len(tuple_domains[0]) and (tuple_domains[0].split("#")[0]) in (tuple_domains[1].split("#")[0]) and (tuple_domains[0].split("#")[0])==(tuple_domains[1].split("#")[0].split(":")[0]): g_nr_domains.add(tuple_domains[1])


                        else: 

                            print(len(set(df_ID["all_vars"].apply(sorted).apply(tuple))), "set of vars")
                            print("!!!!!!!!!!!!!!!!!!!!!!", spp, drug, tuple_domains, all_types_analyses)
                            raise ValueError("df_ID should have 1 line")

                return g_nr_domains

            nr_domains = set.union(*map(lambda x: get_set_nr_domains_one_gene(x[1]), df_gwas_m_domains.groupby("altered_gene")))

        else: nr_domains = set()

        # chaeck
        strange_domains = nr_domains.difference(set(df_gwas_m.group_name))
        if len(strange_domains)>0: raise ValueError("there are strange domains: %s"%strange_domains)

        # filter
        df_gwas_m = df_gwas_m[(df_gwas_m.type_collapsing!="domains") | (df_gwas_m.group_name.isin(nr_domains))]
        print("%s. There are %i associated groups after filtering redundant domains"%(ID_gwas_m, len(df_gwas_m)))


        ###################################################################

        # get the all vars tuple
        df_gwas_m["all_vars_tuple"] = df_gwas_m.all_vars.apply(sorted).apply(tuple)

        # remove vars
        for f in ["all_vars", "type_vars_level_spec", "type_mutations_level_spec"]: df_gwas_m.pop(f)

        # add
        df_gwas_NR_all = df_gwas_NR_all.append(df_gwas_m)

    print("returning df")
    return df_gwas_NR_all

def get_df_gwas_filtered_and_redundancy_removed(spp_drug_to_gwas_df_file, ProcessedDataDir, PlotsDir, threads, min_npheno_transitions, min_OR, DataDir, optimal_filters_df, type_filters, type_muts, species_to_gff, gene_features_df):

    """Gets the filtered GWAS with the redundancy removed."""

    # define the pval fields
    all_pval_fields = ["%s_%s"%(pval_seed, pval_m) for pval_seed in ["pval_chi_square", "pval_GenoAndPheno"] for pval_m in  ["RelToBranchLen", "phenotypes"]] 

    ####### GET DATA #########

    # get file
    df_gwas_filt_file = "%s/df_gwas_filt_redundancy_removed_%i_each_spp_drug_optimal_filters_%s_filters_%s_muts.py"%(ProcessedDataDir, min_npheno_transitions, type_filters, type_muts)
    if file_is_empty(df_gwas_filt_file):

        # debug
        #spp_drug_to_gwas_df_file = {x:y for x,y in spp_drug_to_gwas_df_file.items() if x==("Candida_auris", "VRC")}
        #spp_drug_to_gwas_df_file = {x:y for x,y in spp_drug_to_gwas_df_file.items() if x==("Candida_albicans", "FLC")}
        #spp_drug_to_gwas_df_file = {x:y for x,y in spp_drug_to_gwas_df_file.items() if x==("Candida_auris", "AMB")}

        # get the concatenated data for each df
        print("running get_get_df_gwas_filtered_and_redundancy_removed_one_spp_and_drug in parallel")
        inputs_fn = [(spp, drug, tab_file, min_npheno_transitions, min_OR, all_pval_fields, DataDir, ProcessedDataDir, optimal_filters_df.loc["%s-%s"%(spp,drug)], type_filters, type_muts, species_to_gff[spp],  gene_features_df[(gene_features_df.species==spp)]) for (spp,drug), tab_file in spp_drug_to_gwas_df_file.items() if "%s-%s"%(spp,drug) in set(optimal_filters_df.index)]
        with multiproc.Pool(threads) as pool:
            df_gwas_filt = pd.concat(pool.starmap(get_get_df_gwas_filtered_and_redundancy_removed_one_spp_and_drug, inputs_fn)).reset_index(drop=True)
            pool.close()
            pool.terminate()    

        print("saving")
        save_object(df_gwas_filt, df_gwas_filt_file)


    return load_object(df_gwas_filt_file)



def plot_heatmap_number_significant_results_GWAS(PlotsDir, df_gwas_filt, gwas_method):

    """Plots the number of significant associated groups as a heatmap"""

    # get the df for this gwas method
    df_gwas_filt = df_gwas_filt[df_gwas_filt.gwas_method==gwas_method]

    # redefine the real_type_mutations
    def get_real_type_mutations_from_r(r):
        if r.type_collapsing=="none": return r.type_mutations
        else: return r.real_type_mutations
    df_gwas_filt["real_type_mutations"] = df_gwas_filt.apply(get_real_type_mutations_from_r, axis=1)

    # add the row and col IDs
    col_fields = ["species", "drug"]
    row_fields = ["real_type_vars", "type_genes", "real_type_mutations", "type_collapsing"]

    df_gwas_filt["colID"] = df_gwas_filt[col_fields].apply(lambda r: "-".join(r), axis=1)
    df_gwas_filt["rowID"] = df_gwas_filt[row_fields].apply(lambda r: "-".join(r), axis=1)

    # check
    if len(df_gwas_filt)!=len(df_gwas_filt[["colID", "rowID", "group_name"]].drop_duplicates()): raise ValueError("the combination of col, row and group_name should be unique")

    # add the number of data
    df_gwas_filt = df_gwas_filt[["colID", "rowID", "group_name"]].groupby(["rowID", "colID"]).apply(lambda df: pd.Series({"# groups":len(set(df.group_name)), "rowID":df.rowID.iloc[0], "colID":df.colID.iloc[0]})).reset_index(drop=True)

    # define the df plot
    square_df = df_gwas_filt[["rowID", "colID", "# groups"]].pivot(values="# groups", columns="colID", index="rowID").sort_index()

    # define the sorted columns
    species_to_ID = {"Candida_albicans":0, "Candida_auris":1, "Candida_glabrata":2}
    drug_to_ID= {"FLC":0, "ITR":0.1, "POS":0.2, "VRC":0.3, "ANI":1, "CAS":1.1, "MIF":1.2, "AMB":2}
    sorted_cols = sorted(square_df.columns, key=(lambda x: (species_to_ID[x.split("-")[0]], drug_to_ID[x.split("-")[1]])))
    square_df = square_df[sorted_cols]



    # define annotations
    def get_annot_for_cell(x):
        if pd.isna(x) or x>100: return ""
        else: return str(int(x))
    annot_df = square_df.applymap(get_annot_for_cell)

    # remove nans
    """
    def get_nans_as_0(x):
        if pd.isna(x): return 0
        else: return int(x)
    square_df = square_df.applymap(get_nans_as_0)
    """

    # define the col colors
    drug_to_color= {"FLC":"c", "ITR":"blue", "POS":"aquamarine", "VRC":"cyan", "AMB":"gray", "ANI":"magenta", "CAS":"red", "MIF":"salmon"}
    cols_tuple_list = [("species", species_to_color), ("drug", drug_to_color)]
    col_colors_df = pd.DataFrame({col : {name : color_dict[col.split("-")[I]] for I, (name, color_dict) in enumerate(cols_tuple_list)} for col in sorted(set(df_gwas_filt.colID))}).transpose().loc[square_df.columns]


    type_vars_to_color = {'SVs': 'navy', 'SVs_and_CNVs': 'c', 'all_vars': 'black', 'coverageCNVs': 'cyan', 'small_vars': 'red', 'small_vars_and_CNVs': 'purple', 'small_vars_and_SVs': 'magenta'}
    type_genes_to_color = {'all_genes': 'black', 'only_protein_coding': 'lime'}
    type_mutations_to_color = {'all_muts': 'black', 'non_syn_muts': 'salmon', 'non_syn_non_truncating_muts': 'blue', 'truncating_muts': 'lightgreen'}
    type_collapsing_to_color = {"none":"black", "domains":"khaki", "genes":"royalblue", "GO":"red", "MetaCyc":"olive", "Reactome":"gold"}
    rows_tuple_list = [("type_vars", type_vars_to_color), ("type_genes", type_genes_to_color), ("type_mutations", type_mutations_to_color), ("type_collapsing", type_collapsing_to_color)]
    row_colors_df = pd.DataFrame({row : {name : color_dict[row.split("-")[I]] for I, (name, color_dict) in enumerate(rows_tuple_list)} for row in sorted(set(df_gwas_filt.rowID))}).transpose().loc[square_df.index]


    # make plot
    cmap = "rocket_r"
    cm = sns.clustermap(square_df, col_cluster=False, row_cluster=False, cmap=cmap,  col_colors=col_colors_df, row_colors=row_colors_df, cbar_kws={"label":"# sig. groups"}, annot=annot_df, annot_kws={"size": 10}, fmt="", yticklabels=1) # , xticklabels=1    


    # re-position the panels
    hm_pos = cm.ax_heatmap.get_position()
    rect_width = hm_pos.width / len(square_df.columns)
    spacer = rect_width * 0.25
    hm_height = rect_width*len(square_df)
    cm.ax_heatmap.set_position([hm_pos.x0, cm.ax_col_colors.get_position().y0 - hm_height - spacer, hm_pos.width, hm_height])
    hm_pos = cm.ax_heatmap.get_position()

    rc_width = rect_width * len(row_colors_df.columns)
    cm.ax_row_colors.set_position([hm_pos.x0-rc_width-spacer, hm_pos.y0, rc_width, hm_pos.height])

    # get the ticjlabels
    #cm.ax_heatmap.set_yticklabels([row for row in square_df.index])
    cm.ax_heatmap.set_ylabel("<tyoe vars>-<type genes>-<type_mutations>-<type_collapsing>")

    # title
    cm.ax_col_colors.set_title("gwas_method=%s"%gwas_method)

    # add a legend for the row colors
    def get_lel(facecolor, label, edgecolor="gray"): return mpatches.Patch(facecolor=facecolor, edgecolor=edgecolor, label=label)
    legend_elements = make_flat_listOflists([([get_lel("white", "", edgecolor="white"), get_lel("white", f, edgecolor="white")] + [get_lel(color,label) for label,color in color_dict.items()]) for f, color_dict in rows_tuple_list])
    cm.ax_row_colors.legend(handles=legend_elements, loc="upper right", bbox_to_anchor=(0, 1))

    # add a legend for the col colors
    legend_elements = make_flat_listOflists([([get_lel("white", "", edgecolor="white"), get_lel("white", f, edgecolor="white")] + [get_lel(color,label) for label,color in color_dict.items()]) for f, color_dict in cols_tuple_list])    
    cm.ax_cbar.legend(handles=legend_elements, loc="lower right", bbox_to_anchor=(0, 0))




    plt.show()

    # save
    filename = "%s/GWAS_number_significant_groups_%s.pdf"%(PlotsDir, gwas_method)
    print("saving %s"%filename)
    cm.savefig(filename, bbox_inches="tight")


def write_excel_file_with_rows_all_same_color(df, filename, field_colors, fields_to_remove=[]):

    """Writes an excel where the colors are determined by the row field"""

    from openpyxl.styles import PatternFill, Font

    # define the tmp
    filename_tmp = "%s.tmp.xlsx"%filename

    # define the value to color
    all_vals = pd.unique(df[field_colors])
    value_to_color = get_value_to_color(all_vals, n=len(all_vals), palette="tab10", type_color="hex")[0]



    # edit the excel
    with pd.ExcelWriter(filename_tmp, engine="openpyxl") as writer:

        # define the sheet_name
        sheet_name = "sheet1"

        # Export DataFrame content
        df.to_excel(writer, sheet_name=sheet_name)

        # define the sheet
        sheet = writer.sheets[sheet_name]   

        # map the col_idx (0based) to the colfield
        Icol_to_field = dict(zip(range(len(df.columns)) , df.columns))


        # Set backgrund colors depending on cell values
        #for cell, in sheet[f'B2:B{len(df) + 1}']: # Skip header row, process as many rows as there are DataFrames

        # go through each row
        for Irow, row_tuple in enumerate(sheet):

            # define the real Irow
            real_Irow = Irow-1
            if real_Irow<0: continue

            # defnie the color
            color = value_to_color[df[field_colors].iloc[real_Irow]][1:]
            font = Font(name='Calibri', size=11, bold=True, italic=False, vertAlign=None,  underline='none', strike=False, color=color)


            # go through each col
            for Icol, cell in enumerate(row_tuple):

                # define the real Icol
                real_Icol = Icol-1
                if real_Icol<0 or real_Irow<0: continue

                # define the value
                #col_name = Icol_to_field[real_Icol]
                #value = df[col_name].iloc[real_Irow]

                # fill the color
                #cell.fill = PatternFill("solid", start_color=color) # change background color

                #  change the font color
                cell.font = font

                #cell.fill = PatternFill("solid", start_color=color, fgColor=get_matplotlib_color_as_hex("black")[1:])

                # =("5cb800" if value == "True" else 'ff2800'))

    os.rename(filename_tmp, filename)

def get_gwas_results_df_more_than_1_spp_OGs_filtering_NR_pathways(gwas_results_df_more_than_1_spp_OGs):

    """Takes a df with all the pathways / genes that are >1 dataset and returns it filtered so that there are no redundant pathways"""

    print("running get_gwas_results_df_more_than_1_spp_OGs_filtering_NR_pathways")

    # define types of data
    df_genes = gwas_results_df_more_than_1_spp_OGs[gwas_results_df_more_than_1_spp_OGs.type_collapsing.isin({"genes", "domains", "none"})]  
    df_pathways = gwas_results_df_more_than_1_spp_OGs[gwas_results_df_more_than_1_spp_OGs.type_collapsing.isin({"Reactome", "GO", "MetaCyc"})]
    if len(df_genes)+len(df_pathways)!=len(gwas_results_df_more_than_1_spp_OGs): raise ValueError("no correct sorting")
    if any(df_pathways.genes_in_pathway.apply(len)==0): raise ValueError("there are ps with 0 genes")

    # map each orthogroup to the combinations of datasets that are significant
    if any(df_genes.orthogroups.apply(lambda x: len(x.split(",")))!=1): raise ValueError("there are hits with >1 OG in genes. If this happens the code below makes no sense")
    og_to_tuple_sig_datasets = df_genes.groupby("orthogroups").apply(lambda df: tuple(sorted(set(df.species_and_drug))))
    df_genes["tuple_sig_datasets"] = df_genes.orthogroups.map(og_to_tuple_sig_datasets); check_no_nans_series(df_genes["tuple_sig_datasets"])

    # init the df
    df_pathways_NR  = pd.DataFrame()
    for pname in sorted(set(df_pathways.group_name)):

        # keep df for this pathway
        df_p = df_pathways[df_pathways.group_name==pname]
        if len(set(df_p.species_and_drug))<2: raise ValueError("error in df_p")
        #if len(df_p)!=len(set(df_p.species_and_drug)): raise ValueError("dataset is not unique")


        # get the df genes that has the same combination of datasets
        tuple_sig_datasets = tuple(sorted(set(df_p.species_and_drug)))
        df_g = df_genes[df_genes.tuple_sig_datasets==tuple_sig_datasets]

        # if in all spp_drug there is some overlap between the genes under the pathway and the genes that are hits, continue
        if len(df_g)>0:

            # create a list that states, for each species and drug, if there is some overlap between genes and pathways
            list_overlap_spp_drug = []
            for spp_drug in tuple_sig_datasets:

                genes_p = set.union(*df_p[df_p.species_and_drug==spp_drug].genes_in_pathway)
                genes_g = set.union(*df_g[df_g.species_and_drug==spp_drug].set_altered_genes)
                shared_genes = genes_p.intersection(genes_g)
                list_overlap_spp_drug.append(len(shared_genes)>0)

            if all(list_overlap_spp_drug): continue

        # append if not skipped
        df_pathways_NR = df_pathways_NR.append(df_p)

    return df_genes.append(df_pathways_NR)




def get_Table_GroupsGWAS(df_gwas_filt, DataDir, ProcessedDataDir, TablesDir, gene_features_df, designed_GWAS_filters_df, type_muts):

    """Gets a table with all GWAS results"""

    # make the folder
    make_folder(ProcessedDataDir)

    # redefine the real_type_mutations
    def get_real_type_mutations_from_r(r):
        if r.type_collapsing=="none": return 'all_muts'
        else: return r.real_type_mutations
    df_gwas_filt["real_type_mutations"] = df_gwas_filt.apply(get_real_type_mutations_from_r, axis=1)

    # change the real fields
    df_gwas_filt = df_gwas_filt.rename(columns={"real_type_vars":"type_vars", "real_type_mutations":"type_mutations"})

    # add final name
    def get_final_name_gene_features_df(r):
        if not pd.isna(r.gene_name): return r.gene_name
        elif not pd.isna(r.Scerevisiae_orthologs): return "Scer_%s"%(r.Scerevisiae_orthologs)
        else: return r.gff_upmost_parent
    gene_features_df["final_name"] = gene_features_df.apply(get_final_name_gene_features_df, axis=1)

    # init df
    gwas_results_df = pd.DataFrame()


    # define the interesting pval fields
    interesting_pval_fields = ['pval_chi_square_maxT', 'pval_epsilon_maxT', 'pval_chi_square_phenotypes', 'pval_GenoAndPheno_phenotypes', 'pval_fisher', 'pval_chi_square_phenotypes_bonferroni', 'pval_GenoAndPheno_phenotypes_bonferroni', 'pval_fisher_bonferroni']

    # go through each species
    for species in sorted(set(df_gwas_filt.species)):
        print(species)

        # define dir for this species
        taxID_dir = "%s/%s_%i"%(DataDir, species, sciName_to_taxID[species])

        # get df
        df_spp = df_gwas_filt[df_gwas_filt.species==species]
        df_gf = gene_features_df[gene_features_df.species==species]

        # get the variants with annotations
        associated_vars = set(df_spp[df_spp.type_collapsing=="none"].group_name)

        tmpdir_vars =  "%s/generating_GWAS_df_with_short_var_effect_%s_only_no_vars_%s"%(ProcessedDataDir, species, type_muts)
        df_vars_with_annot =  get_df_vars_with_short_variant_effect(associated_vars, DataDir, species, tmpdir_vars, set(df_gf.gff_upmost_parent).union({"-"}))

        geneID_to_name = dict(df_gf.set_index("gff_upmost_parent").final_name); geneID_to_name["-"] = "-"
        name_to_geneID = dict(df_gf.set_index("final_name").gff_upmost_parent); name_to_geneID["-"] = "-"
        geneID_to_description = dict(df_gf.set_index("gff_upmost_parent").description); geneID_to_description["-"] = "-"

        df_vars_with_annot["final_name"] = df_vars_with_annot.Gene.apply(lambda g: geneID_to_name[g])
        df_vars_with_annot["description"] = df_vars_with_annot.Gene.apply(lambda g: geneID_to_description[g])

        # keep one affected gene per variant
        df_vars_with_annot["consequences_set"] = df_vars_with_annot.Consequence.apply(lambda x: x.split(",")).apply(set)
        df_vars_with_annot["is_truncating"] = df_vars_with_annot.consequences_set.apply(get_is_truncating_from_consequences)
        df_vars_with_annot["is_synonymous"] = df_vars_with_annot.consequences_set.apply(get_is_synonymous_from_consequences)

        def get_df_vars_one_var_maximal_impact_df(df):

            # already df==1
            if len(df)==1: return df

            # if there are some truncating vars
            elif any(df.is_truncating): return df[df.is_truncating]

            # if there are some non syn vars
            elif any(~df.is_synonymous): return df[~(df.is_synonymous)]

            # if there is a mix
            else: return df

        df_vars_with_annot = pd.concat(map(lambda x: get_df_vars_one_var_maximal_impact_df(x[1]), df_vars_with_annot.groupby("variantID_across_samples")))

        # map each var to the effect as a string
        var_to_description = df_vars_with_annot.groupby("variantID_across_samples").apply(lambda df: "\n".join(df.apply(lambda r: "%s in %s (%s, %s)"%(r.short_variant_effect, r.final_name, r.Gene, r.description), axis=1)))

        # map each domain to the description
        df_interpro = load_InterProAnnotation("%s/InterproScan_annotation/interproscan_annotation.out"%taxID_dir)[["signature_accession", "signature_description", "InterPro_annotation_description", "type_analysis"]].drop_duplicates().set_index("signature_accession")
        if len(df_interpro)!=len(set(df_interpro.index)): raise ValueError("the index accesion should be unique")

        def get_description_one_domain(x):
            accession, geneID, location = x.split("#")
            location = location.replace("_", "-")
            if accession.startswith("pChunk"): return "region %s from gene %s (%s, %s)"%(location, geneID, geneID_to_name[geneID], geneID_to_description[geneID])
            else: 

                type_analysis = df_interpro.loc[accession, "type_analysis"]
                description = df_interpro.loc[accession, "signature_description"]
                description_IP = df_interpro.loc[accession, "InterPro_annotation_description"]
                if description!="-": dom_description = description
                else: dom_description = description_IP

                return "IP-signature %s (%s, %s) (region %s) from gene %s (%s, %s)"%(accession, type_analysis,  dom_description, location, geneID, geneID_to_name[geneID], geneID_to_description[geneID])

        all_domains = sorted(set(df_spp[df_spp.type_collapsing=="domains"].group_name))
        domain_to_description = dict(zip(all_domains, map(get_description_one_domain, all_domains)))

        # map pathway IDs to names
        df_reactome = pd.read_csv("%s/annotation_files/ReactomePathways.txt"%(DataDir), sep="\t", names=["ID", "description", "species"], header=None)
        reactome_ID_to_description = dict(df_reactome.set_index("ID").description)

        metacyc_ids_dir = "%s/%s_metacyc_ID_to_description_gwas_final_results_gwas_all_%s.py"%(ProcessedDataDir, species, type_muts)
        metacyc_ID_to_description = get_metacycID_to_description(sorted(set(df_spp[df_spp.type_collapsing=="MetaCyc"].group_name)), metacyc_ids_dir)

        obo_file = "%s/annotation_files/go-basic_30062021.obo"%(DataDir) # this was got from http://purl.obolibrary.org/obo/go/go-basic.obo
        obodag = GODag(obo_file,  optional_attrs={'consider', 'replaced_by'}, load_obsolete=True, prt=None)
        all_GOterms = sorted(set(df_spp[df_spp.type_collapsing=="GO"].group_name))
        name_space_to_ns = {"biological_process":"BP", "cellular_component":"CC", "molecular_function":"MF"}
        GO_ID_to_description = dict(zip(all_GOterms, map(lambda go: "%s (%s)"%(obodag[go].name, name_space_to_ns[obodag[go].namespace]), all_GOterms)))

        # add the description of the group, which is self explanatory
        type_collapsing_to_mapping_dict = {"none": var_to_description, "genes":geneID_to_description, "domains":domain_to_description, "GO":GO_ID_to_description, "Reactome":reactome_ID_to_description, "MetaCyc":metacyc_ID_to_description}

        def get_self_explanatory_group_description(r):
            description = type_collapsing_to_mapping_dict[r.type_collapsing][r.group_name]
            if r.type_collapsing in {"none", "domains"}: return description
            elif r.type_collapsing=="genes": return "%s (%s, %s)"%(r.group_name, geneID_to_name[r.group_name], description)
            elif r.type_collapsing in {"GO", "MetaCyc", "Reactome"}: return "%s-%s (%s)"%(r.type_collapsing, r.group_name, description)
            else: raise ValueError("invalid %s"%r)

        df_spp["description"] = df_spp[["type_collapsing", "group_name"]].apply(get_self_explanatory_group_description, axis=1)

        # go through each drug
        for drug in sorted(set(df_spp.drug)):
            print(species, drug)

            # get df
            df_drug = df_spp[df_spp.drug==drug]

            # define the used p vals
            """
            filters_series = designed_GWAS_filters_df.loc["%s-%s"%(species, drug)]
            """

            relevant_fields = ["species", "drug", "type_vars", "type_mutations", "type_collapsing", "group_name", "epsilon", "OR", "nodes_GenoAndPheno", "nodes_noGenoAndNoPheno", "nodes_GenoAndNoPheno", "nodes_noGenoAndPheno", "type_collapsing_level_spec", "pathway_has_sig_genes", "pathway_is_NR", "genes_in_pathway", "set_altered_genes"] + interesting_pval_fields + ["description"]

            # keep relevant fields
            df_drug = df_drug[relevant_fields]


            # add the altered gene name
            var_to_genes = df_vars_with_annot.set_index("variantID_across_samples").final_name
            def get_final_names_set_for_df_drug_r(r):
                if r.type_collapsing in {"MetaCyc", "Reactome", "GO"}: return set()
                elif r.type_collapsing=="genes": return {geneID_to_name[r.group_name]}
                elif r.type_collapsing=="domains": return {geneID_to_name[r.group_name.split("#")[1]]}
                elif r.type_collapsing=="none": return set(var_to_genes[{r.group_name}])
                else: raise ValueError("no type collapsings: %s"%r.type_collapsing)


            df_drug["final_names_list"] = df_drug.apply(get_final_names_set_for_df_drug_r, axis=1).apply(sorted)

            # add the OC for each gene and the # of species with that OC
            all_genes_df_drug = set(make_flat_listOflists(df_drug["final_names_list"]))
            def get_orthofinder_orthocluster_no_nans(x):
                if pd.isna(x): return "no_OG"
                else: return x
            df_gf["orthofinder_orthocluster_no_nans"] = df_gf.orthofinder_orthocluster.apply(get_orthofinder_orthocluster_no_nans)
            final_name_to_OC = dict(df_gf[df_gf.final_name.isin(all_genes_df_drug)].set_index("final_name").orthofinder_orthocluster_no_nans)
            final_name_to_OC["-"] = "no_OG"
            df_drug["orthofinder_orthoclusters"] = df_drug.final_names_list.apply(lambda x: [final_name_to_OC[g] for g in x])

            # add GO terms of sig genes
            print("adding GO terms to genes")
            all_alt_genes = set.union(*df_drug.set_altered_genes)

            if len(all_alt_genes)>0:

                obodag = GODag("%s/annotation_files/go-basic_30062021.obo"%(DataDir),  optional_attrs={'consider', 'replaced_by'}, load_obsolete=True, prt=None)
                ns_to_gene_to_GOids = get_namespace_to_gene_to_GOterms(dict(df_gf[df_gf.gff_upmost_parent.isin(all_alt_genes)].set_index("gff_upmost_parent").GOterms), obodag)

                # add to df
                for ns, ns_name in [("BP", "biological_process"), ("MF", "molecular_function"), ("CC", "cellular_component")]:
                    gene_to_GOids = ns_to_gene_to_GOids[ns]
                    for g in all_alt_genes.difference(set(gene_to_GOids)): gene_to_GOids[g] = set()
                    df_drug["%s_GO"%(ns_name)] = df_drug.set_altered_genes.apply(lambda genes: "\n".join([", ".join(["%s (%s)"%(go, obodag[go].name) for go in sorted(gene_to_GOids[g])]) for g in genes]))

            gwas_results_df = gwas_results_df.append(df_drug).reset_index(drop=True)

    final_relevant_fields = ["species_and_drug", "type_vars", "type_mutations", "type_collapsing", "group_name", "epsilon", "OR", "nodes_GenoAndPheno", "nodes_noGenoAndNoPheno", "nodes_GenoAndNoPheno", "nodes_noGenoAndPheno", "orthogroups", "n_spp_drug_worthogroups", "n_spp_drug_wpathway"] + interesting_pval_fields + ["description", "biological_process_GO", "cellular_component_GO", "molecular_function_GO"]

    # add OGs as a string
    gwas_results_df["orthogroups"] = gwas_results_df.orthofinder_orthoclusters.apply(lambda x: ", ".join(sorted({og for og in x if og!="no_OG"})))
    gwas_results_df["species_and_drug"] = gwas_results_df.species + "-" + gwas_results_df.drug

    # add the # of species that have orthogoups
    all_OGs = make_flat_listOflists(gwas_results_df.orthofinder_orthoclusters)
    def get_nspp_withOG_altered(og): 

        if og=="no_OG": return 0
        else:
            nspp = len(set(gwas_results_df[gwas_results_df.orthofinder_orthoclusters.apply(lambda x: og in x)].species_and_drug))
            if nspp==0: raise ValueError("0 spp")
            return nspp
    
    og_to_naffected_species = dict(zip(all_OGs, map(get_nspp_withOG_altered, all_OGs)))
    gwas_results_df["n_spp_drug_worthogroups_list"] = gwas_results_df.orthofinder_orthoclusters.apply(lambda x: [og_to_naffected_species[og] for og in x])
    gwas_results_df["n_spp_drug_worthogroups"] = gwas_results_df.n_spp_drug_worthogroups_list.apply(lambda x: ", ".join([str(y) for y in x]))

    # define the number of species that have this pathway
    def get_nspp_withpathway_altered(r):
        if r.type_collapsing in {"none", "genes", "domains"}: return 0
        else:

            nspp = len(set(gwas_results_df[(gwas_results_df.type_collapsing==r.type_collapsing) & (gwas_results_df.group_name==r.group_name)].species_and_drug))
            if nspp==0: raise ValueError("0 spp")
            return nspp

    gwas_results_df["n_spp_drug_wpathway"] = gwas_results_df.apply(get_nspp_withpathway_altered, axis=1)

    # define a gwas df that has only OGs from >1 spp
    gwas_results_df_more_than_1_spp_OGs = gwas_results_df[(gwas_results_df.n_spp_drug_worthogroups_list.apply(lambda x: any([n_spp>1 for n_spp in x]))) | (gwas_results_df.n_spp_drug_wpathway>1)]


    # in gwas_results_df_more_than_1_spp_OGs, remove the pathways that have equivalent gene duplicates across datasets
    #gwas_results_df_more_than_1_spp_OGs = get_gwas_results_df_more_than_1_spp_OGs_filtering_NR_pathways(gwas_results_df_more_than_1_spp_OGs)

    # keep only important fields
    #idx_sigle_dataset_results = (gwas_results_df.type_collapsing.isin({"none", "domains", "genes"})) | ( (~gwas_results_df.pathway_has_sig_genes) & (gwas_results_df.pathway_is_NR) )
    #gwas_results_df = gwas_results_df[idx_sigle_dataset_results] # gene results or NR pathway results
    #gwas_results_df_more_than_1_spp_OGs = gwas_results_df_more_than_1_spp_OGs

    # chcnge nans to -1
    def set_nan_to_minus1(x):
        if type(x)==list or type(x)==set: return x
        elif pd.isna(x): return -1
        else: return x

    gwas_results_df = gwas_results_df.applymap(set_nan_to_minus1)
    gwas_results_df_more_than_1_spp_OGs = gwas_results_df_more_than_1_spp_OGs.applymap(set_nan_to_minus1)

    # write dfs
    gwas_results_df = gwas_results_df.sort_values(by=["species_and_drug", "type_collapsing_level_spec", "epsilon", "OR"] + interesting_pval_fields, ascending=([True, True, False, False] + [True]*len(interesting_pval_fields))).reset_index(drop=True)[final_relevant_fields]
    write_excel_file_with_rows_all_same_color(gwas_results_df, '%s/GroupsGWAS.xlsx'%TablesDir, 'species_and_drug')



    # write >1 spp df


    # add sorting fields
    def add_sorting_fields_to_r(r):
        if r.type_collapsing in {"GO", "Reactome", "MetaCyc"}: 
            r["pname_or_orthogroups"] = r.group_name
            r["sorting_f"] = 1

        else: 
            r["pname_or_orthogroups"] = r.orthogroups
            r["sorting_f"] = 0

        return r

    gwas_results_df_more_than_1_spp_OGs = gwas_results_df_more_than_1_spp_OGs.apply(add_sorting_fields_to_r, axis=1)

    # add combinations of spp
    pname_or_orthogroups_to_comb_spp = gwas_results_df_more_than_1_spp_OGs.groupby("pname_or_orthogroups").apply(lambda df: tuple(sorted(set(df.species_and_drug))))
    gwas_results_df_more_than_1_spp_OGs["tuple_spp_drug_pname_or_orthogroups"] = gwas_results_df_more_than_1_spp_OGs.pname_or_orthogroups.map(pname_or_orthogroups_to_comb_spp)

    # sort and write
    gwas_results_df_more_than_1_spp_OGs_fields = ['pname_or_orthogroups'] + final_relevant_fields
    gwas_results_df_more_than_1_spp_OGs = gwas_results_df_more_than_1_spp_OGs.sort_values(by=["sorting_f", "tuple_spp_drug_pname_or_orthogroups", "pname_or_orthogroups", "type_collapsing_level_spec", "species_and_drug", "epsilon", "OR"] + interesting_pval_fields, ascending=([True, False, True, True, True, False, False] + [True]*len(interesting_pval_fields))).reset_index(drop=True)
    write_excel_file_with_rows_all_same_color(gwas_results_df_more_than_1_spp_OGs[gwas_results_df_more_than_1_spp_OGs_fields], '%s/Shared_GroupsGWAS.xlsx'%TablesDir, 'pname_or_orthogroups', fields_to_remove=["pname_or_orthogroups"])

    return gwas_results_df, gwas_results_df_more_than_1_spp_OGs

def get_tables_GWAS_significant_results(df_gwas_filt, gwas_method, DataDir, ProcessedDataDir, PlotsDir, gene_features_df, n_top_hits, optimal_filters_df, type_filters, type_muts):

    """Gets the descriptive tables with the significant results for GWAS"""

    # get the df for this gwas method
    df_gwas_filt = df_gwas_filt[df_gwas_filt.gwas_method==gwas_method]

    # redefine the real_type_mutations
    def get_real_type_mutations_from_r(r):
        if r.type_collapsing=="none": return r.type_mutations
        else: return r.real_type_mutations
    df_gwas_filt["real_type_mutations"] = df_gwas_filt.apply(get_real_type_mutations_from_r, axis=1)

    # change the real fields
    for f in ["type_vars", "type_mutations"]: df_gwas_filt.pop(f) 
    df_gwas_filt = df_gwas_filt.rename(columns={"real_type_vars":"type_vars", "real_type_mutations":"type_mutations"})

    # add final name
    def get_final_name_gene_features_df(r):
        if not pd.isna(r.gene_name): return r.gene_name
        elif not pd.isna(r.Scerevisiae_orthologs): return "Scer_%s"%(r.Scerevisiae_orthologs)
        else: return r.gff_upmost_parent
    gene_features_df["final_name"] = gene_features_df.apply(get_final_name_gene_features_df, axis=1)

    # init df
    gwas_results_df = pd.DataFrame()
    gwas_results_df_interesting_genes = pd.DataFrame()

    # go through each species
    for species in sorted(set(df_gwas_filt.species)):
        print(species)

        # define dir for this species
        taxID_dir = "%s/%s_%i"%(DataDir, species, sciName_to_taxID[species])

        # get df
        df_spp = df_gwas_filt[df_gwas_filt.species==species]
        df_gf = gene_features_df[gene_features_df.species==species]

        # get the variants with annotations
        associated_vars = set(df_spp[df_spp.type_collapsing=="none"].group_name)
        #var_annot_df = pd.concat([get_tab_as_df_or_empty_df(get_tab_file_only_lines_matching_patterns(associated_vars, "%s/integrated_varcalls/%s_annot.tab"%(taxID_dir, type_var), "%s/extracting_gwas_vars_annot_%s_%s.tab"%(ProcessedDataDir, species, type_var))) for type_var in ["smallVars", "SV_CNV"]])

        tmpdir_vars =  "%s/generating_GWAS_df_with_short_var_effect_%s_only_no_vars_%s_%s"%(ProcessedDataDir, species, type_filters, type_muts)
        df_vars_with_annot =  get_df_vars_with_short_variant_effect(associated_vars, DataDir, species, tmpdir_vars, set(df_gf.gff_upmost_parent).union({"-"}))

        geneID_to_name = dict(df_gf.set_index("gff_upmost_parent").final_name); geneID_to_name["-"] = "-"
        name_to_geneID = dict(df_gf.set_index("final_name").gff_upmost_parent); name_to_geneID["-"] = "-"
        geneID_to_description = dict(df_gf.set_index("gff_upmost_parent").description); geneID_to_description["-"] = "-"

        df_vars_with_annot["final_name"] = df_vars_with_annot.Gene.apply(lambda g: geneID_to_name[g])
        df_vars_with_annot["description"] = df_vars_with_annot.Gene.apply(lambda g: geneID_to_description[g])

        # keep one affected gene per variant
        df_vars_with_annot["consequences_set"] = df_vars_with_annot.Consequence.apply(lambda x: x.split(",")).apply(set)
        df_vars_with_annot["is_truncating"] = df_vars_with_annot.consequences_set.apply(get_is_truncating_from_consequences)
        df_vars_with_annot["is_synonymous"] = df_vars_with_annot.consequences_set.apply(get_is_synonymous_from_consequences)

        def get_df_vars_one_var_maximal_impact_df(df):

            # already df==1
            if len(df)==1: return df

            # if there are some truncating vars
            elif any(df.is_truncating): return df[df.is_truncating]

            # if there are some non syn vars
            elif any(~df.is_synonymous): return df[~(df.is_synonymous)]

            # if there is a mix
            else: return df

        df_vars_with_annot = pd.concat(map(lambda x: get_df_vars_one_var_maximal_impact_df(x[1]), df_vars_with_annot.groupby("variantID_across_samples")))

        # map each var to the effect as a string
        var_to_description = df_vars_with_annot.groupby("variantID_across_samples").apply(lambda df: "\n".join(df.apply(lambda r: "%s in %s (%s, %s)"%(r.short_variant_effect, r.final_name, r.Gene, r.description), axis=1)))

        # map each domain to the description
        df_interpro = load_InterProAnnotation("%s/InterproScan_annotation/interproscan_annotation.out"%taxID_dir)[["signature_accession", "signature_description", "InterPro_annotation_description", "type_analysis"]].drop_duplicates().set_index("signature_accession")
        if len(df_interpro)!=len(set(df_interpro.index)): raise ValueError("the index accesion should be unique")

        def get_description_one_domain(x):
            accession, geneID, location = x.split("#")
            location = location.replace("_", "-")
            if accession.startswith("pChunk"): return "region %s from gene %s (%s, %s)"%(location, geneID, geneID_to_name[geneID], geneID_to_description[geneID])
            else: 

                type_analysis = df_interpro.loc[accession, "type_analysis"]
                description = df_interpro.loc[accession, "signature_description"]
                description_IP = df_interpro.loc[accession, "InterPro_annotation_description"]
                if description!="-": dom_description = description
                else: dom_description = description_IP

                return "IP-signature %s (%s, %s) (region %s) from gene %s (%s, %s)"%(accession, type_analysis,  dom_description, location, geneID, geneID_to_name[geneID], geneID_to_description[geneID])

        all_domains = sorted(set(df_spp[df_spp.type_collapsing=="domains"].group_name))
        domain_to_description = dict(zip(all_domains, map(get_description_one_domain, all_domains)))

        # map pathway IDs to names
        df_reactome = pd.read_csv("%s/annotation_files/ReactomePathways.txt"%(DataDir), sep="\t", names=["ID", "description", "species"], header=None)
        reactome_ID_to_description = dict(df_reactome.set_index("ID").description)


        metacyc_ids_dir = "%s/%s_metacyc_ID_to_description_gwas_final_results_gwas_all_%s_%s.py"%(ProcessedDataDir, species, type_filters, type_muts)
        metacyc_ID_to_description = get_metacycID_to_description(sorted(set(df_spp[df_spp.type_collapsing=="MetaCyc"].group_name)), metacyc_ids_dir)

        obo_file = "%s/annotation_files/go-basic_30062021.obo"%(DataDir) # this was got from http://purl.obolibrary.org/obo/go/go-basic.obo
        obodag = GODag(obo_file,  optional_attrs={'consider', 'replaced_by'}, load_obsolete=True, prt=None)
        all_GOterms = sorted(set(df_spp[df_spp.type_collapsing=="GO"].group_name))
        name_space_to_ns = {"biological_process":"BP", "cellular_component":"CC", "molecular_function":"MF"}
        GO_ID_to_description = dict(zip(all_GOterms, map(lambda go: "%s (%s)"%(obodag[go].name, name_space_to_ns[obodag[go].namespace]), all_GOterms)))

        # add the description of the group, which is self explanatory
        type_collapsing_to_mapping_dict = {"none": var_to_description, "genes":geneID_to_description, "domains":domain_to_description, "GO":GO_ID_to_description, "Reactome":reactome_ID_to_description, "MetaCyc":metacyc_ID_to_description}

        def get_self_explanatory_group_description(r):
            description = type_collapsing_to_mapping_dict[r.type_collapsing][r.group_name]
            if r.type_collapsing in {"none", "domains"}: return description
            elif r.type_collapsing=="genes": return "%s (%s, %s)"%(r.group_name, geneID_to_name[r.group_name], description)
            elif r.type_collapsing in {"GO", "MetaCyc", "Reactome"}: return "%s-%s (%s)"%(r.type_collapsing, r.group_name, description)
            else: raise ValueError("invalid %s"%r)

        df_spp["description"] = df_spp[["type_collapsing", "group_name"]].apply(get_self_explanatory_group_description, axis=1)

        # go through each drug
        for drug in sorted(set(df_spp.drug)):
            print(species, drug)

            # get df
            df_drug = df_spp[df_spp.drug==drug]

            # initialize a field that has the descriptions of other equivalent groupings
            df_drug["descriptions_equivalent_groupings"] = ""


            # define the used p vals
            filters_series = optimal_filters_df.loc["%s-%s"%(species, drug)]
            used_pval_fields = [f for f in all_pval_fields if filters_series[f]==True] 

            # keep one df for each set of variants
            relevant_fields = ["species", "drug", "type_vars", "type_mutations", "type_genes", "type_collapsing", "group_name", "epsilon", "OR", "nodes_GenoAndPheno", "nodes_noGenoAndNoPheno", "nodes_GenoAndNoPheno", "nodes_noGenoAndPheno"] + used_pval_fields + ["description", "descriptions_equivalent_groupings"]

            def get_row_df_drug_one_set_of_variants(df):

                # if there is one row or no collapsing, return the df
                if len(df)==1 or all(df.type_collapsing=="none"): return df[relevant_fields]

                else:

                    # checks
                    if len(df[["type_vars"]].drop_duplicates())!=1:
                        print(df[["type_vars", "type_mutations"]].drop_duplicates())
                        raise ValueError("df should be unique")

                    #if len(set(df.OR))!=1: raise ValueError("The OR should be unique")
                    if len(set(df.epsilon))!=1: raise ValueError("The epsilon should be unique")

                    # sort by p values
                    df = df.sort_values(by=used_pval_fields)
                    descriptions_equivalent_groupings = "\n".join(df.apply(lambda r: "%s-%s-%s (%i vars, %s)"%(r.type_mutations, r.type_genes, r.type_collapsing, len(df.iloc[0].all_vars_tuple), r.description), axis=1))

                    # add df
                    df = df.iloc[0:1]
                    df["descriptions_equivalent_groupings"] = descriptions_equivalent_groupings
                    return df[relevant_fields]


            df_drug = pd.concat(map(lambda x: get_row_df_drug_one_set_of_variants(x[1]), df_drug.groupby("all_vars_tuple")))[relevant_fields]
            check_no_nans_in_df(df_drug)

            # sort by epsilon
            df_drug = df_drug.sort_values(by=["epsilon", "OR"] + used_pval_fields, ascending=([False, False] + [True]*len(used_pval_fields))).reset_index(drop=True)

            # add the altered gene name
            var_to_genes = df_vars_with_annot.set_index("variantID_across_samples").final_name
            def get_final_names_set_for_df_drug_r(r):
                if r.type_collapsing in {"MetaCyc", "Reactome", "GO"}: return set()
                elif r.type_collapsing=="genes": return {geneID_to_name[r.group_name]}
                elif r.type_collapsing=="domains": return {geneID_to_name[r.group_name.split("#")[1]]}
                elif r.type_collapsing=="none": return set(var_to_genes[{r.group_name}])
                else: raise ValueError("no type collapsings: %s"%r.type_collapsing)


            df_drug["final_names_list"] = df_drug.apply(get_final_names_set_for_df_drug_r, axis=1).apply(sorted)

            # add the OC for each gene and the # of species with that OC
            all_genes_df_drug = set(make_flat_listOflists(df_drug["final_names_list"]))
            def get_orthofinder_orthocluster_no_nans(x):
                if pd.isna(x): return "no_OG"
                else: return x
            df_gf["orthofinder_orthocluster_no_nans"] = df_gf.orthofinder_orthocluster.apply(get_orthofinder_orthocluster_no_nans)
            final_name_to_OC = dict(df_gf[df_gf.final_name.isin(all_genes_df_drug)].set_index("final_name").orthofinder_orthocluster_no_nans)
            df_drug["orthofinder_orthoclusters"] = df_drug.final_names_list.apply(lambda x: [final_name_to_OC[g] for g in x])

            # keep the interesting genes:
            interesting_genes = {"ERG11", "TAC1b", "GSC2", "PDR1", "FKS1", "FKS2", "CDR1", "ERG3"}
            df_drug_interesting_genes = df_drug[(df_drug.description.apply(lambda d: any([g in d for g in interesting_genes]))) | (df_drug.descriptions_equivalent_groupings.apply(lambda d: any([g in d for g in interesting_genes])))]

            # keep some fields
            gwas_results_df_interesting_genes = gwas_results_df_interesting_genes.append(df_drug_interesting_genes.iloc[0:n_top_hits])
            gwas_results_df = gwas_results_df.append(df_drug.iloc[0:n_top_hits])

    final_relevant_fields = ["species_and_drug", "type_vars", "type_mutations", "type_genes", "type_collapsing", "group_name", "epsilon", "OR", "nodes_GenoAndPheno", "nodes_noGenoAndNoPheno", "nodes_GenoAndNoPheno", "nodes_noGenoAndPheno", "orthogroups", "n_spp_drug_worthogroups", "n_spp_drug_wpathway"] + [f for f in all_pval_fields if f in gwas_results_df.columns] + ["description", "descriptions_equivalent_groupings"]

    # get the results
    plotsdir = "%s/GWAS_top_hits_results_%s_%s"%(PlotsDir, type_filters, type_muts); 
    #delete_folder(plotsdir)
    make_folder(plotsdir)

    # add OGs as a string
    gwas_results_df["orthogroups"] = gwas_results_df.orthofinder_orthoclusters.apply(lambda x: ", ".join(sorted({og for og in x if og!="no_OG"})))
    gwas_results_df["species_and_drug"] = gwas_results_df.species + "-" + gwas_results_df.drug

    # add the # of species that have orthogoups
    all_OGs = make_flat_listOflists(gwas_results_df.orthofinder_orthoclusters)
    def get_nspp_withOG_altered(og): 

        if og=="no_OG": return 0
        else:
            nspp = len(set(gwas_results_df[gwas_results_df.orthofinder_orthoclusters.apply(lambda x: og in x)].species_and_drug))
            if nspp==0: raise ValueError("0 spp")
            return nspp
    
    og_to_naffected_species = dict(zip(all_OGs, map(get_nspp_withOG_altered, all_OGs)))
    gwas_results_df["n_spp_drug_worthogroups_list"] = gwas_results_df.orthofinder_orthoclusters.apply(lambda x: [og_to_naffected_species[og] for og in x])
    gwas_results_df["n_spp_drug_worthogroups"] = gwas_results_df.n_spp_drug_worthogroups_list.apply(lambda x: ", ".join([str(y) for y in x]))


    # define the number of species that have this pathway
    def get_nspp_withpathway_altered(r):
        if r.type_collapsing in {"none", "genes"}: return 0
        else:

            nspp = len(set(gwas_results_df[(gwas_results_df.type_collapsing==r.type_collapsing) & (gwas_results_df.group_name==r.group_name)].species_and_drug))
            if nspp==0: raise ValueError("0 spp")
            return nspp

    gwas_results_df["n_spp_drug_wpathway"] = gwas_results_df.apply(get_nspp_withpathway_altered, axis=1)

    # define a gwas df that has only OGs from >1 spp
    gwas_results_df_more_than_1_spp_OGs = gwas_results_df[(gwas_results_df.n_spp_drug_worthogroups_list.apply(lambda x: any([n_spp>1 for n_spp in x]))) | (gwas_results_df.n_spp_drug_wpathway>1)]

    # keep only important fields
    gwas_results_df = gwas_results_df[final_relevant_fields]
    #gwas_results_df_interesting_genes = gwas_results_df_interesting_genes[final_relevant_fields]
    gwas_results_df_more_than_1_spp_OGs = gwas_results_df_more_than_1_spp_OGs[final_relevant_fields]

    # chcnge nans to -1
    def set_nan_to_minus1(x):
        if pd.isna(x): return -1
        else: return x

    #gwas_results_df_interesting_genes = gwas_results_df_interesting_genes.applymap(set_nan_to_minus1)
    gwas_results_df = gwas_results_df.applymap(set_nan_to_minus1)
    gwas_results_df_more_than_1_spp_OGs = gwas_results_df_more_than_1_spp_OGs.applymap(set_nan_to_minus1)


    # sort the relevant fields by OGs
    all_used_pval_fields = [f for f in all_pval_fields if f in gwas_results_df.columns]
    gwas_results_df_more_than_1_spp_OGs = gwas_results_df_more_than_1_spp_OGs.sort_values(by=["orthogroups", "species_and_drug", "epsilon", "OR"] + all_used_pval_fields, ascending=([True, True, False, False] + [True]*len(all_used_pval_fields))).reset_index(drop=True)

    # write
    filename = "%s/GWAS_results_top_%i_hits_%s.xlsx"%(plotsdir, n_top_hits, gwas_method)
    print("getting %s"%filename)
    write_excel_file_with_rows_all_same_color(gwas_results_df[final_relevant_fields], filename, 'species_and_drug')
    write_excel_file_with_rows_all_same_color(gwas_results_df_more_than_1_spp_OGs[final_relevant_fields], "%s.only_OGs_or_pathways_moreThan1spp.xlsx"%filename, 'orthogroups')


def load_df_file_only_some_cols(df_file, cols, tmpdir):

    # get the tab file
    if df_file.endswith(".py"):
        convert_py_pandas_df_to_tab(df_file)
        tab_file = "%s.tab"%df_file

    else: tab_file = df_file

    # only get cols file if necessary
    make_folder(tmpdir)
    tab_file_with_relevant_cols = "%s/tab_file_with_relevant_cols.tab"%tmpdir
    if file_is_empty(tab_file_with_relevant_cols):

        # get a file stating which columns exist with the col number
        cols_file  = "%s.cols.txt"%tab_file_with_relevant_cols
        run_cmd("head -n 1 '%s' | sed 's/\\t/\\n/g' | nl > '%s'"%(tab_file, cols_file))

        # get the col numbers
        interesting_col_numbers = [int(l.split()[0]) for l in open(cols_file, "r").readlines() if l.split()[1] in cols]
        if len(interesting_col_numbers)!=len(cols): raise ValueError("all cols %s should be in file %s"%(cols, tab_file))
        remove_file(cols_file)

        # get the cut file
        tab_file_with_relevant_cols_tmp = "%s.tmp"%tab_file_with_relevant_cols
        print("cutting")
        run_cmd("cut -f%s '%s' > %s"%(",".join(list(map(str, interesting_col_numbers))), tab_file, tab_file_with_relevant_cols_tmp))

        # drop duplicates
        print("dropping duplicates")
        df = get_tab_as_df_or_empty_df(tab_file_with_relevant_cols_tmp)[cols].drop_duplicates()
        save_df_as_tab(df, tab_file_with_relevant_cols_tmp)
        os.rename(tab_file_with_relevant_cols_tmp, tab_file_with_relevant_cols)

    return get_tab_as_df_or_empty_df(tab_file_with_relevant_cols)[cols]

def get_gwas_df_only_mutations_with_added_metadata_one_spp_and_drug(spp, drug, tab_file, min_npheno_transitions, DataDir, ProcessedDataDir, gwas_method, reference_genome, df_gwas_sig_results, filters_series, type_muts):

    """
    Gets the GWAS df only of variants of ASR_methods_phenotypes and gwas_method.
    """

    print("\n", spp, drug)

    # load gwas
    df_gwas = get_tab_as_df_or_empty_df(tab_file)

    # define the GWAS dir
    taxID_dir = "%s/%s_%i"%(DataDir, spp, sciName_to_taxID[spp])
    gwas_dir = "%s/ancestral_GWAS_drugResistance/GWAS_%s_resistance"%(taxID_dir, drug)

    # check the min_npheno_transitions
    n_pheno_transitions_set = set(df_gwas[(df_gwas.ASR_methods_phenotypes=="MPPA,DOWNPASS") & (df_gwas.gwas_method=="synchronous") & (df_gwas.min_support==0)].nodes_withPheno)
    if len(n_pheno_transitions_set)!=1: raise ValueError("there are more than one phenotype transitions")
    if next(iter(n_pheno_transitions_set))<min_npheno_transitions: 
        print("WARNING: Skipping %s-%s because it has %i transitions"%(spp, drug, next(iter(n_pheno_transitions_set))))
        return pd.DataFrame()




    # filter to keep only variants
    df_gwas = df_gwas[(df_gwas.type_collapsing=="none") & (df_gwas.gwas_method==gwas_method) & (df_gwas.ASR_methods_phenotypes==filters_series.ASR_methods_phenotypes) & (df_gwas.min_support==filters_series.min_support)]
    if len(df_gwas)!=len(set(df_gwas.group_name)): raise ValueError("the group name should be unique")

    # define the used_pval_fields
    used_pval_fields = [f for f in all_pval_fields if filters_series[f] is True] 
    if len(used_pval_fields)==0: raise ValueError("0 pvals")

    # define the rawly filtered variants
    df_gwas_sig_results_all_vars =  get_filtered_gwas_af_df_consistency_btw_pvals(filters_series, df_gwas)

    # keep important fields
    check_no_nans_in_df(df_gwas[used_pval_fields])
    df_gwas["max_pval"] = df_gwas[used_pval_fields].apply(max, axis=1)
    df_gwas = df_gwas[["group_name", "max_pval"]]

    # load all variants
    df_all_vars = load_object("%s/variants_df.py"%gwas_dir)[["variantID_across_samples", "type_var"]].drop_duplicates().rename(columns={"variantID_across_samples":"group_name"}).reset_index(drop=True)
    missing_vars = set(df_gwas.group_name).difference(set(df_all_vars.group_name))
    if len(missing_vars)>0: raise ValueError("There are missing vars: %s"%missing_vars)

    # add the pvalue as 1 for all data
    df_gwas = df_all_vars.merge(df_gwas, on="group_name", validate="one_to_one", how="left")
    df_gwas["pval_measured"] = ~pd.isna(df_gwas.max_pval)
    df_gwas["max_pval"] = df_gwas.max_pval.apply(get_NaN_to_1)
    check_no_nans_in_df(df_gwas)
    print("There are %i/%i measured variants"%(sum(df_gwas.pval_measured), len(df_gwas)))

    # generate a df that has all the variants and positions
    var_fields = ["#CHROM", "POS", "INFO_END", "variantID_across_samples"]

    df_SV_CNV = load_df_file_only_some_cols("%s/integrated_varcalls/SV_CNV_filt.py"%taxID_dir, var_fields, "%s/%s-%s-load_SV_CNV_some_cols_GWAS_only_none"%(ProcessedDataDir, spp, drug))
    def get_nan_END_to_POS(r):
        if pd.isna(r.INFO_END): return r.POS
        else: return r.INFO_END
    df_SV_CNV["INFO_END"] = df_SV_CNV.apply(get_nan_END_to_POS, axis=1)
    df_SV_CNV = df_SV_CNV[var_fields].drop_duplicates(); check_no_nans_in_df(df_SV_CNV)

    df_small_vars = load_df_file_only_some_cols("%s/integrated_varcalls/smallVars_filt.py"%taxID_dir,  ["#CHROM", "POS", "#Uploaded_variation"], "%s/%s-%s-load_small_vars_some_cols_GWAS_only_none"%(ProcessedDataDir, spp, drug))
    df_small_vars["INFO_END"] = df_small_vars.POS
    df_small_vars["variantID_across_samples"] = df_small_vars["#Uploaded_variation"]
    df_small_vars = df_small_vars[var_fields].drop_duplicates(); check_no_nans_in_df(df_small_vars)

    df_all_vars_wMetaData = df_small_vars.append(df_SV_CNV)[var_fields]

    # add the chromosome deletions and duplications
    chr_to_len = get_chr_to_len(reference_genome)
    all_chromosomes = sorted(set(chr_to_len))
    df_all_vars_wMetaData_chroms = pd.concat([pd.DataFrame({chrom : {"#CHROM":chrom, "POS":1, "INFO_END":chr_to_len[chrom], "variantID_across_samples":"%s_%s"%(chrom, type_chrom_var)} for chrom in all_chromosomes}).transpose() for type_chrom_var in ["deletion", "duplication"]])[var_fields]
    df_all_vars_wMetaData = df_all_vars_wMetaData.append(df_all_vars_wMetaData_chroms)[var_fields]

    # add var metadata to df_gwas
    missing_vars = set(df_gwas.group_name).difference(set(df_all_vars_wMetaData.variantID_across_samples))
    if len(missing_vars)>0: raise ValueError("there are missing vars: %s"%sorted(missing_vars))
    df_gwas = df_gwas.merge(df_all_vars_wMetaData, left_on="group_name", right_on="variantID_across_samples", validate="one_to_many", how="left")
    check_no_nans_in_df(df_gwas)

    # add whether these are significant results according to df_gwas_sig_results
    sig_vars = set(df_gwas_sig_results.group_name)
    if len(sig_vars.difference(set(df_gwas.group_name)))>0: raise ValueError("there can't be missing sig_vars")
    df_gwas["is_significant_var_%s"%type_muts] = df_gwas.group_name.isin(sig_vars)
    print("There are %i/%i significant vars"%(len(sig_vars), len(df_gwas)))


    # add is signifficant according to all other vars
    sig_vars_all_vars = set(df_gwas_sig_results_all_vars.group_name)
    df_gwas["is_significant_var_all_vars"] = df_gwas.group_name.isin(sig_vars_all_vars)
    print("There are %i/%i significant vars for all vars"%(len(sig_vars_all_vars), len(df_gwas)))

    # add data
    df_gwas["species"] = spp
    df_gwas["drug"] = drug

    return df_gwas


def get_gwas_df_only_mutations_with_added_metadata_one_spp_and_drug_get_figure_GWAS_manhattan(spp, drug, tab_file, DataDir, ProcessedDataDir, reference_genome, filters_series):

    """Gets the GWAS df for one species and drug"""

    print(spp, drug)
    #print("\n", spp, drug)
    make_folder(ProcessedDataDir)

    # load gwas synchronous
    #print("loading GWAS df")
    df_gwas = get_tab_as_df_or_empty_df(tab_file)
    df_gwas = df_gwas[df_gwas.gwas_method=="synchronous"]

    # define the GWAS dir
    taxID_dir = "%s/%s_%i"%(DataDir, spp, sciName_to_taxID[spp])
    gwas_dir = "%s/ancestral_GWAS_drugResistance/GWAS_%s_resistance"%(taxID_dir, drug)

    # filter to keep only variants w/ the correct support
    df_gwas = df_gwas[(df_gwas.type_collapsing=="none") & (df_gwas.ASR_methods_phenotypes==filters_series.ASR_methods_phenotypes) & (df_gwas.min_support==filters_series.min_support)]
    if len(df_gwas)!=len(set(df_gwas.group_name)): raise ValueError("the group name should be unique")

    # define the used_pval_fields
    interesting_pval_fields = ['pval_chi_square_phenotypes', 'pval_GenoAndPheno_phenotypes', 'pval_fisher']
    used_pval_fields_normal = ["%s_bonferroni"%f for f in interesting_pval_fields if filters_series[f] is True] 

    used_pval_fields_maxT = [f for f in ['pval_chi_square_maxT', 'pval_epsilon_maxT'] if filters_series[f] is True]

    used_pval_fields = used_pval_fields_normal + used_pval_fields_maxT

    # check that all are bonferroni
    if len(used_pval_fields_normal)>0:
        if filters_series.correction_method!="bonferroni": raise ValueError("All should be bonferroni")

    print( spp, drug, "These are the used pvals: %s"%used_pval_fields)

    #if len(used_pval_fields)!=1: raise ValueError("!=1 pvals")

    # define the rawly filtered variants
    df_gwas_sig_results_all_vars =  get_filtered_gwas_af_df_consistency_btw_pvals(filters_series, df_gwas)

    # keep important fields
    check_no_nans_in_df(df_gwas[used_pval_fields])
    df_gwas["max_pval"] = df_gwas[used_pval_fields].apply(max, axis=1)
    df_gwas = df_gwas[["group_name", "max_pval", "pval_fisher"]]

    # load all variants and check that it makes sense
    df_all_vars = load_object("%s/variants_df.py"%gwas_dir)[["variantID_across_samples", "type_var"]].drop_duplicates().rename(columns={"variantID_across_samples":"group_name"}).reset_index(drop=True)
    missing_vars = set(df_gwas.group_name).difference(set(df_all_vars.group_name))
    if len(missing_vars)>0: raise ValueError("There are missing vars: %s"%missing_vars)

    # add the pvalue as 1 for all data
    df_gwas = df_all_vars.merge(df_gwas, on="group_name", validate="one_to_one", how="left")
    df_gwas["pval_measured"] = ~pd.isna(df_gwas.max_pval)
    df_gwas["max_pval"] = df_gwas.max_pval.apply(get_NaN_to_1)
    df_gwas["pval_fisher"] = df_gwas.pval_fisher.apply(get_NaN_to_1)
    check_no_nans_in_df(df_gwas)
    print("There are %i/%i measured variants"%(sum(df_gwas.pval_measured), len(df_gwas)))

    # generate a df that has all the variants and positions
    var_fields = ["#CHROM", "POS", "INFO_END", "variantID_across_samples"]

    df_SV_CNV = load_df_file_only_some_cols("%s/integrated_varcalls/SV_CNV_filt.py"%taxID_dir, var_fields, "%s/%s-%s-load_SV_CNV_some_cols_GWAS_only_none"%(ProcessedDataDir, spp, drug))
    def get_nan_END_to_POS(r):
        if pd.isna(r.INFO_END): return r.POS
        else: return r.INFO_END
    df_SV_CNV["INFO_END"] = df_SV_CNV.apply(get_nan_END_to_POS, axis=1)
    df_SV_CNV = df_SV_CNV[var_fields].drop_duplicates(); check_no_nans_in_df(df_SV_CNV)

    df_small_vars = load_df_file_only_some_cols("%s/integrated_varcalls/smallVars_filt.py"%taxID_dir,  ["#CHROM", "POS", "#Uploaded_variation"], "%s/%s-%s-load_small_vars_some_cols_GWAS_only_none"%(ProcessedDataDir, spp, drug))
    df_small_vars["INFO_END"] = df_small_vars.POS
    df_small_vars["variantID_across_samples"] = df_small_vars["#Uploaded_variation"]
    df_small_vars = df_small_vars[var_fields].drop_duplicates(); check_no_nans_in_df(df_small_vars)

    df_all_vars_wMetaData = df_small_vars.append(df_SV_CNV)[var_fields]

    # add the chromosome deletions and duplications
    chr_to_len = get_chr_to_len(reference_genome)
    all_chromosomes = sorted(set(chr_to_len))
    df_all_vars_wMetaData_chroms = pd.concat([pd.DataFrame({chrom : {"#CHROM":chrom, "POS":1, "INFO_END":chr_to_len[chrom], "variantID_across_samples":"%s_%s"%(chrom, type_chrom_var)} for chrom in all_chromosomes}).transpose() for type_chrom_var in ["deletion", "duplication"]])[var_fields]
    df_all_vars_wMetaData = df_all_vars_wMetaData.append(df_all_vars_wMetaData_chroms)[var_fields]

    # add var metadata to df_gwas
    missing_vars = set(df_gwas.group_name).difference(set(df_all_vars_wMetaData.variantID_across_samples))
    if len(missing_vars)>0: raise ValueError("there are missing vars: %s"%sorted(missing_vars))
    df_gwas = df_gwas.merge(df_all_vars_wMetaData, left_on="group_name", right_on="variantID_across_samples", validate="one_to_many", how="left")
    check_no_nans_in_df(df_gwas)

    # add is signifficant according to all other vars
    sig_vars_all_vars = set(df_gwas_sig_results_all_vars.group_name)
    df_gwas["is_significant_var_all_vars"] = df_gwas.group_name.isin(sig_vars_all_vars)
    print("There are %i/%i significant vars for all vars"%(len(sig_vars_all_vars), len(df_gwas)))

    # add data
    df_gwas["species"] = spp
    df_gwas["drug"] = drug

    return df_gwas


def get_figure_GWAS_manhattan(DataDir, ProcessedDataDir, PlotsDir, spp_drug_to_gwas_df_file, threads, species_to_ref_genome, filters_df):

    """Gets the GWAS df with the filtered data"""

    # keep
    filters_df = cp.deepcopy(filters_df)

    # debug (add filter)
    #filters_df["min_nodes_GenoAndPheno"] = 3
    #filters_df["min_fraction_nodes_Pheno_wGeno"] = 0.3
    #filters_df["correction_method"] = "bonferroni"

    ####### GET DF ######## 

    # get the df for the 50 resamples filtering with each set of samples
    df_sig_vars_resamples_file = "%s/GWAS_AF_resistance_df_sig_vars_resamples_file.py"%(ProcessedDataDir)

    if file_is_empty(df_sig_vars_resamples_file):

        # getting the resampled GWAS dfs inputs
        df_sig_vars_resamples = pd.DataFrame()
        print("getting inputs")
        for spp, drug in spp_drug_to_gwas_df_file.keys():
            print(spp, drug)

            # get df
            df_gwas_resamples =  load_object("%s/%s_%i/ancestral_GWAS_drugResistance/GWAS_%s_resistance_reshuffled_phenotypes/df_gwas_all_resamples.py"%(DataDir, spp, sciName_to_taxID[spp], drug))
            df_gwas_resamples = df_gwas_resamples[(df_gwas_resamples.gwas_method=='synchronous') & (df_gwas_resamples.type_collapsing=='none')]

            # add the fisher p values
            df_gwas_resamples = get_gwas_df_with_fisher_p_value_and_corrected_pvals(df_gwas_resamples, threads, gwas_unique_id_fields=["ASR_methods_phenotypes", "min_support", "resampleI"])

            # check
            if len(df_gwas_resamples[["resampleI", "group_name", "ASR_methods_phenotypes", "min_support"]].drop_duplicates())!=len(df_gwas_resamples): raise ValueError("group name should be unique")

            # get filtered data
            df_sig_vars_resamples = df_sig_vars_resamples.append(get_filtered_gwas_af_df_consistency_btw_pvals(filters_df.loc["%s-%s"%(spp, drug)], df_gwas_resamples)).reset_index(drop=True)

        print("saving")
        save_object(df_sig_vars_resamples, df_sig_vars_resamples_file)   

    # load
    print("loading df_sig_vars_resamples")
    df_sig_vars_resamples = load_object(df_sig_vars_resamples_file)

    # map each species and drug to the number of sig vars per resample
    spp_drug_to_nsig_vars_resamples = df_sig_vars_resamples.groupby(["species", "drug"]).apply(lambda df: np.array(sorted(df.groupby("resampleI").apply(lambda df_I: len(set(df_I.group_name))))))


    # generate df
    df_manhattan_file = "%s/GWAS_AF_resistance_df_manhattan.py"%(ProcessedDataDir)
    if file_is_empty(df_manhattan_file):

        # debug one example
        #test_spp_drug = ("Candida_albicans", "FLC")
        #spp_drug_to_gwas_df_file = {x:y for x,y in spp_drug_to_gwas_df_file.items() if x==test_spp_drug}

        # get dfs in parallel
        inputs_fn = [(spp, drug, tab_file, DataDir, "%s/getting_vars_filt_manhattan_GWAS_final_settings"%ProcessedDataDir, species_to_ref_genome[spp], filters_df.loc["%s-%s"%(spp, drug)]) for (spp,drug), tab_file in spp_drug_to_gwas_df_file.items()]

        # debug
        #inputs_fn = inputs_fn[0:1]

        # run in parallel
        with multiproc.Pool(threads) as pool:
            df_manhattan = pd.concat(pool.starmap(get_gwas_df_only_mutations_with_added_metadata_one_spp_and_drug_get_figure_GWAS_manhattan, inputs_fn)).reset_index(drop=True)
            pool.close()
            pool.terminate() 

        print("saving")
        save_object(df_manhattan, df_manhattan_file)   

    print("loading manhattan df")
    df_manhattan = load_object(df_manhattan_file)

    # checks and add -log10 pval
    print("adding pvals")
    pseudocount = 1e-5 # the minimum pval should be 1e-4 or 0 for resampled pvals
    df_manhattan["minus_log10_max_pval"] = -(df_manhattan.max_pval + pseudocount).apply(lambda x: min([1.0, x])).apply(np.log10)

    df_manhattan["minus_log10_pval_fisher"] = -(df_manhattan.pval_fisher).apply(lambda x: min([1.0, x])).apply(np.log10)
    print(min( df_manhattan["minus_log10_pval_fisher"]), max( df_manhattan["minus_log10_pval_fisher"]))

    #######################


    ######## PLOT ########

    # keep only SNPs
    #df_manhattan = df_manhattan[df_manhattan.type_var=="SNP"]

    # define plots dir
    plots_dir = "%s/manhattan_plots"%PlotsDir
    make_folder(plots_dir)

    df_manhattan = df_manhattan.set_index("species")

    # one figure for each species 
    for species in sorted(set(df_manhattan.index)):
        print(species)

        #if species!="Candida_albicans": continue

        # get df for this species
        df_spp = df_manhattan.loc[species]

        # add the offset
        all_chromosomes = sorted({x for x in set(df_spp["#CHROM"]) if x!=species_to_mtChromosome[species]})
        all_chromosomes.append(species_to_mtChromosome[species])
        chrom_to_len = get_chr_to_len(species_to_ref_genome[species])

        chrom_to_Xoffset = {}
        current_offset = 0
        offset_btw_chroms = 2*1e5
        for chrom in all_chromosomes:
            chrom_to_Xoffset[chrom] = current_offset
            current_offset += chrom_to_len[chrom] + offset_btw_chroms

        df_spp["Xoffset"] = df_spp["#CHROM"].map(chrom_to_Xoffset)
        df_spp["genome position"] = df_spp.POS + df_spp.Xoffset
        check_no_nans_series(df_spp["genome position"])

        # get a plot that has all the info for this species
        print("getting relplot")
        g = sns.relplot(data=df_spp, kind="scatter", row="drug", x="genome position", y="minus_log10_pval_fisher", hue="is_significant_var_all_vars", palette={False:"gray", True:"red"}, aspect=6, height=1.7, alpha=0.7, size="is_significant_var_all_vars",  sizes={True:5, False:1}, edgecolor="none")
        
        # edit each subplot
        print("editing subplots")
        for ax in g.axes.flat:

            # define the drug
            drug =  ax.get_title().split("=")[1].rstrip().lstrip()


            # calculate the number of significant variants
            n_sig_vars = len(set((df_spp[(df_spp.is_significant_var_all_vars) & (df_spp.drug==drug)].variantID_across_samples)))

            # calculate the probability of observing this number of sig vars under radom 50 resamples of phenotypes
            if (species, drug) in spp_drug_to_nsig_vars_resamples:
                array_random_n_sig_vars = spp_drug_to_nsig_vars_resamples[(species, drug)]
            else: array_random_n_sig_vars = np.array([])

            p_random_nvars = sum(array_random_n_sig_vars>=n_sig_vars)/50
            if p_random_nvars>0: p_random_nvars_str = "=%.3f"%p_random_nvars
            else: p_random_nvars_str = "<%.3f"%(1/50)

            # add the offset positions
            for chrom, x in chrom_to_Xoffset.items(): 
                ax.axvline(x, linestyle="-", linewidth=.5, color="gray") 
                ax.axvline(x+chrom_to_len[chrom], linestyle="-", linewidth=.5, color="gray") 

            ax.set_title("C. %s-%s. $N=%i$ significant variants. $p(N)(random)%s$"%(species.split("_")[1], drug, n_sig_vars, p_random_nvars_str))
            
            # add the chrom name at the last row
            if ax.rowNum==(len(set(df_spp.drug))-1):
                for chrom, Xoffset in chrom_to_Xoffset.items():
                    ax.text(Xoffset + chrom_to_len[chrom]/2, -2, get_shortChrom_from_chrName(chrom, species=species)+"\n\n", ha="center",va="center")

                ax.set_xlabel("\n\ngenome position")


            # label
            if ax.rowNum==({"Candida_glabrata":1, "Candida_albicans":0, "Candida_auris":3}[species]): ax.set_ylabel("$-log\ p_{FISHER}$")
            else: ax.set_ylabel("")

            ax.set_xticklabels([])
            ax.set_xticks([])



        plt.show()

        filename = "%s/manhattan_%s.png"%(plots_dir, species)
        print("saving %s"%filename)
        g.savefig(filename, dpi=600)


    ######################

def get_gwas_manhattan_plots(df_gwas_filt, gwas_method, DataDir, ProcessedDataDir, PlotsDir, spp_drug_to_gwas_df_file, min_npheno_transitions, threads, species_to_ref_genome, filters_df, type_filters, type_muts):

    """Plots the manhattan plot of the simple variants"""

    ####### GET DF ######## 

    # generate df
    df_manhattan_file = "%s/GWAS_AF_resistance_df_manhattan_%s_%s.py"%(ProcessedDataDir, type_filters, type_muts)
    if file_is_empty(df_manhattan_file):

        #spp_drug_to_gwas_df_file = {x:y for x,y in spp_drug_to_gwas_df_file.items() if x==("Candida_auris", "CAS")}
        #spp_drug_to_gwas_df_file = {x:y for x,y in spp_drug_to_gwas_df_file.items() if x==("Candida_glabrata", "MIF")}

        # get the concatenated data for each df
        print("running get_gwas_df_only_mutations_with_added_metadata_one_spp_and_drug in parallel")
        inputs_fn = [(spp, drug, tab_file, min_npheno_transitions, DataDir, ProcessedDataDir, gwas_method, species_to_ref_genome[spp], df_gwas_filt[(df_gwas_filt.species==spp) & (df_gwas_filt.drug==drug) & (df_gwas_filt.gwas_method==gwas_method) & (df_gwas_filt.type_collapsing=="none")], filters_df.loc["%s-%s"%(spp, drug)], type_muts) for (spp,drug), tab_file in spp_drug_to_gwas_df_file.items() if "%s-%s"%(spp, drug) in set(filters_df.index)]
        with multiproc.Pool(threads) as pool:
            df_manhattan = pd.concat(pool.starmap(get_gwas_df_only_mutations_with_added_metadata_one_spp_and_drug, inputs_fn)).reset_index(drop=True)
            pool.close()
            pool.terminate() 


        print("saving")
        save_object(df_manhattan, df_manhattan_file)   

    print("loading manhattan df")
    df_manhattan = load_object(df_manhattan_file)


    # checks and add -log10 pval
    print("adding pvals")
    if any((df_manhattan.max_pval!=0) & (df_manhattan.max_pval<1e-4)): raise ValueError("there are pvals btw 0 - 1e4")
    pseudocount = 1e-5 # the minimum pval should be 1e-4 or 0
    df_manhattan["minus_log10_max_pval"] = -(df_manhattan.max_pval + pseudocount).apply(lambda x: min([1.0, x])).apply(np.log10)

    #######################

    ######## PLOT ########

    # define plots dir
    plots_dir = "%s/manhattan_plots"%PlotsDir
    make_folder(plots_dir)

    df_manhattan = df_manhattan.set_index("species")

    # one figure for each species 
    for species in sorted(set(df_manhattan.index)):
        print(species)

        # get df for this species
        df_spp = df_manhattan.loc[species]

        # add the offset
        all_chromosomes = sorted({x for x in set(df_spp["#CHROM"]) if x!=species_to_mtChromosome[species]})
        all_chromosomes.append(species_to_mtChromosome[species])
        chrom_to_len = get_chr_to_len(species_to_ref_genome[species])

        chrom_to_Xoffset = {}
        current_offset = 0
        offset_btw_chroms = 2*1e5
        for chrom in all_chromosomes:
            chrom_to_Xoffset[chrom] = current_offset
            current_offset += chrom_to_len[chrom] + offset_btw_chroms

        df_spp["Xoffset"] = df_spp["#CHROM"].map(chrom_to_Xoffset)
        df_spp["genome position"] = df_spp.POS + df_spp.Xoffset
        check_no_nans_series(df_spp["genome position"])


        # get a plot that has all the info for this species
        print("getting relplot")
        g = sns.relplot(data=df_spp, kind="scatter", row="drug", x="genome position", y="minus_log10_max_pval", hue="is_significant_var_all_vars", palette={False:"gray", True:"blue"}, style="type_var", aspect=6, height=1.7, alpha=0.7, size="is_significant_var_all_vars",  sizes={True:5, False:1}, edgecolor="none")
        
        # edit each subplot
        print("editing subplots")
        for ax in g.axes.flat:

            # define the drug
            drug =  ax.get_title().split("=")[1].rstrip().lstrip()

            # add the offset positions
            for chrom, x in chrom_to_Xoffset.items(): 
                ax.axvline(x, linestyle="-", linewidth=.5, color="gray") 
                ax.axvline(x+chrom_to_len[chrom], linestyle="-", linewidth=.5, color="gray") 

            if ax.rowNum==0: ax.set_title("%s, %s"%(species, type_muts) + "\n\n")
            else: ax.set_title("")

            # add the chrom name
            if ax.rowNum==0:
                for chrom, Xoffset in chrom_to_Xoffset.items():
                    ax.text(Xoffset + chrom_to_len[chrom]/2, 6, get_shortChrom_from_chrName(chrom, species=species), ha="center",va="center")

            # add the non syn vars
            if type_muts=="only_non_syn":

                df_plot = df_spp[(df_spp.drug==drug) & (df_spp["is_significant_var_%s"%type_muts]==True)]
                for x, pval in df_plot[["genome position", "minus_log10_max_pval"]].values:

                    ax.scatter([x], [pval], edgecolor="red", facecolor="none",  s=12) 


                    #ax.axvline(x, linestyle="--", linewidth=.5, color="red") 

            # label
            ax.set_ylabel("%s\n-log(p)"%drug)


        plt.show()

        filename = "%s/manhattan_%s.pdf"%(plots_dir, species)
        print("saving %s"%filename)
        #g.savefig(filename)



 
        #plt.subplots_adjust(wspace=0.0,  hspace=0.1)
        #sys.exit(0)



    ######################




def get_gwas_optimal_filters_df(filtering_stats_df, valid_spp_drug, PlotsDir):

    """Gets the optimal filters df for GWAS"""

    # filter invalid genes
    filtering_stats_df["spp_and_drug"] = filtering_stats_df.species + "-" + filtering_stats_df.drug
    valid_spp_drug = {"%s-%s"%(x[0], x[1]) for x in valid_spp_drug}
    filtering_stats_df = filtering_stats_df[(filtering_stats_df.spp_and_drug.isin(valid_spp_drug))]

    # check
    if set(filtering_stats_df.spp_and_drug)!=valid_spp_drug: raise ValueError("not all spp_and_drug considered")

    # add fields
    def get_nan_to_0(x):
        if pd.isna(x): return 0.0
        else: return x
    filtering_stats_df["fraction_n_genes_expected_found"]  = (filtering_stats_df.n_genes_expected_found / filtering_stats_df.n_genes_expected).apply(get_nan_to_0)
    filtering_stats_df["n_pval_fields"] = filtering_stats_df[["pval_chi_square_RelToBranchLen", "pval_chi_square_phenotypes", "pval_GenoAndPheno_RelToBranchLen", "pval_GenoAndPheno_phenotypes"]].apply(sum, axis=1)

    # define the speices and drug that have unknown genes
    unknown_spp_and_drug = ["Candida_auris-AMB", "Candida_auris-ANI", "Candida_glabrata-MIF"]


    # for each species and drug get a row that has the valid filters
    def get_optimal_filters_row_one_spp_and_drug(spp_drug, df):


        # rationally designed dfs
        if spp_drug in unknown_spp_and_drug:

            # checks
            if any(df.fraction_n_genes_expected_found>0): raise ValueError("There can't be >0 in fraction_n_genes_expected_found")

            # get a df designed with filters of the other species and drugs. Taking the most common filters for each of them.
            df_filt = df[(df.correction_method=="fdr_bh") & (df.min_epsilon==0.1) & (df.pval_chi_square_RelToBranchLen==True) & (df.pval_chi_square_phenotypes==True) & (df.pval_GenoAndPheno_RelToBranchLen==True) & (df.pval_GenoAndPheno_phenotypes==False) & (df.fdr_threshold==0.1) & (df.min_support==50) & (df.ASR_methods_phenotypes=="MPPA,DOWNPASS") & (df.alpha_pval==0.05)]

            if len(df_filt)!=1: raise ValueError("there should be only one f or 0")

            # get the row
            return df_filt.iloc[0]

        # get the optimal filter
        else:

            # check
            if all(df.fraction_n_genes_expected_found==0): raise ValueError("Some filters should have ==0 in %s"%(spp_drug))

            # get the filters with the max number of expected genes found
            df_filt = df[df.fraction_n_genes_expected_found==max(df.fraction_n_genes_expected_found)]

            # keep the filters that have the minimum number of genes
            df_filt = df_filt[df_filt.n_genes_found==min(df_filt.n_genes_found)]

            # return
            if len(df_filt)==1: return df_filt.iloc[0]

            # sort the df by the different filter types
            list_least_to_most_conservative = [("correction_method", ["none", "fdr_bh", "fdr_by", "bonferroni"]),
                                               ("min_epsilon", [0, 0.1, 0.2, 0.3, 0.4, 0.5]),
                                               ("n_pval_fields", [1, 2, 3, 4]),
                                               ("alpha_pval", [0.05, 0.01, 0.001, 0.0001]),
                                               ("fdr_threshold", [0.2, 0.15, 0.1, 0.05]),
                                               ("min_support", [0, 50, 70]),
                                               ("ASR_methods_phenotypes", ['DOWNPASS', 'MPPA', 'MPPA,DOWNPASS'])]

            sorting_fields = []
            for f, values_f in list_least_to_most_conservative:
                sorting_f = "%s_I"%f
                val_to_I = dict(zip(reversed(values_f), range(len(values_f))))                
                df_filt[sorting_f] = df_filt[f].apply(lambda x: val_to_I[x])
                sorting_fields.append(sorting_f)

            df_filt = df_filt.sort_values(by=sorting_fields, ascending=True)

            # keep the values from df_filt that are most conservative
            first_r = df_filt.iloc[0]
            df_filt = df_filt[df_filt.apply(lambda r: all([r[f]==first_r[f] for f in sorting_fields]), axis=1)]

            if len(df_filt)!=1: raise ValueError("there should be only one f")

            # get the row
            return df_filt.iloc[0]


    optimal_filters_df = pd.DataFrame({spp_drug : get_optimal_filters_row_one_spp_and_drug(spp_drug, df) for spp_drug, df in filtering_stats_df.groupby("spp_and_drug")}).transpose()
    
    #print(optimal_filters_df[optimal_filters_df.spp_and_drug.isin(unknown_spp_and_drug)][["spp_and_drug", "n_genes_found"]])

    # keep some fields
    optimal_filters_df = optimal_filters_df[["species", "drug", "n_genes_found", "fraction_n_genes_expected_found", "correction_method", "min_epsilon", "pval_chi_square_RelToBranchLen", "pval_chi_square_phenotypes", "pval_GenoAndPheno_RelToBranchLen", "pval_GenoAndPheno_phenotypes", "fdr_threshold", "min_support", "ASR_methods_phenotypes", "alpha_pval"]]


    # return
    optimal_filters_df.to_excel("%s/optimal_filters_GWAS_each_spp_drug.xlsx"%PlotsDir)
    return optimal_filters_df


def get_gwas_conservative_filters_df(valid_spp_drug):

    """Returns a df like get_gwas_optimal_filters_df but with all the same and some conservative filters""" 

    # define the expected fields
    fields = ["species", "drug", "correction_method", "min_epsilon", "pval_chi_square_RelToBranchLen", "pval_chi_square_phenotypes", "pval_GenoAndPheno_RelToBranchLen", "pval_GenoAndPheno_phenotypes", "fdr_threshold", "min_support", "ASR_methods_phenotypes", "alpha_pval"]





    # init df
    filters_df = pd.DataFrame({"%s-%s"%(spp, drug) : {"species":spp, "drug":drug} for spp, drug in valid_spp_drug}).transpose()

    # set the conservative filters
    filters_df["correction_method"] = "bonferroni"
    filters_df["min_epsilon"] = 0.3

    for pval_f in ['pval_chi_square_RelToBranchLen', 'pval_chi_square_phenotypes', 'pval_GenoAndPheno_phenotypes']: 
        filters_df.loc[filters_df.index, pval_f] = False
    filters_df.loc[filters_df.index, 'pval_GenoAndPheno_RelToBranchLen'] = True
        
    filters_df["fdr_threshold"] = 0.05
    filters_df["min_support"] = 70
    filters_df["ASR_methods_phenotypes"] = "MPPA,DOWNPASS"
    filters_df["alpha_pval"] = 0.05

    return filters_df[fields]

def get_optimal_and_rationally_designed_filters_df(valid_spp_drug, filtering_stats_df, TablesDir):

    """Returns a df like get_gwas_optimal_filters_df but with all the same and some conservative filters. These have been rationally designed to find the common genes""" 

    # init df
    filters_df = pd.DataFrame({"%s-%s"%(spp, drug) : {"species":spp, "drug":drug} for spp, drug in valid_spp_drug}).transpose()


    this_is_unfisished

    # define the filters fileds
    filters_fields = ["correction_method", "fdr_threshold", "pval_chi_square_phenotypes", "pval_GenoAndPheno_phenotypes", "pval_fisher", "min_support", "min_epsilon", "min_hmean_fraction_nodes_GwP_PwG", "min_fraction_nodes_Pheno_wGeno", "min_fraction_nodes_Geno_wPheno", "min_nodes_GenoAndPheno", "ASR_methods_phenotypes"]


    # set the most common filters 
    filters_df["correction_method"] = "fdr_bh"
    filters_df["fdr_threshold"] = 0.05
    filters_df["pval_chi_square_phenotypes"] = True
    filters_df["pval_GenoAndPheno_phenotypes"] = False
    filters_df["pval_fisher"] = False
    filters_df["min_support"] = 70
    filters_df["min_epsilon"] = 0.33
    filters_df["min_OR"] = 1



    work_here

    #filters_df["min_hmean_fraction_nodes_GwP_PwG"] = 





    #get_filtering_stats_df_good_filters




    for pval_f in ['pval_chi_square_RelToBranchLen', 'pval_GenoAndPheno_RelToBranchLen', 'pval_GenoAndPheno_phenotypes']: 
        filters_df.loc[filters_df.index, pval_f] = False
    filters_df.loc[filters_df.index, 'pval_chi_square_phenotypes'] = True
    filters_df["fdr_threshold"] = 0.05
    filters_df["alpha_pval"] = 0.05
    filters_df["gwas_method"] = "synchronous"

    # set filters that are tuned for each species
    def get_r_with_species_drug_specific_filers(r):

        if r.name=="Candida_albicans-FLC": 
            r["ASR_methods_phenotypes"] = "MPPA"
            r["min_support"] = 70

        elif r.name=="Candida_auris-FLC": 
            r["ASR_methods_phenotypes"] = "MPPA"
            r["min_support"] = 50

        elif r.name=="Candida_auris-VRC": 
            r["ASR_methods_phenotypes"] = "DOWNPASS"
            r["min_support"] = 50

        else:
            r["ASR_methods_phenotypes"] = "DOWNPASS"
            r["min_support"] = 70

        return r

    filters_df = filters_df.apply(get_r_with_species_drug_specific_filers, axis=1)


    # add n genes
    filters_fields = ['species', 'drug', 'gwas_method', 'ASR_methods_phenotypes', 'correction_method', 'min_epsilon', 'min_OR', 'min_support', 'fdr_threshold', 'pval_chi_square_RelToBranchLen', 'pval_chi_square_phenotypes', 'pval_GenoAndPheno_RelToBranchLen', 'pval_GenoAndPheno_phenotypes', 'alpha_pval']
    filters_df = filters_df.merge(filtering_stats_df[filters_fields + ["n_genes_found", "n_genes_expected_found", "n_genes_expected"]], on=filters_fields, how="left", validate="one_to_one")
    check_no_nans_in_df(filters_df) 

    # chenge the index
    filters_df.index = ["%s-%s"%(r.species, r.drug) for I,r in filters_df.iterrows()]

    # write
    filters_df.to_excel("%s/optimal_filters_rationally_designed_GWAS_each_spp_drug.xlsx"%TablesDir)
    save_object(filters_df, "%s/optimal_filters_rationally_designed_GWAS_each_spp_drug.py"%TablesDir)

    return filters_df


def generate_table_allStrainData(metadata_df, species_to_srr_to_sampleID, species_to_tree, TablesDir, DataDir, ProcessedDataDir):

    """Generates a table with all strains with variant calling and also the information"""

    # define the final table
    table_name = "%s/allStrainData.xlsx"%TablesDir
    df_name =  "%s/allStrainData.py"%ProcessedDataDir
    if file_is_empty(table_name) or file_is_empty(df_name) or True:

        # keep
        metadata_df = cp.deepcopy(metadata_df)

        # add coverage
        df_coverage = pd.concat([pd.read_csv("%s/%s_%i/coverage_all_srrs.tab"%(DataDir, species, taxID), sep="\t")[["srr", "mean_coverage", "pct_covered"]] for taxID, species in taxID_to_sciName.items()])
        metadata_df = metadata_df.merge(df_coverage, left_on="Run", right_on="srr", how="left", validate="one_to_one")
        check_no_nans_in_df(metadata_df[["mean_coverage", "pct_covered"]])

        # checks
        for species, tree in species_to_tree.items():
            print(species)
            samples_in_metadata = set(metadata_df[metadata_df.species_name==species].sampleID)
            samples_tree = set(tree.get_leaf_names())
            samples_dict = set(species_to_srr_to_sampleID[species].values())

            if samples_dict!=samples_tree: raise ValueError("samples_dict should have the same samples as samples_tree")

            if samples_in_metadata!=samples_tree: 
                print("samples from metadata %s are missing in tree"%(samples_in_metadata.difference(samples_tree)))
                print("samples from tree %s are missing in metadata"%(samples_tree.difference(samples_in_metadata)))

        # after these checks I conclude that the metadata df does not have the C. auris bad samples

        # add date
        metadata_df = get_metadata_df_with_collection_date(metadata_df)
        def get_str_or_nan(x):
            if pd.isna(x): return np.nan
            else: return "/".join([str(y) for y in x])
        metadata_df["collection_date"] = metadata_df.collection_date_tuple.apply(get_str_or_nan)

        # add location
        metadata_df = get_metadata_df_with_lat_and_lon(metadata_df)
        countries_dict = get_countries_dict_for_reverse_geocoding(); print("countries dict obtained. Adding to df...")
        metadata_df["collection_country"] = metadata_df.apply(lambda r: get_country_from_lat_lon(r["latitude"], r["longitude"], countries_dict), axis=1)
        def get_collection_latitude_longitude_or_nan(r):
            if pd.isna(r.latitude): return np.nan
            else: return "%.2f,%.2f"%(r.latitude, r.longitude)
        metadata_df["collection_latitude_longitude"] = metadata_df.apply(get_collection_latitude_longitude_or_nan, axis=1)

        # rename some fields
        metadata_df = metadata_df.rename(columns={"cladeID_previousPaper":"cladeID_previous", "cladeID_Tree_and_BranchLen":"cladeID_systematic"})

        metadata_df["numeric_sampleID"] = metadata_df.sampleID
        metadata_df["clonal_cluster"] = metadata_df.cladeID_clonal_tshdSNPsKb_1

        # init fields
        mic_fields = [ 'ANI_MIC', 'CAS_MIC', 'MIF_MIC', 'FLC_MIC', 'ITR_MIC', 'POS_MIC', 'VRC_MIC', 'IVZ_MIC', 'KET_MIC', 'MIZ_MIC', 'AMB_MIC', 'BVN_MIC', '5FC_MIC', 'TRB_MIC']
        final_fields = ["species_name", "BioProject", "Run", "numeric_sampleID", "BioSample", "type", "mean_coverage", "pct_covered", "cladeID_systematic", "cladeID_previous", "clonal_cluster", "collection_date", "collection_latitude_longitude", "collection_country", "paper_link"]

        # add the link of each study
        bp_to_link = {'PRJDB6988': 'https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0223433',
                     'PRJEB11905': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4814463',
                     'PRJEB12264': 'not found',
                     'PRJEB14717': 'https://www.sciencedirect.com/science/article/pii/S2052297516300749?via%3Dihub',
                     'PRJEB1755': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3704024',
                     'PRJEB1831': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5780349',
                     'PRJEB20230': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5874254',
                     'PRJEB20459': 'https://pubmed.ncbi.nlm.nih.gov/31059831',
                     'PRJEB21518': 'https://aricjournal.biomedcentral.com/articles/10.1186/s13756-016-0132-5',
                     'PRJEB27862': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6325710',
                     'PRJEB29190': 'https://www.frontiersin.org/articles/10.3389/fmicb.2019.01445/full',
                     'PRJEB40738': 'not found',
                     'PRJEB8387': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5549176',
                     'PRJEB9463': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4505117',
                     'PRJNA120009': 'not found',
                     'PRJNA165021': 'not found',
                     'PRJNA165023': 'not found',
                     'PRJNA165025': 'not found',
                     'PRJNA165027': 'not found',
                     'PRJNA165029': 'not found',
                     'PRJNA165031': 'not found',
                     'PRJNA165033': 'not found',
                     'PRJNA165035': 'not found',
                     'PRJNA165037': 'not found',
                     'PRJNA165039': 'not found',
                     'PRJNA194436': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3812114',
                     'PRJNA194439': 'https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1001692#s1',
                     'PRJNA200311': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6588375',
                     'PRJNA201506': 'not found',
                     'PRJNA210804': 'not found',
                     'PRJNA231221': 'not found',
                     'PRJNA257929': 'https://elifesciences.org/articles/00662',
                     'PRJNA267757': 'https://europepmc.org/article/med/26346253',
                     'PRJNA271803': 'https://journals.plos.org/plospathogens/article?id=10.1371/journal.ppat.1005308#ppat.1005308.s016',
                     'PRJNA277890': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5177745',
                     'PRJNA297263': 'https://mra.asm.org/content/5/10/e00328-16',
                     'PRJNA305340': 'https://www.biorxiv.org/content/10.1101/302943v1.full.pdf+html',
                     'PRJNA310957': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5908403',
                     'PRJNA322245': 'https://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1006404',
                     'PRJNA322852': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5047589',
                     'PRJNA323475': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5922528',
                     'PRJNA324272': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5119031',
                     'PRJNA326748': 'https://www.clinicalmicrobiologyandinfection.com/article/S1198-743X(17)30090-3/fulltext',
                     'PRJNA328792': 'https://www.nature.com/articles/s41467-018-07779-6',
                     'PRJNA329124': 'not found',
                     'PRJNA345600': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6140516',
                     'PRJNA356375': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5832965',
                     'PRJNA361477': 'https://www.cell.com/current-biology/fulltext/S0960-9822(17)31472-0?_returnURL=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS0960982217314720%3Fshowall%3Dtrue',
                     'PRJNA374542': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5555451',
                     'PRJNA393577': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6863853',
                     'PRJNA394762': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5676243',
                     'PRJNA395439': 'not found',
                     'PRJNA400662': 'https://science.sciencemag.org/content/362/6414/589.long',
                     'PRJNA415955': 'https://www.nejm.org/doi/10.1056/NEJMoa1714373',
                     'PRJNA417396': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6336345',
                     'PRJNA421185': 'https://www.biorxiv.org/content/10.1101/537340v1.full',
                     'PRJNA431439': 'not found',
                     'PRJNA432250': 'not found',
                     'PRJNA432884': 'https://www.nature.com/articles/s41467-018-04787-4',
                     'PRJNA433858': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5404545',
                     'PRJNA437988': 'not found',
                     'PRJNA470683': 'https://academic.oup.com/cid/article/68/1/15/4996781',
                     'PRJNA480138': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6287553',
                     'PRJNA480539': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6555181',
                     'PRJNA483064': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6355595',
                     'PRJNA485022': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6302580',
                     'PRJNA485145': 'https://wwwnc.cdc.gov/eid/article/25/9/19-0262_article',
                     'PRJNA485239': 'https://wwwnc.cdc.gov/eid/article/25/9/19-0262_article',
                     'PRJNA485259': 'https://wwwnc.cdc.gov/eid/article/25/9/19-0262_article',
                     'PRJNA485409': 'https://wwwnc.cdc.gov/eid/article/25/9/19-0262_article',
                     'PRJNA485414': 'https://wwwnc.cdc.gov/eid/article/25/9/19-0262_article',
                     'PRJNA485415': 'https://wwwnc.cdc.gov/eid/article/25/9/19-0262_article',
                     'PRJNA489773': 'https://www.nature.com/articles/s41598-019-38768-4',
                     'PRJNA493002': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6236803',
                     'PRJNA493622': 'https://www.thelancet.com/journals/laninf/article/PIIS1473-3099(18)30597-8/fulltext',
                     'PRJNA506893': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6379656',
                     'PRJNA510147': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6591007',
                     'PRJNA516045': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7481391',
                     'PRJNA524686': 'not found',
                     'PRJNA525402': 'https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/31467089',
                     'PRJNA540907': 'https://pubmed.ncbi.nlm.nih.gov/31600556',
                     'PRJNA541007': 'https://wwwnc.cdc.gov/eid/article/25/9/19-0686_article',
                     'PRJNA546118': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6595153',
                     'PRJNA549561': 'https://pubmed.ncbi.nlm.nih.gov/33077664',
                     'PRJNA555042': 'https://bmcbiol.biomedcentral.com/articles/10.1186/s12915-020-00776-6',
                     'PRJNA563885': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7263194',
                     'PRJNA563993': 'https://academic-oup-com.sire.ub.edu/jac/article/75/4/835/5700447',
                     'PRJNA579121': 'not found',
                     'PRJNA593955': 'not found',
                     'PRJNA595978': 'https://pubmed.ncbi.nlm.nih.gov/32345637',
                     'PRJNA596170': 'not found',
                     'PRJNA600925': 'not found',
                     'PRJNA603602': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7255648',
                     'PRJNA604451': 'https://www.biorxiv.org/content/10.1101/2020.11.25.397612v1.full',
                     'PRJNA605578': 'not found',
                     'PRJNA610214': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7449159',
                     'PRJNA634197': 'not found',
                     'PRJNA677456': 'not found',
                     'PRJNA677457': 'not found',
                     'PRJNA677458': 'not found',
                     'PRJNA677459': 'not found',
                     'PRJNA73979': 'not found',
                     'PRJNA75209': 'not found',
                     'PRJNA75211': 'not found',
                     'PRJNA75213': 'not found',
                     'PRJNA75215': 'not found',
                     'PRJNA75217': 'not found',
                     'PRJNA75219': 'not found',
                     'PRJNA75221': 'not found',
                     'PRJNA75223': 'not found',
                     'PRJNA75225': 'not found',
                     'PRJNA75227': 'not found',
                     'PRJNA75229': 'not found',
                     'PRJNA75231': 'not found',
                     'PRJNA75233': 'not found',
                     'PRJNA75235': 'not found',
                     'PRJNA75237': 'not found',
                     'PRJNA75239': 'not found',
                     'PRJNA75241': 'not found',
                     'PRJNA75243': 'not found',
                     'PRJNA75245': 'not found',
                     'PRJNA75247': 'not found'}

        for x in bp_to_link.values():
            if x!="not found" and (not x.startswith("https:/") or " " in x): raise ValueError("invalid %s"%x)

        metadata_df["paper_link"] = metadata_df.BioProject.apply(lambda x: bp_to_link[x])
        if any(metadata_df["paper_link"]==""): raise ValueError("empty links")

        # add resistance and susceptibility
        for type_susc, name_susc in [("R", "resistance"), ("S", "susceptibility"), ("I", "intermediate_susceptibility")]:
            all_drugs = [x.split("_")[0] for x in mic_fields]

            def get_drugs_resistant(r):
                susc_drugs = [d for d in all_drugs if r["%s_resistance"%d]==type_susc]
                if len(susc_drugs)==0: return np.nan
                else: return ",".join(susc_drugs)

            metadata_df[name_susc] = metadata_df.apply(get_drugs_resistant, axis=1)
            final_fields.append(name_susc)

        # add the final fields
        final_fields += mic_fields

        # define the final data
        strains_df = metadata_df[final_fields].sort_values(by=final_fields).reset_index(drop=True)
    
        # save
        print("saving")
        save_object(strains_df, df_name)
        strains_df.to_excel(table_name, index=False)

    return load_object(df_name)

def get_figure_DefineBreakpoints(df_allStrainData, PlotsDir, manually_curated_data, metadata_df):

    """Plots a histgram with the MIC and the breakpoints for all species"""

    # keep
    df_allStrainData = cp.deepcopy(df_allStrainData)

    # load the bps data
    df_eucast_bps = pd.read_excel("%s/Eucast_MIC_breakpoints_perSpecies.xlsx"%manually_curated_data).set_index("species").applymap(float)
    df_eucast_bps.columns = [c.split("_")[0] for c in df_eucast_bps.columns]
    df_manual_MICs_all = pd.read_excel("%s/ManualMIC_breakpoints.xlsx"%manually_curated_data)
    df_manual_MICs = df_manual_MICs_all.pivot(index="species", columns="drug", values="breakpoint").applymap(float)
    def get_systematic_source(x): 
        if pd.isna(x): return x
        return {"Chow2020, large C. auris dataset":"PMC7188998", 'PMC89555. In table 2 there are 2 R isolates':"PMC89555", "Manual":"Manual", "PMC7449159":"PMC7449159", 'Manual. No resistant isolates':"Manual", 'Manual. One resistant ':"Manual", 'Manual. Literature':"Manual"}[x]
    df_manual_MICs_source = df_manual_MICs_all.pivot(index="species", columns="drug", values="source").applymap(get_systematic_source)

    # define the breakpoints
    species_to_drug_to_breakpoint = {}
    species_to_drug_to_type_bp = {}

    for species in sorted(set(df_allStrainData.species_name)):
        for drug in [c.split("_")[0] for c in df_allStrainData.columns if c.endswith("_MIC")]:

            df_spp = df_allStrainData[df_allStrainData.species_name==species]
            if all(pd.isna(df_spp["%s_MIC"%drug])): continue

            bp_eucast = df_eucast_bps.loc[species, drug]
            if drug in df_manual_MICs.columns and species in df_manual_MICs.index: bp_manual = df_manual_MICs.loc[species, drug]
            else: bp_manual = np.nan

            if not pd.isna(bp_eucast): 
                bp = bp_eucast
                type_bp = "EUCAST"

            elif not pd.isna(bp_manual): 
                bp = bp_manual
                type_bp = df_manual_MICs_source.loc[species, drug]

            else: raise ValueError("invalid bp for %s-%s"%(species, drug))

            species_to_drug_to_breakpoint.setdefault(species, {}).setdefault(drug, bp)
            species_to_drug_to_type_bp.setdefault(species, {}).setdefault(drug, type_bp)

    # print the number of combinations
    all_spp_drug = make_flat_listOflists([[(spp, d) for d in d_to_bp.keys()] for spp, d_to_bp in species_to_drug_to_breakpoint.items()])
    print(len(all_spp_drug), "spp-d combinations") # 45

    # define the species and drugs that have GWAS data done
    spp_to_drug_to_samplesGWAS = get_species_to_drug_to_samplesForGWAS_clinicalIsolates(metadata_df)

    # make the figure
    nrows = 8
    ncols = 6
    fig = plt.figure(figsize=(ncols*3, nrows*3))
    Ip = 1


    for Ir in range(nrows):
        for Ic in range(ncols):

            if Ip>len(all_spp_drug): break
            spp, drug = all_spp_drug[Ip-1]

            # init subplot
            ax = plt.subplot(nrows, ncols, Ip); Ip+=1

            # get hist
            df_spp = metadata_df[(metadata_df.species_name==spp) & ~(pd.isna(metadata_df["%s_MIC"%drug]))]
            if any(df_spp["%s_MIC"%drug]==0): raise ValueError("cant be 0s")

            sns.distplot(np.log2(df_spp["%s_MIC"%drug]), rug=True, kde=False, hist=True, color=species_to_color[spp])

            # add bp
            bp = species_to_drug_to_breakpoint[spp][drug]
            for x, color in [(bp/2, "red"), (bp, "black"), (2*bp, "red")]: ax.axvline(np.log2(x), linewidth=1.6, linestyle="--", color=color)

            # add the number of isolates
            text_str = "%s-%s"%(spp.split("_")[1], drug) + " " + "/".join(["%i%s"%(sum((metadata_df.species_name==spp) & (metadata_df["%s_resistance"%drug]==type_s)), type_s) for type_s in ["S", "I", "R"]]) + "\nBP=%.3f (%s)"%(bp, species_to_drug_to_type_bp[spp][drug]) 

            
            if spp in spp_to_drug_to_samplesGWAS and drug in spp_to_drug_to_samplesGWAS[spp]: ax.set_title(text_str, fontweight="bold")
            else: ax.set_title(text_str)

            # add axes
            if Ir==7 and Ic==2: ax.set_xlabel("log2 (Minimum Inhibitory Concentration (mg/L))", size=14)
            else: ax.set_xlabel("")

            if Ir==3 and Ic==0: ax.set_ylabel("Number of isolates", size=14)

            # add legend
            if Ir==7 and Ic==2:
            #if Ir==0 and Ic==0:

                def get_empty_legend(label): return Line2D([0], [0], marker="o", label=label, markersize=0, lw=0)
                def get_legend_element(color, label): return Line2D([0], [0], color=color, linestyle="--", label=label) 

                legend_elements = [get_empty_legend("breakpoints")] + [get_legend_element("black", "actual breakpoint"), get_legend_element("red", "0.5·breakpoint & 2·breakpoint")]
                ax.legend(handles=legend_elements, loc="right", bbox_to_anchor=(3.7, 0.5), fontsize=14)

    plt.subplots_adjust(wspace=0.35,  hspace=0.6)
    fig.savefig("%s/DefineBreakpoints.pdf"%PlotsDir, bbox_inches="tight")



def get_figure_clade_definition(metadata_df, species_to_tree, PlotsDir, ProcessedDataDir):


    """Plots how different branch min_branch lengths define different clades"""


    ######### get data ###########

    df_plot_file = "%s/get_figure_clade_definition_data.py"%ProcessedDataDir
    if file_is_empty(df_plot_file):

        data_dict = {}; I=0
        for species in taxID_to_sciName.values():

            # define the best_min_fraction_branchLen for each species (this was used to define the clades)
            best_min_fraction_branchLen = species_to_min_fraction_branchLen[species]
            min_branch_suport = 95

            # go through different lens, including the best_min_fraction_branchLen
            for min_fraction_branchLen in (list(np.linspace(0.001, 0.2, 30)) + [best_min_fraction_branchLen]):
                print(species, min_fraction_branchLen)

                # get the tree with the clade IDs
                tree_withCladeIDs = get_tree_with_cladeID_from_supportAndBranchLength(species_to_tree[species], min_fraction_branchLen, min_branch_suport)
                leaf_to_clade = pd.Series({l.name : l.cladeID for l in tree_withCladeIDs.get_leaves()})

                fraction_samples_clade = 1-(sum(pd.isna(leaf_to_clade))/len(leaf_to_clade))
                nclades = len(set(leaf_to_clade[~pd.isna(leaf_to_clade)].values))

                data_dict[I] = {"min_fraction_branchLen":min_fraction_branchLen, "fraction_samples_clade":fraction_samples_clade, "nclades":nclades, "species":species}; I+=1

        df_plot = pd.DataFrame(data_dict).transpose().sort_values(by="min_fraction_branchLen")
        save_object(df_plot, df_plot_file)

    df_plot = load_object(df_plot_file)

    ##############################


    #### plot #####

    # init fig
    nrows = 2
    ncols = 3
    fig = plt.figure(figsize=(ncols*4, nrows*4))
    Ip = 1
    iterable_rows_and_cols = [(Ir, Ic) for Ir in range(nrows) for Ic in range(ncols)]

   
    for species in sorted_species_byPhylogeny:
        Ir, Ic = iterable_rows_and_cols[Ip-1]

        # get df
        df_spp = df_plot[df_plot.species==species]

        # get best fraction
        best_min_fraction_branchLen = species_to_min_fraction_branchLen[species]

        # define the title to include the selected number of clades and the numbers of samples with some clade
        selected_r = df_spp[df_spp.min_fraction_branchLen==best_min_fraction_branchLen].iloc[0]
        tree = cp.deepcopy(species_to_tree[species])
        nsamples = len(tree.get_leaf_names())
        nsamples_with_clade = int(nsamples*selected_r.fraction_samples_clade)
        nclades = int(selected_r.nclades)
        expected_nclades = len(set(metadata_df[(metadata_df.species_name==species) & ~(pd.isna(metadata_df.cladeID_previousPaper))].cladeID_previousPaper))


        # init subplot
        ax = plt.subplot(nrows, ncols, Ip); Ip+=1

        # add plot
        ax.plot(df_spp.min_fraction_branchLen, df_spp.fraction_samples_clade, color="royalblue", linewidth=2)
        
        # add line with all
        plt.axhline(1.0, color="royalblue", linestyle="--")

        # switch to ax2
        ax2 = ax.twinx()
        df_spp.plot(x="min_fraction_branchLen", y="nclades", ax=ax2, color="tomato", linewidth=2, legend=False)

        # add best_min_fraction_branchLen
        plt.axvline(best_min_fraction_branchLen, color="k", linestyle="--")

        # add the number of expected clades
        if expected_nclades!=0: ax2.axhline(expected_nclades, color="tomato", linestyle="--")

        # change the axis
        if Ic==0: ax.set_ylabel("fraction samples with clade", color="royalblue", fontsize=12)
        ax.spines['left'].set_color('royalblue')
        ax.tick_params(axis='y', colors='royalblue')

        if Ic==2: ax2.set_ylabel("total # clades", color="tomato", fontsize=12)
        ax2.spines['right'].set_color('tomato')
        ax2.tick_params(axis='y', colors='tomato')

        if Ir==1 and Ic==1: ax.set_xlabel("minimum relative branch length", fontsize=12)

        # title
        ax.set_title("C. %s %i clades (%i previous)\n%i/%i (%.1f%s) strains w/ clade"%(species.split("_")[1], nclades, expected_nclades, nsamples_with_clade, nsamples, (nsamples_with_clade/nsamples)*100, "%"), linespacing=1.8)


    plt.subplots_adjust(wspace=0.4,  hspace=0.3)
    plt.show()

    # save
    fig.savefig("%s/clade_definition.pdf"%PlotsDir, bbox_inches="tight")

    ###############

def get_figure_Dataset_Overview_trees_clades_and_donutplots(metadata_df, PlotsDir, ProcessedDataDir, species_to_tree):

    """Plots, for each species, a tree with the clades as a donut chart with information about the type of strain"""

    metadata_df = cp.deepcopy(metadata_df)

    # one plot for each species
    for species in sorted_species_byPhylogeny:
        print(species)

        #if species not in {"Candida_orthopsilosis", "Candida_albicans"}: continue
        #if species not in {"Candida_auris"}: continue

        ###### GET TREE #####

        # remove bad samples
        tree = cp.deepcopy(species_to_tree[species])
        correct_samples = set(tree.get_leaf_names()).difference({str(x) for x in sciName_to_badSamples[species]})
        tree.prune(correct_samples)

        # remove low support branches
        tree = get_correct_tree(tree, min_support=95)

        # define metadata df for this species
        metadata_df_s = metadata_df[metadata_df.species_name==species]
        metadata_df_s["sampleID"] = metadata_df_s.sampleID.apply(int)

        # define the sample_to_clade
        def get_correct_cladeID_Tree_and_BranchLen(r):
            if pd.isna(r.cladeID_Tree_and_BranchLen): return "unassignedSample_%i"%(r.sampleID)
            else: return int(r.cladeID_Tree_and_BranchLen)

        metadata_df_s["correct_cladeID_Tree_and_BranchLen"] = metadata_df_s.apply(get_correct_cladeID_Tree_and_BranchLen, axis=1)
        sample_to_clade = metadata_df_s[metadata_df_s.sampleID.apply(str).isin(correct_samples)].set_index("sampleID", drop=False).correct_cladeID_Tree_and_BranchLen

        # set clade to color
        sorted_numeric_clades = sorted(set(sample_to_clade[sample_to_clade.apply(type)==int]))
        if len(sorted_numeric_clades)<10: palette="tab10"
        else: palette="tab20"
        clade_to_color = get_value_to_color(sorted_numeric_clades, palette=palette, n=len(sorted_numeric_clades), type_color="hex")[0]

        # add unassigned clades colors
        for c in set(sample_to_clade):
            if str(c).startswith("unassignedSample_"):  clade_to_color[c] = "white"

        # convert to str
        clade_to_color  = {str(cID):c for cID,c in clade_to_color.items()}
        sample_to_clade = {str(s):str(c) for s,c in sample_to_clade.items()}

        # map each clade to the samples
        clade_to_samples = {}
        for s, c in sample_to_clade.items():
            if c not in clade_to_samples: clade_to_samples[c] = list()
            clade_to_samples[c].append(s)

        # add the cladeID
        tree = get_tree_withcladeID_as_str(tree, sample_to_clade)

        """
        # keep only ancestral nodes of each clade
        for n in tree.traverse():
            if n.is_root(): continue
            if pd.isna((n.get_ancestors()[0]).cladeID) and not pd.isna(n.cladeID): 
                for child in n.get_children(): child.detach()
                n.name = n.cladeID

        # add children to the each clade in a collapsed manner
        for l in tree.get_leaves():
            if not l.cladeID.startswith("unassignedSample_"):
                for s in clade_to_samples[l.name]: l.add_child(name=s, dist=0)
                for child in l.get_children(): child.cladeID = l.cladeID
        """

        # get tree (traverse with preorder to get the correct clades)
        clade_group_to_size = {} # this is for the piechart
        clade_group_names = []
        previous_clade = None
        sorted_samples_as_in_tree = []

        # define the number of samples
        nsamples = len(tree.get_leaf_names())

        # iterate through tree
        for n in tree.traverse("preorder"):

            # init style
            nst = NodeStyle()

            # define the bwidth    
            species_to_bwidth = {"Candida_albicans":40, "Candida_glabrata":80, "Candida_auris":20, "Candida_tropicalis":8, "Candida_metapsilosis":8, "Candida_parapsilosis":8, "Candida_orthopsilosis":8}
            bwidth = species_to_bwidth[species]

            nst["hz_line_width"] = bwidth
            nst["vt_line_width"] = bwidth

            # define the size of the node
            nst["size"] = 0

            # add the colors
            if pd.isna(n.cladeID) or n.cladeID.startswith("unassignedSample_"): clade_color = "black"
            else: clade_color = clade_to_color[n.cladeID]

            nst["fgcolor"] = "black"
            nst["hz_line_color"] = clade_color
            nst["vt_line_color"] = clade_color
            #nst["bgcolor"] = bgcolor
            if n.is_leaf(): n.add_face(RectFace(20, 40, fgcolor="white", bgcolor="white"), column=0 , position="aligned") #

            # add style
            n.set_style(nst)


            # keep clade for pie chart
            if n.is_leaf(): 

                if n.cladeID!=previous_clade:
                    clade_group_names.append(n.cladeID)
                    clade_group_to_size[n.cladeID] = 0

                clade_group_to_size[n.cladeID] += 1
                previous_clade = n.cladeID
                sorted_samples_as_in_tree.append(n.name)



        # init treestyle
        ts = TreeStyle()
        ts.show_branch_length = False
        ts.show_branch_support = False
        ts.show_leaf_name = False
        ts.mode = "c"
        ts.root_opening_factor = 1 # the higher the morecompact
        #ts.optimal_scale_level = "full"
        ts.arc_start = 0 # 0 degrees = 3 o'clock
        ts.arc_span = 360

        # show
        #tree.show(tree_style=ts)

        # save the tree
        plots_dir = "%s/Dataset_overwiew_trees_eachSpecies"%PlotsDir; make_folder(plots_dir)
        filename = "%s/%s_onlyTree.pdf"%(plots_dir, species)
        #tree.render(file_name=filename, tree_style=ts, dpi=500,  units='mm', h=len(sample_to_clade)*2) #w=20
        tree.render(file_name=filename, tree_style=ts) #w=20

        #####################


        ###### DONUT CHART ########

        # make the donut chart

        # init
        import matplotlib.pyplot as plt
        fig, ax = plt.subplots()

        # reverse the group names to match the tree
        clade_group_names = list(reversed(clade_group_names))
        sorted_samples_as_in_tree = list(reversed(sorted_samples_as_in_tree))

        # define the general radius proportional to the number of samples
        total_radius = np.log2(nsamples)*0.12

        # define the width of each ring
        width_ring = total_radius*0.2

        # create the pie with the previous clade assignation (a string)

        # define a type of clade for all samples
        labels_previous_clades = []
        type_previous_clade_to_str = {"=":"1-to-1 match with previous clade", "*":"new clade", "":"unassigned strain", "x":"inconsistent w/ previous clades"}
        for clade in clade_group_names:

            # update the data with the number of samples
            df_clade = metadata_df_s[(metadata_df_s.correct_cladeID_Tree_and_BranchLen.apply(str)==clade) & ~(pd.isna(metadata_df_s.cladeID_previousPaper))]

            # unassinged sample
            if clade.startswith("unassignedSample_"): label = ""

            # clades 2,3 of auris are correct (manually curated). It is just SRR10852068, that should be clade 3 (according to Chow202) but it is clade 2 in our dataset
            elif species=="Candida_auris" and clade in {"2", "3"}: label = "="

            # no match
            elif len(df_clade)==0: label = "*"

            # some matches
            elif len(set(df_clade.cladeID_previousPaper))==1 and len(set(metadata_df_s[metadata_df_s.cladeID_previousPaper==(df_clade.cladeID_previousPaper.iloc[0])].correct_cladeID_Tree_and_BranchLen))==1:  label = "="

            else: 

                print("\ninconsistent clades (Run, clade, previous clade)")
                for I, r in metadata_df_s[(metadata_df_s.correct_cladeID_Tree_and_BranchLen==clade) | (metadata_df_s.cladeID_previousPaper.isin(set(df_clade.cladeID_previousPaper)))][["cladeID_Tree_and_BranchLen", "cladeID_previousPaper", "Run"]].drop_duplicates().iterrows(): print(r.Run, r.cladeID_Tree_and_BranchLen, r.cladeID_previousPaper)

                label = "x"

            labels_previous_clades.append(label)

        # create pie with the type of clade as compared to previous
        pie_clades, _ = ax.pie([clade_group_to_size[c] for c in clade_group_names], radius=total_radius, colors=["white" for c in clade_group_names], labels=labels_previous_clades, textprops={'fontsize': 12})
        plt.setp( pie_clades, width=0, edgecolor='white')

        # create with clade assignation
        pie_clades, _ = ax.pie([clade_group_to_size[c] for c in clade_group_names], radius=total_radius, colors=[clade_to_color[c] for c in clade_group_names])
        plt.setp( pie_clades, width=width_ring, edgecolor='white')

        # create the outside ring with the type of data (clinical, environmental or other)
        type_strain_to_color = {"clinical":"black", "environmental":"red", "other":"darkgrey"}
        def get_type_strain_simple(x):
            if x in {"clinical", "environmental"}: return x
            else: return "other"
        metadata_df_s["type_strain_simple"] = metadata_df_s.type.apply(get_type_strain_simple)
        list_tuples_clade_and_strain = make_flat_listOflists([[(c, type_strain)for type_strain in type_strain_to_color] for c in clade_group_names])
        data_type_strain = [sum((metadata_df_s.type_strain_simple==type_strain) & (metadata_df_s.correct_cladeID_Tree_and_BranchLen.apply(str)==c)) for c, type_strain in list_tuples_clade_and_strain]
        if sum(data_type_strain)!=sum(clade_group_to_size.values()): raise ValueError("There are not the same number of strains")

        pie_type, _ = ax.pie(data_type_strain, radius=total_radius-width_ring, labels=[""]*len(list_tuples_clade_and_strain), colors=[type_strain_to_color[type_strain] for c, type_strain in list_tuples_clade_and_strain] )
        plt.setp( pie_type, width=width_ring, edgecolor='white')

        # save
        plt.margins(0,0)
        plots_dir = "%s/Dataset_overwiew_cladePieCharts"%PlotsDir; make_folder(plots_dir)
        fig.savefig("%s/%s.pdf"%(plots_dir, species), bbox_inches="tight")

        ###########################

def get_figure_Dataset_Overview_species_tree(ProcessedDataDir_allCmine, PlotsDir, height_rect=300):

    """Plots the species tree generated by orthofinder"""

    # load the tree
    tree = Tree("%s/orthofinder_all_species/output_orthofinder/Results_Oct03/Species_Tree/SpeciesTree_rooted.txt"%ProcessedDataDir_allCmine)


    # custimize
    for n in tree.traverse():

        # set style
        nst = NodeStyle()
        nst["hz_line_width"] = 12
        nst["vt_line_width"] = 12
        nst["hz_line_color"] = "black"
        nst["vt_line_color"] = "black"
        nst["size"] = 0
        n.set_style(nst)

        # add empty face
        n.add_face(RectFace(20, height_rect, fgcolor="white", bgcolor="white", label={"text":"", "color":"black"}), position="aligned", column=0)


    ts = TreeStyle()
    ts.show_branch_length = False
    ts.show_branch_support = False
    ts.show_leaf_name = False

    ts.draw_guiding_lines = True
    ts.guiding_lines_type = 2 # 0=solid, 1=dashed, 2=dotted.
    ts.guiding_lines_color = "gray" 
     
    # save
    #tree.show(tree_style=ts)
    tree.render(file_name="%s/Dataset_overview_species_tree_rectH=%i.pdf"%(PlotsDir, height_rect), tree_style=ts)


def get_df_pairwise_diversity(DataDir, ProcessedDataDir, species_to_ref_genome, species_to_gff, metadata_df, species_to_tree, threads=4, type_comparison="number_vars"):

    """Gets the pairwise diversity df, comparing the number of variants or number of genes affected by variants """


    # keep
    metadata_df = cp.deepcopy(metadata_df)

    # define an outdir to store processed datasets
    outdir = "%s/outdir_df_pairwise_diversity_%s"%(ProcessedDataDir, type_comparison)
    make_folder(outdir)

    # define  parms
    all_typesVars =  ["SNP", "IN/DEL", "SV", "CNV"]

    ######### GET DF #######

    df_pairwiseDifferences_all_file = "%s/df_pairwiseDifferences.py"%(outdir)

    if file_is_empty(df_pairwiseDifferences_all_file):
        print("generating df_pairwiseDifferences_all_file")

        df_pairwiseDifferences_all = pd.DataFrame()
        for type_var in all_typesVars:        
            for species in sorted_species_byPhylogeny:
                print(type_var, species)

                #if species!="Candida_parapsilosis": continue

                # get the long df with pairwise differences
                df_pairwiseDifferences_file = "%s/%s_%s_df_pairWiseDifferences.py"%(outdir, type_var.replace("/", "-").replace(" ", "_"), species)
                taxID_dir = "%s/%s_%i"%(DataDir, species, sciName_to_taxID[species])

                if type_comparison=="number_vars":

                    # define the df with the pairwise differences
                    df_pairwiseDifferences = get_df_pairwise_differences_oneSpecies_and_typeVar(taxID_dir, df_pairwiseDifferences_file, threads, type_var, taxID_to_ploidy[sciName_to_taxID[species]], set(species_to_tree[species].get_leaf_names()))
                    
                    # add fields
                    genome_len = sum(get_chr_to_len(species_to_ref_genome[species]).values())
                    df_pairwiseDifferences["vars/kb"] = (df_pairwiseDifferences.diffent_positions / genome_len)*1000

                elif type_comparison in {"number_genes_all", "number_genes_protAltering"}:

                    # define a df with the pariwise differences
                    df_pairwiseDifferences = get_df_pairwise_differences_oneSpecies_and_typeVar_nGenes(taxID_dir, df_pairwiseDifferences_file, threads, type_var, taxID_to_ploidy[sciName_to_taxID[species]], set(species_to_tree[species].get_leaf_names()), type_comparison)

                    # define all expected genes
                    gff_df = load_gff3_intoDF(species_to_gff[species])
                    all_genes = set(gff_df[gff_df.feature.isin({"gene", "pseudogene"})].upmost_parent)
                    all_protein_coding_genes = set(gff_df[gff_df.feature.isin({"CDS", "mRNA"})].upmost_parent)
                    expected_geneIDs = {"number_genes_all":all_genes, "number_genes_protAltering":all_protein_coding_genes}[type_comparison]
                    print("There are %i expected genes"%len(expected_geneIDs))

                    # add fields
                    df_pairwiseDifferences["pct genes altered"] = (df_pairwiseDifferences.different_genes / len(expected_geneIDs))*100

                    # check that there is no genes above 100%
                    if any(df_pairwiseDifferences["pct genes altered"]>100): raise ValueError("There are some genes that are not annotated")

                else: raise ValueError("invalid")


                # set as strings    
                for f in ["origin_sample", "target_sample"]: df_pairwiseDifferences[f] = df_pairwiseDifferences[f].apply(str)

                # keep
                df_pairwiseDifferences["type_var"] = type_var
                df_pairwiseDifferences["species"] = species
                df_pairwiseDifferences_all = df_pairwiseDifferences_all.append(df_pairwiseDifferences)


        # add a string with the sorted origin and target
        def get_string_united_list(list_obj): return "_".join(list_obj)
        df_pairwiseDifferences_all["sorted_origin_target_str"] = df_pairwiseDifferences_all[["origin_sample", "target_sample"]].apply(sorted, axis=1).apply(get_string_united_list)
 
        print("saving")
        save_object(df_pairwiseDifferences_all, df_pairwiseDifferences_all_file)

    # load
    df_pairwiseDifferences_all = load_object(df_pairwiseDifferences_all_file)

    # redefine
    df_pairwiseDifferences_all["origin_sample"] = df_pairwiseDifferences_all.origin_sample.apply(str)
    df_pairwiseDifferences_all["target_sample"] = df_pairwiseDifferences_all.target_sample.apply(str)

    return df_pairwiseDifferences_all


def generate_table_Strain_metadata(metadata_df, TablesDir, type_comp_to_df_pairwise_diff):

    """Generates table w overall strain information"""

    metadata_df = cp.deepcopy(metadata_df)

    # init with the number of strains
    df_table = pd.DataFrame({"# strains" : metadata_df.groupby("species_name").apply(len)})
    df_table["species_name"] = df_table.index
    df_table["species"] = ["C. %s"%(x.split("_")[1]) for x in df_table.index]

    # add the type of isolate
    def get_type_strain_simple(x):
        if x in {"clinical", "environmental"}: return x
        else: return "other"
    metadata_df["type_strain_simple"] = metadata_df.type.apply(get_type_strain_simple)

    def get_nan_to_0(x):
        if pd.isna(x): return 0
        else: return x
    for type_s in ["clinical", "environmental", "other"]: df_table["# %s"%type_s] = df_table.species_name.map(metadata_df[metadata_df.type_strain_simple==type_s].groupby("species_name").apply(len)).apply(get_nan_to_0).apply(int)

    # put the drugs that have at least 5R isolates
    """
    all_drugs = [x.split("_")[0] for x in metadata_df.keys() if x.endswith("_MIC")]

    def get_interesting_drugs(s):
        df_spp = metadata_df[(metadata_df.species_name==s) & (metadata_df.type=="clinical")]
        #list_d = ["%s (%iR/%iS)"%(d, sum(df_spp["%s_resistance"%d]=="R"), sum(df_spp["%s_resistance"%d]=="S")) for d in all_drugs if sum(df_spp["%s_resistance"%d]=="R")>=5]
        list_d = ["%s (%i)"%(d, sum(df_spp["%s_resistance"%d]=="R")) for d in all_drugs if sum(df_spp["%s_resistance"%d]=="R")>=5]

        if list_d==[]: return ""
        else: return ", ".join(list_d)

    df_table["drugs >=5 resistant clinical isolates"] = df_table.species_name.apply(get_interesting_drugs)
    """

    # add number of clades
    df_table["# clades"] = df_table.species_name.map(metadata_df[~pd.isna(metadata_df.cladeID_Tree_and_BranchLen)].groupby("species_name").apply(lambda df: len(set(df.cladeID_Tree_and_BranchLen))))

    # add median pairwise diversity
    df_distances = type_comp_to_df_pairwise_diff["number_vars"].drop_duplicates(subset=["species", "type_var", "sorted_origin_target_str"])
    df_table["median pairwise SNPs/kb"] = df_table.species_name.map(df_distances[(df_distances.type_var=="SNP")].groupby("species").apply(lambda df: round(np.median(df["vars/kb"]), 2) ))


    # keep some fields
    final_fields = ["species", "# strains", "# clinical", "# environmental", "# other", "# clades", "median pairwise SNPs/kb"]
    #final_fields = ["species", "# strains", "# clinical", "# environmental", "# other", "drugs >=5 resistant clinical isolates"]

    df_table = df_table.loc[sorted_species_byPhylogeny][final_fields].reset_index(drop=True)

    print(df_table)

    return df_table



def get_figure_Diversity_patterns_heatmap(dict_df_pairwiseDifferences_all, type_comparison, PlotsDir, species_to_tree, metadata_df):

    """Plots the heatmap of the pairwise genetic distance"""

    metadata_df = cp.deepcopy(metadata_df)

    # define the sample_to_clade
    def get_correct_cladeID_Tree_and_BranchLen(r):
        if pd.isna(r.cladeID_Tree_and_BranchLen): return "unassignedSample_%i"%(int(r.sampleID))
        else: return int(r.cladeID_Tree_and_BranchLen)

    metadata_df["correct_cladeID_Tree_and_BranchLen"] = metadata_df.apply(get_correct_cladeID_Tree_and_BranchLen, axis=1)



    # get df
    df_pairwiseDifferences_all = cp.deepcopy(dict_df_pairwiseDifferences_all[type_comparison])

    # define vars
    all_typesVars =  ["SNP", "IN/DEL", "SV", "CNV"]


    # define the yfield depending on the type of data plot
    if type_comparison=="number_vars": 
        zfield = "vars/kb"
        label_text = "log(# vars/kb)"
        pseudocount =  0.001

    elif type_comparison=="number_genes_protAltering":  
        zfield = "pct genes altered"
        label_text = "log(% proteins changing)"
        pseudocount = 1


    # convert to log
    print("pseudocounting %s for %s"%(pseudocount, type_comparison))
    df_pairwiseDifferences_all[zfield] = np.log10(df_pairwiseDifferences_all[zfield] + pseudocount)

    # init fig
    nrows = len(all_typesVars)
    ncols = len(sorted_species_byPhylogeny)
    fig = plt.figure(figsize=(ncols*2.5, nrows*2.5)); Ip=1


    print("plotting")
    for Iv, type_var in enumerate(all_typesVars):
        for Is, species in enumerate(sorted_species_byPhylogeny):
            print(type_var, species)

            #if species!="Candida_parapsilosis": continue # debug

            # get df
            df_pairwiseDifferences = df_pairwiseDifferences_all[(df_pairwiseDifferences_all.species==species) & (df_pairwiseDifferences_all.type_var==type_var)]
            if len(df_pairwiseDifferences)==0: raise ValueError("there are no vars for %s, %s"%(type_var, species))

            # discard the bad samples
            bad_samples = set(map(str, sciName_to_badSamples[species]))
            df_pairwiseDifferences = df_pairwiseDifferences[~(df_pairwiseDifferences.origin_sample.isin(bad_samples)) & ~(df_pairwiseDifferences.target_sample.isin(bad_samples))]

            # define the vmax and vmin so that it is the same for all plots                
            vmax = max(df_pairwiseDifferences_all[zfield])
            vmin = min(df_pairwiseDifferences_all[zfield])
            cbar = (species==sorted_species_byPhylogeny[-1] and type_var=="IN/DEL")

            # get as a square df
            sorted_samples = [s for s in species_to_tree[species].get_leaf_names() if s not in bad_samples]
            df_plot = df_pairwiseDifferences[["origin_sample", "target_sample", zfield]].pivot(index="origin_sample", columns="target_sample", values=zfield).loc[sorted_samples, sorted_samples]

            # define the colorbar label
            if cbar: cbar_label = label_text
            else: cbar_label = ""

            # plot as a heatmap
            ax = plt.subplot(nrows, ncols, Ip); Ip+=1
            mask = np.zeros_like(df_plot)
            mask[np.triu_indices_from(mask)] = True

            if cbar is True: cax = inset_axes(ax, width="10%", height="80%", loc='lower left', bbox_to_anchor=(1.2, -0.5, 1, 1), bbox_transform=ax.transAxes, borderpad=0)
            else: cax = None

            sns.heatmap(df_plot, cbar_kws={"label":cbar_label}, ax=ax, cbar_ax=cax, mask=mask, vmax=vmax, cbar=cbar, vmin=vmin, cmap="rocket_r")

            # get the colors of the samples by clade
            metadata_df_s = metadata_df[metadata_df.species_name==species]
            sample_to_clade = metadata_df_s.set_index("sampleID", drop=False).correct_cladeID_Tree_and_BranchLen

            sorted_numeric_clades = sorted(set(sample_to_clade[sample_to_clade.apply(type)==int]))
            if len(sorted_numeric_clades)<10: palette="tab10"
            else: palette="tab20"
            clade_to_color = get_value_to_color(sorted_numeric_clades, palette=palette, n=len(sorted_numeric_clades), type_color="hex")[0]

            for c in set(sample_to_clade):
                if str(c).startswith("unassignedSample_"):  clade_to_color[c] = "white"

            # convert to str
            clade_to_color  = {str(cID):c for cID,c in clade_to_color.items()}
            sample_to_clade = {str(s):str(c) for s,c in sample_to_clade.items()}

            # add one rect for each sample
            sorted_samples_cladeColors = [clade_to_color[sample_to_clade[s]] for s in sorted_samples]
            width_rect = len(sorted_samples)*0.1


            for Isample, color in enumerate(sorted_samples_cladeColors):

                # add X axis rect
                ax.add_patch(patches.Rectangle((Isample, len(sorted_samples)), 1, width_rect, linewidth=0.1, edgecolor=color, facecolor=color)) # this is (x,y) anchor point and width, height

                # add Y axis rects
                ax.add_patch(patches.Rectangle((-width_rect, Isample), width_rect, 1, linewidth=0.1, edgecolor=color, facecolor=color)) # this is (x,y) anchor point and width, height

            ax.set_xticklabels([])
            ax.set_yticklabels([])
            ax.set_xticks([])
            ax.set_yticks([])
            ax.set_xlabel("")
            ax.set_ylabel("")
            ax.set_ylim([len(sorted_samples_cladeColors)+width_rect, 0])
            ax.set_xlim([-width_rect, len(sorted_samples_cladeColors)])

            if Iv==0: ax.set_title("C. %s"%(species.split("_")[1]))
            if Is==0: ax.set_ylabel(type_var)
            #if species==sorted_species[-1] and 

    # lims of the plot
    #plt.subplots_adjust(wspace=0.025, hspace=0.025)


    # save fig
    print("saving figure")
    filename = "%s/Diversity_patterns_trinagle_heatmaps.png"%(PlotsDir)
    fig.savefig(filename, bbox_inches='tight', dpi=1600)
    #plt.close(fig)

def get_figure_Diversity_patterns_boxplots(dict_df_pairwiseDifferences_all, type_comparison, PlotsDir, species_to_tree):

    """Gets the boxplots of the diversity"""

    # get df
    df_pairwiseDifferences_all = cp.deepcopy(dict_df_pairwiseDifferences_all[type_comparison])

    # define vars
    all_typesVars =  ["SNP", "IN/DEL", "SV", "CNV"]


    # define the yfield depending on the type of data plot
    if type_comparison=="number_vars": 
        zfield = "vars/kb"
        label_text = "log(# vars/kb)"
        pseudocount =  0.001
        boxplot_ylines = [np.log10(y+pseudocount) for y in [1]]

    elif type_comparison=="number_genes_protAltering":  
        zfield = "pct genes altered"
        label_text = "log(% proteins changing)"
        pseudocount = 1
        boxplot_ylines = [np.log10(y+pseudocount) for y in [10, 50, 100]]


    # convert to log
    print("pseudocounting %s for %s"%(pseudocount, type_comparison))
    df_pairwiseDifferences_all[zfield] = np.log10(df_pairwiseDifferences_all[zfield] + pseudocount)


    fig = plt.figure(figsize=(6, 3)) # init fig

    # keep only all_typesVars
    df_pairwiseDifferences_all = df_pairwiseDifferences_all[df_pairwiseDifferences_all.type_var.isin(set(all_typesVars))]


    # discard bad samples (only from C. auris)
    bad_samples_Cauris = set(map(str, sciName_to_badSamples['Candida_auris']))
    df_pairwiseDifferences_all = df_pairwiseDifferences_all[~((df_pairwiseDifferences_all.species=="Candida_auris") & ((df_pairwiseDifferences_all.origin_sample.isin(bad_samples_Cauris)) | (df_pairwiseDifferences_all.target_sample.isin(bad_samples_Cauris))))]

    # sort by species
    species_to_I = dict(zip(sorted_species_byPhylogeny, range(len(sorted_species_byPhylogeny))))
    df_pairwiseDifferences_all["species_I"]  = df_pairwiseDifferences_all.species.map(species_to_I)

    typeVar_to_I = dict(zip(all_typesVars, range(len(all_typesVars))))
    df_pairwiseDifferences_all["type_var_I"]  = df_pairwiseDifferences_all.type_var.map(typeVar_to_I)

    df_pairwiseDifferences_all = df_pairwiseDifferences_all.sort_values(by=["type_var_I", "species_I"])

    """
    meta_data = df_pairwiseDifferences_all[(df_pairwiseDifferences_all.type_var=="SNP") & (df_pairwiseDifferences_all.species=="Candida_metapsilosis")][zfield]

    sns.distplot(meta_data)

    print(meta_data)

    sys.exit(0)
    """

    # get a boxplot
    #ax = sns.boxplot(data=df_pairwiseDifferences_all, x="type_var", y=zfield, hue="species", palette=species_to_color)

    palette = {"SNP":"red", "IN/DEL":"magenta", "SV":"navy", "CNV":"cyan"}
    ax = sns.boxplot(data=df_pairwiseDifferences_all, x="species", y=zfield, hue="type_var", palette=palette)


    ax.set_xticklabels([s.split("_")[1] for s in sorted_species_byPhylogeny], rotation=90)


    # add lines
    for y in boxplot_ylines: plt.axhline(y, color="k", linestyle="--", linewidth=".9")

    # edit boxplot
    for i,artist in enumerate(ax.artists): # each artist has 6 elements. This is from https://stackoverflow.com/questions/36874697/how-to-edit-properties-of-whiskers-fliers-caps-etc-in-seaborn-boxplot

        # get color
        col = artist.get_facecolor()
        r, g, b, a = col
        artist.set_edgecolor(col)
        artist.set_facecolor((r, g, b, 0.3))

        for j in range(i*6,i*6+6):
            line = ax.lines[j]
            line.set_color(col)
            line.set_mfc(col)
            line.set_mec(col)

    ax.set_ylabel(label_text)
    #ax.set_yscale({True:"log", False:"linear"}[log_scale])
    ax.legend(bbox_to_anchor=(1.01, 1), loc=2, borderaxespad=0.)

    # add jitter
    #print("add strip"); sns.stripplot(data=df_pairwiseDifferences_all, x="species", y=zfield, hue="type_var", dodge=True, alpha=.01, palette=palette)

    print("saving figure")
    filename = "%s/Diversity_paterns_pairwise_distances_boxplot_%s.png"%(PlotsDir, type_comparison)
    fig.savefig(filename, bbox_inches='tight', dpi=1200)
    #fig.savefig(filename, bbox_inches='tight')

    plt.show()


def get_figure_variants_MAF(DataDir, ProcessedDataDir, PlotsDir, species_to_tree):

    """This function will plot the MAF distribution of different SVtypes"""

    ####### GET DF SVs ########

    df_varFrequencies_SVs = pd.DataFrame()

    # go through each species
    for taxID, species in taxID_to_sciName.items():
        print(species)

        # debug
        #if species!="Candida_parapsilosis": continue

        # load a df where each row is one unique SV, all samples present
        taxID_dir = "%s/%s_%i"%(DataDir, species, taxID)
        SV_CNV_filt_file = "%s/integrated_varcalls/SV_CNV_filt.py"%(taxID_dir)
        SV_df_oneVarPerRow_file = "%s/%s_SVs_CNVs_oneVarPerRow_allSamples.py"%(ProcessedDataDir, species)
        #remove_file(SV_df_oneVarPerRow_file); continue
        SV_df = get_SV_df_oneVarPerRow_df(SV_CNV_filt_file, SV_df_oneVarPerRow_file, only_unique_SVs=False, threads=threads, var_types={"SV", "coverageCNV"})

        # remove the bad samples
        bad_samples = set(map(str, sciName_to_badSamples[species]))
        SV_df = SV_df[~(SV_df.sampleID.apply(str).isin(bad_samples))]

        # set the fact that all SVs have AF=1
        SV_df["AF_from_GT"] = 1.0

        # get the df with the minor allele freq
        nsamples = len(set(species_to_tree[species].get_leaf_names()).difference(bad_samples))
        df_MAF = get_df_MAF_from_df_vars(SV_df, nsamples, variant_f="variantID_across_samples", var_type_f="svtype")

        # add the species
        df_MAF["species"] = species

        # keep
        df_varFrequencies_SVs = df_varFrequencies_SVs.append(df_MAF)

    #######################

    ########### GET DF VAR FREQUENCIES SMALL VARS ###########

    df_varFrequencies_smallVars_file = "%s/small_varFrequencies_df.py"%ProcessedDataDir
    if file_is_empty(df_varFrequencies_smallVars_file):
        print("getting df_varFrequencies_smallVars_file")

        # init df
        df_varFrequencies_smallVars = pd.DataFrame()

        # go through each species
        for taxID, species in taxID_to_sciName.items():
            print(species)

            #if species!="Candida_auris": continue

            # load the df
            print("loading df")
            df_vars = load_object("%s/%s_%i/integrated_varcalls/smallVars_filt.py"%(DataDir, species, taxID))[["#Uploaded_variation", "sampleID", "ISSNP", "common_GT", "calling_ploidy"]]

            # get the df with the correct ploidy and some GT called
            print("getting the MAFs")
            df_vars = df_vars[(df_vars.calling_ploidy==taxID_to_ploidy[taxID]) & (df_vars.common_GT!=".")]

            # remove the bad samples
            bad_samples = set(map(str, sciName_to_badSamples[species]))
            df_vars = df_vars[~(df_vars.sampleID.apply(str).isin(bad_samples))]

            # add the af_from_GT
            df_vars["common_GT"] = df_vars.common_GT.apply(str)
            strange_GTs = set(df_vars.common_GT).difference({"1", "0/1", "1/1"})
            if len(strange_GTs)>0: raise ValueError("There are strange gts: %s"%strange_GTs)

            df_vars["AF_from_GT"] = df_vars.common_GT.map({"1":1.0, "0/1":0.5, "1/1":1.0})
            if any(pd.isna(df_vars.AF_from_GT)): raise ValueError("There can't be NaNs")

            # add the type_var
            df_vars["type_var"] = df_vars.ISSNP.map({True:"SNP", False:"IN/DEL"})
            if any(pd.isna(df_vars.type_var)): raise ValueError("There can't be NaNs")

            # get the df with the minor allele freq
            nsamples = len(set(species_to_tree[species].get_leaf_names()))
            df_MAF = get_df_MAF_from_df_vars(df_vars, nsamples, variant_f="#Uploaded_variation", var_type_f="type_var")

            # add the species
            df_MAF["species"] = species

            # keep
            df_varFrequencies_smallVars = df_varFrequencies_smallVars.append(df_MAF)

        # save
        save_object(df_varFrequencies_smallVars, df_varFrequencies_smallVars_file)

    # load
    df_varFrequencies_smallVars = load_object(df_varFrequencies_smallVars_file)

    #########################################################

    ####### PLOT DISTPLOT ########

    print("plotting distplot")

    sorted_svtypes = ["deletion", "tandem_duplication", "inversion", "copyPaste_insertion", "cutPaste_insertion", "bal_translocation", "inverted_copyPaste_insertion", "inverted_translocation", "inverted_cutPaste_insertion", "unclassified_breakpoint", "coverage_deletion", "coverage_duplication"]


    sorted_smallVarTypes = ["SNP", "IN/DEL"]

    # define the var iterables
    var_iterables = [("SVs", df_varFrequencies_SVs, sorted_svtypes, [0.01, 0.1, 0.5], [10, 100, 1000]),
                     ("small vars", df_varFrequencies_smallVars, sorted_smallVarTypes, [0.01, 0.1, 0.5], [100, 1000, 10000])]


    # define the minimum AF
    #min_AF = min([min(df_varFrequencies_smallVars.MAF), min(df_varFrequencies_SVs.MAF)])
    #max_AF = max([max(df_varFrequencies_smallVars.MAF), max(df_varFrequencies_SVs.MAF)])

    # get one set of histograms for each plot
    for Iv, (var_type, df_MAF, sorted_typeVars, xticks, yticks)  in enumerate(var_iterables):
        print(var_type)


        filename = "%s/variantsMAF_%s.pdf"%(PlotsDir, var_type)
        plot_histograms_rows_in_species_vatypes_in_cols(df_MAF, sorted_typeVars, "MAF", filename, "var_type", logscale_x=True, logscale_y=True, pseudocount=0, xlabel="Minor Allele Frequency", input_xticks=xticks, input_yticks=yticks, extended_xrange=0.1)



    ##############################



def get_figure_mechanisms_SV_formation_overlap_simple_repeats(unique_SV_CNV_df, unique_small_vars_df, PlotsDir, min_pct_overlap_CNV):

    """This function plots whether each of the vars overlaps simple repeats or not"""

    ########## PREPARE THE DFS ##########
    print("adding the overlaps repeats")

    # debug
    if any(pd.isna(unique_SV_CNV_df[unique_SV_CNV_df.type_var=="coverageCNV"]["repeat#Low_complexity#pctCoverage"])): raise ValueError("There can't be Nans in Low_complexity pct") 
    if any(pd.isna(unique_SV_CNV_df[unique_SV_CNV_df.type_var=="coverageCNV"]["repeat#Simple_repeat#pctCoverage"])): raise ValueError("There can't be Nans in Simple_repeat pct") 

    # get whether the variants overlap simple repeats
    simple_repeats = {"Simple_repeat", "Low_complexity"}
    unique_SV_CNV_df = get_unique_SV_CNV_df_with_overlaps_simple_repeats(unique_SV_CNV_df, min_pct_overlap_CNV, simple_repeats)
    unique_small_vars_df = get_unique_small_vars_df_with_overlaps_simple_repeats(unique_small_vars_df, simple_repeats)

    # add type overlap
    bool_to_typeOverlap = {True:"overlap repeats", False:"no overlap repeats"}
    unique_SV_CNV_df["type_overlap"] = unique_SV_CNV_df.overlaps_simple_repeats.map(bool_to_typeOverlap)
    unique_small_vars_df["type_overlap"] = unique_small_vars_df.overlaps_simple_repeats.map(bool_to_typeOverlap)

    # merge the vars into one single 
    print("merging")

    unique_small_vars_df["variant_type"] = unique_small_vars_df.ISSNP.map({True:"SNP", False:"IN/DEL"})
    unique_SV_CNV_df["variant_type"] = unique_SV_CNV_df.svtype

    var_fields = ["variant_type", "species", "type_overlap"]
    df_plot = pd.concat([d[var_fields] for d in [unique_small_vars_df, unique_SV_CNV_df]])

    # debug
    if any(pd.isna(df_plot.type_overlap)): raise ValueError("there has to be some type overlap")

    ######################################

    ######## MAKE PLOTS ########

    # define things
    print("plotting")

    df_plot["species"] = df_plot.species.apply(lambda x: x.split("_")[1])
    sorted_species = [x.split("_")[1] for  x in sorted_species_byPhylogeny]

    sorted_vartypes = ["SNP", "IN/DEL", "deletion", "tandem_duplication", "inversion", "copyPaste_insertion", "cutPaste_insertion", "inverted_copyPaste_insertion", "inverted_translocation", "inverted_cutPaste_insertion", "coverage_deletion", "coverage_duplication", "unclassified_breakpoint", "bal_translocation"]

    title = "variants overlapping %s\nCNVs >=%1.f%s overlap"%(",".join(sorted(simple_repeats)), min_pct_overlap_CNV, "%")

    palette = {"overlap repeats":"red", "no overlap repeats":"c"}

    # define the plots dir
    plots_dir = "%s/variantsBreakpointsOverlapping_simpleRepeats"%PlotsDir; make_folder(plots_dir)

    #for type_vars in ["all_vars", "only_SVs"]:
    for type_vars in ["only_SVs"]:

        # get the df
        if type_vars=="all_vars": df_p = df_plot
        elif type_vars=="only_SVs": df_p = df_plot[~(df_plot.variant_type.isin({"SNP", "IN/DEL"}))]

        # define the interesting var types
        all_vartypes = set(df_p.variant_type)
        interesting_sorted_vartypes = [v for v in sorted_vartypes if v in all_vartypes]


        #for yfield in ["fraction_events", "n_events"]: # also n_events 
        for yfield in ["n_events"]: # also n_events 

            plot_stacked_bar_subplots_eachEventOneRow(df_p, xfield="variant_type", rowfield="species", hue="type_overlap", yfield=yfield, sorted_rows=sorted_species, sorted_xvalues=interesting_sorted_vartypes, sorted_huevalues=["overlap repeats", "no overlap repeats"], palette=palette, title=title, filename="%s/mechanisms_SV_formation_overlapRepeats_%s_%s.pdf"%(plots_dir, type_vars, yfield))

    ############################


def get_figure_mechanisms_SV_formation_allMechanisms(SV_CNV_df, PlotsDir, repeats_df, min_pct_overlap_CNV_simpleRepeats, min_fraction_to_qualify_as_repeatSVregion=0.1):

    """This function assigns each variant to one mechanism of SV formation: TE, simple repeats, rRNA/tRNA (these three require that there is at least 10% of the SV region covered by the repeat), microhomology, inextact microhomology, others """

    SV_CNV_df = cp.deepcopy(SV_CNV_df)

    ##### GET THE DF PLOT ######

    # add whether the breakpoints overlaps simple repeats (adds "overlaps_simple_repeats")
    simple_repeats = {"Low_complexity", "Simple_repeat"}
    SV_CNV_df = get_unique_SV_CNV_df_with_overlaps_simple_repeats(SV_CNV_df, min_pct_overlap_CNV_simpleRepeats, simple_repeats)
    
    # add the fraction of SV region covered by the repeat
    def get_fraction_SVregion_covered_byRepeat(r):
        if pd.isna(r.BestRepeatSVregion_repeat): return 1.0 # no repeat overlapping
        else: return (r.BestRepeatSVregion_end_repeat-r.BestRepeatSVregion_begin_repeat) / r.len_event

    SV_CNV_df["fraction_SVregion_covered_byRepeat"] = SV_CNV_df.apply(get_fraction_SVregion_covered_byRepeat, axis=1)

    # add the repeat family of the best overlappng repeat. This will only be considered if the SV region is above the threshold min_fraction_to_qualify_as_repeatSVregion
    def get_repeat_family(r):
        if pd.isna(r.BestRepeatSVregion_repeat) or r.fraction_SVregion_covered_byRepeat<min_fraction_to_qualify_as_repeatSVregion: return "no_repeat"

        elif r.BestRepeatSVregion_type!="Unknown" or r.BestRepeatSVregion_clusterID_repeatModeller=="no_cluster" or r.BestRepeatSVregion_clusterID_repeatModeller.startswith("singleton_"): return r.BestRepeatSVregion_type.split("/")[0]
        
        else: return "%s_cluster"%(r.BestRepeatSVregion_clusterID_repeatModeller.split("_")[0])

    SV_CNV_df["repeat_family"] = SV_CNV_df.apply(get_repeat_family, axis=1)

    # redefine microhomologies
    SV_CNV_df["max_micro_homology_length"] = SV_CNV_df.micro_homology_length.apply(max)
    SV_CNV_df["max_inexact_homology_length"] = SV_CNV_df.inexact_homology_length.apply(max)

    # add the mechanism of SV formation
    repeatFamily_to_mechanismSVformation = {'LINE': 'transposable element', 'tRNA': 'tRNA/rRNA', 'DNA': 'transposable element', 'LTR': 'transposable element', 'Low_complexity': 'simple repeat', 'Simple_repeat': 'simple repeat', 'multiSpecies_cluster': 'transposable element', 'rRNA': 'tRNA/rRNA', 'singleSpecies_cluster': 'transposable element', 'Unknown': 'transposable element'}

    def get_mechanism_SV_formation(r):


        # if it has TEs, get them. This can also be rRNA/tRNA or simple repeat
        if r.repeat_family!="no_repeat": return repeatFamily_to_mechanismSVformation[r.repeat_family]

        # short microhomology (btw 2 and 10 bp)
        elif r.max_micro_homology_length>=2 and r.max_micro_homology_length<=10: return "exact microhomology"

        # long homology
        elif r.max_micro_homology_length>10: return "exact homology"

        # short inexact microhomology (btw 2 and 10 bp)
        elif r.max_inexact_homology_length>=2 and r.max_inexact_homology_length<=10: return "inexact microhomology"

        # long inexact homology
        elif r.max_inexact_homology_length>10: return "inexact homology"

        else: return "other"

    SV_CNV_df["mechanism SV formation"] = SV_CNV_df.apply(get_mechanism_SV_formation, axis=1)

    # change the species
    SV_CNV_df["species"] = SV_CNV_df.species.apply(lambda x: x.split("_")[1])


    # define the sorted_mechanisms_SVformation 
    sorted_mechanisms_SVformation = ['transposable element', 'tRNA/rRNA', 'simple repeat', 'exact microhomology', 'inexact microhomology', 'exact homology', 'inexact homology', 'other']
    if set(sorted_mechanisms_SVformation)!=set(SV_CNV_df["mechanism SV formation"]): raise ValueError("there are missing mechanisms of SV formation")

    SVmechanism_to_color = {"transposable element":"black", "tRNA/rRNA":"olive", "simple repeat":"cyan", "exact microhomology":"red", "inexact microhomology":"magenta", 'exact homology':'teal', 'inexact homology':'blue', "other":"gray"}

    ############################

    ######### MAKE PLOTS #############
    print("making plots")

    plots_dir = "%s/all_mechansims_SVformation_onePlot"%PlotsDir; make_folder(plots_dir)

    # one plot with / without SVs overlapping repeats
    #for type_vars_SimpleRepeats in ["all_vars", "only_vars_noSimpleRepeats"]:
    for type_vars_SimpleRepeats in ["only_vars_noSimpleRepeats"]:

        # get the df to plot
        if type_vars_SimpleRepeats=="only_vars_noSimpleRepeats": df_plot = SV_CNV_df[SV_CNV_df.overlaps_simple_repeats==False]
        else: df_plot = SV_CNV_df

        # keep important fields
        df_plot = df_plot[["species", "svtype", "mechanism SV formation"]]
        sorted_svtypes = ["deletion", "tandem_duplication", "inversion", "copyPaste_insertion", "cutPaste_insertion", "inverted_copyPaste_insertion", "inverted_translocation", "inverted_cutPaste_insertion", "bal_translocation", "unclassified_breakpoint", "coverage_deletion", "coverage_duplication"]

        # define the title
        title = "mechanisms SV formation %s\nSV region matching >%.2fx of repeat"%(type_vars_SimpleRepeats, min_fraction_to_qualify_as_repeatSVregion)

        # one plot for absolute and relative vals
        for yfield in ["fraction_events", "n_events"]: # also n_events 
        #for yfield in ["n_events"]: # also n_events 

            # define filename
            filename = "%s/mechanisms_SV_formation_all_%s_%s.pdf"%(plots_dir, type_vars_SimpleRepeats, yfield)

            # redefine the sorted_mechanisms_SVformation
            sorted_mechanisms_SVformation_p = [x for x in sorted_mechanisms_SVformation if x in set(df_plot["mechanism SV formation"])]

            plot_stacked_bar_subplots_eachEventOneRow(df_plot, xfield="svtype", rowfield="species", hue="mechanism SV formation", yfield=yfield, sorted_rows=[x.split("_")[1] for  x in sorted_species_byPhylogeny], sorted_xvalues=sorted_svtypes, sorted_huevalues=sorted_mechanisms_SVformation_p, palette=SVmechanism_to_color, title=title, filename=filename)

    ##################################


def get_figure_threshold_for_clonalSamples(metadata_df, PlotsDir, ProcessedDataDir, PlotsDir_allCmine, species_to_tree):


    """Plots the effect of different clinal sample thresholds on clade definition"""

    metadata_df = cp.deepcopy(metadata_df)

    ##### GET DATA ####

    df_plot_file = "%s/get_metadata_df_with_cladeID_clonal_df_plot.py"%ProcessedDataDir
    if file_is_empty(df_plot_file):

        # load divergence file
        species_to_snps_kb_series = load_object("%s/get_metadata_df_with_cladeID_clonal_species_to_snps_kb_series.py"%PlotsDir_allCmine)

        df_plot = pd.DataFrame()
        for Is, species in enumerate(sorted_species_byPhylogeny):
            print(species)

            for clonal_threshold_SNPs_kb in [0.1, 0.5, 1, 2, 3, 4, 5, 6, 7, 8, 9,  10]:

                # get tree and map each sample to clade
                tree_tshd = get_tree_withcladeID_clonal(species_to_tree[species], species_to_snps_kb_series[species], clonal_threshold_SNPs_kb)
                sample_to_clade_tshd = pd.Series({l.name : l.cladeID for l in tree_tshd.get_leaves()})
                nclades = len(set(sample_to_clade_tshd[~pd.isna(sample_to_clade_tshd)]))
                fraction_samples_no_clade = sum(pd.isna(sample_to_clade_tshd))/len(sample_to_clade_tshd)

                df_plot = df_plot.append(pd.DataFrame({0:{"# clusters":nclades, "nsamples":len(sample_to_clade_tshd), "fraction strains w/out cluster":fraction_samples_no_clade, "species":species, "threshold SNPs/kb":clonal_threshold_SNPs_kb}}).transpose()).reset_index(drop=True)

        save_object(df_plot, df_plot_file)

    df_plot = load_object(df_plot_file)
    for f in ["threshold SNPs/kb", "fraction strains w/out cluster", "# clusters"]: df_plot[f] = df_plot[f].apply(float)

    ###################


    # pseudocount the number of clades
    df_plot["# clusters"] += 1

    # one plot for each
    for yfield in ["fraction strains w/out cluster", "# clusters"]:

        fig = plt.figure()
        ax = sns.lineplot(data=df_plot, x="threshold SNPs/kb", y=yfield, hue="species", style="species", markers=True, dashes=False, palette=species_to_color)
        ax.legend(loc='upper left', bbox_to_anchor=(1.05, 0.5))
        if yfield=="# clusters": ax.set_yscale("log")
        ax.axvline(1, linestyle="--", color="black", linewidth=.8)

        # save
        fig.savefig("%s/threshold_for_clonalSamples_%s.pdf"%(PlotsDir, yfield.replace("/", "_")), bbox_inches="tight")


def check_that_tree_has_no_politomies(tree):

    """Raises an error if tree has politomies"""

    for n in tree.traverse():
        if n.is_leaf() is False and len(n.get_children())!=2: raise ValueError("node %s has !=2 children: %i"%(n.name, len(n.get_children())))



def get_selection_SNPs_distribution_fraction_extreme_values(df_diversity_all, filename, max_fraction_strains_extreme_piS):

    """Plots the diversity df xfield vs yfield. This is useful to assess how the null model works"""

    # keep df that is useful
    df_diversity_all =  df_diversity_all[(df_diversity_all.type_var=="SNP") & (df_diversity_all.threshold_piNpiS==1) & ~(pd.isna(df_diversity_all.mean_p_real_piS_not_extreme)) & (df_diversity_all.type_vars_SimpleRepeats=="only_vars_noSimpleRepeats")  & (df_diversity_all.type_vars_appearance=="only_vars_recent")][["species", "Gene", 'type_var', 'type_vars_SimpleRepeats', 'type_vars_appearance', "selection", 'fraction_samples_extreme_piS_p<0.05']]

    # init figure for each combination of species and type_vars_appearance
    all_species = sorted_species_byPhylogeny
    all_selections = ["positive"]
    nrows = len(all_species)
    ncols = len(all_selections)
    fig = plt.figure(figsize=(ncols*2, nrows*2)); I=1

    # add subplots
    for Ir, species in enumerate(all_species):
        for Ic, selection in enumerate(all_selections):

            # get df and check that Gene is unique
            df_plot = df_diversity_all[(df_diversity_all.species==species) & (df_diversity_all.selection==selection)]
            if len(df_plot)!=len(set(df_plot.Gene)): raise ValueError("Gene should be unique ")
            print(species, selection, len(df_plot))

            # set lims
            lims = [-0.05, 1.05]


            # plot the hist
            ax = plt.subplot(nrows, ncols, I); I+=1

            if len(df_plot)>0:

                #sns.distplot(df_plot[xfield].values, kde=False, rug=False, hist=True, hist_kws=dict(alpha=1, color=species_to_color[species]), size=5)
                sns.distplot(df_plot['fraction_samples_extreme_piS_p<0.05'].values, kde=False, rug=False, hist=True, hist_kws=dict(alpha=1, color='gray', range=lims), bins=15)
                ax.axvline(max_fraction_strains_extreme_piS, linestyle="--", color="black", linewidth=.8)


                ax.set_yscale("log")

            else: 
                plt.text(0.5, 1700, "NA", color='black',  ha='center', va='center', fontsize=13) # fontweight="bold"
                ax.set_yticklabels([])
                #ax.set_yticks([])

            ax.set_ylim([0, 3700])
            ax.set_xlim(lims)

            # x axis
            if species==(all_species[-1]): 
                if Ic==0: ax.set_xlabel('fraction strains w/ extreme 'r'$\pi_S$')
            else:
                ax.set_xticklabels([])
                ax.set_xticks([])
                ax.set_xlabel("")


            # y axis
            if Ic==0 and Ir==2: ax.set_ylabel("# genes\nC. %s"%(species.split("_")[1]))        
            elif Ic==0: ax.set_ylabel("C. %s"%(species.split("_")[1]))                       
            else: ax.set_ylabel("")


    plt.subplots_adjust(wspace=0.35,  hspace=0.04)
    plt.show()

    print("saving %s"%filename)
    fig.savefig(filename, bbox_inches="tight")


def get_selection_score_scatterplot(df_diversity_filt, filename, types_var, selection_str):

    """Plots the correlation between the fraction of strains and clades under selection"""

    # get fields
    xfield = "fraction_samples_selected"; yfield = "fraction_clades_selection"

    # filter to keep the type of vars
    df_diversity = df_diversity_filt[(df_diversity_filt.type_var.isin(set(types_var)))][["species", "Gene", 'type_var', 'type_vars_SimpleRepeats', 'type_vars_appearance', "selection", xfield, yfield, "harmonicMean_fraction_samples_and_clades"]].rename(columns={"harmonicMean_fraction_samples_and_clades":"selection score (S)"})

    xlim = [min(df_diversity[xfield])-0.05, max(df_diversity[xfield])+0.05]
    ylim = [min(df_diversity[yfield])-0.05, max(df_diversity[yfield])+0.05]

    max_selection_score_all = max(df_diversity_filt.harmonicMean_fraction_samples_and_clades)

    # init figure for each combination of species and type_vars_appearance
    all_species = sorted_species_byPhylogeny
    nrows = len(all_species)
    ncols = len(types_var)
    fig = plt.figure(figsize=(ncols*2, nrows*2)); I=1

    # add subplots
    for Ir, species in enumerate(all_species):
        for Ic, type_var in enumerate(types_var):

            # get df and check that Gene is unique
            df_plot = df_diversity[(df_diversity.species==species) & (df_diversity.type_var==type_var)]
            if len(df_plot)!=len(set(df_plot.Gene)): raise ValueError("Gene should be unique ")
            check_no_nans_in_df(df_plot[[xfield, yfield]])

            # plot the density scatter
            ax = plt.subplot(nrows, ncols, I); I+=1
            if len(df_plot)>0:

                ax = sns.scatterplot(data=df_plot, x=xfield, y=yfield, alpha=1, hue="selection score (S)", s=15, palette='rocket_r', hue_norm=(0, max_selection_score_all), edgecolor="gray", linewidth=.4)
                #plt.scatter(df_plot[xfield], df_plot[yfield], s=15, alpha=1,  cmap='rocket_r')



                if Ir==0 and type_var==(types_var[-1]): ax.legend(loc='upper left', bbox_to_anchor=(1.05, 0))
                else: ax.get_legend().remove()



            else: 

                ax.set_yticklabels([])
                plt.text(xlim[0]+(xlim[1]-xlim[0])*0.5, ylim[0]+(ylim[1]-ylim[0])*0.5, "NA", color='black',  ha='center', va='center', fontsize=13) # fontweight="bold"


            # set lims
            ax.set_xlim(xlim)
            ax.set_ylim(ylim)

            # x axis
            if species==(all_species[-1]): 
                if Ic==0: ax.set_xlabel('fraction strains w/ %s'%selection_str)
                else: ax.set_xlabel("")
            else:
                ax.set_xticklabels([])
                ax.set_xticks([])
                ax.set_xlabel("")

            # y axis
            if Ic==0: 
                if Ir==2: ax.set_ylabel("fraction clusters w/ strain %s\nC. %s"%(selection_str, species.split("_")[1]))                       
                else: ax.set_ylabel("C. %s"%(species.split("_")[1]))

            else:
                ax.set_yticklabels([])
                ax.set_yticks([])
                ax.set_ylabel("")

            # title
            if Ir==0: ax.set_title(type_var)



    plt.subplots_adjust(wspace=0.03,  hspace=0.03)
    plt.show()

    print("saving %s"%filename)
    if filename.endswith("png"): fig.savefig(filename, bbox_inches="tight", dpi=900)
    else: fig.savefig(filename, bbox_inches="tight")




def get_selection_SNPs_selection_score_vs_pval(df_diversity_filt, filename):


    """Plots the correlation between the harmonic mean btw fraciton of samples and fraction of clades and the p value"""

    # filter
    df_diversity = df_diversity_filt[(df_diversity_filt.type_var=="SNP")]
    df_diversity["selection score (S)"] = df_diversity.harmonicMean_fraction_samples_and_clades


    df_diversity["-log(p(S) neutral model)"] = df_diversity["minus_log10pval_pval_harmonicMean_fraction_samples_and_clades_fdr"]

    xfield = "selection score (S)"
    yfield = "-log(p(S) neutral model)"

    xlim = [min(df_diversity[xfield])-0.1, max(df_diversity[xfield])+0.1]
    ylim = [min(df_diversity[yfield])-0.1, max(df_diversity[yfield])+0.1]

    # init figure for each combination of species and type_vars_appearance
    all_species = sorted_species_byPhylogeny
    all_thresholds = [1]
    nrows = len(all_species)
    ncols = len(all_thresholds)
    fig = plt.figure(figsize=(ncols*2, nrows*2)); I=1


    # add subplots
    for Ir, species in enumerate(all_species):
        for Ic, threshold_piNpiS in enumerate(all_thresholds):

            # get df and check that Gene is unique
            df_plot = df_diversity[(df_diversity.species==species) & (df_diversity.threshold_piNpiS==threshold_piNpiS)]
            if len(df_plot)!=len(set(df_plot.Gene)): raise ValueError("Gene should be unique ")
            check_no_nans_series(df_plot.minus_log10pval_pval_harmonicMean_fraction_samples_and_clades_fdr)


            # plot the density scatter
            ax = plt.subplot(nrows, ncols, I); I+=1


            #density_scatter(df_plot.harmonicMean_fraction_samples_and_clades.values , df_plot.minus_log10pval_pval_harmonicMean_fraction_samples_and_clades_fdr.values, ax, sort=True, bins=3, s=3)

            df_plot_selected = df_plot[df_plot.gene_significant_by_SNPs]
            df_plot_not_selected = df_plot[~df_plot.gene_significant_by_SNPs]

            sns.scatterplot(data=df_plot_selected, x=xfield, y=yfield, color='none', alpha=1, s=10, edgecolor="red", linewidth=.5)
            sns.scatterplot(data=df_plot_not_selected, x=xfield, y=yfield, color='none', alpha=1, s=10, edgecolor="gray", linewidth=.5)

            # set lims
            ax.set_xlim(xlim)
            ax.set_ylim(ylim)

            # add text with the number of sig genes
            plt.text(0.6, 2.7, "%i/%i\np<0.05"%(sum(df_plot.pval_harmonicMean_fraction_samples_and_clades_fdr<0.05), len(df_plot)), fontsize=8)

            # x axis
            if species==(all_species[-1]): 
                if Ic==0: ax.set_xlabel(xfield)
                else: ax.set_xlabel("")
            else:
                ax.set_xticklabels([])
                ax.set_xticks([])
                ax.set_xlabel("")

            # y axis
            if Ic==0: 
                if Ir==2: ax.set_ylabel("%s\nC. %s"%(yfield, species.split("_")[1]))                       
                else: ax.set_ylabel("C. %s"%(species.split("_")[1]))

            else:
                ax.set_yticklabels([])
                ax.set_yticks([])
                ax.set_ylabel("")

            # legend
            #ax.get_legend().remove()

            # add the line for the pval
            for pval in [0.05]: plt.axhline(-np.log10(pval+1/10000), color="gray", linestyle="--", linewidth=.7)

        
        


    plt.subplots_adjust(wspace=0.03,  hspace=0.1)
    plt.show()

    print("saving %s"%filename)
    if filename.endswith("png"): fig.savefig(filename, bbox_inches="tight", dpi=900)
    else: fig.savefig(filename, bbox_inches="tight")



def get_selection_noSNPs_distribution_selecion_score(df_diversity_filt, filename):

    """Plots the diversity df xfield vs yfield. This is useful to assess how the null model works"""


    # keep interesting df
    df_diversity = df_diversity_filt[df_diversity_filt.type_var!="SNP"]
    df_diversity["selection score (S)"] = df_diversity.harmonicMean_fraction_samples_and_clades

    # init figure for each combination of species and type_vars_appearance
    all_species = sorted_species_byPhylogeny
    all_types_var = ["if_INDEL", "DUP", "DEL"]
    nrows = len(all_species)
    ncols = len(all_types_var)
    fig = plt.figure(figsize=(ncols*2, nrows*2)); I=1

    # add subplots
    for Ir, species in enumerate(all_species):
        for Ic, type_var in enumerate(all_types_var):

            # get df and check that Gene is unique
            df_plot = df_diversity[(df_diversity.species==species) & (df_diversity.type_var==type_var)]
            if len(df_plot)!=len(set(df_plot.Gene)): raise ValueError("Gene should be unique ")

            # set lims
            lims = [-0.05, 1.05]
            ylims = [0, 1650]

            # plot the hist
            ax = plt.subplot(nrows, ncols, I); I+=1

            if len(df_plot)>0:

                #sns.distplot(df_plot[xfield].values, kde=False, rug=False, hist=True, hist_kws=dict(alpha=1, color=species_to_color[species]), size=5)
                sns.distplot(df_plot[~df_plot.gene_significant_by_noSNPs]['selection score (S)'].values, kde=False, rug=False, hist=True, hist_kws=dict(alpha=1, color='gray', range=lims), bins=35)

                sns.distplot(df_plot[df_plot.gene_significant_by_noSNPs]['selection score (S)'].values, kde=False, rug=False, hist=True, hist_kws=dict(alpha=1, color='red', range=lims), bins=35)




                ax.axvline(min(df_plot[df_plot.gene_significant_by_noSNPs]['selection score (S)']), color="gray", linestyle="--", linewidth=.9)
                ax.set_yscale('log', nonposy='clip')

            else: 
                plt.text(0.5, 800, "NA", color='black',  ha='center', va='center', fontsize=13) # fontweight="bold"
                ax.set_yticklabels([])
                #ax.set_yticks([])

            ax.set_ylim(ylims)
            ax.set_xlim(lims)

            # x axis
            if species==(all_species[-1]): 
                if Ic==1: ax.set_xlabel('selection score (S)')
                else: ax.set_xlabel("")
            else:
                ax.set_xticklabels([])
                ax.set_xticks([])
                ax.set_xlabel("")

            # y axis
            if Ic==0: 
                if Ir==2: ax.set_ylabel("# genes\nC. %s"%(species.split("_")[1]))                       
                else: ax.set_ylabel("C. %s"%(species.split("_")[1]))

                yticks = [int(y) for y in [1e0, 1e1, 1e2, 1e3, 1e4] if y<=ylims[1]]
                ax.set_yticks(yticks)
                ax.set_yticklabels([str(get_float_or_int(y)) for y in yticks])

            else:
                ax.set_yticklabels([])
                ax.set_yticks([])
                ax.set_ylabel("")


            # title
            if Ir==0: ax.set_title(type_var)

    plt.subplots_adjust(wspace=0.04,  hspace=0.04)
    plt.show()

    if filename.endswith("png"): fig.savefig(filename, bbox_inches="tight", dpi=900)
    else: fig.savefig(filename, bbox_inches="tight")




def get_figure_selection_tree_recent_clades(metadata_df, PlotsDir, species_to_tree, species):

    """Plots, for each species, a tree with the clades as a donut chart with information about the type of strain"""

    metadata_df = cp.deepcopy(metadata_df)

    ###### GET TREE #####

    # remove bad samples
    tree = cp.deepcopy(species_to_tree[species])
    correct_samples = set(tree.get_leaf_names()).difference({str(x) for x in sciName_to_badSamples[species]})
    tree.prune(correct_samples)

    # remove low support branches
    tree = get_correct_tree(tree, min_support=95)

    # define metadata df for this species
    metadata_df_s = metadata_df[metadata_df.species_name==species]
    metadata_df_s["sampleID"] = metadata_df_s.sampleID.apply(int)

    # define the sample_to_clade
    def get_correct_cladeID_clonal_threshold(r):
        if pd.isna(r.cladeID_clonal_tshdSNPsKb_1): return "unassignedSample_%i"%(r.sampleID)
        else: return int(r.cladeID_clonal_tshdSNPsKb_1)

    metadata_df_s["correct_cladeID_clonal_tshdSNPsKb_1"] = metadata_df_s.apply(get_correct_cladeID_clonal_threshold, axis=1)
    sample_to_clade = metadata_df_s[metadata_df_s.sampleID.apply(str).isin(correct_samples)].set_index("sampleID", drop=False).correct_cladeID_clonal_tshdSNPsKb_1

    # convert to str
    sample_to_clade = {str(s):str(c) for s,c in sample_to_clade.items()}

    # add the cladeID
    tree = get_tree_withcladeID_as_str(tree, sample_to_clade)

    # iterate through tree
    print("traversing tree")
    for n in tree.traverse("preorder"):

        # init style
        nst = NodeStyle()

        # define the bwidth    
        species_to_bwidth = {"Candida_albicans":40, "Candida_glabrata":80, "Candida_auris":20, "Candida_tropicalis":8, "Candida_metapsilosis":8, "Candida_parapsilosis":8, "Candida_orthopsilosis":8}
        bwidth = species_to_bwidth[species]

        nst["hz_line_width"] = bwidth
        nst["vt_line_width"] = bwidth

        # define the size of the node
        nst["size"] = 0

        # add the colors
        if pd.isna(n.cladeID) or n.cladeID.startswith("unassignedSample_"): clade_color = "black"
        else: clade_color = 'black'

        nst["fgcolor"] = "lightgray"
        nst["hz_line_color"] = clade_color
        nst["vt_line_color"] = clade_color
        #nst["bgcolor"] = bgcolor
        if n.is_leaf(): n.add_face(RectFace(20, 40, fgcolor="white", bgcolor="white"), column=0 , position="aligned") #

        # add style
        n.set_style(nst)

    # init treestyle
    ts = TreeStyle()
    ts.show_branch_length = False
    ts.show_branch_support = False
    ts.show_leaf_name = False
    ts.mode = "c"
    ts.root_opening_factor = 1 # the higher the morecompact
    #ts.optimal_scale_level = "full"
    ts.arc_start = 0 # 0 degrees = 3 o'clock
    ts.arc_span = 360

    # show
    #tree.show(tree_style=ts); sys.exit(0)

    # save the tree
    filename = "%s/Selection_clonal_clusters_%s.pdf"%(PlotsDir, species)
    #tree.render(file_name=filename, tree_style=ts, dpi=500,  units='mm', h=len(sample_to_clade)*2) #w=20
    tree.render(file_name=filename, tree_style=ts) #w=20

    #####################


def get_all_valid_genes_selection_one_species(gene_features_df, species_to_gff, species):

    """Returns a set with all valid genes where selection was interrogated"""

    # load gff
    gff_df = load_gff3_intoDF(species_to_gff[species])

    # get the genes
    gene_features_df_s = gene_features_df[(gene_features_df.species==species)]
    pseudogenes = set(gene_features_df_s[gene_features_df_s.feature_type.isin({"pseudogene", "pseudogene|Uncharacterized"})].gff_upmost_parent)
    protein_coding_genes = set(gff_df[gff_df.feature.isin({"CDS", "mRNA"})].upmost_parent).difference(pseudogenes)
    gene_features_df_s = gene_features_df_s[(gene_features_df_s.gff_upmost_parent.isin(protein_coding_genes))]
    all_genes = set(gene_features_df_s.gff_upmost_parent)

    return all_genes

def get_target_and_all_genes_under_recent_selection(df_diversity_filt, species, type_var, gene_features_df, species_to_gff):

    """Gets target and all genes under selection"""


    # define the target genes (selected)
    df_spp_var = df_diversity_filt[(df_diversity_filt.species==species) & (df_diversity_filt.type_var==type_var)]
    type_var_to_selected_f = {"SNP":"gene_significant_by_SNPs", "if_INDEL":"gene_significant_by_noSNPs", "DEL":"gene_significant_by_noSNPs", "DUP":"gene_significant_by_noSNPs"}
    target_genes = set(df_spp_var[df_diversity_filt[type_var_to_selected_f[type_var]]].Gene)

    # get all genes (protein coding that are not pseudogenes)
    all_genes = get_all_valid_genes_selection_one_species(gene_features_df, species_to_gff, species)
  
    # cjeck
    strange_genes = target_genes.difference(all_genes)
    if len(strange_genes)>0: raise ValueError("There are some strange genes: %s"%strange_genes)

    return target_genes, all_genes

def get_minus_1_to_nan(x):
    if x==-1: return np.nan
    else: return x

def get_table_GenesSelection(gene_features_df, df_diversity_filt, TablesDir, species_to_gff, DataDir, ProcessedDataDir, gene_types):

    """Gets the table of genes under selection"""


    # define files
    genes_df_all_file = "%s/genes_under_selection_%s.py"%(ProcessedDataDir, gene_types)
    if file_is_empty(genes_df_all_file):

        # init the df for all genes under selection
        print("getting table with all genes")
        genes_df_all = pd.DataFrame()

        for type_var in ["SNP", "if_INDEL", "DUP", "DEL"]:
            for species in sorted_species_byPhylogeny:
                print(type_var, species)

                # define the significant genes with possel
                sig_genes = get_target_and_all_genes_under_recent_selection(df_diversity_filt, species, type_var, gene_features_df, species_to_gff)[0]

                # get target genes and all_genes to do comparisons
                if gene_types=="under_selection": target_genes = sig_genes
                elif gene_types=="all_genes": target_genes = set(df_diversity_filt[(df_diversity_filt.species==species) & (df_diversity_filt.type_var==type_var)].Gene)
                else: raise ValueError("invalid gene types")

                # debug
                if len(target_genes)==0: continue

                # get a df with the genes under selection
                df_genes = gene_features_df[(gene_features_df.species==species) & (gene_features_df.gff_upmost_parent.isin(target_genes))]
                if set(df_genes.gff_upmost_parent)!=target_genes: raise ValueError("df_genes is missing genes")

                # get the diversity df
                df_diversity = df_diversity_filt[(df_diversity_filt.type_var==type_var) & (df_diversity_filt.species==species) & (df_diversity_filt.Gene.isin(target_genes))]
                if len(df_diversity)!=len(set(df_diversity.Gene)): raise ValueError("gene is not unique")

                # add info
                gene_to_S = dict(df_diversity.set_index("Gene")["harmonicMean_fraction_samples_and_clades"])
                gene_to_p = dict(df_diversity.set_index("Gene")["pval_harmonicMean_fraction_samples_and_clades_fdr"])
                check_no_nans_in_df(df_diversity[["harmonicMean_fraction_samples_and_clades", "pval_harmonicMean_fraction_samples_and_clades_fdr"]])

                df_genes["selection_score_S"] = df_genes.gff_upmost_parent.apply(lambda g: gene_to_S[g])
                df_genes["fdr_p_S"] = df_genes.gff_upmost_parent.apply(lambda g: gene_to_p[g]).apply(get_minus_1_to_nan)
                if any(df_genes.fdr_p_S<0): raise ValueError("negatives in fdr_p_S")

                for f, real_f in [('fraction_clades_selection','fraction_clusters_w_selection'), ('fraction_samples_selected', 'fraction_strains_w_selection'), ('nclades_total', 'total_number_clusters'), ('nsamples', 'total_number_strains')]: 

                    gene_to_fval =  dict(df_diversity.set_index("Gene")[f])
                    df_genes[real_f] = df_genes.gff_upmost_parent.apply(lambda x: gene_to_fval[x])
                    check_no_nans_in_df(df_genes[[real_f]])

                # add whether the selection is sig
                df_genes["significant_selection"] = df_genes.gff_upmost_parent.isin(sig_genes)

                # add the GO terms
                obo_file = "%s/annotation_files/go-basic_30062021.obo"%(DataDir) # this was got from http://purl.obolibrary.org/obo/go/go-basic.obo
                obodag = GODag(obo_file,  optional_attrs={'consider', 'replaced_by'}, load_obsolete=True, prt=None)
                ns_to_gene_to_GOids = get_namespace_to_gene_to_GOterms(dict(df_genes.set_index("gff_upmost_parent").GOterms), obodag)

                for ns, ns_name in [("BP", "biological_process"), ("MF", "molecular_function"), ("CC", "cellular_component")]:
                    gene_to_GOids = ns_to_gene_to_GOids[ns]
                    for g in set(df_genes.gff_upmost_parent).difference(set(gene_to_GOids)): gene_to_GOids[g] = set()
                    df_genes["%s_GO"%(ns_name)] = df_genes.gff_upmost_parent.apply(lambda g: ", ".join(["%s (%s)"%(go, obodag[go].name) for go in sorted(gene_to_GOids[g])]))

                # add fields and keep
                df_genes["type_var"] = type_var
                df_genes["species"] = species

                # sort df and save
                df_genes = df_genes.sort_values(by=["fdr_p_S", "selection_score_S"], ascending=[True, False])
                genes_df_all = genes_df_all.append(df_genes).reset_index(drop=True)

        # save as table and as df
        save_object(genes_df_all, genes_df_all_file)

    # load df
    genes_df_all = load_object(genes_df_all_file)

    # add fields
    og_to_n_species = dict(genes_df_all.groupby("orthofinder_orthocluster").apply(lambda df_og: len(set(df_og.species))))
    genes_df_all["n_species_orthogroup"] = genes_df_all.orthofinder_orthocluster.apply(lambda oc: og_to_n_species[oc])

    spp_to_og_to_ntype_vars = {}
    for spp in sorted_species_byPhylogeny:
        df_spp = genes_df_all[genes_df_all.species==spp]
        spp_to_og_to_ntype_vars[spp] = dict(df_spp.groupby("orthofinder_orthocluster").apply(lambda df_og: len(set(df_og.type_var))))

    genes_df_all["n_types_vars_in_species_orthogroup"] = genes_df_all.apply(lambda r: spp_to_og_to_ntype_vars[r.species][r.orthofinder_orthocluster], axis=1)

    # add whether the f

    # save excel with all genes
    fields = ['species', 'type_var', 'chromosome', 'start', 'end', 'gff_upmost_parent', 'gene_name', 'Scerevisiae_orthologs', "selection_score_S", "fdr_p_S", 'orthofinder_orthocluster', 'n_types_vars_in_species_orthogroup', 'n_species_orthogroup', 'description', 'biological_process_GO', 'molecular_function_GO', 'cellular_component_GO', 'fraction_clusters_w_selection', 'fraction_strains_w_selection', 'total_number_clusters', 'total_number_strains', 'significant_selection']
    genes_df_all[fields].sort_values(by=["species", "type_var", "fdr_p_S", "selection_score_S", "chromosome"], ascending=[True, True, True, False, True]).to_excel("%s/GenesSelection_%s.xlsx"%(TablesDir, gene_types),  index=False)

    # create the table of genes that are shared across species
    genes_df_all_shared_spp = genes_df_all[genes_df_all.n_species_orthogroup>=2]
    genes_df_all_shared_spp[fields].sort_values(by=["n_species_orthogroup", "orthofinder_orthocluster", "species", "type_var", "fdr_p_S", "selection_score_S", "chromosome"], ascending=[False, True, True, True, True, False, True])[fields].to_excel("%s/Shared_GenesSelection_%s.xlsx"%(TablesDir, gene_types),  index=False)

    # create a table of genes that are shared across the same species
    genes_df_all_shared_within_spp = genes_df_all[genes_df_all.n_types_vars_in_species_orthogroup>=2]
    genes_df_all_shared_within_spp[fields].sort_values(by=["n_types_vars_in_species_orthogroup", "orthofinder_orthocluster", "species", "type_var", "fdr_p_S", "selection_score_S", "chromosome"], ascending=[False, True, True, True, True, False, True])[fields].to_excel("%s/DifferentTypeVarsSameSpecies_GenesSelection_%s.xlsx"%(TablesDir, gene_types),  index=False)


    return genes_df_all, genes_df_all_shared_spp


def get_short_spp_name(s): return "C. %s"%(s.split("_")[1])

def get_figure_Selection_overlapping_orthogroups(gene_features_df, ProcessedDataDir, PlotsDir, df_diversity_filt, TablesDir, species_to_gff):

    """This data plots the overlap between orthogroups by different selection sets"""

    # load df with the genes under selection in all species
    all_genes_df = load_object("%s/genes_under_selection.py"%ProcessedDataDir)
    check_no_nans_series(all_genes_df.orthofinder_orthocluster)

    # go through different types of plot
    for target_species, fields, idx_spp in ([(sorted_species_byPhylogeny, ["species"], 0)]  + [([spp], ["type_var"], None) for spp in sorted_species_byPhylogeny]):
        print(target_species, fields)        

        if target_species!=["Candida_glabrata"]: continue

        # keep df
        genes_df = all_genes_df[all_genes_df.species.isin(set(target_species))]

        # add species
        genes_df["species"] = genes_df.species.apply(get_short_spp_name)

        # init general vars
        field_to_color_dict = {"selection":{"positive":"magenta", "negative":"c"},
                               "type_vars_appearance":{"all_vars":"gold", "only_vars_recent":"salmon"},
                               "species":{get_short_spp_name(spp) : c for spp, c in species_to_color.items()},
                               #"type_var":{"SNP":"red", "IN/DEL":"magenta", "SV":"navy", "coverageCNV":"cyan"}
                               "type_var":{"SNP":"red", "if_INDEL":"magenta", "DEL":"grey", "DUP":"navy"}}

        field_to_sorted_items = {"selection":["positive", "negative"],
                                 "type_vars_appearance":["all_vars", "only_vars_recent"],
                                 "species":list(map(get_short_spp_name, sorted_species_byPhylogeny)),
                                 "type_var":["SNP", "if_INDEL", "DUP", "DEL"]}



        # redefine field_to_sorted_items so that they appear in df
        if fields==["type_var"]:  field_to_sorted_items = {"type_var": field_to_sorted_items["type_var"]}
        else: field_to_sorted_items = {f : [x for x in sitems if x in set(genes_df[f])] for f, sitems in field_to_sorted_items.items() if f in fields}


        # only fields that are nonunique
        fields =  [f for f in fields if len(field_to_sorted_items[f])>1]

        # define the sorted combinations
        sorted_combinations = list(itertools.product(*[field_to_sorted_items[f] for f in fields]))

        # kepp valid combinations
        if len(fields)>1: all_combs_set = set(genes_df.set_index(fields).index)
        elif len(fields)==1: all_combs_set = {(x,) for x in set(genes_df[fields[0]])}
        if fields==["type_var"]: all_combs_set = {(x,) for x in set(all_genes_df[fields[0]])}
        sorted_combinations = [c  for c in sorted_combinations if c in all_combs_set] 

        # define dicts
        comb_to_ngenes = dict(genes_df.groupby(fields).apply(lambda df_fs: len(set(df_fs.gff_upmost_parent))))
        if len(fields)==1: comb_to_ngenes = {(c,):ngenes for c,ngenes in comb_to_ngenes.items()}
        spp_to_genes =  {get_short_spp_name(spp): get_all_valid_genes_selection_one_species(gene_features_df, species_to_gff, spp) for spp in sorted_species_byPhylogeny}    
        spp_to_gene_to_orthocluster = {get_short_spp_name(spp) : dict(gene_features_df[(gene_features_df.species==spp) & (gene_features_df.gff_upmost_parent.isin(spp_to_genes[get_short_spp_name(spp)]))].set_index("gff_upmost_parent").orthofinder_orthocluster) for spp in sorted_species_byPhylogeny}

        for g_to_c in spp_to_gene_to_orthocluster.values():
            if any(pd.isna(pd.Series(g_to_c))): raise ValueError("nans in g_to_c: %s"%g_to_c)

        # map each combination to the groups
        comb_to_groups = dict(genes_df.groupby(fields).apply(lambda df_fs: set(df_fs.orthofinder_orthocluster)))
        if len(fields)==1: comb_to_groups = {(c,):gs for c,gs in comb_to_groups.items()}

        missing_combs = set(sorted_combinations).difference(set(comb_to_groups))
        for missing_comb in missing_combs: comb_to_groups[missing_comb] = set()

        # define a function that takes one specific pair of combinations and returns the JC overlap
        def get_jaccard_distance(set_1, set_2):

            # return nan if there are no groups
            if len(set_1)==0 or len(set_2)==0: return 0.0

            # define the all and shared IDs
            all_IDs = set_1.union(set_2)
            shared_IDs = set_1.intersection(set_2)

            return len(shared_IDs)/len(all_IDs)

        def get_jaccard_index_overlap_two_combinations(comb1, comb2):

            # get data for each
            groups1 = comb_to_groups[comb1]
            groups2 = comb_to_groups[comb2]

            return get_jaccard_distance(groups1, groups2)


        # define the square df for each of the combinations
        df_square = pd.DataFrame({comb1 : {comb2 : get_jaccard_index_overlap_two_combinations(comb1, comb2) for comb2 in sorted_combinations} for comb1 in sorted_combinations}).loc[sorted_combinations, sorted_combinations]

        # define the df of the significance of the combinations
        df_significance_combinations_file = "%s/significance_combinations_orthogroups_df_%s_%s.py"%(ProcessedDataDir, "_".join(target_species), "_".join(fields))
        if file_is_empty(df_significance_combinations_file) or True:

            # define all the overlaps to consider
            all_comparisons_combs = sorted({tuple(sorted([comb1, comb2])) for comb1 in sorted_combinations for comb2 in sorted_combinations if comb1!=comb2 and get_jaccard_index_overlap_two_combinations(comb1, comb2)>0})

            def get_pvalue_pair_combs_overlap(combs):
                comb1, comb2 = combs

                # checks
                if comb1==comb2 or get_jaccard_index_overlap_two_combinations(comb1, comb2)==0: raise ValueError("there can't be this")

                # define vars
                ngenes1 = comb_to_ngenes[comb1]
                ngenes2 = comb_to_ngenes[comb2]

                if len(target_species)==1: 
                    short_spp = get_short_spp_name(target_species[0])
                    g_to_oc_1 = g_to_oc_2 = spp_to_gene_to_orthocluster[short_spp]
                    all_genes1 = all_genes2 = list(spp_to_genes[short_spp])

                else:
                    g_to_oc_1 = spp_to_gene_to_orthocluster[comb1[idx_spp]]
                    g_to_oc_2 = spp_to_gene_to_orthocluster[comb2[idx_spp]]

                    all_genes1 = list(spp_to_genes[comb1[idx_spp]])
                    all_genes2 = list(spp_to_genes[comb2[idx_spp]])

                # for 10,000 samples, calculate a jaccard index
                nsamples = 10000
                def get_len_intersection_ocs_random_one_sample(I):
                    random_genes1 = set(random.sample(all_genes1, ngenes1))
                    random_genes2 = set(random.sample(all_genes2, ngenes2))

                    random_ocs1 = set(map(lambda g: g_to_oc_1[g], random_genes1))
                    random_ocs2 = set(map(lambda g: g_to_oc_2[g], random_genes2))

                    return len(random_ocs1.intersection(random_ocs2))

                random_distribution_nocs_overlapping = np.array(list(map(get_len_intersection_ocs_random_one_sample, range(nsamples))))

                # getting p val
                observed_n_intersecting = len(comb_to_groups[comb1].intersection(comb_to_groups[comb2]))
                p = sum(random_distribution_nocs_overlapping>=observed_n_intersecting)/nsamples

                print("pval: %s vs %s: %s. %i intersection. %i genes1, %i genes2."%(comb1, comb2, p, observed_n_intersecting, ngenes1, ngenes2))

                return p

            df_significance_combinations = pd.DataFrame({"raw_p" : list(map(get_pvalue_pair_combs_overlap, all_comparisons_combs)), "compared_combs":all_comparisons_combs})
            df_significance_combinations["fdr_p"] = multitest.fdrcorrection(df_significance_combinations.raw_p.values)[1]

            print("saving")
            save_object(df_significance_combinations, df_significance_combinations_file)
        df_significance_combinations = load_object(df_significance_combinations_file)

        # define the square index for the significant overlaps (15% FDR)
        def get_annot_str(comb1, comb2):
            if comb1==comb2 or get_jaccard_index_overlap_two_combinations(comb1, comb2)==0: return ""
            df_sig = df_significance_combinations[df_significance_combinations.compared_combs==tuple(sorted([comb1, comb2]))]
            if len(df_sig)!=1: raise ValueError("invalid df: %s"%df_sig)
            if (df_sig.iloc[0].raw_p<0.05): return "*"
            else: return ""


        annot_df = pd.DataFrame({comb1 : {comb2 : get_annot_str(comb1, comb2) for comb2 in sorted_combinations} for comb1 in sorted_combinations}).loc[sorted_combinations, sorted_combinations]

        # define the colors df
        row_colors_df = pd.DataFrame({comb : {f : field_to_color_dict[f][comb[I]]  for I,f in enumerate(fields)} for comb in sorted_combinations}).transpose()

        # the col colors in the same as row but inverted
        col_colors_df = row_colors_df[list(reversed(row_colors_df.columns))]

        # get the 0s to nans
        def get_0s_to_nan(x):
            if x==0: return np.nan
            else: return x
        df_square = df_square.applymap(get_0s_to_nan)

        # define the clustermap
        mask = df_square.isnull()
        mask = np.zeros_like(df_square)
        mask[np.triu_indices_from(mask)] = True

        colorbar_label = "fraction ovelapping Orthologous Groups (OGs)"
        cm = sns.clustermap(df_square, col_cluster=False, row_cluster=False, yticklabels=False, xticklabels=False, col_colors=col_colors_df, row_colors=row_colors_df, linecolor="white", linewidth=.05, cbar_kws={'label': colorbar_label, "orientation":"vertical"}, annot=annot_df, mask=mask, cmap="rocket_r", center=0.5, vmin=0, vmax=1, annot_kws={"size": 13}, fmt=""); # row_colors = row_colors_df

        # remove the row colors
        cm.ax_row_colors.set_xticklabels([])
        cm.ax_col_colors.set_yticklabels([])

        # add numbers
        cm.ax_row_colors.set_yticks([I+0.5 for I, comb in enumerate(df_square.index)])
        cm.ax_row_colors.set_yticklabels([str(len(comb_to_groups[comb])) for comb in df_square.index], rotation=0) # rotation 45

        cm.ax_col_colors.set_xticks([I+0.5 for I, comb in enumerate(df_square.index)])
        cm.ax_col_colors.set_xticklabels([str(len(comb_to_groups[comb])) for comb in df_square.columns], rotation=90) # rotation 45

        # add lines to unite the dashed lines
        for Ic in range(len(sorted_combinations)):

            # horizontal line
            cm.ax_heatmap.plot([0, Ic], [Ic, Ic], linewidth=.7, linestyle="--", color="gray")

            # vertical line
            cm.ax_heatmap.plot([Ic, Ic], [Ic, len(sorted_combinations)], linewidth=.7, linestyle="--", color="gray")

        # add extra horizontal line
        print(Ic)
        Ic+=0.95
        cm.ax_heatmap.plot([0, Ic], [Ic, Ic], linewidth=.7, linestyle="--", color="gray")

        # labels
        cm.ax_heatmap.set_xlabel("")
        cm.ax_heatmap.set_ylabel("")

        # adjust
        square_w = 0.025
        distance_btw_boxes = 0.007
        adjust_cm_positions(cm, df_square, hm_height_multiplier=square_w, hm_width_multiplier=square_w, cc_height_multiplier=square_w, rc_width_multiplier=square_w, cbar_width=0.12, cbar_height=0.02, distance_btw_boxes=distance_btw_boxes)

        # def adjust_cm_positions(cm, df_square, hm_height_multiplier=0.0002, hm_width_multiplier=0.01, cc_height_multiplier=0.017, rc_width_multiplier=0.017, idx_delimiter="-", distance_btw_boxes=0.0025, cd_height=0.07, rd_width=0.07, cbar_width=0.08, cbar_height=0.015):

        # change the position of the col colors (be in the bottom)
        hm_pos = cm.ax_heatmap.get_position()
        cc_pos = cm.ax_col_colors.get_position()
        cm.ax_col_colors.set_position([hm_pos.x0, hm_pos.y0-cc_pos.height-distance_btw_boxes, cc_pos.width, cc_pos.height])

        # change the location of the bar
        cb_pos = cm.ax_cbar.get_position()
        cm.ax_cbar.set_position([cb_pos.x0-cb_pos.width-square_w*13, cb_pos.y0-distance_btw_boxes, square_w*2, square_w*6])

        # add title
        #cm.ax_heatmap.set_title(title+"\n%s"%("; ".join(["%s=%s"%(f, sitems[0]) for f, sitems in field_to_sorted_items.items() if len(sitems)==1])))

        # add the legend

        # add legend in the lft of the heatmap
        def get_empty_legend(label): return Line2D([0], [0], marker="o", label=label, markersize=0, lw=0)
        legend_elements = []
        for f in fields: 
            color_dict = field_to_color_dict[f]
            legend_elements.append(get_empty_legend(f))
            legend_elements += [mpatches.Patch(facecolor=color_dict[val], edgecolor="gray", label=str(val)) for val in field_to_sorted_items[f]]
            if len(fields)>1: legend_elements.append(get_empty_legend(""))


        cm.ax_row_colors.legend(handles=legend_elements, bbox_to_anchor=[-4, 1], loc="upper right")

        #def adjust_cm_positions(cm, df_square, hm_height_multiplier=0.0002, hm_width_multiplier=0.01, cc_height_multiplier=0.017, idx_delimiter="-", distance_btw_boxes=0.0025, cd_height=0.07, rd_width=0.07, cbar_width=0.08, cbar_height=0.015):

        # add title
        if len(target_species)==1: title_str = "%s\n(%i OGs)"%(get_short_spp_name(target_species[0]), len(set(genes_df.orthofinder_orthocluster)))
        else: title_str = "all species\n(%i OGs)"%(len(set(genes_df.orthofinder_orthocluster)))

        cm.ax_heatmap.set_title(title_str)
        plt.show()

        # save
        print("saving")
        cm.savefig('%s/Selection_overlap_orthogroups_%s_%s.pdf'%(PlotsDir, "_".join(target_species), "_".join(fields)), bbox_inches="tight")


def get_table_enrichments_selection(DataDir, gene_features_df, ProcessedDataDir, PlotsDir, df_diversity_filt, species_to_gff, max_fraction_genes_pathway_toBeConsidered, TablesDir):

    """get df with enrichments for selected genes"""

    # load df with the genes under selection in all species
    genes_under_selection_df = load_object("%s/genes_under_selection.py"%ProcessedDataDir)

      # define the parameters string, useful to not repeat things
    parms_string = "maxFractGenes=%s"%(max_fraction_genes_pathway_toBeConsidered)

    # define the outdir
    outdir = "%s/enriched_domains_or_pathways_genes_selection_files_%s"%(ProcessedDataDir, parms_string)
    make_folder(outdir)

    # define the obofile
    obo_file = "%s/annotation_files/go-basic_30062021.obo"%(DataDir) # this was got from http://purl.obolibrary.org/obo/go/go-basic.obo
    
    # define file
    df_enrichment_all_file = "%s/df_diversity_df_enrichment_file_%s.py"%(outdir, parms_string)

    # get the enrichments
    if file_is_empty(df_enrichment_all_file):

        # init dfs
        df_enrichment_all = pd.DataFrame()

        # go through each species
        for species in sorted_species_byPhylogeny:

            #if species!="Candida_tropicalis": continue

            # define the dir of the ancestral GWAS, which is where I'd get the annotations
            taxID_dir = "%s/%s_%i"%(DataDir, species, sciName_to_taxID[species])
            outdir_species = "%s/%s"%(outdir, species)
            make_folder(outdir_species)

            # go through each type of grouping
            for type_grouping in ["MetaCyc", "GO_BP", "GO_CC", "GO_MF", "Reactome", "IP_domains"]:
                print(species, type_grouping)

                # load the interpro df, seed for the other groupings
                df_interpro = load_InterProAnnotation("%s/InterproScan_annotation/interproscan_annotation.out"%taxID_dir)
                df_interpro_pathways_all = get_df_pathways_from_df_interpro(df_interpro, "%s/interproscan_pathway_df.py"%outdir_species)

                # load a df that has Gene, group_name, ID
                if type_grouping=="MetaCyc":

                    # load the metacyc pathways from interpro
                    df_metacyc_all = df_interpro_pathways_all[df_interpro_pathways_all.pathway_type=="MetaCyc"]
                    df_metacyc_all = get_df_metacyc_with_pathways_from_parents(df_metacyc_all, outdir_species, max_fraction_genes_pathway_toBeConsidered)

                    # define the grouping df
                    df_grouping = df_metacyc_all.rename(columns={"pathway_ID":"ID"})
                    df_grouping["group_name"] = df_grouping.ID.map(get_metacycID_to_description(list(df_grouping.ID), "%s/%s_metacyc_ID_to_description.py"%(outdir, species)))

                elif type_grouping=="Reactome":

                    # load the reactome pathways from interptro
                    reactome_pathways_file =  "%s/annotation_files/ReactomePathways.txt"%(DataDir) 
                    reactome_pathwayRelations_file = "%s/annotation_files/ReactomePathwaysRelation.txt"%DataDir

                    df_reactome_all = df_interpro_pathways_all[df_interpro_pathways_all.pathway_type=="Reactome"]
                    df_reactome_all = get_df_reactome_with_Parents_transferred(df_reactome_all, reactome_pathways_file, reactome_pathwayRelations_file, max_fraction_genes_pathway_toBeConsidered, outdir_species)

                    # get the grouping df
                    df_grouping = df_reactome_all.rename(columns={"pathway_ID":"ID"})

                    df_reactome_pathways = pd.read_csv(reactome_pathways_file, sep="\t", header=None, names=["ID", "group_name", "species"])
                    df_reactome_pathways["group_name"] = df_reactome_pathways.species.apply(lambda x: "%s. %s"%(x.split()[0][0], x.split()[1])) + " " + df_reactome_pathways.group_name
                    
                    df_grouping = df_grouping.merge(df_reactome_pathways[["ID", "group_name"]], on="ID", validate="many_to_one", how="left")

                elif type_grouping=="IP_domains":
                    df_grouping = df_interpro.rename(columns={"proteinID":"Gene", "signature_accession":"ID"})
                    df_grouping["group_name"] = df_grouping.type_analysis + "; " + df_grouping.signature_description + "; IPaccession=" + df_grouping.InterPro_accession + "(" + df_grouping.InterPro_annotation_description + ")"

                elif type_grouping in {"GO_MF", "GO_CC", "GO_BP"}:

                    # load GO terms
                    df_GOterms_per_gene = get_df_GOterms_all_species(DataDir, {species:species_to_gff[species]}, outdir_species) # each row is one GO term
                    df_GOterms_all = get_df_GOterms_transfering_across_theGraph(df_GOterms_per_gene, outdir_species, obo_file, max_fraction_genes_pathway_toBeConsidered)

                    # add the description and the namespace
                    df_GOterms_all = get_df_GOterms_all_with_NS_and_description(df_GOterms_all, "%s/df_GOterms_all_w_adds.py"%outdir_species, obo_file)

                    # keep final file
                    df_grouping = df_GOterms_all[df_GOterms_all.NS==(type_grouping.split("_")[1])].rename(columns={"GO":"ID", "description":"group_name"})


                else: raise ValueError("type grouping is invalid")

                if len(df_grouping)==0: raise ValueError("df_grouping can't be 0")
                df_grouping = df_grouping[["Gene", "ID", "group_name"]]

                # checks
                for f in ["Gene", "ID"]: check_no_nans_series(df_grouping[f])
                IDs_no_description = set(df_grouping[pd.isna(df_grouping.group_name)].ID)
                if len(IDs_no_description)>0: print( "WARNING: There are these IDs with no description: %s"%IDs_no_description )

                valid_genes = set(gene_features_df[gene_features_df.species==species].gff_upmost_parent)
                strange_genes = set(df_grouping.Gene).difference(valid_genes)
                if len(strange_genes)>0: raise ValueError("%s. There are %i strange genes: %s"%(species, len(strange_genes), strange_genes))

                # log
                print("%s. There are %i/%i genes with some %s group assigned"%(species, len(set(df_grouping.Gene)), len(valid_genes), type_grouping))

                # go through each set of variants
                for type_var in sorted(set(df_diversity_filt.type_var)):
                    print(species, type_grouping, type_var)

                    # get the genes under selection
                    target_genes, all_genes = get_target_and_all_genes_under_recent_selection(df_diversity_filt, species, type_var, gene_features_df, species_to_gff)
                    if target_genes!=set(genes_under_selection_df[(genes_under_selection_df.species==species) & (genes_under_selection_df.type_var==type_var)].gff_upmost_parent): raise ValueError("the target_genes are not valid")

                    # skip if there are no genes
                    if len(target_genes)==0:
                        print("There are %i/%i genes. Considering %s %s"%(len(target_genes), len(all_genes), type_var, species))
                        continue


                    # get the df with the results of the enrichment
                    df_enrichment = get_df_enrichment_particular_grouping(target_genes, all_genes, df_grouping, "%s/%s_%s"%(outdir_species, type_var.replace("/", "_"), type_grouping))


                    # keep
                    df_enrichment["type_var"] = type_var
                    df_enrichment["species"] = species
                    df_enrichment["type_grouping"] = type_grouping
                    df_enrichment_all = df_enrichment_all.append(df_enrichment).reset_index(drop=True)    

        # save df
        save_object(df_enrichment_all, df_enrichment_all_file)

    df_enrichment_all = load_object(df_enrichment_all_file)


    # write table
    df_enrichment_all["genes"] = df_enrichment_all.genes_group_and_target.apply(lambda x: ",".join(sorted(x)))
    df_enrichment_all[['type_grouping', 'species', 'type_var', 'ID', 'ngenes_group_and_target', 'ngenes_no_group_target', 'ngenes_group_no_target', 'ngenes_no_group_no_target', 'OR', 'p_raw', 'p_fdr', 'group_name', 'genes']].sort_values(by=["type_grouping", "species", "type_var", "p_fdr", "OR", "ID"], ascending=[True, True, True, True, False, True]).to_excel("%s/enrichments_selection.xlsx"%TablesDir,  index=False)

    return df_enrichment_all




def get_figures_Selection_enrichments(df_enrichment_all, max_fraction_genes_pathway_toBeConsidered, ProcessedDataDir, PlotsDir, gene_features_df, DataDir):

    """Performs enrichments of genes under selection. INspired by plot_enriched_domains_or_pathways_genes_selection"""

    # define parameters
    min_OR = 2
    alpha = 0.05
    keep_oneTerm_per_group = True # for each group of similar GO terms, keep only one group

    # redefine the parms_string to include alpha and OR
    parms_string = "max_fraction_genes=%s_minOR=%s_alpha=%s"%(max_fraction_genes_pathway_toBeConsidered, min_OR, alpha)

    # define the interesting types of variants
    interesting_type_grouping =  ["MetaCyc", "IP_domains", "Reactome", "GO_BP", "GO_CC", "GO_MF"]

    # define the obofile
    obo_file = "%s/annotation_files/go-basic_30062021.obo"%(DataDir) # this was got from http://purl.obolibrary.org/obo/go/go-basic.obo
    

    # define the GO terms of each species
    species_to_gene_to_GOterms = dict(gene_features_df.groupby("species").apply(lambda df_s: dict(df_s.set_index("gff_upmost_parent").GOterms)))

    # one plot for each grouping
    for type_grouping in interesting_type_grouping:

        #if type_grouping!="GO_BP": continue


        # get the enrichment df
        df_enrichment = df_enrichment_all[(df_enrichment_all.type_grouping==type_grouping) & (df_enrichment_all.p_fdr<alpha) & (df_enrichment_all.OR>=min_OR) & (df_enrichment_all.p_raw<0.05)]
        if len(df_enrichment)==0: continue

        # log n enriched domains
        print(type_grouping, len(set(df_enrichment.ID)))

        # for Reactome, only S. cerevisiae
        #if type_grouping=="Reactome": df_enrichment = df_enrichment[df_enrichment.group_name.apply(lambda x: x.split()[1])=="cerevisiae"]

        # map each ID to the n species
        ID_to_nspecies = df_enrichment[["ID", "species"]].drop_duplicates().groupby("ID").apply(len)

        # log species-specific things
        print(type_grouping, len(set(ID_to_nspecies[ID_to_nspecies==1].index)), 'species-specific')

        # for domains, only keep only domains affected in >1 species
        if type_grouping=="IP_domains":
            df_enrichment["nspp"] = df_enrichment.ID.apply(lambda x: ID_to_nspecies[x])
            df_enrichment = df_enrichment[df_enrichment.nspp>1]

        if len(df_enrichment)==0: continue


        # map each groupID to the orthogroups
        print("getting groupID_to_orthogroups")
        df_enrichment["all_genes_group"] = (df_enrichment["genes_group_no_target"] + df_enrichment["genes_group_and_target"]).apply(set)

        groupID_to_orthogroups = {ID:set() for ID in set(df_enrichment.ID)}
        for spp in sorted_species_byPhylogeny:
            df_enrichment_spp = df_enrichment[df_enrichment.species==spp]
            if len(df_enrichment_spp)==0: continue

            gene_to_og = dict(gene_features_df[gene_features_df.species==spp].set_index("gff_upmost_parent").orthofinder_orthocluster)
            df_enrichment_spp["ogs"] = df_enrichment_spp.all_genes_group.apply(lambda genes: set(map(lambda g: gene_to_og[g], genes)))
            df_enrichment_spp["ogs"] = df_enrichment_spp["ogs"].apply(lambda ogs: {og for og in ogs if not pd.isna(og)})

            for ID, ogs in df_enrichment_spp[["ID", "ogs"]].values: groupID_to_orthogroups[ID].update(ogs)

        # define the plot
        plots_dir = "%s/plots_selection_enriched_domains_pathways_%s"%(PlotsDir, parms_string); make_folder(plots_dir)
        filename_plot = "%s/%s.pdf"%(plots_dir, type_grouping)
        title = "%s\np(fdr)<%.4f, min_OR=%.4f"%(type_grouping, alpha, min_OR)


        # if it is a GO term, add info
        groups_are_GO = type_grouping.startswith("GO_")        
        if groups_are_GO is True: 

            # define the semantic_similarity_tsh
            if len(set(df_enrichment.ID))<15: semantic_similarity_tsh = 1.0
            else: semantic_similarity_tsh = 0.5


            # add info to filename_plot
            filename_plot += "_semSim=%.2f_onlyRepTerm%s.pdf"%(semantic_similarity_tsh, keep_oneTerm_per_group)


            # add to title
            title += "\nsemantic_similarity_tsh=%s"%semantic_similarity_tsh

            # define the filename data
            filename_data = "%s.data.py"%filename_plot

        else: semantic_similarity_tsh = filename_data = None

        # make the plot
        print("plotting")
        plot_enriched_groups_clustermap(df_enrichment, filename_plot, title, groupID_to_orthogroups, obo_file, species_to_gene_to_GOterms, filename_data, semantic_similarity_tsh, keep_oneTerm_per_group, col_cluster=False, groups_are_GO=groups_are_GO)

def get_nsites_iqstree_file(iqtree_file):

    """Get number of sites"""   

    interesting_lines = [l for l in open(iqtree_file, "r").readlines() if l.startswith("Input data:") and "sites" in l]
    if len(interesting_lines)!=1: raise ValueError(">1 lines")
    l = interesting_lines[0]
    return int(l.split("sequences with")[1].split()[0])



def get_gwas_filtering_df(spp_drug_to_gwas_df_file, ProcessedDataDir, PlotsDir, gene_features_df, DataDir, threads, species_to_gff, min_npheno_transitions, max_n_genes, max_p_nsignificant_vars):

    """Takes the raw GWAS results and generates the filtering df and the plot of the filters effects. Inspired by plot_GWAS_AFresistance_heatmap_how_do_various_filtering_strategies_work_on_genes_consideringConsistencyBtwPvalues"""


    # define an outdir for this data
    outdir = "%s/processed_data_plot_GWAS_AFresistance_how_do_various_filtering_strategies_consideringConsistencyBtwPvalues"%ProcessedDataDir
    make_folder(outdir)

    ######### GET DF #######

    # define the target pval fields
    all_pval_fields = ["%s_%s"%(pval_seed, pval_m) for pval_seed in ["pval_chi_square", "pval_GenoAndPheno"] for pval_m in  ["phenotypes"]] + ["pval_fisher"]

    # define the correction methods
    all_correction_methods = ["bonferroni", "fdr_bh"]

    # define the interesting fields
    df_gwas_af_fields = ["species", "drug", "type_collapsing", "type_vars", "type_mutations", "type_genes", "ASR_methods_phenotypes", "min_support", "gwas_method", "group_name", "epsilon", "OR", 'nodes_withGeno', 'nodes_withPheno', 'nodes_GenoAndPheno', 'nodes_noGenoAndNoPheno', 'nodes_GenoAndNoPheno', 'nodes_noGenoAndPheno']
    df_gwas_af_fields += ["%s_%s"%(pval_f, corr_method) for corr_method in all_correction_methods for pval_f in all_pval_fields]    
    df_gwas_af_fields += all_pval_fields
    df_gwas_af_fields += ["pval_epsilon_maxT", "pval_chi_square_maxT"]

    # define the target drugs
    target_drugs = {d for s,d in spp_drug_to_gwas_df_file.keys()}
    #target_drugs = {"MIF"} # debug

    # get filtering df
    filtering_stats_df_file = "%s/df_trying_different_filter_combinations_gwas_af_resistance_stats_%s.py"%(outdir, "_".join(sorted(target_drugs)))
    if file_is_empty(filtering_stats_df_file):
        print("getting filtering_stats_df_file")

        # create the df_gwas_af with some target drugs and fields
        df_gwas_af_file = "%s/df_gwas_af_%s.py"%(outdir, "_".join(sorted(target_drugs)))
        if file_is_empty(df_gwas_af_file):

            all_tab_files = [tab for (spp, drug), tab in spp_drug_to_gwas_df_file.items() if drug in target_drugs]
            print("getting the gwas df for %i tab files"%(len(all_tab_files)))
            df_gwas_af = pd.concat(map(lambda x: load_df_with_some_fields(x[0], x[1]), [(t, df_gwas_af_fields) for t in all_tab_files]))

            save_object(df_gwas_af, df_gwas_af_file)
        print("loading df_gwas_af_all...")
        df_gwas_af_all = load_object(df_gwas_af_file)

        # keep only some drugs to test
        if set(df_gwas_af_all.drug)!=target_drugs: raise ValueError("target drugs are not the same")

        # create a gwas df with the domains, mutations and genes
        df_gwas_af_genes_domains_muts_file = df_gwas_af_file+".onlyGenesMutsDomains.py"
        if file_is_empty(df_gwas_af_genes_domains_muts_file):

            # define the df that has mutations, adding the per-gene alteration
            print("getting df mutations")
            df_gwas_af_mutations = get_gwas_df_no_collapsing_with_collapsing_info(df_gwas_af_all, DataDir, threads, outdir, target_type_collapsing="genes", target_type_vars="all_vars", target_type_mutations="non_syn_muts", target_type_genes="all_genes").rename(columns={"target_group_name":"gene_name"})[df_gwas_af_fields + ["gene_name"]]

            # get the filtered df, with genes and domains
            df_gwas_af_genes_and_domains = df_gwas_af_all[(df_gwas_af_all.type_vars=="all_vars") & (df_gwas_af_all.type_mutations=="non_syn_muts")]

            df_gwas_af_genes = df_gwas_af_genes_and_domains[(df_gwas_af_genes_and_domains.type_collapsing=="genes") & (df_gwas_af_genes_and_domains.type_genes=="all_genes")]
            df_gwas_af_domains = df_gwas_af_genes_and_domains[(df_gwas_af_genes_and_domains.type_collapsing=="domains") & (df_gwas_af_genes_and_domains.type_genes=="only_protein_coding")]

            def get_gene_name(r):
                if r.type_collapsing=="genes": return r.group_name
                elif r.type_collapsing=="domains": return r.group_name.split("#")[1]
                else: raise ValueError("error in gene: %s"%r)

            df_gwas_af_genes["gene_name"] = df_gwas_af_genes[["type_collapsing", "group_name"]].apply(get_gene_name, axis=1)
            df_gwas_af_domains["gene_name"] = df_gwas_af_domains[["type_collapsing", "group_name"]].apply(get_gene_name, axis=1)

            # merge together the info from domains, genes and small variants, all centered around a gene
            df_gwas_af = pd.concat([d[df_gwas_af_fields + ["gene_name"]] for d in [df_gwas_af_genes, df_gwas_af_domains, df_gwas_af_mutations]]).reset_index(drop=True)
            save_object(df_gwas_af, df_gwas_af_genes_domains_muts_file)

        print("loading df_gwas_af...")
        df_gwas_af = load_object(df_gwas_af_genes_domains_muts_file)

        # go through different combinations of filters and define a dataframe with them
        filters_df_file = "%s/filters_df_all_filters.py"%outdir
        if file_is_empty(filters_df_file):

            print("getting filters")
            filters_dict = {}; I = 0

            for ASR_methods_phenotypes in ['DOWNPASS', 'MPPA,DOWNPASS', 'MPPA']: #, 'MPPA,DOWNPASS']:
                #for min_n_sig_pvals in [1, 2, 3, 4]: # old way with n sig p vals
                for min_support in [50, 70]:

                    # define the pval fileds
                    for pval_chi_square_RelToBranchLen in [False]:
                        for pval_chi_square_phenotypes in [True, False]:
                            for pval_GenoAndPheno_RelToBranchLen in [False]:
                                for pval_GenoAndPheno_phenotypes in [True, False]:
                                    for pval_fisher in [True, False]:

                                        for pval_epsilon_maxT in [True, False]:
                                            for pval_chi_square_maxT in [True, False]:

                                                # require at least one p val field
                                                if sum([pval_chi_square_RelToBranchLen, pval_chi_square_phenotypes, pval_GenoAndPheno_RelToBranchLen, pval_GenoAndPheno_phenotypes, pval_fisher, pval_epsilon_maxT, pval_chi_square_maxT])==0: continue

                                                for correction_method in (all_correction_methods + ["none"]):

                                                    array_fdr_tshds = [0.05]
                                                    for fdr_threshold in {"none":[0.05], "bonferroni":[0.05], "fdr_bh":array_fdr_tshds, "fdr_by":array_fdr_tshds}[correction_method]:
                                                        for min_epsilon in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]:

                                                            for alpha_pval in {"none": [0.05], "bonferroni":[0.05], "fdr_bh":[0.05], "fdr_by":[0.05]}[correction_method]:
                                                                for min_fraction_nodes_Pheno_wGeno in [0]:
                                                                    for min_fraction_nodes_Geno_wPheno in [0]:

                                                                        for min_nodes_GenoAndPheno in [2, 3]:
                                                                            for min_hmean_fraction_nodes_GwP_PwG in [0]:

                                                                                # keep filters
                                                                                #filters_dict[I] = {"ASR_methods_phenotypes":ASR_methods_phenotypes, "correction_method":correction_method, "min_epsilon":min_epsilon, "min_OR":1, "min_support":min_support, "fdr_threshold":fdr_threshold, "pval_chi_square_RelToBranchLen":pval_chi_square_RelToBranchLen, "pval_chi_square_phenotypes":pval_chi_square_phenotypes, "pval_GenoAndPheno_RelToBranchLen":pval_GenoAndPheno_RelToBranchLen, "pval_GenoAndPheno_phenotypes":pval_GenoAndPheno_phenotypes, "pval_fisher":pval_fisher, "alpha_pval":alpha_pval, "min_fraction_nodes_Pheno_wGeno":min_fraction_nodes_Pheno_wGeno, "min_nodes_GenoAndPheno":min_nodes_GenoAndPheno, "min_fraction_nodes_Geno_wPheno":min_fraction_nodes_Geno_wPheno, "min_hmean_fraction_nodes_GwP_PwG":min_hmean_fraction_nodes_GwP_PwG, "filter_I":I}

                                                                                filters_dict[I] = {"ASR_methods_phenotypes":ASR_methods_phenotypes, "correction_method":correction_method, "min_epsilon":min_epsilon, "min_support":min_support, "fdr_threshold":fdr_threshold, "pval_chi_square_RelToBranchLen":pval_chi_square_RelToBranchLen, "pval_chi_square_phenotypes":pval_chi_square_phenotypes, "pval_GenoAndPheno_RelToBranchLen":pval_GenoAndPheno_RelToBranchLen, "pval_GenoAndPheno_phenotypes":pval_GenoAndPheno_phenotypes, "pval_fisher":pval_fisher, "alpha_pval":alpha_pval, "min_nodes_GenoAndPheno":min_nodes_GenoAndPheno, "pval_epsilon_maxT":pval_epsilon_maxT, "pval_chi_square_maxT":pval_chi_square_maxT,  "filter_I":I}


                                                                                I+=1


            filters_df = pd.DataFrame(filters_dict).transpose()
            if len(filters_df)!=len(filters_df.drop_duplicates()): raise ValueError("df is not unique")
            
            print("saving")
            save_object(filters_df, filters_df_file)

        print("loading filters")
        filters_df = load_object(filters_df_file).reset_index(drop=True)

        # keep gene_features_df
        gene_features_df = gene_features_df[gene_features_df.species.isin({"Candida_albicans", "Candida_glabrata", "Candida_auris"})]

        # init files
        print("generating cmds to test each set of filters")
        all_species_drug_method = sorted(set(df_gwas_af.species + "#" + df_gwas_af.drug + "#" + df_gwas_af.gwas_method))
        filtering_stats_df = pd.DataFrame()
        spp_drug_to_highconf_transitions = {}

        all_cmds_greasy = []
        greasy_outdir = "%s/greasy_running_jobs"%outdir; make_folder(greasy_outdir)
        jobs_file = "%s/jobs.running_cmds"%greasy_outdir
        stddir = "%s/STDfiles"%greasy_outdir
        threads_per_job = 4

        for species_drug_method in all_species_drug_method: 
            species, drug, gwas_method = species_drug_method.split("#")
            print(species, drug)

            # debug
            #if drug!="ITR": continue

            # only focus on synchronous
            if gwas_method!="synchronous": continue

            # get df and check that the tested unit is unique
            df_gwas_af_s = df_gwas_af[(df_gwas_af.species==species) & (df_gwas_af.drug==drug) & (df_gwas_af.gwas_method==gwas_method)]
            if len(df_gwas_af_s)==0: raise ValueError("df_gwas_af_s can' be 0")

            # discard the species and drug that have insufficient data
            n_transitions_set = set(df_gwas_af_all[(df_gwas_af_all.species==species) & (df_gwas_af_all.drug==drug) & (df_gwas_af_all.gwas_method=='synchronous') & (df_gwas_af_all.ASR_methods_phenotypes=="MPPA,DOWNPASS") & (df_gwas_af_all.min_support==70)].nodes_withPheno)

            if len(n_transitions_set)==0:
                print("WARNING: No high-confidence transitions")
                continue

            if len(n_transitions_set)!=1: 
                print(n_transitions_set, species, drug)
                raise ValueError("There should be only one value")

            n_transitions = next(iter(n_transitions_set))
            spp_drug_to_highconf_transitions[(species, drug)] = n_transitions
            if n_transitions<min_npheno_transitions:
                print("has %i transitions. Discarding..."%n_transitions)
                continue

            # define the file
            filtering_stats_df_sppDrug_file = "%s.%s_%s.py"%(filtering_stats_df_file, species, drug)
            print("getting filtering_stats_df_sppDrug_file")

            # keep only protein coding genes
            gff_df = load_gff3_intoDF(species_to_gff[species])
            gene_features_df_s = gene_features_df[gene_features_df.species==species]
            pseudogenes = set(gene_features_df_s[gene_features_df_s.feature_type.isin({"pseudogene", "pseudogene|Uncharacterized"})].gff_upmost_parent)
            protein_coding_genes = set(gff_df[gff_df.feature.isin({"CDS", "mRNA"})].upmost_parent).difference(pseudogenes)
            gene_features_df_s = gene_features_df_s[gene_features_df_s.gff_upmost_parent.isin(protein_coding_genes)]

            df_gwas_af_s = df_gwas_af_s[df_gwas_af_s.gene_name.isin(protein_coding_genes)]

            # get the resampled GWAS data for this species and drug
            df_gwas_af_no_collapsing_resamples =  load_object("%s/%s_%i/ancestral_GWAS_drugResistance/GWAS_%s_resistance_reshuffled_phenotypes/df_gwas_all_resamples.py"%(DataDir, species, sciName_to_taxID[species], drug))
            df_gwas_af_no_collapsing_resamples = df_gwas_af_no_collapsing_resamples[(df_gwas_af_no_collapsing_resamples.gwas_method=='synchronous') & (df_gwas_af_no_collapsing_resamples.type_collapsing=='none')]
            if len(df_gwas_af_no_collapsing_resamples[["resampleI", "group_name", "ASR_methods_phenotypes", "min_support"]].drop_duplicates())!=len(df_gwas_af_no_collapsing_resamples): raise ValueError("group name should be unique")

            # get the gwas df with the uncollapsed vars
            df_gwas_af_no_collapsing = df_gwas_af_all[(df_gwas_af_all.type_collapsing=="none") & (df_gwas_af_all.species==species) & (df_gwas_af_all.drug==drug) & (df_gwas_af_all.gwas_method=="synchronous")]

            # define parms
            length_chunks = 100

            # define files
            tmpdir_all = "%s.generating_chunks_size_%i"%(filtering_stats_df_sppDrug_file, length_chunks)
            make_folder(tmpdir_all)
            df_gwas_af_no_collapsing_file = "%s/df_gwas_af_no_collapsing.py"%tmpdir_all
            df_gwas_af_s_file = "%s/df_gwas_af_s.py"%tmpdir_all
            gene_features_df_s_file = "%s/gene_features_df_s.py"%tmpdir_all
            df_gwas_af_no_collapsing_resamples_file = "%s/df_gwas_af_no_collapsing_resamples.py"%tmpdir_all

            if any([file_is_empty(f) for f in [df_gwas_af_no_collapsing_file, df_gwas_af_s_file, gene_features_df_s_file, df_gwas_af_no_collapsing_resamples_file]]):

                # add the fisher p values to the no collapsing gwas
                print("adding fisher pvals")
                df_gwas_af_no_collapsing_resamples = get_gwas_df_with_fisher_p_value_and_corrected_pvals(df_gwas_af_no_collapsing_resamples, threads, gwas_unique_id_fields=["type_vars", "type_genes", "type_mutations", "type_collapsing", "ASR_methods_phenotypes", "min_support", "resampleI"])

                df_gwas_af_no_collapsing = get_gwas_df_with_fisher_p_value_and_corrected_pvals(df_gwas_af_no_collapsing, threads, gwas_unique_id_fields=["type_vars", "type_genes", "type_mutations", "type_collapsing", "ASR_methods_phenotypes", "min_support"])

                # save files for greasy running
                print("saving files for greasy")

                # keep some fields
                interesting_pval_fields = ['pval_chi_square_phenotypes_bonferroni', 'pval_GenoAndPheno_phenotypes_bonferroni', 'pval_fisher_bonferroni', 'pval_chi_square_phenotypes_fdr_bh', 'pval_GenoAndPheno_phenotypes_fdr_bh', 'pval_fisher_fdr_bh', 'pval_chi_square_phenotypes', 'pval_GenoAndPheno_phenotypes', 'pval_fisher', 'pval_epsilon_maxT', 'pval_chi_square_maxT']
                df_gwas_af_no_collapsing = df_gwas_af_no_collapsing[interesting_pval_fields + ['ASR_methods_phenotypes', 'min_support', 'group_name', 'epsilon', 'nodes_GenoAndPheno']]

                df_gwas_af_no_collapsing_resamples = df_gwas_af_no_collapsing_resamples[interesting_pval_fields + ['ASR_methods_phenotypes', 'min_support', 'group_name', 'epsilon', 'nodes_GenoAndPheno', 'resampleI']]

                df_gwas_af_s = df_gwas_af_s[interesting_pval_fields + ['ASR_methods_phenotypes', 'min_support', 'group_name', 'epsilon', 'nodes_GenoAndPheno', 'gene_name']]

                # save files
                print("saving")
                save_object(df_gwas_af_no_collapsing, df_gwas_af_no_collapsing_file)
                save_object(df_gwas_af_s, df_gwas_af_s_file)
                save_object(df_gwas_af_no_collapsing_resamples, df_gwas_af_no_collapsing_resamples_file)
                save_object(gene_features_df_s, gene_features_df_s_file)

            # one job for each chunk of filters
            filtering_stats_df_sppDrug = pd.DataFrame()
            all_chunks_jobs_done = True

            for Ic, chunk_filters_idxs in enumerate(chunks(list(filters_df.index), length_chunks)):
                print("chunk", Ic)

                # get the filters df chunk
                filters_df_chunk = filters_df.loc[chunk_filters_idxs]
                filters_df_chunk_file = "%s/filters_df_chunk%i.py"%(tmpdir_all, Ic)
                if file_is_empty(filters_df_chunk_file): save_object(filters_df_chunk, filters_df_chunk_file)

                # define the final_file (actual df)
                filtering_stats_df_sppDrug_chunk_file = "%s/filtering_stats_df_sppDrug_chunk%i.py"%(tmpdir_all, Ic)

                # if the job was not created, run it
                if file_is_empty(filtering_stats_df_sppDrug_chunk_file):
                    all_chunks_jobs_done = False

                    stfile = "%s/std_%s_%s_chunk%i.out"%(stddir, species, drug, Ic)
                    cmd_prefix = "source /gpfs/projects/bsc40/mschikora/anaconda3/etc/profile.d/conda.sh && conda activate Candida_mine_env"
                    cmd = "%s && %s/CandidaMine_data_generation/v1/run_function_get_filtering_stats_df_one_species_drug_gwas_method_consistency_btw_pvals.py %s %s %s %s %s %s %s %s %s %i > %s 2>&1"%(cmd_prefix, ParentDir, df_gwas_af_s_file, species, drug, gwas_method, filters_df_chunk_file, gene_features_df_s_file, df_gwas_af_no_collapsing_file, df_gwas_af_no_collapsing_resamples_file, filtering_stats_df_sppDrug_chunk_file, threads_per_job, stfile)

                    all_cmds_greasy.append(cmd)

                # keep
                else: filtering_stats_df_sppDrug = filtering_stats_df_sppDrug.append(load_object(filtering_stats_df_sppDrug_chunk_file))

            # if all jobs are done, append to filtering_stats_df
            if all_chunks_jobs_done is True: 
                print("appending to main df")
                if sorted(filtering_stats_df_sppDrug.filter_I)!=sorted(filters_df.filter_I): raise ValueError("there should be one filter as in filter_I")
                filtering_stats_df = filtering_stats_df.append(filtering_stats_df_sppDrug).reset_index(drop=True)

        # run greasy jobs if some are to be done
        if len(all_cmds_greasy)>0:
            print('running greasy', jobs_file)
            open(jobs_file, "w").write("\n".join(all_cmds_greasy))
            run_jobarray_file_MN4_greasy(jobs_file, 'gwas_filtering', time="02:00:00", queue="debug", threads_per_job=threads_per_job, nodes=16, submit=True)
            sys.exit(0)

        # add the number of transitions
        filtering_stats_df["n_pheno_transitions_MPPA,DOWNPASS_min_support70"] = filtering_stats_df.apply(lambda r: spp_drug_to_highconf_transitions[(r.species, r.drug)], axis=1)

        # save
        print("saving")
        save_object(filtering_stats_df, filtering_stats_df_file)

    ########################

    # load
    filtering_stats_df = load_object(filtering_stats_df_file)


    # add general fields
    filtering_stats_df["only_maxT_pvals"] = (filtering_stats_df[["pval_chi_square_phenotypes", "pval_GenoAndPheno_phenotypes", "pval_fisher"]]==False).apply(all, axis=1)
    filtering_stats_df["spp_and_drug"] = filtering_stats_df.species + "-" + filtering_stats_df.drug
    filtering_stats_df["correct_row_p_nsignificant_vars"] = (filtering_stats_df.nsignificant_vars==0) | (filtering_stats_df.p_nsignificant_vars<max_p_nsignificant_vars)
    filtering_stats_df["correct_row_p_nsignificant_vars_and_max_ngenes"] = (filtering_stats_df.correct_row_p_nsignificant_vars) & (filtering_stats_df.n_genes_found<=max_n_genes)

    # remove filters that are redundant
    filtering_stats_df = filtering_stats_df[~((filtering_stats_df.only_maxT_pvals) & (filtering_stats_df.correction_method!="none"))] # from filters that only have maxT, only keep corrected pvals

    return filtering_stats_df


def get_figure_GWAS_ChooseFilters_heatmap(filtering_stats_df, filename, title_str, figsize, max_n_genes_gwas, designed_GWAS_filters_df=None, discard_non_bonferroni=True):

    """Gets the filtering GWAS stats df"""

    # debug
    #filtering_stats_df = filtering_stats_df[filtering_stats_df.correction_method=="bonferroni"]
    
    # keep
    print("keeping")
    filtering_stats_df = cp.deepcopy(filtering_stats_df)

    # only synchronous
    print("filtering")
    filtering_stats_df = filtering_stats_df[filtering_stats_df.gwas_method=="synchronous"]

    # only keep bonferroni
    if discard_non_bonferroni:
        valid_corrections = {"bonferroni"}
        col_fields = ["ASR_methods_phenotypes", "min_support", "min_epsilon", "min_nodes_GenoAndPheno", "pval_chi_square_maxT", "pval_epsilon_maxT", "pval_chi_square_phenotypes", "pval_GenoAndPheno_phenotypes", "pval_fisher"]
        suffix_pvals = "(bonf)"

    else:
        valid_corrections = {"bonferroni", "none"}
        col_fields = ["correction_method", "pval_chi_square_maxT", "pval_epsilon_maxT", "ASR_methods_phenotypes", "min_support", "min_epsilon", "min_nodes_GenoAndPheno", "pval_chi_square_phenotypes", "pval_GenoAndPheno_phenotypes", "pval_fisher"]
        suffix_pvals = ""




    filtering_stats_df = filtering_stats_df[(filtering_stats_df.correction_method.isin(valid_corrections)) | (filtering_stats_df.only_maxT_pvals)]

    # general filters
    #filtering_stats_df = filtering_stats_df[(filtering_stats_df.min_support>0) & (filtering_stats_df.fdr_threshold==0.05) & (filtering_stats_df.pval_fisher==False)] 
    filtering_stats_df = filtering_stats_df[(filtering_stats_df.min_support>0) & (filtering_stats_df.fdr_threshold==0.05)] 

    # extra filters (testing)
    #filtering_stats_df = filtering_stats_df[(filtering_stats_df[["pval_chi_square_phenotypes", "pval_GenoAndPheno_phenotypes", "pval_fisher"]]==False).apply(all, axis=1)]
    #filtering_stats_df = filtering_stats_df[filtering_stats_df.correction_method=="none"]

    # checks
    print("minimum n_pheno_transitions", min(filtering_stats_df["n_pheno_transitions_MPPA,DOWNPASS_min_support70"]))

    # add spp and drug
    print("adding fields")
    filtering_stats_df["spp_and_drug"] = filtering_stats_df.species + "-" + filtering_stats_df.drug
    all_spp_and_drug = sorted(set(filtering_stats_df.spp_and_drug))

    # check that all filters have some p val
    all_pval_fields = ["pval_chi_square_phenotypes", "pval_GenoAndPheno_phenotypes", "pval_fisher", "pval_chi_square_maxT", "pval_epsilon_maxT"]
    if any(filtering_stats_df[all_pval_fields].apply(sum, axis=1)==0): raise ValueError("there are 0s some pvals")

    # real fields 

    # change fields
    for f in ["min_epsilon"]: filtering_stats_df[f] = filtering_stats_df[f].apply(float)

    # add row and col fileds for heatmap
    print("addiing IDs")
    row_fields = ["species", "drug"]
    def add_r_c_ID(r): return "-".join(r)
    filtering_stats_df["rowID"] = filtering_stats_df[row_fields].applymap(str).apply(add_r_c_ID, axis=1)
    filtering_stats_df["colID"] = filtering_stats_df[col_fields].applymap(str).apply(add_r_c_ID, axis=1)


    # sort
    filtering_stats_df = filtering_stats_df.sort_values(by=col_fields, ascending=True)

    # checks
    if len(filtering_stats_df)!=(len(set(filtering_stats_df.rowID + filtering_stats_df.colID))): raise ValueError("combinations of row and col are not unique")
    if len(filtering_stats_df)==0: raise ValueError("empty df")

    # map each row or col f to the color dict
    s_to_color = {s:c for s,c in species_to_color.items() if s in set(filtering_stats_df.species)}
    drug_to_color= {"FLC":"teal", "ITR":"cyan", "POS":"dodgerblue", "VRC":"navy", 
                    "AMB":"gray", "ANI":"magenta", "MIF":"salmon"}

    t_to_color = {"0.0":"white",  "0.1": "silver", "0.2":"dimgrey", "0.3":"rosybrown", "0.4":"red", "0.5":"black"}
    bool_to_color = {"True":"black", "False":"white"}

    f_to_color_dict = {"species": s_to_color,
                       "drug": drug_to_color,

                       "pval_chi_square_maxT":bool_to_color,
                       "pval_epsilon_maxT":bool_to_color,
                       
                       "correction_method": {"bonferroni":"black", "none":"gray"},
                       "fdr_threshold": {"0.05":"dimgray", "0.1":"salmon"},
                       "ASR_methods_phenotypes": {"DOWNPASS":"red", "MPPA":"blue", 'MPPA,DOWNPASS':"purple"},
                       "min_support": {"50":"salmon", "70":"black"},

                       "pval_chi_square_phenotypes":bool_to_color,
                       "pval_GenoAndPheno_phenotypes":bool_to_color,
                       "pval_fisher":bool_to_color,


                       "min_epsilon": t_to_color,
                       "min_nodes_GenoAndPheno":{"2":"dimgray", "3":"black"}}

    # row colors df
    print("creating row and col colors")
    row_colors_df_all = pd.DataFrame({row : {name : f_to_color_dict[name][row.split("-")[I]] for I, name in enumerate(row_fields)} for row in sorted(set(filtering_stats_df.rowID))}).transpose()

    # col colors df
    list_tuples_f_and_color_dict_cols = [(f, f_to_color_dict[f]) for f in col_fields]
    col_colors_df_all = pd.DataFrame({col : {name : color_dict[col.split("-")[I]] for I, (name, color_dict) in enumerate(list_tuples_f_and_color_dict_cols)} for col in sorted(set(filtering_stats_df.colID))}).transpose()

    # remove important stats for the cases with no significant nsignificant_vars
    def get_0genes_according_to_p_nsignificant_vars(r, target_f):
        if r.correct_row_p_nsignificant_vars_and_max_ngenes is False: return 0
        else: return r[target_f]

    for f in ["n_genes_found", "n_genes_expected_found"]: filtering_stats_df[f] = filtering_stats_df.apply(get_0genes_according_to_p_nsignificant_vars, target_f=f, axis=1)

    # make square df
    print("make square df")
    square_df = filtering_stats_df.pivot(values="n_genes_found", columns="colID", index="rowID").sort_index(); check_no_nans_in_df(square_df)
    sorted_rows = sorted(square_df.index)
    sorted_cols = list(square_df.columns)
    square_df = square_df.loc[sorted_rows, sorted_cols]

   # define the annot df
    filtering_stats_df["fraction_n_genes_expected_found"] = filtering_stats_df.n_genes_expected_found / filtering_stats_df.n_genes_expected
    def get_fraction_n_genes_expected_found_str(r):
        if pd.isna(r.fraction_n_genes_expected_found): return ""
        else: return {0:"", 0.5:"~", 1:"*"}[r.fraction_n_genes_expected_found]

    filtering_stats_df["fraction_n_genes_expected_found_str"] = filtering_stats_df.apply(get_fraction_n_genes_expected_found_str, axis=1)
    annot_df = filtering_stats_df.pivot(values="fraction_n_genes_expected_found_str", columns="colID", index="rowID").loc[square_df.index, square_df.columns]


    # print the datasets that have no sig filters
    dataset_to_hasGoodFilters = annot_df.apply(lambda r: any(r!=""), axis=1)
    datasets_no_good_filters = set(dataset_to_hasGoodFilters[dataset_to_hasGoodFilters==False].index).difference({"Candida_auris-ANI", "Candida_auris-AMB", "Candida_glabrata-MIF"})
    print('The following datasets have no filters yielding expected genes:\n%s'%("\n".join(sorted(datasets_no_good_filters))))

    # set 0s to nans
    def set_0_to_nan(x):
        if x==0: return np.nan
        else: return x
    square_df = square_df.applymap(set_0_to_nan)

    # make plot
    print("plotting...")
    cmap = "rocket_r"

    max_val = max({x for x in get_uniqueVals_df(square_df) if not pd.isna(x)})
    cm = sns.clustermap(square_df, col_cluster=False, row_cluster=False, cmap=cmap,  col_colors=col_colors_df_all, row_colors=row_colors_df_all, cbar_kws={"label":"# sig. genes"}, annot=annot_df, annot_kws={"size": 12}, fmt="", figsize=figsize, vmin=0, vmax=min([max_n_genes_gwas])) 


    # set the ticklabels of the heatmap
    print("modifying ticks")
    def get_yticklabel(row):
        species, drug = row.split("-")
        df = filtering_stats_df[filtering_stats_df.spp_and_drug==row]
        if drug=="AMB": string_genes = 'no exp.'
        elif species=="Candida_auris" and drug in {"ANI", "MIF"}: string_genes = 'exp. FKS1'
        else: string_genes = 'exp. %s'%(",".join(sorted(df.genes_expected.iloc[0])))
        return "%s. %i S<->R"%(string_genes, df['n_pheno_transitions_MPPA,DOWNPASS_min_support70'].iloc[0])

    cm.ax_heatmap.set_yticklabels([get_yticklabel(row) for row in square_df.index])
    cm.ax_heatmap.set_ylabel("")
    cm.ax_heatmap.set_xlabel("")
        
    # add filter names
    colID_to_filterI = dict(filtering_stats_df[["colID", "filter_I"]].drop_duplicates().set_index('colID').filter_I)
    colID_to_I = dict(zip(square_df.columns, range(len(square_df.columns))))

    if len(square_df.columns)<15:
        cm.ax_heatmap.set_xticklabels(["filter "+str(colID_to_filterI[c]) for c in square_df.columns])

    else:

        cm.ax_heatmap.set_xticklabels([])
        cm.ax_heatmap.set_xticks([])

    # add the designed_GWAS_filters_df
    if designed_GWAS_filters_df is not None:


        for spp_drug, r in designed_GWAS_filters_df.iterrows():
            print(spp_drug)

            rowI = dict(zip(square_df.index, range(len(square_df.index))))[spp_drug]
            colI = [colID_to_I[cID] for cID, filterI in colID_to_filterI.items() if filterI==r.filter_I][0]

            #cm.ax_heatmap.add_patch(patches.Rectangle((colI-0.1, rowI-0.25), 1+0.2, 1+0.5, edgecolor='black', facecolor='none', lw=2))
            cm.ax_heatmap.add_patch(patches.Rectangle((colI, rowI), 1, 1, edgecolor='black', facecolor='none', lw=2))




    # define a functiion that modifies ticks
    def get_modified_tick(x):

        x_to_modified_x = {"ASR_methods_phenotypes":"ASR method", "correction_method":"p correction", "min_support":"min support", "min_epsilon":"min $\epsilon$", "True":"yes", "False":"no", "MPPA":"ML", "DOWNPASS":"MP", "MPPA,DOWNPASS":"ML/MP", "pval_fisher":"$p_{FISHER}%s$"%suffix_pvals, "pval_chi_square_phenotypes":"$p(X^2)%s$"%suffix_pvals, "pval_GenoAndPheno_phenotypes":"$p(n_{Gt,Ph})%s$"%suffix_pvals, "min_nodes_GenoAndPheno":"min $n_{Gt,Ph}$", "min_fraction_nodes_Pheno_wGeno": "min $\epsilon _{Pt}$", "min_fraction_nodes_Geno_wPheno":"min $\epsilon _{Gt}$", "min_hmean_fraction_nodes_GwP_PwG":"min $H(\epsilon _{Pt} , \epsilon _{Gt})$", "pval_chi_square_maxT":"$p(X^2) (maxT)$", "pval_epsilon_maxT":"$p(\epsilon) (maxT)$"}


        if x in x_to_modified_x: return x_to_modified_x[x]

        elif x in s_to_color: return 'C. %s'%(x.split("_")[1])
        else: return x

    # add the 
    cm.ax_col_colors.set_yticklabels([get_modified_tick(y) for y in col_colors_df_all.columns])


    # add a legend for the row colors
    def get_lel(facecolor, label, edgecolor="gray"): return mpatches.Patch(facecolor=facecolor, edgecolor=edgecolor, label=get_modified_tick(label))
    legend_elements = make_flat_listOflists([([get_lel("white", f, edgecolor="white")] + [get_lel(color,label) for label,color in f_to_color_dict[f].items()]) for f in row_fields])
    cm.ax_row_colors.legend(handles=legend_elements, loc="upper right", bbox_to_anchor=(0, 1))

    # add a legend for the col colors
    legend_elements = make_flat_listOflists([([get_lel("white", f, edgecolor="white")] + [get_lel(color,label) for label,color in color_dict.items()]) for f, color_dict in list_tuples_f_and_color_dict_cols])
    
    cm.ax_cbar.legend(handles=legend_elements, loc="lower right", bbox_to_anchor=(0, 0))


    # set title
    cm.ax_col_colors.set_title(title_str)

    # show
    print("showing plot")
    plt.show()
    #sys.exit(0)

    # save
    print("saving %s"%filename)
    cm.savefig(filename, bbox_inches="tight", dpi=600)


def get_filtering_stats_df_good_filters(filtering_stats_df, TablesDir):

    """Gets a subset of filters that are good.r"""

    # add fields
    #filtering_stats_df["fraction_n_genes_expected_found"] = filtering_stats_df.n_genes_expected_found / filtering_stats_df.n_genes_expected

    ####################### DEFINE CONSERVATIVENESS OF THE FILTERS ##########################

    # generate a df with the sorted filters by conservativeness
    sorted_fields = ["pval_chi_square_maxT", "pval_epsilon_maxT", "correction_method", "fdr_threshold", "pval_chi_square_phenotypes", "pval_GenoAndPheno_phenotypes", "pval_fisher", "min_support", "min_epsilon", "min_nodes_GenoAndPheno", "ASR_methods_phenotypes"]

    filters_df = filtering_stats_df[sorted_fields + ["filter_I"]].drop_duplicates()
    if len(filters_df)!=len(set(filters_df.filter_I)): raise ValueError("filter_I should be unique")

    list_field_idx_list = [("pval_chi_square_maxT", [True, False]),
                           ("pval_epsilon_maxT", [True, False]),
                           ("correction_method", ["bonferroni", "fdr_bh", "none"]), 
                           ("fdr_threshold", [0.05]),
                           ("ASR_methods_phenotypes", ["MPPA", "DOWNPASS", "MPPA,DOWNPASS"]),
                           ("min_support", [70, 50]),
                           ("pval_chi_square_phenotypes", [True, False]),
                           ("pval_GenoAndPheno_phenotypes", [True, False]),
                           ("pval_fisher", [True, False]),
                           ("min_epsilon", [0.5, 0.4, 0.3, 0.2, 0.1, 0.0]),
                           ("min_nodes_GenoAndPheno", [3, 2])]

    for field, idx_list in list_field_idx_list:
        dict_idx = dict(zip(idx_list, range(len(idx_list))))
        filters_df["%s_idx"%field] = filters_df[field].map(dict_idx)
        check_no_nans_series(filters_df["%s_idx"%field])

    filters_df = filters_df.sort_values(by=["%s_idx"%f for f in sorted_fields], ascending=True)
    filters_df["filter_idx_conservative"] = list(range(len(filters_df)))

    # map each filter_I_to the conservativeness. The higher the index the less conservative
    filter_I_to_idxConservative = dict(filters_df.set_index("filter_I").filter_idx_conservative)
    filtering_stats_df["filter_idx_conservative"] = filtering_stats_df.filter_I.apply(lambda x: filter_I_to_idxConservative[x])

    ##############################################################################

    ####################### DEFINE OVERALL GOOD FILTERS ##########################

    # only keep bonferroni correction
    df_filt = filtering_stats_df[(filtering_stats_df.correction_method=="bonferroni") | (filtering_stats_df.only_maxT_pvals)] # keep only corrected pvalues

    print("there are %i filters"%(len(set(df_filt.filter_I))))

    # keep only filters that have either no vars or a sig number of vars
    df_filt = df_filt[df_filt.correct_row_p_nsignificant_vars_and_max_ngenes]

    # keep filters
    #df_filt = df_filt[df_filt.min_support>0]
    #df_filt = df_filt[df_filt.pval_fisher==False]
    #df_filt = df_filt[df_filt.fdr_threshold==0.05]
    #df_filt = df_filt[(df_filt.correction_method!="none") | (df_filt.only_maxT_pvals)] # keep only corrected pvalues
    #df_filt = df_filt[(df_filt.pval_epsilon_maxT) | (df_filt.pval_chi_square_maxT)] # keep some maxT pvals
    #df_filt = df_filt[(df_filt.only_maxT_pvals)] # keep only corrected pvalues

    # define the filters that are correct_row_p_nsignificant_vars_and_max_ngenes in all all_spp_and_drug
    filterI_to_spp_drug_correct = df_filt.groupby("filter_I").apply(lambda df: set(df.spp_and_drug))
    all_spp_and_drug = set(filtering_stats_df["spp_and_drug"])    
    filtersI_all_spp_drug_correct = set(filterI_to_spp_drug_correct[filterI_to_spp_drug_correct.apply(lambda x: x==all_spp_and_drug)].index)

    # define the species and drugs on which it is unerasonable to benchmark
    #non_benchmark_spp_and_drug = {"Candida_auris-AMB", "Candida_auris-ANI", "Candida_glabrata-MIF", "Candida_glabrata-POS", "Candida_auris-POS", 'Candida_albicans-FLC', 'Candida_auris-ITR'} # POS, ITR and C. albicans-FLC would require only p_nsig<0.05. Manual definition
    benchmark_spp_and_drug = set(df_filt[df_filt.n_genes_expected_found>0].spp_and_drug)
    non_benchmark_spp_and_drug = all_spp_and_drug.difference(benchmark_spp_and_drug)

    # add whether it is a good filter
    spp_drug_to_max_n_genes_expected_any_filter = dict(df_filt[(df_filt.n_genes_expected_found>0) & (df_filt.spp_and_drug.isin(benchmark_spp_and_drug))].groupby("spp_and_drug").apply(lambda df: max(df.n_genes_expected_found)))
    for spp_drug in non_benchmark_spp_and_drug: spp_drug_to_max_n_genes_expected_any_filter[spp_drug] = 0

    df_filt["good_filter_benchmarkSppDrug"] = (df_filt.spp_and_drug.isin(benchmark_spp_and_drug)) & (df_filt[["spp_and_drug", "n_genes_expected_found"]].apply(lambda r: r.n_genes_expected_found==spp_drug_to_max_n_genes_expected_any_filter[r.spp_and_drug], axis=1)) # in benchmarking spp_and_drug, you should find the maximum spp and drug

    df_filt["good_filter_non_benchmarkSppDrug"] =  (df_filt.spp_and_drug.isin(non_benchmark_spp_and_drug)) & (df_filt.n_genes_found>0)
    if any((df_filt.good_filter_benchmarkSppDrug) & (df_filt.good_filter_non_benchmarkSppDrug)): raise ValueError("there can't be in both")

    # keep good filters
    df_filt = df_filt[(df_filt.good_filter_benchmarkSppDrug) | (df_filt.good_filter_non_benchmarkSppDrug)] # there can be spp-drug with no filters
    print("datasets with no genes:", all_spp_and_drug.difference(set(df_filt.spp_and_drug)))
    print("non_benchmark_spp_and_drug:", non_benchmark_spp_and_drug)

    # create a df with information about how the filters perform
    filters_df = filters_df[filters_df.filter_I.isin(set(df_filt.filter_I))].set_index("filter_I", drop=False)
    for type_spp_and_drug_set, spp_and_drug_set in [("non_benchmark_spp_and_drug", non_benchmark_spp_and_drug), ("benchmark_spp_and_drug", benchmark_spp_and_drug), ("all_spp_and_drug", set(filtering_stats_df["spp_and_drug"]))]:

        filters_df['sig_%s'%type_spp_and_drug_set] = df_filt[df_filt.spp_and_drug.isin(spp_and_drug_set)][["filter_I", "spp_and_drug"]].drop_duplicates().groupby("filter_I").apply(lambda df: set(df.spp_and_drug))

        def get_nan_to_empty_set(x):
            if pd.isna(x): return set()
            else: return x

        filters_df['sig_%s'%type_spp_and_drug_set] = filters_df['sig_%s'%type_spp_and_drug_set].apply(get_nan_to_empty_set)
        filters_df['n_sig_%s'%type_spp_and_drug_set] = filters_df['sig_%s'%type_spp_and_drug_set].apply(len)

    # define the generally optimal filters. Those that give the maximum numbers of significant genes
    #best_filters_series = filters_df[filters_df.filter_I.isin(filtersI_all_spp_drug_correct)].sort_values(by=["n_sig_all_spp_and_drug", "n_sig_benchmark_spp_and_drug", "n_sig_non_benchmark_spp_and_drug", "filter_idx_conservative"], ascending=[False, False, False, True]).iloc[0]


    # define the best filters as some rationally designed ones
    best_filters_df = filters_df[(filters_df.pval_fisher==False) & (filters_df.pval_chi_square_phenotypes==False) & (filters_df.pval_GenoAndPheno_phenotypes==False) & (filters_df.pval_epsilon_maxT==True) & (filters_df.pval_chi_square_maxT==True) & (filters_df.min_epsilon==0.1) & (filters_df.ASR_methods_phenotypes=="MPPA,DOWNPASS") & (filters_df.min_support==70) & (filters_df.min_nodes_GenoAndPheno==2)]
    if len(best_filters_df)!=1: raise ValueError("best_filters_df should have one")
    best_filters_series = best_filters_df.iloc[0]
    print("best filter is ", best_filters_series.filter_I)

    # init the spp_drug to filter_I final
    spp_drug_to_final_filter_I = {}

    # for each spp,drug pick the filters that are best and are closest to the best_filters_series
    for spp_drug in all_spp_and_drug:

        # pick the correct filters
        filters_df_s = filters_df[filters_df.sig_all_spp_and_drug.apply(lambda x: spp_drug in x)]

        # if there are no filters, pick the best ones
        if len(filters_df_s)==0:
            print("WARNING: There are no good filters for %s. Taking the general ones"%spp_drug)
            good_filterI = best_filters_series.filter_I

        else:


            # add the number of changes to the best filters
            filters_df_s["number_filter_changes_to_best_filter"] = filters_df_s[sorted_fields].apply(lambda r: sum([r[f]!=best_filters_series[f] for f in r.keys()]), axis=1)

            # add the number of genes found
            for f in ["n_genes_expected_found", "n_genes_found"]:
                filter_to_f = dict(df_filt[df_filt.spp_and_drug==spp_drug].set_index("filter_I")[f])
                filters_df_s[f] = filters_df_s.filter_I.apply(lambda x: filter_to_f[x])


            # pick the filters that yield some gene but are closest to number_filter_changes_to_best_filter
            if spp_drug in non_benchmark_spp_and_drug:
                if any(filters_df_s.n_genes_expected_found>0): raise ValueError("n_genes_expected_found should be 0")
                good_filterI = filters_df_s[filters_df_s.n_genes_found>0].sort_values(by=["number_filter_changes_to_best_filter", "filter_idx_conservative"], ascending=[True, False]).iloc[0].filter_I


            # pick the filters that are least conservative and yield the expected genes
            else:
                if any(filters_df_s.n_genes_expected_found==0): raise ValueError("n_genes_expected_found should be >0")
                good_filterI = filters_df_s.sort_values(by=["n_genes_expected_found", "number_filter_changes_to_best_filter", "filter_idx_conservative"], ascending=[False, True, False]).iloc[0].filter_I


        # keep
        spp_drug_to_final_filter_I[spp_drug] = good_filterI

    ##############################################################################

    ######### GET THE FINAL GOOD FILTERS #########

    # keep the good filters
    good_filters = set(spp_drug_to_final_filter_I.values())
    print("There are %i good filters"%len(good_filters))
    filtering_stats_df_filt = filtering_stats_df[(filtering_stats_df.filter_I.isin(good_filters))]

    # define the df with the optimal filters
    df_optimal_filters = pd.concat([filtering_stats_df[(filtering_stats_df.spp_and_drug==spp_drug) & (filtering_stats_df.filter_I==filter_I)] for spp_drug, filter_I in spp_drug_to_final_filter_I.items()])
    df_optimal_filters.index = df_optimal_filters.spp_and_drug

    # log
    print("\n")
    for I,r in df_optimal_filters.iterrows(): print(I, r.n_genes_found, "/", r.n_genes_expected_found, "total/expected genes found")

    # check that the pvalues are all correct
    if any(df_optimal_filters.correct_row_p_nsignificant_vars==False): 

        df = df_optimal_filters[df_optimal_filters.correct_row_p_nsignificant_vars==False]
        print(df[["nsignificant_vars", "p_nsignificant_vars"]])
        raise ValueError("there can't be false values in correct_row_p_nsignificant_vars")


    ##############################################

    return filtering_stats_df_filt, df_optimal_filters

def get_table_Drugs_for_GWAS(filtering_stats_df, metadata_df, TablesDir, DataDir):

    """Gets the for each species-drug combination, show the number of R strains, R clades, S strains, S clades and # transitions."""

    # keep
    metadata_df = cp.deepcopy(metadata_df)

    #sort
    drug_to_type = {"ANI":"echinocandin", "MIF":"echinocandin", "AMB":"polyene", "FLC":"azole", "VRC":"azole", "POS":"azole", "ITR":"azole"}
    drug_to_name = {"ANI":"anidulafungin", "MIF":"micafungin", "AMB":"amphotericin B", "FLC":"fluconazole", "VRC":"voriconazole", "POS":"posaconazole", "ITR":"itraconazole"}
    def sorting_fun_spp_drug(x):
        species, drug = x
        return (species, {"echinocandin":2, "polyene":1, "azole":0}[drug_to_type[drug]], drug)

    all_spp_drug = sorted(filtering_stats_df[["species", "drug"]].drop_duplicates().apply(tuple, axis=1), key=sorting_fun_spp_drug)


    # get the dict that maps the samples for GWAS
    species_to_drug_to_samplesForGWAS = get_species_to_drug_to_samplesForGWAS_clinicalIsolates(metadata_df)

    # init table
    df_table = pd.DataFrame()

    for (species, drug) in all_spp_drug:

        # define the transitions
        n_transitions = filtering_stats_df[(filtering_stats_df.species==species) & (filtering_stats_df.drug==drug)]["n_pheno_transitions_MPPA,DOWNPASS_min_support70"].iloc[0]
        print(species, drug, n_transitions)

        # get the metadata df for this dataset
        metadta_df_species = metadata_df[(metadata_df.species_name==species)]
        metadata_df_drug = metadta_df_species[(metadta_df_species.sampleID.apply(str).isin(species_to_drug_to_samplesForGWAS[species][drug]))]
        resistance_field = "%s_resistance"%drug
        if set(metadata_df_drug[resistance_field])!={"R", "S"}: raise ValueError("the resistance is not properly defined")

        # define samples and clades R/S
        df_R = metadata_df_drug[metadata_df_drug[resistance_field]=="R"]
        df_S = metadata_df_drug[metadata_df_drug[resistance_field]=="S"]

        nclades_R = len(set(df_R[~pd.isna(df_R.cladeID_Tree_and_BranchLen)].cladeID_Tree_and_BranchLen))
        nclades_S = len(set(df_S[~pd.isna(df_S.cladeID_Tree_and_BranchLen)].cladeID_Tree_and_BranchLen))

        total_n_clades = len(set(metadta_df_species[~pd.isna(metadta_df_species.cladeID_Tree_and_BranchLen)].cladeID_Tree_and_BranchLen))

        # define rep samples
        n_representative_samples = len(get_tab_as_df_or_empty_df("%s/%s_%i/ancestral_GWAS_drugResistance/GWAS_%s_resistance/resistance_df.tab"%(DataDir, species, sciName_to_taxID[species], drug)))


        # get data
        data_dict = {"species":species, "drug":"%s (%s)"%(drug, drug_to_name[drug]), "S strains":len(df_S), "R strains":len(df_R), "clades w/ S strain":"%i/%i"%(nclades_S, total_n_clades),  "clades w/ R strain":"%i/%i"%(nclades_R, total_n_clades), "R>S or S>R transitions":n_transitions, "drug class":drug_to_type[drug]}
        df_table = df_table.append(pd.DataFrame({0 : data_dict}).transpose()).reset_index(drop=True)

    
    df_table[["species", "drug", "drug class", "S strains", "R strains", "clades w/ S strain", "clades w/ R strain", "R>S or S>R transitions"]].to_excel("%s/table_Drugs_for_GWAS.xlsx"%TablesDir, index=False)
    print(df_table)


    return df_table


def get_figure_GWAS_significant_vars_on_trees_df_vars(species, drug, ProcessedDataDir, DataDir, df_manhattan_spp_drug):

    """Gets the df with the variant, sample, common_GT for all the GWAS variants in one species and drug"""

    df_vars_file = "%s/df_var_sample_and_GT_importantGWASvars_%s_%s.py"%(ProcessedDataDir, species, drug)
    #print(species, drug)

    if file_is_empty(df_vars_file):

        # define dirs
        taxID_dir = "%s/%s_%i"%(DataDir, species, sciName_to_taxID[species])
        gwas_dir = "%s/ancestral_GWAS_drugResistance/GWAS_%s_resistance"%(taxID_dir, drug)

        # load the vars with the GWAS 
        print("loading GWAS vars")
        df_vars = load_object("%s/variants_df.py"%gwas_dir)[["variantID_across_samples", "type_var", "sampleID"]].drop_duplicates().rename(columns={"variantID_across_samples":"#Uploaded_variation"}).reset_index(drop=True)

        # keep only SNPs
        df_vars = df_vars[df_vars.type_var=="SNP"]
        df_vars = df_vars[["#Uploaded_variation", "sampleID"]]

        # check that the manhattan plot info is there
        print("adding manhattan info")
        df_manhattan_spp_drug = df_manhattan_spp_drug.rename(columns={"variantID_across_samples":"#Uploaded_variation"})
        if set(df_manhattan_spp_drug["#Uploaded_variation"])!=set(df_vars["#Uploaded_variation"]): raise ValueError("the vars should be the same")
        if len(set(df_manhattan_spp_drug["#Uploaded_variation"]))!=len(df_manhattan_spp_drug): raise ValueError("vars should be unique")

        # add manhattanplot info
        df_vars = df_vars.merge(df_manhattan_spp_drug[["#Uploaded_variation", "#CHROM", "POS", "is_significant_var_all_vars"]], on="#Uploaded_variation", how="left", validate="many_to_one")

        # load the small vars for all samples
        print("getting all the variants")
        df_all_vars = load_df_file_only_some_cols("%s/integrated_varcalls/smallVars_filt.py"%taxID_dir,  ["#Uploaded_variation", "sampleID", "common_GT", "calling_ploidy"], "%s/%s-load_small_vars_some_cols_GWAS_viusalize_vars_wGT"%(ProcessedDataDir, species))

        # modify the sampleID
        print("changing sampleID fields")
        df_all_vars["sampleID"] = df_all_vars.sampleID.apply(int)
        df_vars["sampleID"] = df_vars.sampleID.apply(int)

        # keep only GWAS relevant vars
        print("filtering and checking")
        df_all_vars = df_all_vars[df_all_vars["#Uploaded_variation"].isin(set(df_vars["#Uploaded_variation"]))]
        df_all_vars =  df_all_vars[df_all_vars.sampleID.isin(set(df_vars.sampleID))]

        # checks
        print("checks")
        if set(df_all_vars["#Uploaded_variation"])!=set(df_vars["#Uploaded_variation"]): raise ValueError("the vars should be the same")
        if set(df_all_vars["sampleID"])!=set(df_vars["sampleID"]): raise ValueError("the sampleID should be the same")

        # drop duplicates of uploaded variation, sampleID, keeping the lowest calling ploidy
        print("removing redundant calls")
        df_all_vars = df_all_vars.sort_values(by=["sampleID", "#Uploaded_variation", "calling_ploidy"])
        df_all_vars = df_all_vars.drop_duplicates(subset=["sampleID", "#Uploaded_variation"], keep="first")

        # merge
        print("merging")
        df_vars = df_vars.merge(df_all_vars, on=["sampleID", "#Uploaded_variation"], how="left", validate="one_to_one")

        print('saving')
        save_object(df_vars, df_vars_file)

    #print("loading df with all vars")
    return load_object(df_vars_file)

def get_df_vars_file_with_biallelic_positions(df_all_vars, filename):

    """Takes a df with SNPs and gets the biallelic vars"""

    if file_is_empty(filename):

        # add fields
        print('keeping biallelic positions')
        df_all_vars["chrom_and_pos"] = df_all_vars["#CHROM"] + "_" + df_all_vars["POS"].apply(str)

        # define the bialleic postions
        df = df_all_vars[["chrom_and_pos", "#Uploaded_variation"]].drop_duplicates()
        pos_to_nvars = df.groupby('chrom_and_pos').apply(len)
        biallelic_positions = set(pos_to_nvars[pos_to_nvars==1].index)
        print("there are %i/%i biallelic positions"%(len(biallelic_positions), len(pos_to_nvars)))

        # keep them and return
        df_all_vars = df_all_vars[df_all_vars.chrom_and_pos.isin(biallelic_positions)]
        save_object(df_all_vars, filename)

def set_nan_to_0(x):
    if pd.isna(x): return 0.0
    else: return x


def get_var_in_sample_is_interesting_GWAS(r, tree, df_ASR):

    """Returns if a variant is significant in a given sample"""

    # convert to str
    sampleID = str(r.sampleID)

    # get the ASR of this mutation
    df_ASR = df_ASR[df_ASR.group_name==r["#Uploaded_variation"]]
    df_ASR["gt_and_ph_transition"] = (df_ASR.genotype_transition==1.0) & (df_ASR.phenotype_transition==1.0)

    # define transition nodes (both geno and pheno)
    transition_nodes = set(df_ASR[df_ASR.gt_and_ph_transition].node_name)
    if len(transition_nodes)<2: raise ValueError("there should be >2 transition_nodes")

    # go through each transition node
    for t_node in transition_nodes:

        # define the interesting samples (all the children of the parents)
        interesting_sampleIDs = set((tree&(t_node)).get_ancestors()[0].get_leaf_names())

        # if the sample is in the interesting samples, return
        if sampleID in interesting_sampleIDs: return True


    return False

def get_df_all_vars_only_positions_close_to_significant_vars_one_chrom(chrom, df_c, ld_block_size):

    """Run for one chrom"""
    print(chrom)

    # change pos
    df_c["POS"] = df_c.POS.apply(int)

    # define the positions with significant SNPs
    positions_sig_snps = set(df_c[df_c.is_significant_var_all_vars].POS)
    if len(positions_sig_snps)==0: return pd.DataFrame()

    # define positions close to the sig ones
    positions_around_sig_snps = set.union(*map(lambda sig_pos: set(range(sig_pos-ld_block_size, sig_pos+ld_block_size+1)), positions_sig_snps))

    # keep only these positions
    return df_c[df_c.POS.isin(positions_around_sig_snps)]

def get_df_all_vars_only_positions_close_to_significant_vars(df_all_vars, ld_block_size, threads):

    """Get only variants that are close (<ld_block_size) to significant variants"""

    print("remiving variants that are not close to the significant vars")

    # run independently on each chrom
    inputs_fn = [(chrom, df_c, ld_block_size) for chrom,df_c in df_all_vars.groupby("#CHROM")]

    # run not in parallel
    #df_all_vars = pd.concat(map(lambda x: get_df_all_vars_only_positions_close_to_significant_vars_one_chrom(x[0], x[1], x[2]), inputs_fn))

    # run in parallel
    with multiproc.Pool(threads) as pool:

        df_all_vars = pd.concat(pool.starmap(get_df_all_vars_only_positions_close_to_significant_vars_one_chrom, inputs_fn))
        pool.close()
        pool.terminate()

    return df_all_vars

def get_figure_GWAS_significant_vars_on_trees(DataDir, ProcessedDataDir, PlotsDir, spp_drug_to_gwas_df_file, threads, species_to_ref_genome, filters_df, df_manhattan, metadata_df, gene_features_df):

    """Makes one tree for each species"""

    # redef
    make_folder(ProcessedDataDir)

    # load the manhattan plot for SNPs
    df_manhattan = df_manhattan[df_manhattan.type_var=="SNP"]

    # debug one example
    #spp_drug_to_gwas_df_file = {x:y for x,y in spp_drug_to_gwas_df_file.items() if x==("Candida_albicans", "FLC")}
    #spp_drug_to_gwas_df_file = {x:y for x,y in spp_drug_to_gwas_df_file.items() if x==("Candida_glabrata", "MIF")}
    #spp_drug_to_gwas_df_file = {x:y for x,y in spp_drug_to_gwas_df_file.items() if x==("Candida_auris", "MIF")}

    for (species, drug), gwas_df_file in spp_drug_to_gwas_df_file.items():
        #print(species, drug)

        #if species!="Candida_auris" or drug!="AMB": continue

        ########## GET DATA ###############

        # get genes
        gene_features_df_s = gene_features_df[gene_features_df.species==species]

        # get the manhattan plot for this spp, drug
        df_manhattan_spp_drug = df_manhattan[(df_manhattan.species==species) & (df_manhattan.drug==drug)]

        # keep the sig vars
        df_manhattan_spp_drug_sig = df_manhattan_spp_drug[df_manhattan_spp_drug.is_significant_var_all_vars]
        all_sig_vars = set(df_manhattan_spp_drug_sig.group_name)
        if len(all_sig_vars)==0: continue

        # verify that all sig snps are biallelic
        pos_to_sig_nvars = df_manhattan_spp_drug_sig.groupby(["#CHROM", "POS"]).apply(lambda df: len(set(df["variantID_across_samples"])))
        if any(pos_to_sig_nvars!=1): print("WARNING: there are some biallelic significant SNPs: %s"%(pos_to_sig_nvars[pos_to_sig_nvars!=1]))

        # define the filename with only biallelic positions
        ld_block_size = 1000
        df_all_vars_file_biallelic  = "%s/df_all_vars_GWAS_only_biallelic_positions_%s_%s_LD%ibp.py"%(ProcessedDataDir, species, drug, ld_block_size)

        if file_is_empty(df_all_vars_file_biallelic):

            # get the GWAS vars with GT and minimal info
            df_all_vars = get_figure_GWAS_significant_vars_on_trees_df_vars(species, drug, ProcessedDataDir, DataDir, df_manhattan_spp_drug)

            # change GTs
            print("changing GTs")
            #print(set(df_all_vars["common_GT"]))
            df_all_vars["common_GT"] = df_all_vars["common_GT"].map({"0/1":"0/1", "1/1":"1/1", "1":"1/1", ".":"0/1", 1:"1/1"})
            check_no_nans_series(df_all_vars["common_GT"])

            strange_gts = set(df_all_vars.common_GT).difference({"1/1", "0/1"})
            if len(strange_gts)>0: raise ValueError("strange_gts: %s"%strange_gts)

            # keep only positions that are close to the significant variants
            df_all_vars = get_df_all_vars_only_positions_close_to_significant_vars(df_all_vars, ld_block_size, threads)

            # check 
            if set(df_all_vars[df_all_vars.is_significant_var_all_vars]["#Uploaded_variation"])!=all_sig_vars: raise ValueError("You lost some sig vars")

            # discard non-biallelic positions
            get_df_vars_file_with_biallelic_positions(df_all_vars, df_all_vars_file_biallelic)

        #print("loading vars")
        df_all_vars = load_object(df_all_vars_file_biallelic)

        # print the lost vars
        missing_sig_vars = all_sig_vars.difference(set(df_all_vars[df_all_vars.is_significant_var_all_vars]["#Uploaded_variation"]))
        #print("There are %i/%i significant SNPs that are not biallelic and not considered in %s-%s"%(len(missing_sig_vars), len(all_sig_vars), species, drug))

        # define dirs
        taxID_dir = "%s/%s_%i"%(DataDir, species, sciName_to_taxID[species])
        outdir_drug = "%s/ancestral_GWAS_drugResistance/GWAS_%s_resistance"%(taxID_dir, drug)

        filters_series = filters_df.loc['%s-%s'%(species, drug)]
        outdir_all_muts = "%s/all_vars-all_genes-all_muts-none/%s-%s-min_support=%i/gwas_jobs/"%(outdir_drug, filters_series.ASR_methods_phenotypes, filters_series.ASR_methods_phenotypes, filters_series.min_support)

        # load the resistance data
        resistance_data = get_tab_as_df_or_empty_df("%s/resistance_df.tab"%outdir_drug)

        # the tree with internal node names
        tree = get_tree_with_internalNodeNames(Tree("%s/correct_tree.nw"%outdir_all_muts))
        tree.name = "root"
        check_that_tree_has_no_politomies(tree)


        # create a df with the sig variants (only in samples where it is driving significance)
        df_all_vars_sig = df_all_vars[df_all_vars.is_significant_var_all_vars]
        if len(set(df_all_vars_sig["#Uploaded_variation"]))<2: continue


        # define ehthether the vars are relevant in a given sample
        initial_sig_vars = set(df_all_vars_sig["#Uploaded_variation"])
        df_ASR =get_tab_as_df_or_empty_df(
        get_tab_file_only_lines_matching_patterns(sorted(initial_sig_vars), '%s/integrated_ASR.tab'%outdir_all_muts, '%s/get_figure_GWAS_significant_vars_on_trees_df_ASR_sig_vars_%s_%s.tab'%(ProcessedDataDir, species, drug)))
        df_ASR = df_ASR[df_ASR.group_name.isin(initial_sig_vars)]
        df_all_vars_sig["var_interesting_in_sample"] = df_all_vars_sig.apply(get_var_in_sample_is_interesting_GWAS, tree=tree, df_ASR=df_ASR, axis=1)

        #df_all_vars_sig = df_all_vars_sig[df_all_vars_sig.var_interesting_in_sample]
        #if set(df_all_vars_sig["#Uploaded_variation"])!=initial_sig_vars: raise ValueError("all sig variants should be assigned to some node")

        # print the fraction of vars that have signs of recombination with another significant variant
        all_sig_vars_sorted = sorted(set(df_all_vars_sig[df_all_vars_sig.var_interesting_in_sample]["#Uploaded_variation"]))
        sample_to_vars = df_all_vars_sig[df_all_vars_sig.var_interesting_in_sample].groupby("sampleID").apply(lambda df: set(df["#Uploaded_variation"]))
        nsig_vars = 0
        for var in all_sig_vars_sorted:

            # check if there is another variant that is significant and it is linked
            for other_var in all_sig_vars_sorted:
                if var==other_var: continue

                # if there are >=2 samples with both variants they are linked
                n_samples_w_both_vars = len(set(sample_to_vars[sample_to_vars.apply(lambda x: var in x and other_var in x)].index))
                if n_samples_w_both_vars>=2: 
                    nsig_vars += 1
                    break

        print("%s-%s. There are %.3fpct of sig variants linked to another sig variant"%(species, drug, (nsig_vars/len(all_sig_vars_sorted))*100))

        # load annotations df
        interesting_genes = set(gene_features_df_s.gff_upmost_parent)
        tmpdir_annot = '%s/generating_annot_df_wshort_vars_gwas_%s_%s'%(ProcessedDataDir, species, drug)

        print("getting short variant effect")
        df_annot =  get_df_vars_with_short_variant_effect(set(df_all_vars_sig["#Uploaded_variation"]), DataDir, species, tmpdir_annot, interesting_genes, allow_vars_no_annot=True)

        # add shot chrom
        gene_features_df_s["final_name"] = gene_features_df_s.apply(get_interesting_name_FeatsDF, axis=1)
        geneID_to_geneName = dict(gene_features_df_s.set_index("gff_upmost_parent").final_name)
        def get_corrected_short_variant_effect_by_chrom(r):
            chrom = "_".join(r.variantID_across_samples.split("_")[:-1])
            chrom_short = 'chr'+get_shortChrom_from_chrName(chrom, species)
            return "%s (%s)"%(r.short_variant_effect.replace(chrom, chrom_short), geneID_to_geneName[r.Gene])
        df_annot["short_variant_effect"] = df_annot.apply(get_corrected_short_variant_effect_by_chrom, axis=1)

        # checks
        #if set(df_annot["variantID_across_samples"])!=set(df_all_vars_sig["#Uploaded_variation"]): raise ValueError("the vars are not the same in all")

        df_annot = df_annot.set_index("variantID_across_samples")

        ###################################

        ######## PLOT #######
        print("getting tree plot")

        # map each node to the ASR phenotype
        df_phenotypes_ASR = load_df_file_only_some_cols('%s/integrated_ASR.tab'%outdir_all_muts, ["node_name", "phenotype"], '%s/obtaining_phenotypesASR_fromGWASruns_%s_%s'%(ProcessedDataDir, species, drug)).drop_duplicates()

        if len(df_phenotypes_ASR)!=len(set(df_phenotypes_ASR.node_name)): raise ValueError("node names should be unique")
        node_to_pheno = dict(df_phenotypes_ASR.set_index("node_name").phenotype)

        # check nodes
        tree_nodes = set([n.name for n in tree.traverse()])
        asr_nodes = set(df_phenotypes_ASR.node_name)
        if tree_nodes!=asr_nodes: raise ValueError("nodes are not the same")
        pheno_to_color = {"1.0":'red', "0.0":"blue", "nan":"gray"}

        # go through each node
        print("defining the tree")
        for n in tree.traverse():

            # set general node appearance
            nst = NodeStyle()
            nst["hz_line_width"] = 4
            nst["vt_line_width"] = 4
            nst["size"] = 0

            color = pheno_to_color[str(node_to_pheno[n.name])]

            nst["hz_line_color"] = color
            nst["vt_line_color"] = color
            n.set_style(nst)

            # add the face
            border_color = color
            if n.support>=70 or n.is_leaf(): filler_color = color
            else: filler_color = "white"

            var_color = filler_color
            width_rect = 15
            shape = 'o'
            width = width_rect
            height = width_rect
            label = "arial|12|white|"
            motif = [1, width_rect, shape, width, height, border_color, var_color, label] # [seq.start, seq.end, shape, width, height, fgcolor, bgcolor, text_label ]

            circ_face = SeqMotifFace(seq=None, motifs=[motif])

            if n.is_leaf(): n.add_face(circ_face, column=0, position="aligned")
            else: n.add_face(circ_face, column=0)

            # add leaf I
            leaf_to_i = dict(zip(tree.get_leaf_names(), list(range(len(tree.get_leaf_names())))))
            #if n.is_leaf(): n.add_face(TextFace(str(leaf_to_i[n.name])), column=1, position="aligned")


        # init treestyle
        ts = TreeStyle()

        # define general faces
        rect_width = 12
        rect_height = 20

        # add variant info
        current_col = 1
        sorted_chroms = sorted(set(df_all_vars["#CHROM"]))
        previos_chrom = sorted_chroms[0]
        for chrom in sorted_chroms:
            print("adding vars for", chrom)

            # get dfs
            df_c = df_all_vars[df_all_vars["#CHROM"]==chrom]
            df_c_sig = df_all_vars_sig[df_all_vars_sig["#CHROM"]==chrom]
            if len(df_c_sig)==0: raise ValueError("there should be some sig variants in the chrom")

            # define the positions colors
            sorted_positions = sorted(set(df_c_sig.POS))
            sorted_positions = list(map(str, sorted_positions))
            pos_to_color = get_value_to_color(sorted_positions, palette='tab10', n=len(sorted_positions), type_color="hex")[0]

            # define combinations of sample and position that are interesting
            combinations_sample_pos_interesting = set(df_c_sig[df_c_sig.var_interesting_in_sample][["sampleID", "POS"]].apply(tuple, axis=1))
            combinations_sample_pos_not_interesting = set(df_c_sig[~df_c_sig.var_interesting_in_sample][["sampleID", "POS"]].apply(tuple, axis=1))

            # map each position to the samples that have the mutation
            pos_to_samples = df_c_sig.groupby("POS").apply(lambda df_p: set(df_p.sampleID))
            pos_sample_to_GT = df_c_sig.set_index(["POS", "sampleID"]).common_GT
            gt_to_label = {"0/1":"/", "1/1":""}
            pos_to_var = dict(df_c_sig[["POS", "#Uploaded_variation"]].drop_duplicates().set_index("POS")["#Uploaded_variation"])

            # go through each pos
            for pos in sorted_positions:

                # add header
                ts.aligned_header.add_face(RectFace(rect_width, rect_height, bgcolor=pos_to_color[pos], fgcolor=pos_to_color[pos]), column=current_col)


                # add the footer
                ts.aligned_foot.add_face(RectFace(rect_width, rect_height, bgcolor=pos_to_color[pos], fgcolor=pos_to_color[pos]), column=current_col)

                # add the annots
                abb_to_consequence = {a:c for c,a in consequence_to_abbreviation.items()}
                var_this_pos = pos_to_var[int(pos)]
                if var_this_pos not in df_annot.index: important_annots = ['intergenic '+var_this_pos.replace(chrom, "chr"+get_shortChrom_from_chrName(chrom, species))]
                else:
                    all_annots = list(df_annot.loc[{var_this_pos}].short_variant_effect)
                    annot_to_IDx = pd.Series(dict(zip(all_annots, list(map(lambda x: var_to_IDX[abb_to_consequence[x.split("|")[0]]], all_annots)))))                    
                    important_annots = sorted(annot_to_IDx[annot_to_IDx==max(annot_to_IDx)].index)

                    # redefine the annots if all are intergenic
                    if all([x.split("|")[0] in {"up", "down"} for x in important_annots]): important_annots = ['intergenic '+var_this_pos.replace(chrom, "chr"+get_shortChrom_from_chrName(chrom, species))]

                # add annots
                text_f = TextFace(', '.join(important_annots), fsize=7, fgcolor=pos_to_color[pos], bold=True)
                text_f.rotation = 90
                ts.aligned_foot.add_face(text_f, column=current_col)


                # add each of the mutations
                for leaf in tree.get_leaves():

                    # if the variant is there
                    if int(leaf.name) in pos_to_samples[int(pos)]:

                        # define the label according to the GT
                        label = gt_to_label[pos_sample_to_GT[(int(pos), int(leaf.name))]]

                        # color according to the type
                        if (int(leaf.name), int(pos)) in combinations_sample_pos_interesting: border_color = pos_to_color[pos]
                        elif (int(leaf.name), int(pos)) in combinations_sample_pos_not_interesting: border_color = "white"
                        else: raise ValueError("invalid combination")

                        # add face
                        leaf.add_face(RectFace(rect_width, rect_height, bgcolor=border_color, fgcolor=pos_to_color[pos], label={"text":label, "color":"black", "fontsize":9, "bold":True}), column=current_col,  position="aligned")

                    # add a generic var
                    else:

                        color_resistance = pheno_to_color[str(node_to_pheno[leaf.name])]
                        leaf.add_face(RectFace(rect_width, rect_height*0.05, bgcolor=color_resistance, fgcolor=color_resistance), column=current_col,  position="aligned")



                # update the column for each position
                current_col+=1

            # at the end of every chrom add a sepparators          
            for color, wmult in [("white", 1), ("k", 0.15), ("white", 1)]:
                ts.aligned_header.add_face(RectFace(rect_width*wmult, rect_height, bgcolor=color, fgcolor=color), column=current_col)
                ts.aligned_foot.add_face(RectFace(rect_width*wmult, rect_height, bgcolor=color, fgcolor=color), column=current_col)
                for leaf in tree.get_leaves(): leaf.add_face(RectFace(rect_width*wmult, rect_height, bgcolor=color, fgcolor=color), column=current_col, position="aligned")
                current_col+=1


            #ts.aligned_foot.add_face(white_face, column=current_col); current_col+=1

        # add title
        ts.title.add_face(TextFace("C. %s-%s"%(species.split("_")[1], drug)), column=0)

        # get tree
        ts.show_branch_length = False
        ts.show_branch_support = False
        ts.show_leaf_name = False
        ts.draw_guiding_lines = True # 0=solid, 1=dashed, 2=dotted.
        ts.guiding_lines_type = 2

        #print("showing"); tree.show(tree_style=ts); # sys.exit(0)

        print("rendering")
        plots_dir = "%s/GWAS_variants_trees"%PlotsDir; 
        make_folder(plots_dir)
        tree.render(file_name="%s/%s_%s.pdf"%(plots_dir, species, drug), tree_style=ts)#, dpi=500,  units='mm', h=nleafs*2) #w=20


        #####################

        #### CREATE HEATMAP ####
        """
        print("heatmap")

        # keep variants that are in chromosomes with sig vars
        df_all_vars = df_all_vars[df_all_vars["#CHROM"].isin(set(df_manhattan_spp_drug_sig["#CHROM"]))].sort_values(by=["#CHROM", "POS"])

        # numeric GT
        df_all_vars['GT'] = df_all_vars.common_GT.map({"0/1":0.5, "1/1":1}); check_no_nans_series(df_all_vars['GT'])

        # sort samples as in the tree an positions
        sorted_positions = list(df_all_vars.drop_duplicates(subset="chrom_and_pos", keep="first").chrom_and_pos)
        sorted_samples = [int(x) for x in tree.get_leaf_names()]

        # get squared df with the plot
        print("square df")
        df_square = df_all_vars.pivot(index="sampleID", columns="chrom_and_pos", values="GT").loc[sorted_samples, sorted_positions]#.applymap(set_nan_to_0)

        # map clades and colors
        sample_to_clade = dict(metadata_df[(metadata_df.species_name==species)][["sampleID", "cladeID_Tree_and_BranchLen"]].set_index("sampleID").cladeID_Tree_and_BranchLen)
        sorted_numeric_clades = sorted({c for c in sample_to_clade.values() if not pd.isna(c)})
        if len(sorted_numeric_clades)<10: palette="tab10"
        else: palette="tab20"
        clade_to_color = get_value_to_color(sorted_numeric_clades, palette=palette, n=len(sorted_numeric_clades), type_color="hex")[0]

        # map resistance and colors
        sample_to_resistance = dict(resistance_data.set_index("sampleID").phenotype)
        resistance_to_color = {1:"red", 0:"blue"}


        # define the row colors df
        row_colors_df = pd.DataFrame({"resistance" : {s : resistance_to_color[sample_to_resistance[s]]  for s in sorted_samples}})
        def get_clade_color_for_s(s):
            clade = sample_to_clade[str(s)]
            if pd.isna(clade): return 'white'
            else: return clade_to_color[clade]
        row_colors_df["clade"] = pd.Series({s : get_clade_color_for_s(s)  for s in sorted_samples})
        row_colors_df = row_colors_df[["clade", "resistance"]]

        # define colors
        print("colors dfs")
        sorted_chroms = sorted(set(df_all_vars["#CHROM"]))
        chrom_to_color = get_value_to_color(sorted_chroms, palette="tab10", n=len(sorted_chroms))[0]
        def get_chr_color_from_pos(x): return chrom_to_color["_".join(x.split("_")[0:-1])]
        col_colors_df = pd.DataFrame({"chrom" : dict(zip(sorted_positions, map(get_chr_color_from_pos, sorted_positions)))})


        # get clustermap
        print("creating clustermap")
        cm = sns.clustermap(df_square, col_cluster=False, row_cluster=False, cmap=["olive", "salmon"], col_colors=col_colors_df, row_colors=row_colors_df)

        # adjust
        print("adjust_cm_positions")
        square_w = 0.02
        distance_btw_boxes = 0.007
        modifier_width = 0.05
        df_square.index = list(map(str, df_square.index))
        adjust_cm_positions(cm, df_square, hm_height_multiplier=square_w, hm_width_multiplier=square_w*modifier_width, cc_height_multiplier=square_w, rc_width_multiplier=square_w*2, cbar_width=0.12, cbar_height=0.02, distance_btw_boxes=distance_btw_boxes)

        print("showing")
        plt.show() 
        """

        ########################


def get_df_with_sorted_values_as_first_r(df, sorting_fields, sorting_fields_ascending):

    """Takes a df and returns the sorted df that has all the values as in the fisrts row"""


    # sort df
    check_no_nans_in_df(df[sorting_fields])
    df = df.sort_values(by=sorting_fields, ascending=sorting_fields_ascending)

    # get the df that has the general stats as the firts row
    first_r = df.iloc[0]
    df = df[df.apply(lambda r: all([r[f]==first_r[f] for f in sorting_fields]), axis=1)]

    return df

# case with the same domain in two different forms
def get_from_gwas_df_domains_whether_one_domain_is_a_subset_of_the_other(df_gwas):

    """takes a gwas of domains and checks if one is the subset of the other"""

    # situations where the domains are not as expected
    if len(df_gwas)!=2 or len(set(df_gwas.domain_start))!=1 or len(set(df_gwas.range_protein_covered))!=1 or len(set(df_gwas.altered_gene))!=1: return False

    # get the names
    d1 = df_gwas.group_name.iloc[0].split("#")[0]
    d2 = df_gwas.group_name.iloc[1].split("#")[0]

    if d1.startswith(d2) or d2.startswith(d1): return True
    else: return False



def get_NR_gwas_hits_old(df_gwas, current_nr_hits, already_considered_vars, obodag, taxID_dir):

    """Gets a df with gwas hits and the currently defined nr hits. It adds the NR hits to current_nr_hits """


    # check that the gwas is not 0
    if len(df_gwas)==0: raise ValueError("the gwas can't be 0 at this point")

    # If some hits have already been defined, remove them
    if len(already_considered_vars)>0: df_gwas = df_gwas[df_gwas.all_vars.apply(lambda x: x.intersection(already_considered_vars)).apply(len)==0]

    print("running get_NR_gwas_hits. Current hits are %i. Still %i gwas lines to process. Already considered vars are %i"%(len(current_nr_hits), len(df_gwas), len(already_considered_vars)))

    # if there are no GWAS dfs to work on, return
    if len(df_gwas)==0: return

    # check that the hits are unique
    if len(df_gwas)!=len(set(df_gwas.index)): raise ValueError("index should be unique")




    # define hits of overlapping hits
    hit_to_vars = df_gwas.all_vars
    if any(hit_to_vars.apply(len)==0): raise ValueError("There are hits with 0 vars")
    def get_hits_w_overlapping_vars(set_vars): return set(hit_to_vars[hit_to_vars.apply(lambda x: x.intersection(set_vars)).apply(len)>0].index)
    hit_to_overlapping_hits = dict(hit_to_vars.apply(get_hits_w_overlapping_vars))
    hit_to_overlapping_hits = {h : {x for x in o_hits if x!=h} for h,o_hits in hit_to_overlapping_hits.items()} # remove the self overlapping hits
    print(hit_to_overlapping_hits)

    jhgdajadgjgad


    ID_to_overlappingIDs = dict(zip(all_domains, map(lambda d : set(df_g[df_g.all_vars.apply(lambda x: x.intersection(df_g.loc[d, "all_vars"])).apply(len)>0].index), all_domains)))



    dadakgdahgdjhdagjadg
    list_clusters_hits = get_list_clusters_from_dict(hit_to_overlapping_hits, integrate_keyAndValues=True)

    # check that each hit is only in one cluster
    list_nclusters_w_hit = [sum(list(map(lambda c: hit in c, list_clusters_hits))) for hit in hit_to_vars.index]

    for hit in hit_to_vars.index:

        clusters = [c for c in list_clusters_hits if hit in c]
        if len(clusters)!=1:

            print("\n\n", hit, clusters)





    dajtydajadjadjdag

    print(list_nclusters_w_hit)
    if set(list_nclusters_w_hit)!={1}: raise ValueError("all hits should be in 1, and only 1 cluster")

    print(list_clusters_hits)

    dakhdagdajhgadgdjdgjhg

    # for each cluster of hits keep adding to current_nr_hits
    for cluster_hits in list_clusters_hits:

        # keep df with hits of this cluster
        df_gwas_c = df_gwas.loc[cluster_hits]
        if len(df_gwas_c)==0: raise ValueError("can't be 0 len")

        # sort by general things, generating df that has all the highest levels
        sorting_fields = ["epsilon", "OR", "type_collapsing_level_spec", "type_vars_level_spec", "type_mutations_level_spec", "n_all_vars"]
        sorting_fields_ascending = [False, False, True, True, True, True] # keep the highest epsilon and OR, but the lowest specificity index
        df = get_df_with_sorted_values_as_first_r(df_gwas_c, sorting_fields, sorting_fields_ascending)

        # if there are more than 1 lines, apply some contingency plans
        if len(df)>1:

            # if there are only domains that are all in the same gene, keep the most important ones
            if all(df.type_collapsing=="domains"):

                # if there is more than 1 gene, keep the most important one
                if len(set(df.altered_gene))>1: df = get_df_with_sorted_values_as_first_r(df, ["n_spp_w_ortholog",  "has_gene_name", "has_Scerevisiae_ortholog", "n_GOterms_cgdCurated", "gene_length"], [False, False, False, False, True])

                # check that there is only one gene
                if len(set(df.altered_gene))!=1: raise ValueError("there should only be one gene")

                # keep the most important domains
                df = get_df_with_sorted_values_as_first_r(df, ["domain_type_importance_idx", "range_protein_covered", "domain_start"], [False, False, True])

                # if there are two equal domains where one is the subset of the other, take the longest one
                if len(df)>1 and get_from_gwas_df_domains_whether_one_domain_is_a_subset_of_the_other(df):

                    df["len_group_name"] = df.group_name.apply(len)
                    df = df.sort_values(by="len_group_name", ascending=False).iloc[0:1]

            # for pathways, keep the most 
            elif all(df.type_collapsing=="Reactome") or all(df.type_collapsing=="GO") or all(df.type_collapsing=="MetaCyc"):


                # load dfs
                outdir_ancestralGWAS = "%s/ancestral_GWAS_drugResistance"%taxID_dir
                
                if df.type_collapsing.iloc[0]=="Reactome": df_pathways = load_object("%s/df_reactome_per_gene_with_all_parents.py"%outdir_ancestralGWAS).rename(columns={"pathway_ID":"pathway"})
                elif df.type_collapsing.iloc[0]=="MetaCyc": df_pathways = load_object("%s/df_metacyc_per_gene_with_all_parents.py"%outdir_ancestralGWAS).rename(columns={"pathway_ID":"pathway"})
                elif df.type_collapsing.iloc[0]=="GO": df_pathways = load_object("%s/df_GOterms_per_gene.py"%outdir_ancestralGWAS).rename(columns={"GO":"pathway"})

                # keep cols
                if len(df_pathways.columns)!=2: raise ValueError("there have to be 2 cols")
                df_pathways = df_pathways[["pathway", "Gene"]].drop_duplicates()

                # map each pathway to the fraction of genes that have it
                ngenes = len(set(df_pathways.Gene))
                pathway_to_fractionGenes = dict(df_pathways.groupby("pathway").apply(len)/ngenes)
                df["fraction_genes_pathway"] = df.group_name.apply(lambda x: pathway_to_fractionGenes[x])

                # keep the most specific pathway (in the minimum fraction of genes)
                df = get_df_with_sorted_values_as_first_r(df, ["fraction_genes_pathway"], [True])

                # in Reactome cases where the fraction of genes is not enough
                if len(df)>1 and df.type_collapsing.iloc[0]=="Reactome":

                    # add the number of parents
                    reactome_pathwayRelations_file = "%s/../annotation_files/ReactomePathwaysRelation.txt"%taxID_dir
                    pathway_relations_df = pd.read_csv(reactome_pathwayRelations_file, sep="\t", header=None, names=["parent_pathway_ID", "child_pathway_ID"])
                    childID_to_parentIDs = dict(pathway_relations_df.groupby("child_pathway_ID").apply(lambda df_c: set(df_c.parent_pathway_ID)))
                    all_pathways = sorted(set.union(*[set(pathway_relations_df.parent_pathway_ID), set(pathway_relations_df.child_pathway_ID)]))
                    pathway_to_allParentPathways = pd.Series(dict(zip(all_pathways, map(lambda ID: get_all_parents_pathwayID(ID, childID_to_parentIDs), all_pathways))))
                    pathway_to_nParentPathways = dict(pathway_to_allParentPathways.apply(len))
                    df["n_parent_pathways"] = df.group_name.apply(lambda x: pathway_to_nParentPathways[x])

                    # add the length of the pathway description
                    reactome_pathways_df = pd.read_csv("%s/../annotation_files/ReactomePathways.txt"%taxID_dir, sep="\t", header=None, names=["pathway", "pathway_name", "species"])
                    df = df.merge(reactome_pathways_df, left_on="group_name", right_on="pathway", how="left", validate="one_to_one"); check_no_nans_series(df.pathway_name)
                    df["len_pathway_name"] = df.pathway_name.apply(len)

                    # sort by n_parent_pathways
                    df = get_df_with_sorted_values_as_first_r(df, ["n_parent_pathways", "len_pathway_name"], [False, False])

            # if there are only GO terms, keep the most specific ones
            """
            elif all(df.type_collapsing=="GO"):

                # add GO items
                ns_to_idx = {"biological_process":0, "cellular_component":1, "molecular_function":2}
                df["namespace_idx"] = df.group_name.apply(lambda x: ns_to_idx[obodag[x].namespace])

                #df["n_parents"] = df.group_name.apply(lambda x: len(obodag[x].get_all_parents()))
                df["n_children"] = df.group_name.apply(lambda x: len(obodag[x].get_all_children()))
                df["go_level"] = df.group_name.apply(lambda x: obodag[x].level)
                df["go_depth"] = df.group_name.apply(lambda x: obodag[x].depth)

                # sort
                df = get_df_with_sorted_values_as_first_r(df, ["namespace_idx", "n_children", "go_level", "go_depth"], [True, True, False, False])

            """


        # check that there is only one row
        if len(df)!=1 or len(df)==0 or type(df)!=pd.DataFrame:
            print(df, df.group_name)
            for I,r in df.iterrows(): print(r.var_type_ID, r.group_name, sorted(r.all_vars), df.epsilon)
            print(df.fraction_genes_pathway)
            raise ValueError("df should have a value of 1") # check that there is only one filed as big as this

        # keep the first line of the df
        current_nr_hits.add(df.iloc[0].name)

        # keep the vars of the already considered 
        already_considered_vars.update(df.iloc[0].all_vars)

        # run again on df_gwas_c but now considering that there arlready some nr hits
        get_NR_gwas_hits(df_gwas_c, current_nr_hits, already_considered_vars, obodag, taxID_dir)


    return


def get_graph_subcomponents(graph):

    """Takes a graph and returns a list of unique subcomponents (ordered tuples)"""

    components_set = set()

    for node in graph.vs.indices:

        # get the component
        all_graph_positions = tuple(sorted(set(graph.subcomponent(node, mode="ALL"))))

        # add
        components_set.add(all_graph_positions)

    return components_set

def get_list_unique_clusters_igraph(ID_to_overlappingIDs):

    """Takes a mapping between IDs and returns a list of connected IDs that are unique"""

    # define all the IDs
    all_IDs = sorted(set.union(*[set(ID_to_overlappingIDs.keys())] + [x for x in ID_to_overlappingIDs.values()]))
    ID_to_numericID = dict(zip(all_IDs, range(len(all_IDs))))
    numericID_to_ID = {nID:ID for ID,nID in ID_to_numericID.items()}

    # initialize a graph
    import igraph
    graph = igraph.Graph(directed=False)

    # add one vertex for each ID
    graph.add_vertices(len(all_IDs))

    # add connections
    edges = []
    for ID, oIDs in ID_to_overlappingIDs.items():
        for oID in oIDs:
            if ID!=oID: edges.append((ID_to_numericID[ID], ID_to_numericID[oID]))

    graph.add_edges(edges)

    # get all the components of this graph
    numericIDs_set_tuples = get_graph_subcomponents(graph)

    # get back to the ID definition
    list_clusters = [set([numericID_to_ID[nID] for nID in set_tuples]) for set_tuples in numericIDs_set_tuples]

    # check that all IDs are in one, and only one cluster
    list_nclusters_w_ID = [sum(list(map(lambda c: ID in c, list_clusters))) for ID in all_IDs]
    if set(list_nclusters_w_ID)!={1}: raise ValueError("all IDs should be in 1, and only 1 cluster")

    return list_clusters

def get_best_df_out_of_redundant_df_gwas(df_gwas, taxID_dir, obodag, tmpdir):

    """Takes a df gwas with many redundant results and it returns a df with one line, which contains the best df"""

    # sort by general things, generating df that has all the highest levels
    sorting_fields = ["epsilon", "OR", "type_collapsing_level_spec", "type_vars_level_spec", "type_mutations_level_spec", "n_all_vars"]
    sorting_fields_ascending = [False, False, True, True, True, True] # keep the highest epsilon and OR, but the lowest specificity index
    df = get_df_with_sorted_values_as_first_r(df_gwas, sorting_fields, sorting_fields_ascending)

    # if there are more than 1 lines, apply some contingency plans
    if len(df)>1:

        # if there are only domains that are all in the same gene, keep the most important ones
        if all(df.type_collapsing=="domains"):

            # if there is more than 1 gene, keep the most important one
            if len(set(df.altered_gene))>1: df = get_df_with_sorted_values_as_first_r(df, ["n_spp_w_ortholog",  "has_gene_name", "has_Scerevisiae_ortholog", "n_GOterms_cgdCurated", "gene_length"], [False, False, False, False, True])

            # check that there is only one gene
            if len(set(df.altered_gene))!=1: raise ValueError("there should only be one gene")

            # keep the most important domains
            df = get_df_with_sorted_values_as_first_r(df, ["domain_type_importance_idx", "range_protein_covered", "domain_start"], [False, False, True])

            # if there are two equal domains where one is the subset of the other, take the longest one
            if len(df)>1 and get_from_gwas_df_domains_whether_one_domain_is_a_subset_of_the_other(df):

                df["len_group_name"] = df.group_name.apply(len)
                df = df.sort_values(by="len_group_name", ascending=False).iloc[0:1]

            # if there are domains affecting the same gene and range and they are not sobsets, take the one with the longest description
            if len(df)>1: df = get_df_with_sorted_values_as_first_r(df, ["len_signature_description"], [False])

        # for genes
        elif all(df.type_collapsing=="genes"):

            # get the most important gene
            df = get_df_with_sorted_values_as_first_r(df, ["n_spp_w_ortholog",  "has_gene_name", "has_Scerevisiae_ortholog", "n_GOterms_cgdCurated", "gene_length"], [False, False, False, False, True])

        # for pathways
        elif all(df.type_collapsing=="Reactome") or all(df.type_collapsing=="GO") or all(df.type_collapsing=="MetaCyc"):


            # load dfs
            outdir_ancestralGWAS = "%s/ancestral_GWAS_drugResistance"%taxID_dir
            
            if df.type_collapsing.iloc[0]=="Reactome": df_pathways = load_object("%s/df_reactome_per_gene_with_all_parents.py"%outdir_ancestralGWAS).rename(columns={"pathway_ID":"pathway"})
            elif df.type_collapsing.iloc[0]=="MetaCyc": df_pathways = load_object("%s/df_metacyc_per_gene_with_all_parents.py"%outdir_ancestralGWAS).rename(columns={"pathway_ID":"pathway"})
            elif df.type_collapsing.iloc[0]=="GO": df_pathways = load_object("%s/df_GOterms_per_gene.py"%outdir_ancestralGWAS).rename(columns={"GO":"pathway"})

            # keep cols
            if len(df_pathways.columns)!=2: raise ValueError("there have to be 2 cols")
            df_pathways = df_pathways[["pathway", "Gene"]].drop_duplicates()

            # map each pathway to the fraction of genes that have it
            ngenes = len(set(df_pathways.Gene))
            pathway_to_fractionGenes = dict(df_pathways.groupby("pathway").apply(len)/ngenes)
            df["fraction_genes_pathway"] = df.group_name.apply(lambda x: pathway_to_fractionGenes[x])

            # keep the most specific pathway (in the minimum fraction of genes)
            df = get_df_with_sorted_values_as_first_r(df, ["fraction_genes_pathway"], [True])

            # in Reactome cases where the fraction of genes is not enough
            if len(df)>1 and df.type_collapsing.iloc[0] in {"Reactome", "MetaCyc"}:

                # define the pathway relations
                if df.type_collapsing.iloc[0]=="Reactome":

                    # get prelations data
                    reactome_pathwayRelations_file = "%s/../annotation_files/ReactomePathwaysRelation.txt"%taxID_dir
                    pathway_relations_df = pd.read_csv(reactome_pathwayRelations_file, sep="\t", header=None, names=["parent_pathway_ID", "child_pathway_ID"])
                    childID_to_parentIDs = dict(pathway_relations_df.groupby("child_pathway_ID").apply(lambda df_c: set(df_c.parent_pathway_ID)))
                    all_pathways = sorted(set.union(*[set(pathway_relations_df.parent_pathway_ID), set(pathway_relations_df.child_pathway_ID)]))

                    # get p_to_name
                    reactome_pathways_df = pd.read_csv("%s/../annotation_files/ReactomePathways.txt"%taxID_dir, sep="\t", header=None, names=["pathway", "pathway_name", "species"])
                    p_to_name = dict(reactome_pathways_df.set_index("pathway").pathway_name)

                    # add the species IDX
                    species_to_idx = {"SCE":0, "SPO":1}
                    df["species_idx"] = df.group_name.apply(lambda x: species_to_idx[x.split("-")[1]])

                    # define sorting fields
                    s_fields = ["species_idx", "n_parent_pathways", "len_pathway_name", "pathway_name"]; s_fields_ascending = [True, False, False, True]

                elif df.type_collapsing.iloc[0]=="MetaCyc":

                    # get prelations data
                    df_pathway_relations = load_object("%s/ancestral_GWAS_drugResistance/metacyc_child_to_parent_relations_df.py"%taxID_dir)
                    childID_to_parentIDs = dict(df_pathway_relations.groupby("childID").apply(lambda df_c: set(df_c.parentID)))
                    all_pathways = sorted(set.union(*[set(df_pathway_relations.parentID), set(df_pathway_relations.childID)]))

                    # get p_to_name
                    delete_folder(tmpdir); make_folder(tmpdir)
                    p_to_name = get_metacycID_to_description(sorted(set(df.group_name)), "%s/metacyc_ID_to_description.py"%tmpdir)

                    # define sorting fields
                    s_fields = ["n_parent_pathways", "len_pathway_name", "pathway_name"]; s_fields_ascending = [False, False, True]

                # define the number of parent pathways
                pathway_to_allParentPathways = pd.Series(dict(zip(all_pathways, map(lambda ID: get_all_parents_pathwayID(ID, childID_to_parentIDs), all_pathways))))
                pathway_to_nParentPathways = dict(pathway_to_allParentPathways.apply(len))
                df["n_parent_pathways"] = df.group_name.apply(lambda x: pathway_to_nParentPathways[x])

                # add the length of the pathway description
                df["pathway_name"] = df.group_name.apply(lambda x: p_to_name[x])
                df["len_pathway_name"] = df.pathway_name.apply(len)

                # sort by n_parent_pathways
                df = get_df_with_sorted_values_as_first_r(df, s_fields, s_fields_ascending)

            # in GO cases where the fraction of genes is not enough
            elif len(df)>1 and df.type_collapsing.iloc[0]=="GO":

                # add GO items
                ns_to_idx = {"biological_process":0, "cellular_component":1, "molecular_function":2}
                df["namespace_idx"] = df.group_name.apply(lambda x: ns_to_idx[obodag[x].namespace])

                #df["n_parents"] = df.group_name.apply(lambda x: len(obodag[x].get_all_parents()))
                df["n_children"] = df.group_name.apply(lambda x: len(obodag[x].get_all_children()))
                df["go_level"] = df.group_name.apply(lambda x: obodag[x].level)
                df["go_depth"] = df.group_name.apply(lambda x: obodag[x].depth)
                df["len_pathway_name"] = df.group_name.apply(lambda x: obodag[x].name).apply(len)

                check_no_nans_series(df["len_pathway_name"])
                if any(df["len_pathway_name"]==0): raise ValueError("no name for pathways")

                # sort
                df = get_df_with_sorted_values_as_first_r(df, ["namespace_idx", "n_children", "go_level", "go_depth", "len_pathway_name", "group_name"], [True, True, False, False, False, True])

    # check that there is only one row
    if len(df)!=1 or len(df)==0 or type(df)!=pd.DataFrame:
        print(df, df.group_name)
        print_df_keys(df)
        for I,r in df.iterrows(): print(r.group_name, sorted(r.all_vars), r.epsilon)
        raise ValueError("df should have a value of 1") # check that there is only one filed as big as this

    # define the index as the hit IDd
    df.index = df.hitID

    return df


def get_NR_gwas_hits(df_gwas, current_nr_hits, already_considered_vars, obodag, taxID_dir):

    """Gets a df with gwas hits and the currently defined nr hits. It adds the NR hits to current_nr_hits """

    # check that the gwas is not 0
    if len(df_gwas)==0: raise ValueError("the gwas can't be 0 at this point")

    # If some hits have already been defined, remove them
    if len(already_considered_vars)>0: df_gwas = df_gwas[df_gwas.all_vars.apply(lambda x: x.intersection(already_considered_vars)).apply(len)==0]

    print("running get_NR_gwas_hits. Current hits are %i. Still %i gwas lines to process. Already considered vars are %i"%(len(current_nr_hits), len(df_gwas), len(already_considered_vars)))

    # if there are no GWAS dfs to work on, return
    if len(df_gwas)==0: return

    # check that the hits are unique
    if len(df_gwas)!=len(set(df_gwas.hitID)): raise ValueError("hitID should be unique")

    # define hits of overlapping hits
    hit_to_vars = df_gwas.all_vars
    if any(hit_to_vars.apply(len)==0): raise ValueError("There are hits with 0 vars")
    def get_hits_w_overlapping_vars(set_vars): return set(hit_to_vars[hit_to_vars.apply(lambda x: x.intersection(set_vars)).apply(len)>0].index)
    hit_to_overlapping_hits = dict(hit_to_vars.apply(get_hits_w_overlapping_vars))
    list_clusters_hits = get_list_unique_clusters_igraph(hit_to_overlapping_hits)
    list_clusters_hits = sorted(list_clusters_hits, key=(lambda x: (len(x), sorted(x)[0])))

    # add the unique cluster ID
    hit_ID_to_clusterID = {}
    for hit in hit_to_vars.keys():
        cIDs = [Ic for Ic, c in enumerate(list_clusters_hits) if hit in c]
        if len(cIDs)!=1: raise ValueError("there are more than 1 cluster in clusters")
        hit_ID_to_clusterID[hit] = cIDs[0]

    df_gwas["clusterID"] = [hit_ID_to_clusterID[x] for x in df_gwas.index]
    print("There are %i cluster IDs"%len(set(df_gwas["clusterID"])))

    # get the best one-liner df
    df = get_best_df_out_of_redundant_df_gwas(df_gwas[df_gwas.clusterID==max(df_gwas["clusterID"])], taxID_dir, obodag)
  
    # keep the first line of the df
    current_nr_hits.add(df.iloc[0].hitID)

    # keep the vars of the already considered 
    already_considered_vars.update(df.iloc[0].all_vars)

    # run again on df_gwas but now considering that there arlready some nr hits
    get_NR_gwas_hits(df_gwas, current_nr_hits, already_considered_vars, obodag, taxID_dir)


def get_df_gwas_with_gene_info_fields(df_gwas, DataDir, spp, gene_features_df_s, oc_to_nspp):

    """Gets the gwas df with gene info fields"""
    print("adding gene info")

    # set index
    gene_features_df_s = gene_features_df_s.set_index("gff_upmost_parent", drop=False)

    # add several fields
    def get_r_df_gwas_w_adds(r):

        if r.type_collapsing in {"genes", "domains"}:

            # define gene name
            if r.type_collapsing=="genes": gene_name = r.group_name
            elif r.type_collapsing=="domains": gene_name = r.altered_gene

            # add fields
            g_feats = gene_features_df_s.loc[gene_name]
            r["has_Scerevisiae_ortholog"] = (not pd.isna(g_feats.Scerevisiae_orthologs))
            r["has_gene_name"] = (not pd.isna(g_feats.gene_name))

            if pd.isna(g_feats.orthofinder_orthocluster): r["n_spp_w_ortholog"] = 0
            else: r["n_spp_w_ortholog"] = oc_to_nspp[g_feats.orthofinder_orthocluster]
            
            r["gene_length"] = g_feats.gene_length
            r["n_GOterms_cgdCurated"] = len(g_feats.GOterms_cgdCurated)

        # set all fields to False or no
        elif r.type_collapsing in {"none", "GO", "Reactome", "MetaCyc"}:
            r["has_Scerevisiae_ortholog"] = False
            r["has_gene_name"] = False
            r["n_spp_w_ortholog"] = 0
            r["gene_length"] = 0
            r["n_GOterms_cgdCurated"] = 0

        else: raise ValueError("no type_collsping")

        return r

    df_gwas = df_gwas.apply(get_r_df_gwas_w_adds, axis=1)

    return df_gwas

def get_df_gwas_with_domain_info_fields(df_gwas, DataDir, spp):

    """Gets the domain fields"""

    print("adding domain info")

    # get defs
    df_gwas_domains = df_gwas[df_gwas.type_collapsing=="domains"]
    df_gwas_no_domains = df_gwas[df_gwas.type_collapsing!="domains"]

    # add fields to the domains dataset
    if len(df_gwas_domains)>0:

        # add name
        df_gwas_domains["domain_name"] = df_gwas_domains.group_name.apply(lambda x: x.split("#")[0])

        # load interpro df
        taxID_dir =  "%s/%s_%i"%(DataDir, spp, sciName_to_taxID[spp])
        df_interpro = load_InterProAnnotation("%s/InterproScan_annotation/interproscan_annotation.out"%taxID_dir)
        df_interpro = df_interpro[df_interpro.signature_accession.isin(set(df_gwas_domains.domain_name))]

        # add fields
        df_gwas_domains["altered_gene"] = df_gwas_domains.group_name.apply(lambda x: x.split("#")[1])
        df_gwas_domains["list_range"] = df_gwas_domains.group_name.apply(lambda x: list(map(int, x.split("#")[2].split("_"))))
        df_gwas_domains["domain_start"] = df_gwas_domains.list_range.apply(lambda x: x[0])
        df_gwas_domains["range_protein_covered"] = df_gwas_domains.list_range.apply(lambda x: x[1]-x[0])
        if any(df_gwas_domains.range_protein_covered<=0): raise ValueError("there should be no proteins with no range covered")

        # add the type analysis field
        sigAcc_to_type_analysis = dict(df_interpro[["type_analysis", "signature_accession"]].drop_duplicates().set_index("signature_accession").type_analysis)
        sorted_types_analysis =  ["chunk", "PIRSF", "CDD", "MobiDBLite", "Hamap", "Coils", "PRINTS", "Gene3D", "SUPERFAMILY", "ProSiteProfiles", "ProSitePatterns", "SMART", "SFLD", "TIGRFAM", "Pfam" , "PANTHER"]
        type_analysis_to_importance_idx = dict(zip(sorted_types_analysis, range(0, len(sorted_types_analysis))))

        def get_domain_type(gname):
            if gname.startswith("pChunk_"): return 'chunk'
            else: return sigAcc_to_type_analysis[gname.split("#")[0]]

        df_gwas_domains["domain_type"] = df_gwas_domains.group_name.apply(get_domain_type) # the higher the number the better
        df_gwas_domains["domain_type_importance_idx"] = df_gwas_domains.domain_type.apply(lambda x: type_analysis_to_importance_idx[x])

        # add the domain description
        sigAcc_to_description = dict(df_interpro[["signature_description", "signature_accession"]].drop_duplicates().set_index("signature_accession").signature_description)
        def get_signature_description(r):

            if r.domain_type=="chunk": return "chunk"
            else: return sigAcc_to_description[r.domain_name]

        df_gwas_domains["len_signature_description"] = df_gwas_domains.apply(get_signature_description, axis=1).apply(len)

    # return both
    return pd.concat([df_gwas_domains, df_gwas_no_domains])



def get_df_gwas_NR(obodag, df_gwas, ProcessedDataDir, spp, drug, type_muts, taxID_dir):


    """Takes a GWAS df and returns it without redundant groups, so that each variant is mapped tp one df"""

    # keep
    df_gwas = cp.deepcopy(df_gwas)

    # get the hits by adding them recursively
    """
    df_gwas["hitID"] = 'hit' + df_gwas.unique_hit_ID.apply(str)
    NR_gwas_hits = set()
    already_considered_vars = set()
    get_NR_gwas_hits(df_gwas, NR_gwas_hits, already_considered_vars, obodag, taxID_dir)
    if len(NR_gwas_hits)==0: raise ValueError("there should be some NR hits")
    """

    # iterate through the variants
    all_vars = sorted(set.union(*df_gwas.all_vars))
    all_hit_ids = set(df_gwas.hitID)
    var_to_max_epsilon = pd.Series(dict(zip(all_vars, map(lambda var: max(df_gwas[df_gwas.all_vars.apply(lambda x: var in x)].epsilon), all_vars))))
    already_considered_vars = set()
    NR_gwas_hits = set()
    df_gwas_current = cp.deepcopy(df_gwas)

    for Iv, (var, max_epsilon) in enumerate(var_to_max_epsilon.sort_values(ascending=False).iteritems()):

        # debug vars already considered
        if var in already_considered_vars: continue

        # check that the gwas is not 0
        if len(df_gwas_current)==0: raise ValueError("the gwas can't be 0 at this point")

        # only keep hits that do not have already_considered_vars
        df_gwas_current = df_gwas_current[df_gwas_current.all_vars.apply(lambda x: x.intersection(already_considered_vars)).apply(len)==0]

        # if there are no GWAS dfs to work on, return
        if len(df_gwas_current)==0: break

        # add var
        already_considered_vars.add(var)

        # from this variant, get all groups thtat have it 
        df_var = df_gwas_current[df_gwas_current.all_vars.apply(lambda x: var in x)]
        if len(df_var)==0: continue

        # get the best one-liner df
        df = get_best_df_out_of_redundant_df_gwas(df_var, taxID_dir, obodag, "%s/getting_metacyc_descriptions_nr_gwas_gen_%s_%s_%s_%s_%s"%(ProcessedDataDir, spp, drug, type_muts, Iv, var.replace("/", "_")))
        if len(df)!=1: raise ValueError("df should have 1")
        if df.iloc[0].hitID not in all_hit_ids: raise ValueError('invalid hit: %s'%(df.iloc[0].name))      

        # keep the first line of the df
        NR_gwas_hits.add(df.iloc[0].hitID)

        # keep the vars of the already considered 
        already_considered_vars.update(df.iloc[0].all_vars)
        if var not in df.iloc[0].all_vars: raise ValueError("var should be in all_vars")

    # filter again at the end
    df_gwas_current = df_gwas_current[df_gwas_current.all_vars.apply(lambda x: x.intersection(already_considered_vars)).apply(len)==0]

    # debug
    if len(df_gwas_current)!=0: raise ValueError("there should not be any df_gwas_current")

    # get df with NR hits
    if len(set(df_gwas.hitID))!=len(df_gwas): raise ValueError("the hitID should be unique")
    df_gwas = df_gwas.set_index("hitID", drop=False)
    df_gwas_NR = df_gwas.loc[NR_gwas_hits]

    # check that  from the NR variants there are no overlaps
    for Ia, set_varsA in df_gwas_NR.all_vars.iteritems():
        for Ib, set_varsB in df_gwas_NR.all_vars.iteritems():
            if Ia==Ib: continue
            if len(set_varsA.intersection(set_varsB))>0: 
                print(Ia, Ib, set_varsA, set_varsB)
                print(df_gwas_NR.loc[Ia].group_name)
                print(df_gwas_NR.loc[Ib].group_name)
                raise ValueError("there is intersection btw hits")

    # check that all the vars are in some group
    for I, r in df_gwas.iterrows():
        df_w_var = df_gwas_NR[df_gwas_NR.all_vars.apply(lambda x: x.intersection(r.all_vars)).apply(len)>0]
        if len(df_w_var)==0: raise ValueError("for vars in hit %s, there are no hits in NR that do contain some of them"%I)

    # remove some fields and return
    for f in ["type_vars", "type_mutations", "type_genes", "all_vars_tuple", "pval_chi_square_phenotypes_fdr_bh", "pval_chi_square_phenotypes_fdr_by", "pval_GenoAndPheno_phenotypes_fdr_bh", "pval_GenoAndPheno_phenotypes_fdr_by", 'pval_fisher_fdr_bh', 'pval_fisher_fdr_by', 'var_type_ID', 'pval_chi_square_phenotypes_is_significant', 'pval_chi_square_maxT_is_significant', 'var_type_ID_without_collapsing']: 

        if f in df_gwas_NR.keys(): df_gwas_NR.pop(f)

    return df_gwas_NR


def get_df_gwas_pathways_with_genes(df_gwas, taxID_dir):

    """Gets a gwas with pathways and adds the genes that are in these pathways"""

    print("adding genes from pathways")

    # load dfs
    outdir_ancestralGWAS = "%s/ancestral_GWAS_drugResistance"%taxID_dir
    df_reactome = load_object("%s/df_reactome_per_gene_with_all_parents.py"%outdir_ancestralGWAS).rename(columns={"pathway_ID":"pathway"})
    df_metacyc = load_object("%s/df_metacyc_per_gene_with_all_parents.py"%outdir_ancestralGWAS).rename(columns={"pathway_ID":"pathway"})
    df_go = load_object("%s/df_GOterms_per_gene.py"%outdir_ancestralGWAS).rename(columns={"GO":"pathway"})

    # get the genes
    type_collapsing_to_df = {"Reactome":df_reactome, "MetaCyc":df_metacyc, "GO":df_go}
    type_collapsing_to_p_to_genes = {t : df.groupby("pathway").apply(lambda df_p: set(df_p.Gene)) for t,df in type_collapsing_to_df.items()}

    df_gwas["genes_in_pathway"] = df_gwas.apply(lambda r: type_collapsing_to_p_to_genes[r.type_collapsing][r.group_name], axis=1)

    # check and return
    if any(df_gwas["genes_in_pathway"].apply(len)==0): raise ValueError("there are no genes in the pathway")

    return df_gwas





def get_df_gwas_filtered_and_redundancy_removed_eachVarOnlyInOneGroup_one_spp_and_drug(spp, drug, tab_file, DataDir, ProcessedDataDir, optimal_filters_series, type_muts, gff, gene_features_df_s, oc_to_nspp, df_gwas_NR_file):

    """Gets the filtered df for one species and drug with redundacny removed. Inspired from get_df_gwas_filtered_and_redundancy_removed_one_spp_and_drug"""

    print("\n", spp, drug)

    # redefine file
    if df_gwas_NR_file is None: df_gwas_NR_file = "%s/df_gwas_NR_%s_%s_%s.py"%(DataDir, spp, drug, type_muts)


    if file_is_empty(df_gwas_NR_file):
        print("generating %s"%df_gwas_NR_file)

        ###### GET FILTERED DATA #####

        # load gwas
        df_gwas = get_tab_as_df_or_empty_df(tab_file)
        print("gwas loaded")

        # define the GWAS dir
        taxID_dir = "%s/%s_%i"%(DataDir, spp, sciName_to_taxID[spp])
        gwas_dir = "%s/ancestral_GWAS_drugResistance/GWAS_%s_resistance"%(taxID_dir, drug)

        # keep only synchronous
        df_gwas = df_gwas[df_gwas.gwas_method=="synchronous"]
        print(len(df_gwas), "lines in df_gwas initially, before filtering")

        # filter gwas df
        df_gwas =  get_filtered_gwas_af_df_consistency_btw_pvals(optimal_filters_series, df_gwas)
        print("There are %i total lines in df_gwas"%(len(df_gwas)))

        ###############################

        ##### KEEP ONLY PROT-ALTERING CHANGES #####

        # this is at the level of variants

        # keep only non-syn vars and genes
        if type_muts=="only_non_syn":
            print("keeping only prot-altering vars")

            # get the tab file that greps to none,
            tab_file_no_collapsing = "%s/gwas_df_no_collapsing_%s_%s_eachVarOnlyInOneGroup_only_non_syn.tab"%(ProcessedDataDir, spp, drug)
            get_tab_file_only_lines_matching_patterns(['none'], tab_file, tab_file_no_collapsing)
            df_gwas_no_collapsing = get_tab_as_df_or_empty_df(tab_file_no_collapsing); df_gwas_no_collapsing = df_gwas_no_collapsing[df_gwas_no_collapsing.type_collapsing=="none"]

            # define the variants that are protein altering
            non_syn_vars = set()
            for target_type_mutations in ["non_syn_muts", "truncating_muts", "non_syn_non_truncating_muts"]:

                df_gene_alteration = get_gwas_df_no_collapsing_with_collapsing_info(df_gwas_no_collapsing, DataDir, 1, '%s/%s_%s_%s_gwas_mapping_muts_to_genes_nonsyn_considering_all_genes_eachVarOnlyInOneGroup_onlynsyn'%(ProcessedDataDir, spp, drug, target_type_mutations), target_type_collapsing="genes", target_type_vars="all_vars", target_type_mutations=target_type_mutations, target_type_genes="all_genes", run_in_parallel=False)
                non_syn_vars.update(set(df_gene_alteration.group_name))

            # define a function that says if the row is nonsyn
            def row_is_non_syn(r):

                if r.type_collapsing=="none":

                    # define based on the mutations df
                    if r.group_name in non_syn_vars: return True
                    else: return False

                else:

                    # only keep non-syn vars
                    if r.type_mutations in {"non_syn_muts", "truncating_muts", "non_syn_non_truncating_muts"}: return True
                    elif r.type_mutations in {"all_muts"}: return False
                    else: raise ValueError("invalid type_mutations")

            print("filtering out non coding alterations")
            df_gwas = df_gwas[df_gwas.apply(row_is_non_syn, axis=1)]
            print("%s-%s. There are %i associated groups after filtering non-syn alterations"%(spp, drug, len(df_gwas)))

        elif type_muts=="all_muts": pass
        else: raise ValueError("invalid type_muts")

        # debug
        if len(df_gwas)==0: 
            print("WARNING: %s-%s has 0 significant results"%(spp, drug))
            print("returning df")
            save_object(pd.DataFrame(), df_gwas_NR_file)
            return pd.DataFrame()

        ###########################################

        #### ADD FIELDS #####

        # define the raw filtered df
        df_gwas_filt_raw = cp.deepcopy(df_gwas)

        # define dicts that define the level of specificity of the variants
        type_vars_to_level_specificity = {"all_vars": 3,
                                          "SVs_and_CNVs":2, "small_vars_and_SVs":2, "small_vars_and_CNVs":2,
                                          "small_vars":1, "SVs":1, "coverageCNVs":1}

        type_collapsing_to_level_specificity = {"Reactome":6, "GO":5, "MetaCyc":4, # I consider Metacyc to be more important than GO, and GO more than Reactome
                                                "genes":3, 
                                                "domains":2,
                                                "none":1}

        type_mutations_to_level_specificity = {"all_muts": 3,
                                               "syn_muts": 2, "non_syn_muts": 2,
                                               "non_syn_non_truncating_muts": 1.5, "truncating_muts": 1} # we put 1.5 in non_syn_non_truncating_muts because truncating are more relevant

        # add the variant type ID
        var_type_ID_fields = ["type_vars", "type_genes", "type_mutations", "type_collapsing"]
        df_gwas["var_type_ID"] = df_gwas[var_type_ID_fields].apply(lambda r: "-".join(r), axis=1)

        # add the variant type w no collapsing
        df_gwas["var_type_ID_without_collapsing"] = df_gwas[["type_vars", "type_genes", "type_mutations"]].apply(lambda r: "-".join(r), axis=1)

        # load the df that maps groups to variants
        all_ProcessedDataDir = "%s/CandidaMine_data_generation/v1/ProcessedData_RecentEvolPaper"%ParentDir # this was generated before

        varTypeID_group_vars_df_file = "%s/%s_%s_varTypeID_group_vars_df_GWAS.py"%(all_ProcessedDataDir, spp, drug)
        varTypeID_group_vars_df = load_object(varTypeID_group_vars_df_file)
        varTypeID_group_vars_df["type_vars"] = varTypeID_group_vars_df.var_type_ID.apply(lambda x: x.split("-")[0])
        varTypeID_group_vars_df["type_genes"] = varTypeID_group_vars_df.var_type_ID.apply(lambda x: x.split("-")[1])
        varTypeID_group_vars_df["type_mutations"] = varTypeID_group_vars_df.var_type_ID.apply(lambda x: x.split("-")[2])
        varTypeID_group_vars_df["type_collapsing"] = varTypeID_group_vars_df.var_type_ID.apply(lambda x: x.split("-")[3])

        # map each gene to the vars
        interesting_type_muts = {"only_non_syn" : {"non_syn_muts", "truncating_muts", "non_syn_non_truncating_muts"}, "all_muts":set(varTypeID_group_vars_df.type_mutations)}[type_muts]
        gene_to_vars = varTypeID_group_vars_df[(varTypeID_group_vars_df.type_collapsing=="genes") & (varTypeID_group_vars_df.type_mutations.isin(interesting_type_muts))].groupby("group_name").apply(lambda df_g: set.union(*df_g.all_vars))

        # filter df
        varTypeID_group_vars_df = varTypeID_group_vars_df[(varTypeID_group_vars_df.var_type_ID.isin(set(df_gwas.var_type_ID))) & (varTypeID_group_vars_df.group_name.isin(set(df_gwas.group_name)))] # keep important lines

        varTypeID_group_vars_df["all_vars_tuple"] = varTypeID_group_vars_df.all_vars.apply(sorted).apply(tuple)

        # create the level of specificity
        varTypeID_group_vars_df["type_mutations_level_spec"] = varTypeID_group_vars_df.type_mutations.map(type_mutations_to_level_specificity); check_no_nans_series(varTypeID_group_vars_df.type_mutations_level_spec)

        # if there are only none vars
        if set(df_gwas.var_type_ID)=={'all_vars-all_genes-all_muts-none'}:

            df_gwas["all_vars"] = [set()]*len(df_gwas)
            df_gwas["real_type_mutations"] = df_gwas.type_mutations

        else:

            # add the real_type_mutations
            def get_varTypeID_group_vars_df_one_type_vars_and_collapsing_and_group_wTypeMutations(df):

                # if there is only one df
                if len(df)==1: 
                    df["real_type_mutations"] = df.type_mutations
                    return df

                # if there are different levels of specificity and unique
                elif len(df)==len(set(df.type_mutations_level_spec)) and len(df)<=4: 
                    df["real_type_mutations"] = df.sort_values(by="type_mutations_level_spec", ascending=True).iloc[0].type_mutations
                    return df

                else:

                    print(spp, drug)
                    print(df[["type_mutations", "all_vars_tuple", "group_name"]])
                    raise ValueError("something went wrong in get_varTypeID_group_vars_df_one_type_vars_and_collapsing_and_group_wTypeMutations")

            varTypeID_group_vars_df = pd.concat(map(lambda x: get_varTypeID_group_vars_df_one_type_vars_and_collapsing_and_group_wTypeMutations(x[1]), varTypeID_group_vars_df.groupby(["type_vars", "type_genes", "type_collapsing", "group_name", "all_vars_tuple"])))
            check_no_nans_series(varTypeID_group_vars_df.real_type_mutations)

            # add the all_vars
            inital_len_df_gwas = len(df_gwas)
            df_gwas = df_gwas.merge(varTypeID_group_vars_df[["var_type_ID", "group_name", "all_vars", "real_type_mutations"]], on=["var_type_ID", "group_name"], validate="many_to_one", how="left")
            if len(df_gwas)!=inital_len_df_gwas: raise ValueError("merge changed len")
            if any(pd.isna(df_gwas[df_gwas.type_collapsing!="none"].all_vars)): raise ValueError("nas in all_vars")
     
            def get_all_vars_set(x):
                if pd.isna(x): return set()
                else: return x
            df_gwas["all_vars"] = df_gwas.all_vars.apply(get_all_vars_set) # get all sets

        # set the all_vars of no collapsing to be the actual variant
        def get_all_vars_correcting_no_collapsing(r):
            if r.type_collapsing=="none": 
                if len(r.all_vars)!=0: raise ValueError("there should be 0 vars")
                return {r.group_name}

            else: return r.all_vars

        df_gwas["all_vars"] = df_gwas[["type_collapsing", "all_vars", "group_name"]].apply(get_all_vars_correcting_no_collapsing, axis=1)

        # check that the all_Vars are meaningful
        if not all(df_gwas[df_gwas.type_collapsing=="none"].all_vars.apply(len)==1): raise ValueError("The all_vars for none should be all 0")
        if not all(df_gwas[df_gwas.type_collapsing!="none"].all_vars.apply(len)>=2): raise ValueError("The all_vars for !none should be all >=2")

        # reset the index
        df_gwas = df_gwas.reset_index(drop=True)

        # check that the type_genes does not play a role
        if  list(df_gwas[["group_name", "type_vars", "type_mutations", "type_collapsing"]].drop_duplicates().index)!=list(df_gwas.index): raise ValueError("type_genes could play a role in the IDs")

        # check that there is only unique group names
        if len(df_gwas[["group_name", "var_type_ID"]])!=len(df_gwas): raise ValueError("the combination of group_name and varID should be unique")

        # add the domain info
        df_gwas = get_df_gwas_with_domain_info_fields(df_gwas, DataDir, spp)

        # add unique hit ID
        df_gwas["unique_hit_ID"] = list(range(len(df_gwas)))
        df_gwas = df_gwas.set_index("unique_hit_ID", drop=False)

        # add levels of specificity
        df_gwas["type_vars_level_spec"] = df_gwas.type_vars.apply(lambda x: type_vars_to_level_specificity[x])
        df_gwas["type_mutations_level_spec"] = df_gwas.type_mutations.apply(lambda x: type_mutations_to_level_specificity[x])
        df_gwas["type_collapsing_level_spec"] = df_gwas.type_collapsing.apply(lambda x: type_collapsing_to_level_specificity[x])

        # add the number of vars
        df_gwas["n_all_vars"] = df_gwas.all_vars.apply(len)

        # add the gene rankings
        df_gwas = get_df_gwas_with_gene_info_fields(df_gwas, DataDir, spp, gene_features_df_s, oc_to_nspp)

        #####################

        ########### KEEP NR SET OF VARS ############

        print("getting NR vars...")

        # add the hitID
        df_gwas["hitID"] = 'hit' + df_gwas.unique_hit_ID.apply(str)

        # add the altered gene
        def get_alt_gene_vars_domains_genes(r):
            if r.type_collapsing=="genes": return {r.group_name}
            elif r.type_collapsing=="domains": return {r.altered_gene}
            elif r.type_collapsing=="none": return set(gene_to_vars[gene_to_vars.apply(lambda x: r.group_name in x)].index)
            elif r.type_collapsing in {"GO", "Reactome", "MetaCyc"}: return set()
            else: raise ValueError("%s"%r.type_collapsing)

        df_gwas["set_altered_genes"] = df_gwas.apply(get_alt_gene_vars_domains_genes, axis=1)

        # load GO info
        obodag = GODag("%s/annotation_files/go-basic_30062021.obo"%(DataDir),  optional_attrs={'consider', 'replaced_by'}, load_obsolete=True, prt=None)

        # apply different filtering for each type
        df_gwas_genes = df_gwas[df_gwas.type_collapsing.isin({"none", "genes", "domains"})]
        df_gwas_pathways = df_gwas[df_gwas.type_collapsing.isin({"GO", "Reactome", "MetaCyc"})]

        # for genes, domains, mutations. Keep NR hits for each gene
        print("getting gene NR hits...")
        if any(df_gwas_genes.set_altered_genes.apply(len)==0): 
            print(df_gwas_genes[df_gwas_genes.set_altered_genes.apply(len)==0].group_name)
            raise ValueError("there are groups with no genes")

        if len(df_gwas_genes)>0: 
            all_sig_genes = sorted(set.union(*df_gwas_genes.set_altered_genes))
            df_gwas_NR_genes = pd.concat(map(lambda g: get_df_gwas_NR(obodag, df_gwas_genes[df_gwas_genes.set_altered_genes.apply(lambda x: g in x)], ProcessedDataDir, spp, drug, type_muts, taxID_dir), all_sig_genes))
            df_gwas_NR_genes["pathway_has_sig_genes"] = False # whether the pathway 
            df_gwas_NR_genes["pathway_is_NR"] = False
            vars_sig_by_genes = set.union(*df_gwas_genes.all_vars)

        else: 
            df_gwas_NR_genes = pd.DataFrame()
            vars_sig_by_genes = set()

        # for pathways, do the same, but also add whether the pathway is considered for a gene, and also whether the pathway is NR in itself
        print("getting pathway NR hits...")
        all_sig_pathways = sorted(set(df_gwas_pathways.group_name))
        df_gwas_NR_pathways = pd.concat(map(lambda p: get_df_gwas_NR(obodag, df_gwas_pathways[df_gwas_pathways.group_name==p], ProcessedDataDir, spp, drug, type_muts, taxID_dir), all_sig_pathways))

        df_gwas_NR_pathways["pathway_has_sig_genes"] = (df_gwas_NR_pathways.all_vars.apply(lambda x: x.intersection(vars_sig_by_genes)).apply(len))>0

        hits_nr_pathways_consideringAllPathways = set.union(*[set(get_df_gwas_NR(obodag, df_gwas_NR_pathways[df_gwas_NR_pathways.type_collapsing==col], ProcessedDataDir, spp, drug, type_muts, taxID_dir).hitID) for col in {"GO", "Reactome", "MetaCyc"} if col in set(df_gwas_NR_pathways.type_collapsing)])
        if len(hits_nr_pathways_consideringAllPathways.difference(set(df_gwas_NR_pathways.hitID)))>0: raise ValueError("some of the hit IDs are invalid")
        df_gwas_NR_pathways["pathway_is_NR"] = df_gwas_NR_pathways.hitID.isin(hits_nr_pathways_consideringAllPathways)

        # add the genes that belong to the pathway
        df_gwas_NR_pathways = get_df_gwas_pathways_with_genes(df_gwas_NR_pathways, taxID_dir)

        # keep both
        df_gwas_NR = df_gwas_NR_genes.append(df_gwas_NR_pathways)
        print("There are %i GWAS hits. %i NR gene hits and %i NR pathway hits. From these pathway hits %i don't have sig genes"%(len(df_gwas), len(df_gwas_NR_genes), len(df_gwas_NR_pathways), sum(~df_gwas_NR_pathways.pathway_has_sig_genes)))

        ############################################

        # remove fields and return
        df_gwas_NR.pop("all_vars")

        # save
        save_object(df_gwas_NR, df_gwas_NR_file)

    return load_object(df_gwas_NR_file)




def get_df_gwas_filtered_and_redundancy_removed_eachVarOnlyInOneGroup(spp_drug_to_gwas_df_file, ProcessedDataDir, PlotsDir, threads, DataDir, optimal_filters_df, type_muts, species_to_gff, gene_features_df, saving_dir=None):

    """Gets the filtered GWAS with the redundancy removed."""

    # define the pval fields
    #all_pval_fields = ["%s_%s"%(pval_seed, pval_m) for pval_seed in ["pval_chi_square", "pval_GenoAndPheno"] for pval_m in  ["RelToBranchLen", "phenotypes"]] 

    # debug
    #spp_drug_to_gwas_df_file = {x:y for x,y in spp_drug_to_gwas_df_file.items() if x==("Candida_glabrata", "VRC")}
    #spp_drug_to_gwas_df_file = {x:y for x,y in spp_drug_to_gwas_df_file.items() if x==("Candida_glabrata", "VRC")}
    #spp_drug_to_gwas_df_file = {x:y for x,y in spp_drug_to_gwas_df_file.items() if x==("Candida_glabrata", "FLC")}

    # map each OC to the number of species
    oc_to_nspp = dict(gene_features_df.groupby("orthofinder_orthocluster").apply(lambda df: len(set(df.species))))

    # get the concatenated data for each df
    print("running get_df_gwas_filtered_and_redundancy_removed_eachVarOnlyInOneGroup_one_spp_and_drug in parallel")
    inputs_fn = [(spp, drug, tab_file, DataDir, ProcessedDataDir, optimal_filters_df.loc["%s-%s"%(spp,drug)], type_muts, species_to_gff[spp], gene_features_df[(gene_features_df.species==spp)], oc_to_nspp) for (spp,drug), tab_file in spp_drug_to_gwas_df_file.items() if "%s-%s"%(spp,drug) in set(optimal_filters_df.index)]

    # add the df_gwas_NR_file
    inputs_fn_final = []
    for x in inputs_fn:

        if saving_dir is None: df_gwas_NR_file = None
        else: df_gwas_NR_file = "%s/df_gwas_NR_%s_%s_%s.py"%(ProcessedDataDir, x[0], x[1], type_muts)

        inputs_fn_final.append(tuple(list(x)+[df_gwas_NR_file]))

    """
    with multiproc.Pool(threads) as pool:
        df_gwas_filt = pd.concat(pool.starmap(get_df_gwas_filtered_and_redundancy_removed_eachVarOnlyInOneGroup_one_spp_and_drug, inputs_fn_final)).reset_index(drop=True)
        pool.close()
        pool.terminate()  

    """

    df_gwas_filt = pd.concat(list(map(lambda x: get_df_gwas_filtered_and_redundancy_removed_eachVarOnlyInOneGroup_one_spp_and_drug(x[0], x[1], x[2], x[3], x[4], x[5], x[6], x[7], x[8], x[9], x[10]), inputs_fn_final))).reset_index(drop=True)

    return df_gwas_filt


def get_figure_GWAS_all_results(PlotsDir, df_gwas_filt):

    """Gets the heatmap with all gwas results"""


    df_gwas_filt = cp.deepcopy(df_gwas_filt)

    # redefine the real_type_mutations
    def get_real_type_mutations_from_r(r):
        if r.type_collapsing=="none": return 'all_muts'
        else: return r.real_type_mutations
    df_gwas_filt["real_type_mutations"] = df_gwas_filt.apply(get_real_type_mutations_from_r, axis=1)

    # add f
    df_gwas_filt["species_and_drug"] = df_gwas_filt.species + "_" + df_gwas_filt.drug
    df_gwas_filt["type_vars"] = df_gwas_filt.real_type_vars
    df_gwas_filt["type_mutations"] = df_gwas_filt.real_type_mutations


    ######### STACKED BARS INSET ###########

    # graphics
    type_vars_to_color = {'all_vars': 'black', 'SVs': 'navy', 'SVs_and_CNVs': 'c', 'coverageCNVs': 'cyan', 'small_vars': 'red', 'small_vars_and_CNVs': 'pink', 'small_vars_and_SVs': 'magenta'}
    type_mutations_to_color = {'all_muts': 'black', 'non_syn_muts': 'purple', 'non_syn_non_truncating_muts': 'violet', 'truncating_muts': 'indigo'}
    type_collapsing_to_color = {"none":"black", "domains":"peru", "genes":"orange", "GO":"olive", "MetaCyc":"lightgreen", "Reactome":"green"}
    palette_plot = {**type_vars_to_color, **type_mutations_to_color, **type_collapsing_to_color}

    # create df
    df_plot = pd.DataFrame()
    for f in ["type_collapsing", "type_mutations", "type_vars"]:

        all_values = sorted(set(df_gwas_filt[f]))
        for val in all_values:

            df_val = df_gwas_filt[df_gwas_filt[f]==val][["species_and_drug", "group_name"]].drop_duplicates()
            df_val["grouping"] = f
            df_val["val"] = val 

            df_plot = df_plot.append(df_val).reset_index(drop=True)

    df_plot["rowf"] = 'all_rows'
    plot_stacked_bar_subplots_eachEventOneRow(df_plot, xfield="grouping", rowfield="rowf", hue="val", yfield='n_events', sorted_rows=None, sorted_xvalues=["type_collapsing", "type_mutations", "type_vars"], sorted_huevalues=None, palette=palette_plot, title=None, filename= "%s/GWAS_all_results_inset.pdf"%(PlotsDir), multiplier_width=0.6, multiplier_height=5)


    ############ HETAMAP ###############

    # add the row and col IDs
    col_fields = ["species", "drug"]
    row_fields = ["real_type_vars", "real_type_mutations", "type_collapsing"]

    df_gwas_filt["colID"] = df_gwas_filt[col_fields].apply(lambda r: "-".join(r), axis=1)
    df_gwas_filt["rowID"] = df_gwas_filt[row_fields].apply(lambda r: "-".join(r), axis=1)

    # check
    #if len(df_gwas_filt)!=len(df_gwas_filt[["colID", "rowID", "group_name"]].drop_duplicates()): raise ValueError("the combination of col, row and group_name should be unique")

    # add the number of data
    df_gwas_filt = df_gwas_filt[["colID", "rowID", "group_name"]].groupby(["rowID", "colID"]).apply(lambda df: pd.Series({"# groups":len(set(df.group_name)), "rowID":df.rowID.iloc[0], "colID":df.colID.iloc[0]})).reset_index(drop=True)

    # define the df plot
    square_df = df_gwas_filt[["rowID", "colID", "# groups"]].pivot(values="# groups", columns="colID", index="rowID")

    # define the sorted columns
    species_to_ID = {"Candida_albicans":0, "Candida_auris":1, "Candida_glabrata":2}
    drug_to_ID= {"FLC":0, "ITR":0.1, "POS":0.2, "VRC":0.3, "ANI":1, "CAS":1.1, "MIF":1.2, "AMB":2}
    sorted_cols = sorted(square_df.columns, key=(lambda x: (species_to_ID[x.split("-")[0]], drug_to_ID[x.split("-")[1]])))
    square_df = square_df[sorted_cols]
    initial_len_square_df = len(square_df)

    # define annotations
    def get_annot_for_cell(x):
        if pd.isna(x): return ""
        else: return str(int(x))
    annot_df = square_df.applymap(get_annot_for_cell)

    # remove nans
    """
    def get_nans_as_0(x):
        if pd.isna(x): return 0
        else: return int(x)
    square_df = square_df.applymap(get_nans_as_0)
    """

    # define the col colors
    drug_to_color= {"FLC":"c", "ITR":"blue", "POS":"aquamarine", "VRC":"cyan", "AMB":"gray", "ANI":"magenta", "CAS":"red", "MIF":"salmon"}
    cols_tuple_list = [("species", species_to_color), ("drug", drug_to_color)]
    col_colors_df = pd.DataFrame({col : {name : color_dict[col.split("-")[I]] for I, (name, color_dict) in enumerate(cols_tuple_list)} for col in sorted(set(df_gwas_filt.colID))}).transpose().loc[square_df.columns]

    rows_tuple_list = [("type_vars", type_vars_to_color), ("type_mutations", type_mutations_to_color), ("type_collapsing", type_collapsing_to_color)]
    row_colors_df = pd.DataFrame({row : {name : color_dict[row.split("-")[I]] for I, (name, color_dict) in enumerate(rows_tuple_list)} for row in sorted(set(df_gwas_filt.rowID))}).transpose().loc[square_df.index]

    # change order
    row_colors_df = row_colors_df[["type_collapsing", "type_mutations", "type_vars"]]

    # define the sorted rows
    sorted_rows = ["%s-%s-%s"%(x,y,z) for x in type_vars_to_color for y in type_mutations_to_color for z in type_collapsing_to_color if "%s-%s-%s"%(x,y,z) in square_df.index]
    square_df = square_df.loc[sorted_rows]
    if initial_len_square_df!=len(square_df): raise ValueError("len changed")

    # reorder
    #rows_tuple_list = [(x, {k : y[k] for k in sorted(y.keys(), key=(lambda val: val.lower()))}) for x,y in rows_tuple_list]

    # make plot
    cmap = "rocket_r"
    cm = sns.clustermap(square_df, col_cluster=False, row_cluster=False, cmap=cmap,  col_colors=col_colors_df, row_colors=row_colors_df, cbar_kws={"label":"# sig. groups"}, annot=annot_df.loc[square_df.index, square_df.columns], annot_kws={"size": 10}, fmt="", yticklabels=1) # , xticklabels=1    


    # re-position the panels
    hm_pos = cm.ax_heatmap.get_position()
    rect_width = hm_pos.width / len(square_df.columns)
    spacer = rect_width * 0.25
    hm_height = rect_width*len(square_df)
    cm.ax_heatmap.set_position([hm_pos.x0, cm.ax_col_colors.get_position().y0 - hm_height - spacer, hm_pos.width, hm_height])
    hm_pos = cm.ax_heatmap.get_position()

    rc_width = rect_width * len(row_colors_df.columns)
    cm.ax_row_colors.set_position([hm_pos.x0+hm_pos.width+spacer, hm_pos.y0, rc_width, hm_pos.height])

    # get the ticjlabels
    #cm.ax_heatmap.set_yticklabels([row for row in square_df.index])
    #cm.ax_heatmap.set_ylabel("<tyoe vars>-<type genes>-<type_mutations>-<type_collapsing>")
    cm.ax_heatmap.set_ylabel("")
    cm.ax_heatmap.set_xlabel("")

    cm.ax_heatmap.set_yticklabels([])
    cm.ax_heatmap.set_xticklabels([])

    cm.ax_heatmap.set_yticks([])
    cm.ax_heatmap.set_xticks([])

    # title
    cm.ax_col_colors.set_title("")

    # add a legend for the row colors
    def get_lel(facecolor, label, edgecolor="gray"): return mpatches.Patch(facecolor=facecolor, edgecolor=edgecolor, label=label)
    legend_elements = make_flat_listOflists([([get_lel("white", "", edgecolor="white"), get_lel("white", f, edgecolor="white")] + [get_lel(color,label) for label,color in color_dict.items()]) for f, color_dict in rows_tuple_list])
    cm.ax_heatmap.legend(handles=legend_elements, loc="upper right", bbox_to_anchor=(0, 1))

    # add a legend for the col colors
    legend_elements = make_flat_listOflists([([get_lel("white", "", edgecolor="white"), get_lel("white", f, edgecolor="white")] + [get_lel(color,label) for label,color in color_dict.items()]) for f, color_dict in cols_tuple_list])    
    cm.ax_cbar.legend(handles=legend_elements, loc="lower right", bbox_to_anchor=(0, 0))

    plt.show()

    # save
    filename = "%s/GWAS_all_results.pdf"%(PlotsDir)
    print("saving %s"%filename)
    cm.savefig(filename, bbox_inches="tight")


def get_figures_trees_with_association_info(DataDir, PlotsDir, species_to_gff, gene_features_df, metadata_df, designed_GWAS_filters_df, df_gwas_filt, spp_drug_to_gwas_df_file):

    """Plots, for some associations, the info about GWAS data in hits"""

    # define plots dir
    plots_dir = "%s/trees_ASR_gwas_hits"%(PlotsDir); make_folder(plots_dir)
    tmpdir = "%s/tmp_creating"%plots_dir; make_folder(tmpdir)

    # add final name
    gene_features_df = cp.deepcopy(gene_features_df)
    def get_final_name_gene_features_df(r):
        if not pd.isna(r.gene_name): return r.gene_name
        elif not pd.isna(r.Scerevisiae_orthologs): return "Scer_%s"%(r.Scerevisiae_orthologs)
        else: return r.gff_upmost_parent

    gene_features_df["final_name"] = gene_features_df.apply(get_final_name_gene_features_df, axis=1)


    # go through different combinations

    # exploratory
    #for species, drug, common_name, type_group, only_mutations_associated_to_resistance, mic_fields_plot in [("Candida_albicans", "FLC", "ERG11", "gene", True, [])]:

    #("Candida_auris", "ANI", "gene-B9J08_004470", "gene", True, ["ANI"], "r")# second top hit interesting gene
    #("Candida_auris", "ANI", "GO:0015294", "pathway", False, ["ANI"], "r")# fourth top hit interesting gene

    # paper relevant trees
    list_items = [("Candida_glabrata", "VRC", "PDR1", "gene", True, [], "r"),
    
                  ("Candida_glabrata", "MIF", "FKS1", "gene", False, ["MIF"], "r"),
                  ("Candida_glabrata", "MIF", "FKS2", "gene", False, ["MIF"], "r"),
                  ("Candida_auris", "ANI", "GSC2", "gene", False, ["ANI"], "r"),
                  ("Candida_glabrata", "MIF", "NET1", "gene", False, ["MIF"], "r"), # interesting gene
                  ("Candida_auris", "ANI", "gene-B9J08_003526", "gene", False, ["ANI"], "r") # top hit interesting gene
                 ]


    # one plot for each species
    for species, drug, common_name, type_group, only_mutations_associated_to_resistance, mic_fields_plot, rotation_mode in list_items:
        print(common_name)

        # define the outdir of the gwas
        outdir_gwas = "%s/%s_%i/ancestral_GWAS_drugResistance"%(DataDir, species, sciName_to_taxID[species])       
        outdir_drug = "%s/GWAS_%s_resistance"%(outdir_gwas, drug)

        # define the parameters
        ASR_methods_phenotypes = designed_GWAS_filters_df.loc[(species+"-"+drug), "ASR_methods_phenotypes"]
        min_support = designed_GWAS_filters_df.loc[(species+"-"+drug), "min_support"]

        # define the interesting attributes (add to the tree)
        interesting_attributes = None

        # define the features
        gene_features_df_s = gene_features_df[gene_features_df.species==species]

        # only keep mutations linked to resistance
        if type_group=="gene":

            # the common name is the gene name
            gene_name = common_name

            # get geneID
            gene_ID = get_geneID_for_gName_from_gene_features_df(gene_features_df_s, gene_name)

            # define association data
            df_assoc = df_gwas_filt[(df_gwas_filt.species==species) & (df_gwas_filt.drug==drug) & (df_gwas_filt.set_altered_genes.apply(lambda x: x=={gene_ID}))]

            # if there is assoc data
            if len(df_assoc)>0:
                print("working on assoc data")

                if len(df_assoc)>1: raise ValueError("there should be 1 row in df_assoc")
                r_assoc = df_assoc.iloc[0]
          
                target_type_vars = r_assoc.real_type_vars
                target_type_collapsing = r_assoc.type_collapsing
                if target_type_collapsing=="none": target_type_mutations = "all_muts"
                else: target_type_mutations = r_assoc.real_type_mutations
                target_group_name = r_assoc.group_name

            # if not, pick the hits with higher association
            else:

                print("loading df")
                # load gwas df

                target_type_vars = "all_vars"
                target_type_mutations = "non_syn_muts"
                target_type_collapsing = "genes"
                target_group_name = gene_ID

            # get the mapping
            target_interesting_geneID_to_geneName = {gene_ID : gene_name}

        elif type_group=="variant":

            # definfe no grouping
            target_type_vars = "all_vars"
            target_type_mutations = "all_muts"
            target_type_collapsing = "none"
            target_group_name = {"chrE_duplication":"ChrE_C_glabrata_CBS138_duplication", "chrA_duplication":"ChrA_C_glabrata_CBS138_duplication", "chrL_duplication":"ChrL_C_glabrata_CBS138_duplication", "chrI_duplication":"ChrI_C_glabrata_CBS138_duplication"}[common_name]

            # no genes involved
            target_interesting_geneID_to_geneName = None

        # only keep mutations linked to resistance
        elif type_group=="pathway":

            # define association data
            df_assoc = df_gwas_filt[(df_gwas_filt.species==species) & (df_gwas_filt.drug==drug) & (df_gwas_filt.group_name==common_name)]
            if len(df_assoc)>1: raise ValueError("there should be 1 row in df_assoc")
            r_assoc = df_assoc.iloc[0]

            target_type_vars = r_assoc.real_type_vars
            target_type_collapsing = r_assoc.type_collapsing
            target_type_mutations = r_assoc.real_type_mutations
            target_group_name = r_assoc.group_name

            # define dict of genes
            if common_name.startswith("GO:"):

                # get the genes with GO
                df_GOterms_all = load_object("%s/%s_%i/ancestral_GWAS_drugResistance/df_GOterms_per_gene.py"%(DataDir, species, sciName_to_taxID[species]))
                genes_pathway = set(df_GOterms_all[df_GOterms_all.GO==common_name].Gene)

            else: raise ValueError("invalid: %s"%common_name)

            if len(genes_pathway)==0: raise ValueError("genes_w_GO cant be 0")
            target_interesting_geneID_to_geneName = {gID : gene_features_df_s[gene_features_df_s.gff_upmost_parent==gID].final_name.iloc[0]  for gID in genes_pathway}

        else: raise ValueError("invalid type_group")

        # define the target type genes based on available data
        list_target_type_genes = {x.split("-")[1] for x in os.listdir(outdir_drug) if len(x.split("-"))==4 and x.split("-")[0]==target_type_vars and x.split("-")[2]==target_type_mutations and x.split("-")[3]==target_type_collapsing}
        if len(list_target_type_genes)!=1: raise ValueError("list_target_type_genes should be 1")
        target_type_genes = next(iter(list_target_type_genes))

        # get plot
        plot_tree_all_mutations_one_group_gwas_AF(outdir_drug, ASR_methods_phenotypes, target_type_vars, target_type_genes, target_type_mutations, target_type_collapsing, target_group_name, "%s/%s_%s_%s.pdf"%(plots_dir, species, drug, common_name), DataDir, species, drug, "%s/%s_%s_%s_generateData"%(tmpdir, species, drug, common_name), target_interesting_geneID_to_geneName, metadata_df, min_support, interesting_attributes=interesting_attributes, only_mutations_associated_to_resistance=only_mutations_associated_to_resistance, mic_fields_plot=mic_fields_plot, rotation_mode=rotation_mode)




# tree.write(outfile=treefile_rooted, format=2)


def get_merged_table_strains(df_allStrainData, species_to_tree, TablesDir, df_Strain_metadata, df_table_drugs_gwas, CurDir):

    """
    # Writes the table with strain data
    """

    print("getting strains table")

    # init the excel writer
    writer = pd.ExcelWriter('%s/TableS1.xlsx'%TablesDir, engine='openpyxl') 

    # first tab, all strains
    df_allStrainData[['species_name', 'BioProject', 'Run', 'numeric_sampleID', 'BioSample', 'type', 'mean_coverage', 'pct_covered', 'cladeID_systematic', 'cladeID_previous', 'clonal_cluster', 'collection_date', 'collection_latitude_longitude', 'collection_country', 'paper_link', 'resistance', 'susceptibility', 'intermediate_susceptibility', 'ANI_MIC', 'CAS_MIC', 'MIF_MIC', 'FLC_MIC', 'ITR_MIC', 'POS_MIC', 'VRC_MIC', 'IVZ_MIC', 'KET_MIC', 'MIZ_MIC', 'AMB_MIC', 'BVN_MIC', '5FC_MIC', 'TRB_MIC']].to_excel(writer, sheet_name=f"All strains", index=False)

    # second tab, overview about strain metadata
    df_Strain_metadata[['species', '# strains', '# clinical', '# environmental', '# other', '# clades', 'median pairwise SNPs/kb']].to_excel(writer, sheet_name=f"Strains overview", index=False)

    # drug resistance information
    df_table_drugs_gwas[["species", "drug", "drug class", 'R strains', 'S strains', 'R>S or S>R transitions', 'clades w/ R strain', 'clades w/ S strain']].to_excel(writer, sheet_name=f"GWAS drugs overview", index=False)

    # reference genomes
    ref_genomes_df = pd.read_excel("%s/manually_curated_data/reference_genomes.xlsx"%CurDir)
    ref_genomes_df.to_excel(writer, sheet_name=f"Reference genomes", index=False)

    # tree
    trees_df = pd.DataFrame()
    for species in sorted_species_byPhylogeny:

        # get the tree with correct samples
        tree = cp.deepcopy(species_to_tree[species])
        correct_samples = set(tree.get_leaf_names()).difference({str(x) for x in sciName_to_badSamples[species]})
        tree.prune(correct_samples)

        if set(map(int, tree.get_leaf_names()))!=set(df_allStrainData[df_allStrainData.species_name==species].numeric_sampleID.apply(int)): raise ValueError("different samples %s"%species)


        df = pd.DataFrame({"species":[species], "tree":[tree.write(format=2)]})
        trees_df = trees_df.append(df)

    trees_df.to_excel(writer, sheet_name=f"Strain trees", index=False)

    # strains
    print("writing")
    writer.save()


def get_merged_table_selection(gene_features_df, selection_dfs_dict, df_enrichment_all, TablesDir):

    """Writes the table with recent selection data"""

    print("getting selection table")

    # init the excel writer
    writer = pd.ExcelWriter('%s/TableS2.xlsx'%TablesDir, engine='openpyxl') 

    # parse sel datasets
    df_sel, df_sel_shared = selection_dfs_dict["under_selection"]
    df_sel_all = selection_dfs_dict["all_genes"][0]

    # define the fields of the selection analysis
    fields_sel = ['species', 'type_var', 'chromosome', 'start', 'end', 'gff_upmost_parent', 'gene_name', 'Scerevisiae_orthologs', "selection_score_S", "fdr_p_S", 'orthofinder_orthocluster', 'n_types_vars_in_species_orthogroup', 'n_species_orthogroup', 'description', 'biological_process_GO', 'molecular_function_GO', 'cellular_component_GO', 'fraction_clusters_w_selection', 'fraction_strains_w_selection', 'total_number_clusters', 'total_number_strains', 'significant_selection']

    # checks
    for df in [df_sel, df_sel_shared, df_sel_all]:
        if any(df.selection_score_S!=(df.apply(lambda r: get_harmonic_mean(r.fraction_clusters_w_selection, r. fraction_strains_w_selection), axis=1))): raise ValueError("the selection scores do not match")

    print("checks passed")

    # Genes under selection
    df_sel[fields_sel].sort_values(by=["species", "type_var", "fdr_p_S", "selection_score_S", "chromosome"], ascending=[True, True, True, False, True]).to_excel(writer,  index=False, sheet_name=f"Genes under selection")

    # genes under selection under >1 spp
    df_sel_shared[fields_sel].sort_values(by=["n_species_orthogroup", "orthofinder_orthocluster", "species", "type_var", "fdr_p_S", "selection_score_S", "chromosome"], ascending=[False, True, True, True, True, False, True])[fields_sel].to_excel(writer,  index=False, sheet_name=f"Genes under selection >1 species")

    # All genes under selection
    df_sel_all[fields_sel].sort_values(by=["species", "type_var", "fdr_p_S", "selection_score_S", "chromosome"], ascending=[True, True, True, False, True]).to_excel(writer,  index=False, sheet_name=f"Selection scores all genes")

    # enrichment data
    df_enrichment_all[['type_grouping', 'species', 'type_var', 'ID', 'ngenes_group_and_target', 'ngenes_no_group_target', 'ngenes_group_no_target', 'ngenes_no_group_no_target', 'OR', 'p_raw', 'p_fdr', 'group_name', 'genes']].sort_values(by=["type_grouping", "species", "type_var", "p_fdr", "OR", "ID"], ascending=[True, True, True, True, False, True]).to_excel(writer,  index=False,  sheet_name=f"Functional enrichments")

    # gene features df
    gene_features_df[["species", "gff_upmost_parent", "gene_name", "Scerevisiae_orthologs", "orthofinder_orthocluster", "description"]].to_excel(writer, index=False, sheet_name=f"Gene features")

    # write
    print("writing")
    writer.save()


def get_df_gwas_results_low_confidence(spp_drug_to_gwas_df_file, ProcessedDataDir, DataDir, type_muts,  species_to_gff, gene_features_df, threads, TablesDir):

    """Uses the same method that we used for high-confidence variants to get a table of GWAS results, but using some more relaxed filters

    This is what was done for highconfience results:

    type_muts = "only_non_syn"
    df_gwas_filt = fun.get_df_gwas_filtered_and_redundancy_removed_eachVarOnlyInOneGroup(spp_drug_to_gwas_df_file, ProcessedDataDir, PlotsDir, threads, DataDir, designed_GWAS_filters_df, type_muts, species_to_gff, gene_features_df)

    df_gwas_filt = df_gwas_filt[(df_gwas_filt.type_collapsing.isin({"none", "domains", "genes"})) | ( (~df_gwas_filt.pathway_has_sig_genes) & (df_gwas_filt.pathway_is_NR) )]

    gwas_table_df, gwas_results_df_more_than_1_spp_OGs = fun.get_Table_GroupsGWAS(df_gwas_filt, DataDir, "%s/get_Table_GroupsGWAS_data_nsyn_muts_NR"%ProcessedDataDir, TablesDir, gene_features_df, designed_GWAS_filters_df, type_muts)

    """
    
    # define outdir
    outdir = "%s/getting_low_confidence_gwas_results_%s"%(ProcessedDataDir, type_muts)
    make_folder(outdir)

    # define dir
    df_gwas_low_confidence_all_file = "%s/df_gwas_low_confidence_all.py"%outdir
    if file_is_empty(df_gwas_low_confidence_all_file):

        # init gwas df
        df_gwas_low_confidence_all = pd.DataFrame()

        # go through different filter types
        for ASR_methods_phenotypes in ["MPPA,DOWNPASS", "MPPA", "DOWNPASS"]:
            for min_support in [50, 70]:
                print(ASR_methods_phenotypes, min_support)

                # define a dir
                outdir_ASR_ms = "%s/%s_%i"%(outdir, ASR_methods_phenotypes, min_support)
                make_folder(outdir_ASR_ms)

                TablesDir_ms = "%s/%s_%i"%(TablesDir, ASR_methods_phenotypes, min_support)
                make_folder(TablesDir_ms)

                # define the dataframe with the filters
                filters_df = pd.DataFrame(index=["%s-%s"%(x[0], x[1]) for x in sorted(spp_drug_to_gwas_df_file)])

                filters_df["ASR_methods_phenotypes"] = ASR_methods_phenotypes
                filters_df["min_support"] = min_support
                filters_df["gwas_method"] = "synchronous"
                filters_df["correction_method"] = "none"
                filters_df["min_epsilon"] = 0.0
                filters_df["pval_chi_square_phenotypes"] = [True]*len(filters_df)
                filters_df["min_nodes_GenoAndPheno"] = 0
                for f in ["pval_GenoAndPheno_phenotypes", "pval_fisher", "pval_epsilon_maxT", "pval_chi_square_maxT"]: filters_df[f] = False
                filters_df["alpha_pval"] = 0.05
                #filters_df["only_maxT_pvals"] = False

                # get the filtered gwas df
                df_gwas_filt = get_df_gwas_filtered_and_redundancy_removed_eachVarOnlyInOneGroup(spp_drug_to_gwas_df_file, outdir_ASR_ms, None, threads, DataDir, filters_df, type_muts, species_to_gff, gene_features_df, saving_dir=outdir_ASR_ms)

                # keep genes or pathways that have no sig genes
                df_gwas_filt = df_gwas_filt[(df_gwas_filt.type_collapsing.isin({"none", "domains", "genes"})) | ( (~df_gwas_filt.pathway_has_sig_genes) & (df_gwas_filt.pathway_is_NR) )]

                # get the format of the table to write
                df_gwas_low_confidence = get_Table_GroupsGWAS(df_gwas_filt, DataDir, "%s/get_Table_GroupsGWAS_dataR"%outdir_ASR_ms, TablesDir_ms, gene_features_df, None, type_muts)[0]

                # keep
                df_gwas_low_confidence["ASR_methods_phenotypes"] = ASR_methods_phenotypes
                df_gwas_low_confidence["min_support"] = min_support
                df_gwas_low_confidence_all = df_gwas_low_confidence_all.append(df_gwas_low_confidence)

        save_object(df_gwas_low_confidence_all, df_gwas_low_confidence_all_file)

    return load_object(df_gwas_low_confidence_all_file)



def get_merged_table_GWAS(gwas_table_df, gwas_results_df_more_than_1_spp_OGs, gwas_table_df_low_confidence, TablesDir):

    """Writes the low-confindence GWAS results"""

    print("getting GWAS table")

    # init the excel writer
    writer = pd.ExcelWriter('%s/TableS3.xlsx'%TablesDir, engine='openpyxl') 

    # add necessary fields
    for df in [gwas_table_df, gwas_results_df_more_than_1_spp_OGs, gwas_table_df_low_confidence]:

        type_collapsing_to_level_specificity = {"Reactome":6, "GO":5, "MetaCyc":4, "genes":3, "domains":2, "none":1}

        df["type_collapsing_level_spec"] = df.type_collapsing.apply(lambda x: type_collapsing_to_level_specificity[x])

        df["species"] = df.species_and_drug.apply(lambda x: x.split("-")[0])
        df["drug"] = df.species_and_drug.apply(lambda x: x.split("-")[1])

    # define the pval fields
    interesting_pval_fields = ['pval_chi_square_maxT', 'pval_epsilon_maxT', 'pval_chi_square_phenotypes', 'pval_GenoAndPheno_phenotypes', 'pval_fisher', 'pval_chi_square_phenotypes_bonferroni', 'pval_GenoAndPheno_phenotypes_bonferroni', 'pval_fisher_bonferroni']

    # define the overly general fields
    final_relevant_fields = ["species", "drug", "type_vars", "type_mutations", "type_collapsing", "group_name", "epsilon", "OR", "nodes_GenoAndPheno", "nodes_noGenoAndNoPheno", "nodes_GenoAndNoPheno", "nodes_noGenoAndPheno", "orthogroups", "n_spp_drug_worthogroups", "n_spp_drug_wpathway"] + interesting_pval_fields + ["description", "biological_process_GO", "cellular_component_GO", "molecular_function_GO"] # species_and_drug

    # write the high-confidence GWAS results
    gwas_table_df.sort_values(by=["species_and_drug", "type_collapsing_level_spec", "epsilon", "OR"] + interesting_pval_fields, ascending=([True, True, False, False] + [True]*len(interesting_pval_fields))).reset_index(drop=True)[final_relevant_fields].to_excel(writer, index=False, sheet_name=f"High-confidence GWAS hits")

    # write the high confidence hits shared across >1 dataset
    gwas_results_df_more_than_1_spp_OGs_fields = ['pname_or_orthogroups'] + final_relevant_fields

    gwas_results_df_more_than_1_spp_OGs.sort_values(by=["sorting_f", "tuple_spp_drug_pname_or_orthogroups", "pname_or_orthogroups", "type_collapsing_level_spec", "species_and_drug", "epsilon", "OR"] + interesting_pval_fields, ascending=([True, False, True, True, True, False, False] + [True]*len(interesting_pval_fields))).reset_index(drop=True)[gwas_results_df_more_than_1_spp_OGs_fields].to_excel(writer, index=False, sheet_name=f"High-confidence GWAS hits >1 dataset")

    # write the low-confidence GWAS results
    gwas_table_df_low_confidence["ASR_method"] = gwas_table_df_low_confidence.ASR_methods_phenotypes
    gwas_table_df_low_confidence.sort_values(by=["ASR_methods_phenotypes", "min_support", "species_and_drug", "type_collapsing_level_spec", "epsilon", "OR"] + interesting_pval_fields, ascending=([True, False, True, True, False, False] + [True]*len(interesting_pval_fields))).reset_index(drop=True)[["ASR_method", "min_support"] + final_relevant_fields].to_excel(writer, index=False, sheet_name=f"Low-confidence GWAS hits")

    # write
    print("writing")
    writer.save()